id,date,author,title,text,comment_on,parent_id,parent,likes,type,ups,downs,score
5ld8eh,2016-12-31 22:04:32-05:00,Kwabena7,Physics,"For the people that are currently taking Computer Science in uni, how important was it that you took physics in High School? I'm thinking of taking both chem and bio but some uni's recommend taking physics so I'm stuck between a rock and a hard place right now.",,,,,Submission,5,0,5
dbuvv3p,2016-12-31 23:26:27-05:00,BrokeDiamond,,"I think in most universities physics is just a general/college specific requirement, and not part of the major really. For me I only have to take as much physics as other engineers since CS is in the School of Engineering at my university.",5ld8eh,t3_5ld8eh,Kwabena7,,Comment,3,0,3
dbxwcyx,2017-01-03 07:19:05-05:00,5arToto,,"Historically CS university programmes in my country (Croatia, but the same probably goes for all ex-Yugoslavia and possibly a lot of other countries) evolved exclusively from electrical engineering and therefore there are some leftovers from that in the Bachelor programme that, by me, is incorrectly translated to Computer Science because it's closer to Computer Engineering or the broader field of Computing. I had to take an extra Physics test (basic high school Physics) on my state graduation to enroll for the programme and I had two semesters of Physics during my first two years. It was all pretty basic, we covered the theoretical basics of every field and the only difficult problems were mathematically difficult because most people were learning the necessary maths in parallel. I liked those two courses and found the problem solving interesting, but the knowledge itself brings no direct benefits to CS.",5ld8eh,t3_5ld8eh,Kwabena7,,Comment,2,0,2
dbutfhw,2016-12-31 22:08:05-05:00,Xudda,,"For me, physics is required for the engineering associates. However, it has to b taken at either the AP or the college level. ",5ld8eh,t3_5ld8eh,Kwabena7,,Comment,1,0,1
dbuytlq,2017-01-01 01:15:43-05:00,Yourothercat,,"Physics was a prerequisite of my program, and I only needed to take first year Physics in University.  Once you begin working you won't likely ever use it again, at least I haven't.",5ld8eh,t3_5ld8eh,Kwabena7,,Comment,1,0,1
dbv0tsc,2017-01-01 02:41:14-05:00,Kwabena7,,It's also a prerequisite for my university but will I get punished severely if I didn't take the class in high school and then later take the course in university? Also do you think taking Bio and Chem is alright?,5ld8eh,t1_dbuytlq,Yourothercat,,Reply,1,0,1
dbv18on,2017-01-01 03:01:43-05:00,Yourothercat,,"If by punished you mean severely challenged, then yes. However, if you can take your calc classes prior to taking Physics you'll be fine.  Bio and Chem won't prepare you for Physics at all though.",5ld8eh,t1_dbv0tsc,Kwabena7,,Reply,1,0,1
dbzokm7,2017-01-04 11:13:58-05:00,andybmcc,,"HS physics classes usually cover basic classical mechanics without a calculus requirement.  Taking physics will give you a slight advantage on problem solving techniques, but as long as your math background is adequate, I'd just take whatever interests you more.  Make sure to check with the University that a physics deficiency upon admission won't negatively impact your scheduling or requirements.",5ld8eh,t1_dbv0tsc,Kwabena7,,Reply,1,0,1
5lccf9,2016-12-31 18:34:08-05:00,swinkid,Genetic Regulatory Networks,"Hey,

I've got a bicomputation Exam in January and I've got through most of the course material well, and I understand alot of the concepts taught to us, such as GA's, ACO, Reinforcement learning etc.

One concept I'm really struggling with is Genetic Regulatory Networks and I can't seem to find much online about it either.

The sort of things covered in this part of the course are things like Gene encoding, Boolean Regulatory Networks and State Spaces of Boolean Networks.

I was wondering if someone can point me towards so material I can read online; Revising from lecture slides isn't easy.

Thanks!
",,,,,Submission,2,0,2
5lb0xx,2016-12-31 13:47:18-05:00,Kwabena7,Pursuing Computer Science,I'm thinking of going to university for computer science but I was wondering if it's worth it? I'm about to choose my courses for my post secondary school learning and Computer Science stands out the most to me mostly because I'm really good with computers and I find all the algorithms interesting. Is it a worth while job and how many years does it take for the course? Also what are other jobs I should be looking towards that are related to Computer Science?,,,,,Submission,1,0,1
dbubxtp,2016-12-31 14:26:25-05:00,None,,[deleted],5lb0xx,t3_5lb0xx,Kwabena7,,Comment,3,0,3
dbuhtw7,2016-12-31 16:52:59-05:00,Kwabena7,,How much are you enjoying your time at university? Also what are the hardest classes and do you have any tips for new students choosing it as a undergrad?,5lb0xx,t1_dbubxtp,None,,Reply,1,0,1
dbukfq0,2016-12-31 18:00:54-05:00,liquidify,,"Just understand that CS is no joke unless you are going to a joke college.  You will need to complete and excel at math classes up through differential equations and linear algebra.  You will need to understand operating systems on a deep level as well as enough computer architecture and electronics to be able to understand hardware and how it relates to computations.  You will need to take ownership over at least one language while understanding the principles of it deeply enough to translate it to other languages.  You will need to use creativity to solve difficult problems, and you will also have to think like an engineer.  

On the other hand, if you just want to be a software designer or engineer, you will just need to know how to program, and know some basic math.  CS is not programming, so don't confuse the two.  Many colleges seem to be confused about this, but it makes sense because there are so many jobs available to people who just need decent programming skills. ",5lb0xx,t1_dbuhtw7,Kwabena7,,Reply,2,0,2
dbudfc9,2016-12-31 15:02:27-05:00,KronktheKronk,,"How about you do some actual research on opportunities and how they line up with your interests and what's required to get from A to B in your specific situation.

You're asking a bunch of computer scientists if they think computer science is worth it. Guess what response you're going to get.",5lb0xx,t3_5lb0xx,Kwabena7,,Comment,3,0,3
dbul6lr,2016-12-31 18:20:57-05:00,RatherPleasent,,"Don't do it. I'm majoring in it and regret it. The major is cool, job prospect are nice, but that'll be over in three years. 

When 2020 happens computers won't know what to do with the extra 0 and everything is going to go kablam. I'm hoping to get a $9,000,000 a year job at a big four company that way I can live off of that for the rest of my life. 

 ",5lb0xx,t3_5lb0xx,Kwabena7,,Comment,-3,0,-3
dbus1z1,2016-12-31 21:27:50-05:00,Kwabena7,,What do you think I should be looking at? I kinda want to do something aimed at computers at least.,5lb0xx,t1_dbul6lr,RatherPleasent,,Reply,1,0,1
dbuu60f,2016-12-31 22:30:00-05:00,chasecaleb,,Read that last paragraph again... this guy is either drunk or crazy: ignore him. ,5lb0xx,t1_dbus1z1,Kwabena7,,Reply,5,0,5
dbv0yyq,2017-01-01 02:48:09-05:00,RatherPleasent,,Get an MIS degree. Its Business + Computers,5lb0xx,t1_dbus1z1,Kwabena7,,Reply,1,0,1
dbut1da,2016-12-31 21:56:32-05:00,Kwabena7,,"I was also wondering, how important is physics to get into computer science? I don't like it that much and the uni im applying to says One of: Physics (SPH4U), Chemistry (SCH4U), Biology (SBI4U). I'm thinking of taking both chem and bio instead.",5lb0xx,t1_dbul6lr,RatherPleasent,,Reply,1,0,1
dc0i3a3,2017-01-04 20:46:35-05:00,Sonder777,,Not really that important to be honest. You'll be fine with just taking chemistry and biology. ,5lb0xx,t1_dbut1da,Kwabena7,,Reply,2,0,2
5l9bxf,2016-12-31 06:38:35-05:00,mortdecaivancleef,Efficient algorithm for determining existence of perfect matching in a bipartite graph?,"I've been looking around and found some properties of bipartite graphs that indicate the existence of a perfect matching, but not really any concrete algorithms I can use for implementation. ",,,,,Submission,7,0,7
dbtyih6,2016-12-31 08:00:44-05:00,LimivorousArbour,,[Hopcroft-Karp](https://en.wikipedia.org/wiki/Hopcroft%E2%80%93Karp_algorithm) algorithm?,5l9bxf,t3_5l9bxf,mortdecaivancleef,,Comment,4,0,4
dbu1q43,2016-12-31 10:10:53-05:00,mortdecaivancleef,,"Thanks, ended up using Ford-Fulkerson to calculate the maximum matching, if the size of the matching is equal to half of the amount of vertices in the graph, it is a perfect matching. It is slightly slower than Hopcroft-Kraft, but simpler to implement and reading the input has higher complexity anyway(unavoidable, I didn't get to decide the input format) .",5l9bxf,t3_5l9bxf,mortdecaivancleef,,Comment,3,0,3
dbtyxlr,2016-12-31 08:22:06-05:00,thewataru,,"I can't remember the name of the algorithm, but if you don't need the matching itself, and just need to check if there is a perfect matching, there is a fast way.

I vaguely remember it: construct an adjacency matrix, but fill it with random numbers instead of 1, then calculate determinant. It should not be zero. For better results you should run this algorithm several times using different random numbers because it is probabilistic.

I think, I forgot something up, but I am sure there was a matrix with random numbers and determinant of it were checked.",5l9bxf,t3_5l9bxf,mortdecaivancleef,,Comment,2,0,2
dbu6vx3,2016-12-31 12:25:35-05:00,crisader,,"Mulmuley Vazirani Vazirani 87, Im on mobile so i cant link the paper.

It is parallized though, doubt it is useful in most cases.

It can also construct a PM",5l9bxf,t1_dbtyxlr,thewataru,,Reply,2,0,2
dbuaca3,2016-12-31 13:48:16-05:00,thewataru,,"Yes, thank you. It was probably based on that algorithm. Although weights were not chosen as a powers of 2.",5l9bxf,t1_dbu6vx3,crisader,,Reply,1,0,1
5l8pbm,2016-12-31 02:49:51-05:00,kinukin,Does Wake On Lan work if the laptop lid is closed?,"Can I use Wake On Lan with the laptop lid closed?

Thanks",,,,,Submission,0,0,0
dbtz7y5,2016-12-31 08:35:45-05:00,Cznielsen,,This has nothing to do with computer science.,5l8pbm,t3_5l8pbm,kinukin,,Comment,6,0,6
5l4rsp,2016-12-30 12:59:11-05:00,MrOaiki,What is the future of computer hardware?,"Processors will be faster, storage will be bigger and so on. But I'm not talking about that, I mean what will be a leap in computing? Do we see a hypothetical future where storage has become so fast that RAM no longer exists as a component? How do future file systems work?",,,,,Submission,22,0,22
dbtddz5,2016-12-30 19:36:34-05:00,drummer_ash,,"Could the shift instead go the other way? Replacing all HDDs and SSDs with flash memory the whole way?. 
Is there a real downside to flash memory? ",5l4rsp,t1_dbt5obh,PastyPilgrim,,Reply,1,0,1
dbt5obh,2016-12-30 16:31:12-05:00,PastyPilgrim,,"No one can really speak to the far off future because the future of computing likely relies on knowledge that literally doesn't exist right now. But as far as the near-ish (5-10 years) future goes:

Processors aren't likely to be that much faster. For years now, we've relied on just improving things like branch prediction, parallelism, etc. to increase the power of our CPUs but changes like those are riddled with limitations. We can't make them faster (as far as clock speed) because then they become impossible to cool. We could cool them if we could make them larger, but the speed of light is a huge bottleneck (thus why chips keep getting smaller because then the light has a shorter distance to travel) and that speed limit is never going to change.

>Do we see a hypothetical future where storage has become so fast that RAM no longer exists as a component?

No. Our fast storage right now is with SSDs but SSDs have a limited number of writes that they can perform. Program memory is *constantly* being read/written (millions or billions of times per second) and would kill an SSD immediately. Further, an SSD will never be as fast as RAM because as their speed/bandwidth increase, the speed/bandwidth of RAM is increasing too (which has advantages in speed that an SSD, by definition, can't have).

Power consumption is something that is getting better in a hurry thanks to the demand and rapid turnover of cell phones, so that's something we can probably expect though.",5l4rsp,t3_5l4rsp,MrOaiki,,Comment,10,0,10
dbt6udn,2016-12-30 16:57:36-05:00,soggybiscuit93,,"To expand on this, expect the immediate future to be a shift from increasing processing power to lowering power usage and heat.

Market trends right now are to increase the portability. For most use cases, the power of a general use desktop is much more powerful than what the majority of the population needs, by a large margin. Shrinking that power down into tablets, ultrabooks, and phones is the goal atm.

We can already see this shift happening with the new Intel Kaby Lake chips coming out now.",5l4rsp,t1_dbt5obh,PastyPilgrim,,Reply,5,0,5
dbteb93,2016-12-30 19:59:54-05:00,PastyPilgrim,,"SSDs are flash memory. It's flash memory that has the limited writes deficiency.

RAM uses volatile (power is required to retain information) memory that does not have limitations on the number of writes. SSDs would never use this kind of storage because they need to be able to retain information without power, and RAM would never use flash because of the limited writes problem. Flash might also be slower than volatile memory cells.",5l4rsp,t1_dbtddz5,drummer_ash,,Reply,6,0,6
dbth9tw,2016-12-30 21:14:30-05:00,drummer_ash,,Thanks for clarifying! ,5l4rsp,t1_dbteb93,PastyPilgrim,,Reply,1,0,1
dbtq2zu,2016-12-31 01:04:37-05:00,bacondev,,"I think that I don't fully agree with everything that has been stated in the other comments. For example, there will always be a demand for more processing power. Programmers will push the limits of available hardware. Hardware designers will respond accordingly by creating designs that can comfortably handle the increased usage.

It's similar to the Internet. There was a time in which people strove to have their web pages be less than 100 kB in size. Now, it's not uncommon for web pages to be 1 MB or more. As Internet speeds keep increasing, web developers will be more liberal in how much data they elect to send.

It's a perpetual cycle. With the rise of artificial intelligence and machine learning, there is a serious demand for more powerful hardware. And we're responding. For example, DeepMind created a new type of processor called a tensor processing unit (TPU), multiple research teams are developing quantum computers, etc.

We will find plenty of limit-pushing uses for these technologies. And the cycle will repeat. We might not know the what or the how, but if history is any indicator, then we can bet on the will.",5l4rsp,t3_5l4rsp,MrOaiki,,Comment,3,0,3
dbyy9i4,2017-01-03 21:08:26-05:00,jollytopper,,"I don't think anyone is saying that there will not be more demand for processing power. 

The ""what"" and ""how"" is what's being questioned. Looking at historical trends, it's true that Moore's law is dying. We've been relying on parallelism more and more (AVX, multicore), but even that has its limitations (Amdahl's). ",5l4rsp,t1_dbtq2zu,bacondev,,Reply,1,0,1
5l4854,2016-12-30 11:22:04-05:00,HounerX,i need a computer scientist to review my essay about why I intend to major in CS,,,,,,Submission,0,0,0
dbssiza,2016-12-30 11:46:03-05:00,None,,Any general professional scientist will do.,5l4854,t3_5l4854,HounerX,,Comment,1,0,1
dbssp6t,2016-12-30 11:49:48-05:00,HounerX,,yea so can i send u it ?,5l4854,t1_dbssiza,None,,Reply,-4,0,-4
dbvkt66,2017-01-01 15:28:06-05:00,None,,I don't want to read it. Are you a minor?,5l4854,t1_dbssp6t,HounerX,,Reply,2,0,2
dbszeki,2016-12-30 14:12:54-05:00,Steve132,,You can PM me. ,5l4854,t3_5l4854,HounerX,,Comment,1,0,1
dbtrig5,2016-12-31 01:51:44-05:00,swilliamseu,,Pm me if you want another opinion,5l4854,t3_5l4854,HounerX,,Comment,1,0,1
5l1jus,2016-12-29 23:41:43-05:00,throwawayqss2,How much faster is RAM than a modern SSD (like PCIe based one in a macbook pro for example)?,I'd like to understand how the speeds compare. I realize that SSD is not mechanical (like an old spinning platter) so are speeds and latencies comparable? especially over like PCIe and other fast connections as within a modern macbook? Thanks!,,,,,Submission,19,0,19
dbscdfo,2016-12-30 01:26:57-05:00,Jedakiah,,"There are three benchmarks that would be especially relevant when comparing PCIe SSDs to DDR4: throughput, latency, and IOPs. Let's compare the [PCIe Intel P3700](http://www.anandtech.com/show/8104/intel-ssd-dc-p3700-review-the-pcie-ssd-transition-begins-with-nvme/3) with [Crucials DDR4 claims](http://www.crucial.com/usa/en/memory-performance-speed-latency) in those three areas.

### Small Read Throughput

RAM is often read and written to in extremely small ""blocks"", or amounts of data. Hard drives are optimized for reading and writing large chunks of data, but are often very slow on small ones. RAM basically doesn't care what size the data is since it's controller is typically the CPU itself. Here the higher numbers are better.

* 60000 MB/s @ 21 million IOPs - DDR4
* 36.7 MB/s @ 8900 IOPs - PCIe Intel P3700

### Average Latency

When you are a CPU that can do over 3 billion calculations per second in a single pipeline, every billionth of a second counts. Here units are in `ns`, or billionths of a second. The lower the better.

* 14ns - DDR4
* 121300ns* - PCIe Intel P3700

### Conclusion

Obviously there is no comparison here. If we were able to find a proper benchmark comparing a RAM drive to a single PCIe SSD, the numbers might not be quite as disparate. But, it is still reasonable to think that RAM will be many orders of magnitude faster in a lot of areas. 

For one thing physical distance from the CPU plays a hugely important factor here, since electrical signals can not propagate faster than light speed. RAM pathways are designed with very short routes direct to the CPU to help accommodate this. 

Further RAM is directly controlled by modern CPUs with an integrated memory controller, helping reduce latency. While an SSD will have it's own controller, often a 2-4 core ARM CPU that will have to do some processing before it sends data out to the main CPU. This too introduces latency.

\* Edit: Updated my SSD latency figure. Originally it said the SSD only had 121ns latency. See Comments below for details.

**tl:dr** There is no comparison, RAM is hands down faster.",5l1jus,t3_5l1jus,throwawayqss2,,Comment,14,0,14
dbso01v,2016-12-30 09:57:36-05:00,IgnorantPlatypus,,"Where do you get either of those latency numbers? DDR4 latency is on the order of 50ns to 80s (see [here](http://www.corsair.com/en-us/blog/2015/september/ddr3_vs_ddr4_generational). 14ns is more like L3 cache.)  Flash latency is on the order of 250,000ns usually (see [here](http://www.anandtech.com/show/8104/intel-ssd-dc-p3700-review-the-pcie-ssd-transition-begins-with-nvme/3) from your own link, which shows 280us).

EDIT: fixed a parenthesis",5l1jus,t1_dbscdfo,Jedakiah,,Reply,3,0,3
dbsujfi,2016-12-30 12:29:19-05:00,Jedakiah,,"My mistake about SSD latency. I thought my link said .121µs, it was actually 121µs. But that's only off by three decimal places! 

I would also trust your DDR4 latency numbers more than the Crucial one I shared. Crucial did not even share benchmark data, they were just the top result on Google.

Thanks for pointing this out. Even with your numbers, without a really good benchmark comparing them I still expect a margin of error of an order of magnitude or three. OP is basically wondering what a single PCIe SSD can do if it was handling a RAM workload. I found benchmarks that compared 16 DIMM RAM drives to SSD arrays. But that's not a great comparison for this question, and none of them were trying to use that space for system memory. If think it would be necessary to simulate this usage in order to see how truly far apart the two are in terms of performance. And I suspect they will remain that far apart until the market starts selling non-volatile memory. At that point your CPUs IMC would likely be extended to cover all sorts of raw storage, and storage devices might start becoming dumb hunks of memory which the CPU directly controls.",5l1jus,t1_dbso01v,IgnorantPlatypus,,Reply,1,0,1
dbsv5bx,2016-12-30 12:42:18-05:00,IgnorantPlatypus,,"Three orders of magnitude (1000x) is much larger than the margin of error here.

Raw NAND reads have a limit, even with the fastest SLC chips. And pretty much no one does SLC anymore, it's all MLC and TLC which are much slower to read. The denser NAND chips get the slower they are, too.  Unless you are reading from cache, SSD/NAND reads will always be on the order of 100us or more.

Reading from DRAM also can only get so fast: it's farther away from the CPU and it has to run over an arbitrated bus, so there's some inherent limits.

Future hardware (storage class memory) will be faster, and will definitely need new architectures for storage/filesystems: at some point the cost of reading persistent memory is fast enough that it's not worth a context switch. But that hardware is not yet cost effective for large amounts of data.

Source: I work on the software side of flash memory.",5l1jus,t1_dbsujfi,Jedakiah,,Reply,1,0,1
dbsvze9,2016-12-30 13:00:09-05:00,Jedakiah,,"I was making a joke when I said I was only off by three decimal places. 

If we were just talking about latency, then yes I do agree we could narrow the margin of error for the average read. But we are also talking about writes, throughput, and IOPs, hence why I was saying there is still a decent margin. SSD controllers would not handle that workload well, and with how NAND blocks work write performance in particular will be a choke point.",5l1jus,t1_dbsv5bx,IgnorantPlatypus,,Reply,1,0,1
dbsioit,2016-12-30 06:30:11-05:00,BonzoESC,,"It is really cool that there's only a magnitude of difference in latency. Compared with [the double digit milliseconds a mechanical disk takes,](http://www.anandtech.com/show/9606/wd-red-pro-6-tb-review-a-nas-hdd-for-the-performance-conscious/3) it's easy to see why there's been a lot of research into data structures that minimize seek count. ",5l1jus,t1_dbscdfo,Jedakiah,,Reply,2,0,2
dbsuth6,2016-12-30 12:35:21-05:00,Jedakiah,,"I was actually wrong about that. See the comment by /u/IgnorantPlatypus above. It is more like 4 orders of magnitude difference. And that sounds more like it to me. Even I was surprised by the number I shared. But I did not spend too much time researching this, since it's highly speculative without proper benchmarks.",5l1jus,t1_dbsioit,BonzoESC,,Reply,1,0,1
dbsme25,2016-12-30 09:10:54-05:00,throwawayqss2,,Thank you for this.  For some reason you gave only the small-read throughoutput.  I also don't understand why.  What about large reads?  What about large writes?,5l1jus,t1_dbscdfo,Jedakiah,,Reply,2,0,2
dbsvn37,2016-12-30 12:52:54-05:00,Jedakiah,,"Basically the smaller the average chunk, the wider the performance gap. If we looked at large read throughput, we could narrow the gap a little. However, memory is often accessed in extremely tiny chunks, often less than the 4Kb chunks in the benchmark I shared. I would guess that the extreme majority of operations are in chunks smaller than 4Kb. Thus sharing a benchmark of even larger chunks would be less indicative of the performance gap.

Either way it is pretty theoretical. If you want to see how RAM comares when used in place of an HD, then we can google ""RAM drive benchmarks"" and get lots of numbers, mostly comparing large numbers of DIMMs with RAID arrays (which is a different ball game than your example laptop memory vs. storage). But presumably you were wondering if you can get by on less RAM when you have a PCIe SSD. In that case we would need to compare how an SSD would fair under a RAM workload, hence my above comparison.",5l1jus,t1_dbsme25,throwawayqss2,,Reply,1,0,1
dbs9tlc,2016-12-30 00:04:44-05:00,jollytopper,,"Samsung 850 EVO has a sequential read 540 MB/s and a sequential write of 520 MB/s. DRAM reads and writes in the order of G/s (2 GB/s - 20 GB/s, don't quote me). ",5l1jus,t3_5l1jus,throwawayqss2,,Comment,4,0,4
dbss9tt,2016-12-30 11:40:25-05:00,None,,"Fundamentally RAM is using electricity to keep time while the SSD uses electricity only when you want it to.

In computer science the hardware characteristics can be abstracted with the transistor switch network model as one view into the physics of computers.",5l1jus,t3_5l1jus,throwawayqss2,,Comment,-1,0,-1
5kztcr,2016-12-29 17:47:23-05:00,RustArtist,"Question: Can we now program so well compared to past, we can make old hardware do better things?","Hello,

Don't know if this is the right place to ask this but it felt informal to ask in /r/AskScience - Computing.

So I wonder if we hired the best programmers and gave them enough resources, could they make old hardware perform in a way considered almost impossible at the day? Like, old gaming consoles running games with better graphics, or even unmanned spacecrafts just performing better?

Or we already did the best?",,,,,Submission,21,0,21
dbry78c,2016-12-29 19:14:24-05:00,whowereyouexpecting,,"Actually I think the opposite is happening. Modern hardware is so powerful that programmers have become lazy and write poorly optimised code. So current generation coders wouldn't even be able to match the achievements of the past.

Not to disrespect current coders. The challenges of the past are no longer a problem that needs to be solved. Current challenges are more likely to be security and privacy related rather than optimisation.",5kztcr,t3_5kztcr,RustArtist,,Comment,33,0,33
dbrz1kh,2016-12-29 19:34:20-05:00,tRfalcore,,everything is virtual.  installing docker roms onto virtual cloud servers.  I don't even know what computer it's running on nor where and I don't care.,5kztcr,t1_dbry78c,whowereyouexpecting,,Reply,6,0,6
dbssqe7,2016-12-30 11:50:29-05:00,coolthesejets,,I'm familiar with docker but what's a docker rom?,5kztcr,t1_dbrz1kh,tRfalcore,,Reply,1,0,1
dbsyq9u,2016-12-30 13:58:23-05:00,scriptmonkey420,,"Don't know anything about it myself, but from a quick google search, it looks like its for building android roms.",5kztcr,t1_dbssqe7,coolthesejets,,Reply,1,0,1
dbsz8nk,2016-12-30 14:09:24-05:00,coolthesejets,,Yea I found that too. It sounded a little too specific for his/her use case though.,5kztcr,t1_dbsyq9u,scriptmonkey420,,Reply,1,0,1
dbt74v6,2016-12-30 17:04:12-05:00,tRfalcore,,"nothing special, that s what i just call docker packages",5kztcr,t1_dbssqe7,coolthesejets,,Reply,1,0,1
dbsyw4o,2016-12-30 14:01:52-05:00,okiyama,,"I think that even if you wanted to uber optimize stuff you couldn't really do it anymore. Programs used to written for extremely specific sets of hardware, so programmers could push that hardware to the limit since they knew what they were working with.

Now, you write some code and you don't even know necessarily what processor architecture it will run on!

If you want that sort of optimization like they had in the old days, look at game console programming. Since they have known constraints and known hardware they will push it extremely far.",5kztcr,t1_dbry78c,whowereyouexpecting,,Reply,2,0,2
dbrwccm,2016-12-29 18:30:28-05:00,tyggerjai,,"Generally, I would say most old systems like that were tweaked to within an inch of their life already. People working on a project back then had a fairly intimate relationship with the hardware they were using, and the fundamentals of computer science - the theoretical, algorithmic underpinnings - haven't really seen much development at a level that hardware could take advantage of. 

So I would say you wouldn't see much improvement. The hardware has limits, and programmers have fairly consistently hit them. ",5kztcr,t3_5kztcr,RustArtist,,Comment,17,0,17
dbrx08h,2016-12-29 18:46:00-05:00,Treyzania,,"I don't have a link but a developer of one of the early Crash Bandicoot games ran into save corruption bugs when there was controller input when the game was saving.

Why?  Turns out the electrical induction from the lines coming from the inputs was causing problems with the memory bus and corrupting data.  But this was because they had turned up the frequency of a certain clock typically used for physics calculation but was being used for saving data, but most games kept it at the default lower speed and never ran into the issue.

Sony Engineers wouldn't believe it until he sent them a quick and short C program that would replicate the issue.",5kztcr,t1_dbrwccm,tyggerjai,,Reply,10,0,10
dbryrwp,2016-12-29 19:27:49-05:00,saintchrit,,[Here is the original Quora link to those who are interested](https://www.quora.com/Programming-Interviews/Whats-the-hardest-bug-youve-debugged/answer/Dave-Baggett?srid=pxH3&share=1),5kztcr,t1_dbrx08h,Treyzania,,Reply,4,0,4
dbry0fy,2016-12-29 19:09:51-05:00,daymi,,"It depends on the machine. There are very old very limited machines like the Commodore C64 whose limits people took as a challenge and made insanely good demos and games (3D graphics!! With a 1 MHz CPU - yeah, *Mega*Hz, not GigaHz. Seriously. Also, text to speech etc).

Now in 2016, that machine is known down to the level of individual gates and where exactly the gates are on the chip etc. With that kind of knowledge much is possible (if you have insane attention to detail).

A programmer gets better the more experience he has. So while he couldn't use that information when he was a rookie he sure can use it after 15 years! And people see it and can't believe it's possible at all. I certainly can't believe it. l'll give examples - but from today's perspective they don't look so great compared to a modern computer with a 2532 MHz CPU - well duh. But I had a C64 as a child and know just how shitty they were. And yet someone managed to build this: https://www.youtube.com/watch?v=PSztkwjKPKg - Yep, that's a 3D game on a 160x200 screen without GPU acceleration and with 1 MHz CPU and 64 kiB (that's 0.000061 GiB) of RAM. Plays fluently.

That said, you won't get any faster than 1 MHz with this machine. But turns out with 1 MHz you can do quite a lot of things per second.

Also, back in the day proper software engineering was... almost unknown. So I expect that many things could be much faster on the same machine just by using some clever non-brute-force algorithms.",5kztcr,t3_5kztcr,RustArtist,,Comment,6,0,6
dbrzkih,2016-12-29 19:47:05-05:00,dizzydizzy,,"I think this proves the case. At the time a wire frame renderer was state of the art on the C64.

Back then 3d coding was fairly esoteric knowledge and the wolfenstein raycasting engine hadnt been invented.

Also tools are so much better now, you can write C64 code on a pc with an emulator, with full debugging, and near instant edit/assemble/run/debug cycles.



",5kztcr,t1_dbry0fy,daymi,,Reply,3,0,3
dbs1kct,2016-12-29 20:34:42-05:00,BonzoESC,,"The biggest change for old hardware capability is that much current development on old hardware is done without time or budget constraints by enthusiasts who want to explore the system rather than ship software that pays the bills. Consider [""Overdrive"" by Titan](https://megabitesblog.wordpress.com/2014/03/26/titandemogroup/). Some of the techniques they use wouldn't see light of day in a production Genesis/Mega Drive game because the Titan group's goals were very different from most Genesis developers.

The difference is economic goals, not programming quality. The biggest advances in software development in the last few decades have been accessibility, safety, and networked capabilities, not hardware utilization. If there are more programmers interested in old hardware, it's probably because if 1% of all programmers are interested in retro computing, 1% of the current population of programmers is much bigger than 1% of programmers a decade ago.",5kztcr,t3_5kztcr,RustArtist,,Comment,2,0,2
dbsmry1,2016-12-30 09:22:32-05:00,swilliamseu,,"Modern game consoles are possibly the only place in industry where hardware optimisation matters to any large degree. Developers are completely limited in what their games must run on so they have to ensure that if their game has good graphics it must be efficient. These efficiencies improve over time.

Compare top of the line games at the start of the generation vs end of the generation. Oblivion vs Skyrim. Uncharted vs Last of Us. Assassin's Creed vs Black Flag.",5kztcr,t3_5kztcr,RustArtist,,Comment,2,0,2
dbs1443,2016-12-29 20:23:58-05:00,lead999x,,"I think the modern virtual machine based programming languages are actually slower than the native languages of the past with harware held equal. If anything we could make more efficient use of hardware today by using C, C++, or other lower level languages than with Java or Python or Haskell. ",5kztcr,t3_5kztcr,RustArtist,,Comment,1,0,1
dbsbxj9,2016-12-30 01:12:02-05:00,cguess,,"Most of that stuff is so precise you can barely move in the code without breaking things at the memory map level. However! my favorite story of this not being true is Woz (because, of course it's Woz, he's probably one of the best to ever do this stuff) realizing he could both have removed two chips from the Apple II while also giving it better grayscale... 38 years later: https://techcrunch.com/2014/11/04/nearly-40-years-later-steve-wozniak-still-brainstorms-ways-the-apple-ii-could-have-been-better/",5kztcr,t3_5kztcr,RustArtist,,Comment,1,0,1
dbwafo9,2017-01-02 02:11:08-05:00,Ralith,,Advancements in programming technologies and techniques have focused more on improving developer productivity and software reliability than on software performance.,5kztcr,t3_5kztcr,RustArtist,,Comment,1,0,1
5kz73s,2016-12-29 15:55:35-05:00,Ionutlng,Game-network programming,"I have to do a multiplayer game (C++, client/server app in UNIX) for next week for my computer network class. I am familiar with TCP, socket and thread but i don't have any idea how to connect two clients and play against each other on the server. I've only managed to do a simple echo client/server app, but I don't know what should I change in that program in order to make a game. Do you know a good tutorial, or some examples with games programs, to inspire me, it would be very helpful. Thanks for your attention and sorry for my english.",,,,,Submission,3,0,3
dbs0hye,2016-12-29 20:09:16-05:00,TheGrue,,"Alright, so you already have an echo client/server app, where the client sends data to the server, and the server responds back with the same data.

If you have two (or more) clients that want to play a game together, then when you receive data from one client, you want to echo it to all of the others in the same game instance so that they know what 'move' was played by the sending client.  In this way, all clients can maintain the same state, as they all know what states they sent (moves made by the client), and what states they have received (moves made by other clients).",5kz73s,t3_5kz73s,Ionutlng,,Comment,2,0,2
dbsotf7,2016-12-30 10:19:07-05:00,Ionutlng,,"Yes, I got it, but i don't know exactly how to change my code to do this.. this is my [server](http://pastebin.com/a1KDYHrg) and [client](http://pastebin.com/bejq6D5L) code, if you can help me ..",5kz73s,t1_dbs0hye,TheGrue,,Reply,1,0,1
dbsuilo,2016-12-30 12:28:49-05:00,lordvadr,,"One of the first thing you need to do is decide what kind of data you're going to be sending back and forth because this affects design quite a bit.  From an academic standpoint, you don't have to worry much about the practical implications of any of these decisions, but you'll still have to look at it from that aspect a little.

So let's design a network chess game.

Are you going to ship the whole contents of the board back and forth?  What happens if the board contents one player is sent essentially contains an illegal move by the other player?

Are you going to just send the moves to each other?  What happens if each player's boards get out of sync?

Maybe you designate one side as the ""server"" and the other as the ""client"" (how?), where the server is authoritative for the state of the board.

While this doesn't scale well for larger games, in this scenario, let's say we design it so that at each move, both the new contents of the entire board as well as the moves are what's sent back and forth, which means each side could undo the move and compare it to the board he thinks is correct as well as independently verify that the move is valid.  (Of course, then what you do you, reject it?  What if the other player insists its a valid move?...these are really beyond the academic nature of this, but its still something to consider...)

Next you have to figure out how you're going to format the data.  Essentially you need to write a to_network_format and a from_network_format method for your object.  You can think of this a lot like when you overloaded the `<<` operator to make your object print nicely, and likewise when you overloaded the `>>` operator so you could read a date object (or whatever it was you read).  You'll actually probably want a constructor for the network data so that you can make a new board object

Things like JSON, XML, are ways to do this while still being human readable.  The downside is that you then have to parse strings.  You can pack it in binary and send it, but you then have to worry about (not academically at least) type-sizes and host ordering (if you do this, PLEASE look at [network vs host byte ordering](http://beej.us/guide/bgnet/output/html/multipage/htonsman.html)).

If you're allowed, just light up a JSON library for C++ and be done with it.

I chose a chess game so that you don't have to worry about asynchronous I/O or any of the other interesting things that come along, but that's something to consider too.

Lastly you'll have to decide how to handle improperly formatted messages, short messages, garbage data, etc, but your code should look something like...

    Board *theBoard:

    make_connections()

    theBoard = new Board():

    while( ! theBoard->game_over() ) {

        while( theBoard->my_turn() ) {
            wait_for_move():
            //error checking
        }

        char *netdata = theBoard->to_network_format():
        send(sock, netdata, strlen(netdata), 0):
        free(netdata):

        while( ! theBoard->my_turn() ) {

            char buff[8192]: // static buffer == bad
            int len:
            Board *newBoard:

            len = recv(sock, buff, 8192, 0):
            buff[len] = 0: // null terminate the string

            newBoard = new Board(buff):

            if( am_happy_with(newBoard) ) {
                delete theBoard:
                theBoard = newBoard:
            } else {
                 bitch_about_other_guys_move():
                 delete newBoard:
            }
        }
    }

    if( theBoard->i_won() )
        cout << ""You win!\n"":
    else
        cout << ""You suck!\n"":

    delete theBoard:",5kz73s,t3_5kz73s,Ionutlng,,Comment,2,0,2
5ky1s7,2016-12-29 12:39:28-05:00,FOKvothe,Help with J- /k flip-flops,"Hi. Could someone give me some pinters on, how to simplify [this](https://s28.postimg.org/sumbwmr4t/flipflop.png), when /PR and /CLR are both 1.",,,,,Submission,2,0,2
dbrklww,2016-12-29 14:18:20-05:00,Randolpho,,"This looks like homework.

How about you tell us what you've figured out so far and what confuses you, and we'll try to help show you what you've got right and wrong. 

As a general hint, don't forget to follow *all* of the wires, and remember your inversions. It may also help to label each of your components and their inputs, as that may make it easier to discuss each. ",5ky1s7,t3_5ky1s7,FOKvothe,,Comment,2,0,2
5kxui6,2016-12-29 12:04:00-05:00,beatbrot,Is using an antivirus still considered good practice?,"More and more people keep telling that antivirus programs actually do introduce more security problems than they prevent. They say it's way more important to use up to date software (which I generally do). 

So what is your opinion? Is it needed? Are there oses that need it more or less? (Linux, Android, Windows)",,,,,Submission,0,0,0
dbrgt7e,2016-12-29 13:00:00-05:00,CoopNine,,I'd say that this isn't an AskComputerScience Question...  Maybe something you'd want to ask in netsec or other sub reddits.  We aren't as interested in security as it pertains to IT infrastructure as we are into how it pertains to implementations. ,5kxui6,t3_5kxui6,beatbrot,,Comment,3,0,3
5kxcxa,2016-12-29 10:34:01-05:00,Candljack,Would it be worth my while to go back to school for Computer Science?,"Hello,

I have been considering for a while returning to school to seek a degree in Computer Science (or possibly Information Tech, but most likely CS). I graduated several years ago with my degree in Film Production and it has not been the greatest experience. I don't blame the degree itself, more like my lack of foresight. I have bound myself to my S.O. (happily) and she has an established career at a location where I can't really hope to make much money. I've actually been unemployed for a few months now. I am very lucky to be in a position where my S.O. is able and okay with supporting me while I figure things out. I am also looking into a few other career options unrelated to this.

I have always enjoyed sort of simple level coding. In high school I took Java I and Java II and loved those classes. I actually won an award for my 2 player Rock 'em Sock 'em Robots game, which may be child's play to you guys, but I was just a sophomore then. It was also more than a decade ago. Since then I have maintained an interest in HTML, writing addons for World of Warcraft, and also a strange hobby in seeing how far I can push Excel formulas.

Before I sink more money into my education, I was simply wondering if this (in your opinion) is a solid career move. Is the job market pretty solid just about everywhere? Or will I likely have to move somewhere else to do well in this field? Is the career actually nothing like what I think it is?

While I, like so many others, do like having money, it is not all that important to me. I would rather stay with my S.O. and make enough to contribute to our lives together over moving somewhere else to make the ""big bucks.""

Any input you guys might have would be greatly appreciated.

TL;DR How easy is it to find employment anywhere with CS and would you recommend it to someone who's experience so far is tinkering with minor coding?

EDIT: In my research I'm finding that some schools offer both a B.S. and a B.A. in Computer Science. Is there a significant difference?",,,,,Submission,2,0,2
dbrgyo2,2016-12-29 13:03:11-05:00,spatialdestiny,,"How old are you?  10 years out of sophmore year puts you around 26.  I'd say you have plenty of time to go get a degree in CS. 


Check with your preferred school (probably local, hopefully not too much driving) to see if the credits you have already taken will allow you to finish school faster.  In the meantime, start reading up on a programming language you want to focus on.  In my experience, that would probably be c# or java.  Plus learn SQL.  It's near-required for any medium and smaller business for developers to be good at SQL.  


Apply to school.  By the time you need to make a decision, you should have a 2-3 months of studying to help you decide whether you think it will be something you want.  And if you get rejected from school or decide you can't/won't do it for any reason, but still want to go into software development, you will have a good start.


A degree isn't required, it just makes it easier to get your first few jobs.  


Edit: I live in a municipal area of 80,000 that is relatively modern for technology jobs.  I've had 3 developer jobs and could probably go through 3 more before I had difficulty finding a job here.  I don't think I could find a good development job in a town of 1000, but I think you can find plenty of opportunities in any city of 50,000+.   ",5kxcxa,t3_5kxcxa,Candljack,,Comment,2,0,2
dbrn6vf,2016-12-29 15:11:31-05:00,Candljack,,26 is right on. I have only heard of SQL so I'll have to put some work in on that. I'll be living in an area that is well populated so I would imagine my chances of finding a job will be fine. One question that has just come up that I've edited into the main post is the difference between a BS and a BA in Computer Science. I did not realize that was an option until I looked into the nearby colleges. Do you know anything about the differences between them in terms of post graduation employment?,5kxcxa,t1_dbrgyo2,spatialdestiny,,Reply,1,0,1
dbrqp5g,2016-12-29 16:24:40-05:00,spatialdestiny,,"Just from some quick Googling, BA is usually more theoretical, and BS is more applied.  Since the theoretical part of CS is already not very useful in the workplace, but nice for figuring out what you want to do with your career, I'd stick with BS since you will also want the math classes because they usually complement the CS degree.  


I got a BS and I bet 95% of others here did as well.  


Don't be surprised when you need to take a few Calculus classes though.  Make the math study lab your second home for that semester.  ",5kxcxa,t1_dbrn6vf,Candljack,,Reply,1,0,1
dbrs6i7,2016-12-29 16:56:30-05:00,Candljack,,"Thanks for that input. I found the same results in terms of theoretical vs applied, but I wasn't sure on the usefulness of each in the professional world. 

I knew I was going to have to do some math. I have this weird thing where I'm actually really good at math, but I hate math for the sake of math. Like doing the equations for that Rock 'em Sock 'em Robots game I mentioned was actually fun, I also had fun in college with Astronomy and making calculations for basic astrophysics, but my math classes were always a struggle. I know I can manage the calculus classes though, I just won't enjoy them.",5kxcxa,t1_dbrqp5g,spatialdestiny,,Reply,1,0,1
dbrs3p0,2016-12-29 16:54:49-05:00,myfavoriteanimal,,"You may be better served by reposting your question on /r/cscareerquestions.

I would wholeheartedly recommend you go back for your degree. I worked as a developer for eight years before returning to finish my BS part time to strengthen my CS fundamentals. It was the correct decision, and this spring I'll be finishing an MS on my employer's dime.

I find that a BS typically requires more numerous and difficult math and CS courses than a BA, but this depends on the school. I always recommend the more challenging route - taking the tougher math and CS and science classes - as they'll generally teach you better problem solving skills. 

If you have some Java experience, you might spend some time brushing up on it, maybe learn some introductory data structures and study some math on [Khan Academy](http://www.khanacademy.com). A huge portion of undergrad CS programs teach with Java, so depending on where you go, your experience may help you through your first few classes.

Best of luck, and have fun!",5kxcxa,t3_5kxcxa,Candljack,,Comment,1,0,1
5kwdx6,2016-12-29 06:30:58-05:00,axtheandalite,Can I go through my first year of Comp Sci undergrad only with the university computers?,"Well simple question, can I? Or maybe just the first session? I'm studying in Australia btw and my uni's computers are quite good. Thank you guys

Edit: just background info that my laptop's hard drive failed to boot today and I figure it really went flatline this time (it'd shown signs before). The problem is I don't have a pc and with my current part-time wage it'd take a few months for me to save enough money ",,,,,Submission,14,0,14
dbr3zwx,2016-12-29 07:09:31-05:00,hemenex,,"Probably doable, but terribly uncomfortable. I would rather spend a little money to fix a laptop (you could buy used parts for cheap) or try to somehow buy a new one.",5kwdx6,t3_5kwdx6,axtheandalite,,Comment,4,0,4
dbr4uwe,2016-12-29 07:50:52-05:00,axtheandalite,,"Well it does sound hideous not having access to a computer between 10 pm and 8 am everyday. Yea i think fixing it or buying a new one would be better, thanks anyway :)",5kwdx6,t1_dbr3zwx,hemenex,,Reply,1,0,1
dbr5ndn,2016-12-29 08:24:15-05:00,hemenex,,"BTW, before you buy a hard drive, make sure it is the problem. Try to boot from USB or CD, if that doesn't work, it could be any other part.

EDIT: Also, as other suggested, you could work from Ubuntu on USB.",5kwdx6,t1_dbr4uwe,axtheandalite,,Reply,3,0,3
dbr48sb,2016-12-29 07:21:56-05:00,pencan,,"I got through the first 3 years of a computer engineering degree with only a $200 chromebook. Definitely can be done, remote into everything. ",5kwdx6,t3_5kwdx6,axtheandalite,,Comment,3,0,3
dbr4pin,2016-12-29 07:44:05-05:00,ugknite,,My first year of grad school was spent mostly in the computer lab. I used to go home just to sleep and shower. I didn't have a computer.,5kwdx6,t3_5kwdx6,axtheandalite,,Comment,1,0,1
dbr52wz,2016-12-29 08:00:44-05:00,axtheandalite,,Actually I'm about to do undergrad so it can be different... this gave me some hope tho that I can survive a few months to save up money for a new one,5kwdx6,t1_dbr4pin,ugknite,,Reply,1,0,1
dbr586w,2016-12-29 08:07:07-05:00,UntrustedProcess,,"Got a DVD drive 4 bucks to spend on a pen drive?  You can boot to a Linux live CD on your laptop and save any work to a pen drive.  You can also install everything to the pen drive if its large enough.

It isn't the best experience but could be enough and only set you back less than $10 if you can find a good deal on a pen drive.",5kwdx6,t3_5kwdx6,axtheandalite,,Comment,1,0,1
dbrqwsn,2016-12-29 16:29:09-05:00,61746162626f7474,,"If you really can't afford to get a new hard drive, your laptops takes standard 2.5"" drives and you feel you can replace the drive yourself I can probably ship you one, I've got loads lying around. It won't be fancy, probably a 256GB or 128GB spinner at  7200rpm but it would be better than having no laptop. 

I'm in the UK so shipping would be about a week from 5th Jan when I get home. I'm currently an undergrad doing CS so I have some sympathy. ",5kwdx6,t3_5kwdx6,axtheandalite,,Comment,1,0,1
dbrvuq9,2016-12-29 18:18:56-05:00,axtheandalite,,Thanks for the offer! I think I'll just bring it to the repair store to make sure it's the hard drive's fault first,5kwdx6,t1_dbrqwsn,61746162626f7474,,Reply,1,0,1
dbry2hl,2016-12-29 19:11:11-05:00,61746162626f7474,,"Okay sure, get back me when you know more if you still need a hard drive",5kwdx6,t1_dbrvuq9,axtheandalite,,Reply,1,0,1
dbrs0k5,2016-12-29 16:52:57-05:00,notUrAvgITguy,,I bought a lenovo x220 on ebay for like 180. It runs Fedora like a dream. I would look into that. ,5kwdx6,t3_5kwdx6,axtheandalite,,Comment,1,0,1
dbrx2zb,2016-12-29 18:47:42-05:00,Xian9,,"It should be fine, some people actually seem to like spending time in the library or computer labs. It might be worth checking if there's any cosy hideaways with computers too.   
  
> not having access to a computer between 10 pm and 8 am everyday    
  
Are you sure they aren't 24 hours?",5kwdx6,t3_5kwdx6,axtheandalite,,Comment,1,0,1
dbs1358,2016-12-29 20:23:21-05:00,axtheandalite,,The library aren't 24 hours but I'm not sure if there is a 24h lab dedicated to only compsci students,5kwdx6,t1_dbrx2zb,Xian9,,Reply,1,0,1
dbr3meg,2016-12-29 06:49:52-05:00,HotEspresso,,"My degree (IT) shared half of it's classes with CS. I'd say its possible, just very inconvenient. Depends on if your school computers have the environments/software you need.

If your hard drive went bad, you can spend a small amount of money to replace it. Put a solid state in there and you'll feel like its a new PC. Check out /r/buildapcsales",5kwdx6,t3_5kwdx6,axtheandalite,,Comment,1,0,1
dbr4q1m,2016-12-29 07:44:47-05:00,axtheandalite,,"Thanks I'll consider replacing the hard drive, at least it wouldn't cost too much",5kwdx6,t1_dbr3meg,HotEspresso,,Reply,1,0,1
dbr4txy,2016-12-29 07:49:43-05:00,HotEspresso,,"I got a 128gb solid state a year or two ago for $55. I've never really needed anymore space to be honest. I think it would be your best bet, instead of restricting yourself to school computers.",5kwdx6,t1_dbr4q1m,axtheandalite,,Reply,1,0,1
dbr56xh,2016-12-29 08:05:36-05:00,axtheandalite,,"I figure around 60gb would go for windows and 10gb for linux, which'll leave me 58gb right?

Btw quick question do you think 4gb RAM is alright for running a Linux VM in Windows?",5kwdx6,t1_dbr4txy,HotEspresso,,Reply,1,0,1
dbr5i38,2016-12-29 08:18:27-05:00,hemenex,,You are running a tight budget with those sizes and it will be a pain to manage it. But that RAM for Windows and Linux should be enough.,5kwdx6,t1_dbr56xh,axtheandalite,,Reply,3,0,3
dbs0um4,2016-12-29 20:17:41-05:00,axtheandalite,,Thanks :),5kwdx6,t1_dbr5i38,hemenex,,Reply,1,0,1
dbr679r,2016-12-29 08:44:47-05:00,HotEspresso,,"The Microsoft website says a windows 10 installation is 20gb. Linux also doesn't require much. I've always run mine in a VM and only given it a few gb of storage. I think allotting 60gb for Windows & Linux is more than enough. My work VM (which doesn't really have much on it on Centos 6.7 is currently using 6.2gb)


4gb of ram is enough. The Linux VM on my work PC and i've given it 1gb of ram. I don't use it for much besides fiddling around with, but the 1gb is fine. That being said, when I was in school, some of my classes (usually networking, so maybe nothing you'd take for CS), I used multiple VMs, as many as 4. I had 12gb of ram in my laptop, so it ran very well. Most of my friends had to shut down/boot up VMs as needed. You'd probably benefit from more RAM. Does the laptop have a decent processor? 



TL:DR: 60gb for Windows+Linux is probably a high but safe estimation. 4gb of ram should be enough, but ram is fairly cheap and more is always helpful.",5kwdx6,t1_dbr56xh,axtheandalite,,Reply,2,0,2
dbs0tgr,2016-12-29 20:16:57-05:00,axtheandalite,,Thanks heaps! :),5kwdx6,t1_dbr679r,HotEspresso,,Reply,2,0,2
dbrco8t,2016-12-29 11:33:42-05:00,FenPhen,,"> Depends on if your school computers have the environments/software you need.

This probably varies by school and it's been awhile for me, but my school had multiple computer labs, including engineering specific (and sometimes exclusive) ones that had the environments and software needed for most classes. For OP's first year, see if there are dedicated labs for CS students.",5kwdx6,t1_dbr3meg,HotEspresso,,Reply,1,0,1
5kw47u,2016-12-29 05:05:49-05:00,PoofByContradiction,How can I prepare for my coming undergrad Data Structures and Algorithms class?,"I want to prepare a bit for my coming semester, and from what I've heard, the hardest class I will be taking is data structures and algorithms. In effort to be more prepared, I was wondering which concepts I should introduce myself to sooner than later, you know, to get those brain juices flowin!

So far, my best guess is that we're going to look at big-theta stuff briefly, and then move on to coding projects, implementing a bunch of different algorithms to see how they're effective for specific problems(?). So, I was thinking of taking that [Sedgewick text ""Algorithms""](https://www.amazon.com/Algorithms-4th-Robert-Sedgewick/dp/032157351X), and implementing all of the algorithms discussed in the book in java and C++.

Someone also told me I should have a visual understanding of pointers and linked lists, as the class focuses on those a lot.

Bad approach?

Thanks!",,,,,Submission,2,0,2
dbr5s8h,2016-12-29 08:29:27-05:00,Infernal_NightGaunt,,"Hard to say what the focus will be on in your particular program but pointers and linked lists are good concepts to understand. When I took it we did a lot of linked lists, implemented a bunch of sorts, we did heaps, trees (as an introduction to graphs), and hashes. If you are able to implement 'all' of the algorithms discussed in Sedgewick's book your understanding of these concepts is well ahead of where you would be expected to be. Good luck!",5kw47u,t3_5kw47u,PoofByContradiction,,Comment,1,0,1
dbrdxrm,2016-12-29 12:00:40-05:00,beatbrot,,"If you maybe need a book recommendation you can take a look at: 

https://www.amazon.com/Introduction-Algorithms-3rd-MIT-Press/dp/0262033844

Hands down the best book in the business. 

Even the Java Developers used this exact book to improve their algorithms.",5kw47u,t3_5kw47u,PoofByContradiction,,Comment,1,0,1
dbru9c1,2016-12-29 17:42:18-05:00,BlueFootedBoobyBob,,"If this is similar to my course: trees, you are going to hate trees.",5kw47u,t3_5kw47u,PoofByContradiction,,Comment,1,0,1
5kskg0,2016-12-28 15:56:29-05:00,Leeds1234,Help with Turing Machines?,"What would the state diagram of `M= a^n b^n c^2n` look like? 
if you could also detail the implementation level description to make it easier to understand that would be great.. 
     
I can get `a^n b^n c^n` fine, the `c^2n` is just throwing me off in this Turing Machine   ",,,,,Submission,2,0,2
dbqm5bl,2016-12-28 20:38:38-05:00,Salusa,,"Sounds like a homework question :)

Post what you have and I'll try to guide you in the right direction.",5kskg0,t3_5kskg0,Leeds1234,,Comment,2,0,2
dbr2d3x,2016-12-29 05:43:15-05:00,Leeds1234,,"well I know you make sure its in `a* b* c *` order first, else reject,
then I cross an `a` move right cross a `b` , cross a `c` then move left for `a^n b^n c^n` and repeat, for this one I tried doing the same only for when we have the `c's` crossed there obviously should be 2 times more extra `c's` , so then I tried just basically once `a` and `b` were crossed and the equal `c's` crossed, repeating the process till I got all the `c's` crossed, but I'm not sure if that's the right way, and how I exactly show this in my state diagram..",5kskg0,t1_dbqm5bl,Salusa,,Reply,1,0,1
dbr2f7u,2016-12-29 05:46:36-05:00,Leeds1234,,"So basically once all `a's, b's and c's` are crossed for `^n`, I wanted to go back skip all crossed, for `c^2n`, if I see a `c` move left and repeat, but I don't know how I'm meant to check if it's double, I'm just confused and new to TM's",5kskg0,t1_dbr2d3x,Leeds1234,,Reply,1,0,1
dbs58q9,2016-12-29 22:04:41-05:00,Salusa,,"This is for a course right? I can help you through this but you've got to reach out to your professor or TAs for help. Turing Machines can be confusing and when I TAed a course in them, lots of people came by for guidance. Please get some.

As for this specific problem, it often helps to describe things as specifically as possible. Do you already have a transition table or a state diagram for `a^n b^n c^n`?

Let me reword the solution you proposed for `a^n b^n c^n` with more details and see if that helps you figure this out.

1. Move to the right while I see `a`. If I see anything other than an `a` or a `b` then reject
2.  Move to the right while I see `b`. If I see anything other than an `b` or a `c` then reject
3. Move to the right while I see `c`. If I see anything other than an `c` or a `<BLANK>` then reject
4. Once I see a blank, return to the start
5. Move right till I see an `a`
6. Cross out one `a`
7. Move right until I see a `b`
8. Cross out one `b`
9. Move right until I see a `c`
10. Cross out one `c`
11. If the next symbol is `<BLANK>`, check to see if everything has been crossed out, if so, ACCEPT
12. else, Return to the start
13. Go to step 5

Now, there are still lots of details missing. However, the shape is there in a decent amount of detail. Could it be modified to accept a string with twice as many `c`s?",5kskg0,t1_dbr2f7u,Leeds1234,,Reply,1,0,1
dbsfqdy,2016-12-30 03:49:45-05:00,Leeds1234,,"Thanks for this bit I already have the answer for that question, could you give me any tips with the state diagram I asked for? Ye they're pretty confusing, I understand them it's just I'm not sure what to do with the `a^n b^n c^2n` because when I have all `a,b,c's` crossed equal to just `n` I don't know what to do with the `c^2n` do I literally just cross out 2 `c's` for every time I cross one `a` and `b`? Ye it's for college too!! /r/Salusa    [state diagram for a^n b^n c^n](http://imgur.com/a/zuJPj)

",5kskg0,t1_dbs58q9,Salusa,,Reply,1,0,1
dbtmaxn,2016-12-30 23:23:35-05:00,Salusa,,What happens if you cross out two C's every time?,5kskg0,t1_dbsfqdy,Leeds1234,,Reply,1,0,1
dbtw737,2016-12-31 05:42:47-05:00,Leeds1234,,"[state diagram for a^n b^2n c^n](http://imgur.com/a/DAN1b)    
I just started a different problem, it's basically the same only the b has the ^2n, not the c, is this right?",5kskg0,t1_dbtmaxn,Salusa,,Reply,1,0,1
dbubj6b,2016-12-31 14:17:01-05:00,Salusa,,"I'm not going to fully check it. It is at least very close (if not fully correct). There's at least one incorrect transition, but it may not actually matter.",5kskg0,t1_dbtw737,Leeds1234,,Reply,1,0,1
dbsfycr,2016-12-30 04:00:53-05:00,Leeds1234,,"Or when I have all `as,bs,cs` crossed first time around do I just look for a `c`, cross a `c` move left, pass all x's where `a and b and some c` values will be crossed and keep repeating till we cross double the `n of c's` /r/Salusa",5kskg0,t1_dbs58q9,Salusa,,Reply,1,0,1
dbupi6x,2016-12-31 20:16:41-05:00,Leeds1234,,"Is that the blank going right? It should be blank, blank->R",5kskg0,t3_5kskg0,Leeds1234,,Comment,1,0,1
5kq4hv,2016-12-28 08:01:33-05:00,greenz1,How to use Intel Media SDK?,"Can anyone tell me where to start learning Intel Media SDK? Also, how to develop screen capture application using the same?",,,,,Submission,2,0,2
dbwgra3,2017-01-02 07:36:44-05:00,greenz1,,It is funny that Reddit community has people interested in this question but there are no comments on the same question posted on the forum related to this SDK.,5kq4hv,t3_5kq4hv,greenz1,,Comment,1,0,1
dbxs1c8,2017-01-03 03:38:55-05:00,greenz1,,"https://github.com/chintan-mishra/Tutorial/blob/master/SimpleIntelMediaSDKFile.cpp

This is the simplest program written using Intel Media SDK I could find on the internet.",5kq4hv,t3_5kq4hv,greenz1,,Comment,1,0,1
5kooe8,2016-12-28 00:54:07-05:00,xMcNerdx,I need help choosing a major track.,"I'm not sure if this is how most universities structure their CS degree, but at my university, you choose an upper division track to specialize in. I have not been able to find much info through my universities website, but I figured since they were pretty general fields other people would be able to give me information on them. The choices are as follows:

    Architecture & Hardware Systems
    Artificial Intelligence / Robotics
    Big Data
    Bioinformatics & Computational Biology
    Computational Science
    Databases
    Geographical Information Systems
    Graphics & Visualization
    Human Computer Interaction
    Networks
    Security
    Software & Data Systems Development
    Software Engineering & Programming Languages
    Systems
    Theory

I'm a sophomore currently and I just finished my first semester of CS as I switched from aerospace. I took discrete mathematics and an into programming course in Python and enjoyed both greatly. I had a ton of fun doing homework and labs in Python. I'm not really sure what I'd like to do for a career yet because I don't know enough about the field as a whole yet. If anyone could give me some insight on these sub tracks and maybe provide info on which ones lead to certain careers and so forth. Thanks!",,,,,Submission,4,0,4
dbplsde,2016-12-28 04:06:45-05:00,epictylerone808,,"I would say to choose which track suits your interests. For my school, there is only majoring in CS and a database management certificate. My advice is to go with whatever interested you. ",5kooe8,t3_5kooe8,xMcNerdx,,Comment,1,0,1
dbpsc8f,2016-12-28 09:22:57-05:00,xMcNerdx,,"Would you say in general it doesn't matter which track I choose? For example, if I'm interested in doing the AI track, I should still be able to find work as a programmer as long as can meet requirements?",5kooe8,t1_dbplsde,epictylerone808,,Reply,1,0,1
dbq1tr0,2016-12-28 13:05:09-05:00,epictylerone808,,"Yeah, the specialization shouldn't matter in the future, just have the work experience, projects, and you should be fine. ",5kooe8,t1_dbpsc8f,xMcNerdx,,Reply,1,0,1
dbpms88,2016-12-28 05:02:31-05:00,Vesper32,,"You're better off looking at university's website for information about the specialized tracks, or talking to a counselor or your fellow peers for a better description of these tracks.",5kooe8,t3_5kooe8,xMcNerdx,,Comment,1,0,1
dbpsa54,2016-12-28 09:21:06-05:00,xMcNerdx,,"That's the thing, there's nothing more than a couple sentences for each (some don't have anything). I figured I could ask around online before talking to someone in person. I also emailed my counselor and she just told me to email the department directly. Thanks!",5kooe8,t1_dbpms88,Vesper32,,Reply,1,0,1
dbprz9n,2016-12-28 09:11:38-05:00,Boehemyth,,"I actually was Computer Engineering in college (kind of a hybrid of electrical engineering / CS), but we had a similar system (although it wasn't very rigid).  I honestly didn't have any idea what I wanted and kind of picked something that seemed the most interesting from the single paragraph descriptions in the pamphlet. It wasn't until my senior year when I realized I probably should have just picked full-on CS rather than deal with all of the Electrical Engineering aspects that didn't interest me as much.  Upon graduation, I was especially interested in low-level systems software, operating systems, and kernel development, but it took me almost all of college to figure that out while I spent most of my time on digital hardware design courses that I now found tedious and frustrating.

It made it kind of hard to find a job I wanted after that, and I kind of bounced around a lot.  I often joke that I learned more about myself in the years immediately after college than during college.  From internships on, I've worked with FPGAs, embedded software, custom kernels, and even a brief stint as entry IT.  I'm now a respected developer at a small shop in the Midwest, which ironically has nothing to do with anything but my intro to programming class freshman year (and data-structures, but that was more like intro-2).  But, I'm very happy, despite this being one of the least interesting jobs I've had technically.

My point is kind of two fold: unless you know exactly what you want and that doesn't change for you in the next 3 years or so, and unless you're planning on going for an advanced degree immediately after undergrad, this is probably not something that will ultimately affect you all that much 10 years from now, so don't stress out over it.  Secondly, I've really learned in the last few years that, at least for me, the ""what I do"" matters a lot less to me than where and whom I'm doing whatever that is with.  Plus, the aspects of myself that all of my employers have seemed to value the most have had very little to do with my formal education.  I think if I had known that earlier, I would have approached college differently and explored a lot more rather than try to pick a focus early on.  Maybe then I would have figured out what I liked earlier.",5kooe8,t3_5kooe8,xMcNerdx,,Comment,1,0,1
dbpshn7,2016-12-28 09:27:29-05:00,xMcNerdx,,"Thanks for the response! So what I'm getting is that it largely doesn't matter which one I choose as long as I can found myself out and make myself hireable for whatever careers are available near me. I'm interested in taking the AI/Robotics track because I think that would be very neat, but I also considered just doing the software track because that seems like the most general track that would make me hireable. I'm in Minnesota and I'm not sure how many companies around here do work with AI/Robotics. ",5kooe8,t1_dbprz9n,Boehemyth,,Reply,1,0,1
dbptt24,2016-12-28 10:04:24-05:00,Cabrill,,"In my opinion AI/Robotics is literally the best choice [for the coming years](https://backchannel.com/the-ai-takeover-is-coming-lets-embrace-it-d764d61f83a#.xj6lypppr).  More and more jobs will be automated as technology improves, and you'll be at the forefront of that industry.  ",5kooe8,t1_dbpshn7,xMcNerdx,,Reply,1,0,1
dbqg4pn,2016-12-28 18:13:08-05:00,Boehemyth,,"AI/Machine learning is a huge field across a lot of industries.  The jobs will exist when you're done, just maybe not exactly what you want right away (quite possibly because you won't actually know what you really want).  I'd worry more about finding things that interest you and pursuing that.",5kooe8,t1_dbpshn7,xMcNerdx,,Reply,1,0,1
5kgdmv,2016-12-26 18:12:45-05:00,Particlepancake,"Approaching Graduation...questions on ""whats next""","So I'm going to be getting my undergrad degree in computer science, and I'm seeking some guidance. I am going to be taking the GRE soon and apply to grad school is the plan. However I'm concerned that this may be unnecessary for what I want to do. To be to the point, I want to work for some renewable energy type of company or a ""green"" company. So....how do i accomplish this? What companies are looking for people like me? Is there a platform where companies like this are actively looking developers and such?  Is grad school a better path to take? Is grad school necessary for me to do ""this""? any help, suggestion is appreciated and useful; anything from schools, companies, locations, websites, languages to learn, really anything i should learn :P.  

some background information:
going to be graduating from a smaller private college
under a 3.0 gpa (2.9) 
participated in athletics (if that matters) 
ive had one internship 
some languages - c++, java, php(html, css, javascript), MySQL plus encounters with others. 

CHEER! and thanks!",,,,,Submission,8,0,8
dbnrdsd,2016-12-26 18:53:18-05:00,TheSageMage,,"I'm not sure about Grad School(never researched it as an option myself), but I think there's a larger question of what do you want to do in your day-to-day?

You mention working for an energy company, but what does this look like to you? Would this be helping researchers create systems for research? Is this just working for a large energy company as a software engineer? 

",5kgdmv,t3_5kgdmv,Particlepancake,,Comment,4,0,4
dbnwn48,2016-12-26 21:12:50-05:00,alfred500,,r/cscareerquestions will probably tell you that a masters is unnecessary..,5kgdmv,t3_5kgdmv,Particlepancake,,Comment,3,0,3
5kcrgi,2016-12-26 02:56:04-05:00,heymynameisben,"What, as a reasonably competent computer scientist, should I know about security?","I have a degree in CS but I chose to take modules that didn't really touch on security (they seemed focused on the implementation of cryptographic techniques anyway). Reading about the hacks that seem to be getting more common I'd like to at least have a basic working knowledge. 

In a practical sense, what should I know to avoid any costly mistakes in the future? For example don't store passwords in plain text/ guard for SQL injection. 

Thanks a lot",,,,,Submission,13,0,13
dbni3gc,2016-12-26 14:41:18-05:00,Salusa,,"* Don't trust any input
* Properly escape all output
* Don't build your own password storage or crypto
* Build positive and negative tests for all your code
* Know some security people to ask questions of 

That's pretty much it. I'm a security guy and if all the devs I worked with knew (and did) just that, my life would be 100 times easier and our software better.",5kcrgi,t3_5kcrgi,heymynameisben,,Comment,9,0,9
dbn8v5e,2016-12-26 10:30:08-05:00,BonzoESC,,"* Don't implement your own crypto, use TLS, NaCl, or GPG
* Don't guard for SQL injection, use tools that make allowing SQL injection more difficult than not
* Password storage is bcrypt, scrypt, or PBKDF2 with a SHA-2 hash and lots of iterations.
* If in doubt about a specific situation, don't be afraid to ask around outside of work. ",5kcrgi,t3_5kcrgi,heymynameisben,,Comment,8,0,8
dbn1c1k,2016-12-26 03:41:07-05:00,_open,,"Computer science is a **very** broad field. So it's pretty important on what you're planning to effectively do before you can know if security will be an important issue to you.

I used to work as a software developer and because we were working with a very old software environment, I had to deal with lots of security questions. 

As frontend dev now, I have more to deal with performance improvement instead of security issues, which is the problem of the backend dev. And even my backend dev doesnt have to deal with a lot of security issues, cause most is taken care of by frameworks we're using already.

So yeah, there are lots of different security issues in the world of computer science. So it hardly depends on what exactly you're doing and with what tools / environment you're working with.",5kcrgi,t3_5kcrgi,heymynameisben,,Comment,8,0,8
dbnav3o,2016-12-26 11:31:02-05:00,Filmore,,Don't store production passwords in plain text in git repositories?,5kcrgi,t3_5kcrgi,heymynameisben,,Comment,2,0,2
5k7fnx,2016-12-25 02:27:26-05:00,josue804,Resources for learning the inner workings of the web?,"Hi all, I've been working as a full-time backend web developer for the last year. 

I'm good at development and yet I still don't fully understand how the web works as a whole and can even less explain it in its actual terms.

I resort to saying things like ""Oh yeah and a request response has a message thingy that has the information you asked for in your message."" Ask me what all the attributes of a message are and you've lost me. ""Uhh there's a status code and 200 means is all good and 404 means you sent a bad message?"" 

TL;DR: Where can I find a good resource on how http works and even how the web works in it's proper technical terms, so that I can talk a out these things to other developers?",,,,,Submission,14,0,14
dblx4ks,2016-12-25 03:13:40-05:00,whowereyouexpecting,,"Have you tried [YouTube](https://www.youtube.com/watch?v=eesqK59rhGA)?

See also [HTTP response codes](https://en.wikipedia.org/wiki/List_of_HTTP_status_codes) and [protocol description](https://en.wikipedia.org/wiki/Hypertext_Transfer_Protocol) on wikipedia.",5k7fnx,t3_5k7fnx,josue804,,Comment,2,0,2
dblzy3c,2016-12-25 06:14:00-05:00,Yorshelf,,"http://marksheet.io/internet.html  
or more advanced  
https://launchschool.com/books/http  

my guide to discover how the web works, part 2 with the web overview articles could help you
https://www.reddit.com/r/learnprogramming/comments/4navmj/if_you_dont_know_how_browserserver_communication/",5k7fnx,t3_5k7fnx,josue804,,Comment,2,0,2
dbmjfwa,2016-12-25 17:47:45-05:00,josue804,,"This is perfect, thanks! ",5k7fnx,t1_dblzy3c,Yorshelf,,Reply,1,0,1
5k2fr2,2016-12-24 05:04:28-05:00,forgive2forget,Degree in Comp Sci at lesser known University,"My situation if that I have nearly no chance to get into the computer science program at the University Of Washington. Will I realistically get a job in programming if I transfer to a lesser university? I've got good grades, but I'm no 4.0 student. ",,,,,Submission,13,0,13
dbl26ur,2016-12-24 12:01:27-05:00,MCPtz,,"Cheaper path:

Community college and transfer to state university.

Get internships and/or research work on campus. Real work experience is important.",5k2fr2,t3_5k2fr2,forgive2forget,,Comment,6,0,6
dblhzy7,2016-12-24 18:38:39-05:00,systemnate,,"This is great advice, OP. On your resume it will just show the school you actually graduated from. If you want to go to University of Washington, just go to a CC and then transfer there after a year or two. However, you will be able to find a job regardless.",5k2fr2,t1_dbl26ur,MCPtz,,Reply,2,0,2
dbksxvs,2016-12-24 05:35:16-05:00,PM_ME_YOUR_SHELLCODE,,"As long as you keep your skills up you'll be fine. Outside of the Ivy League schools school name doesn't usually matter much, what matters is just that you do have a degree.",5k2fr2,t3_5k2fr2,forgive2forget,,Comment,8,0,8
dbkw34a,2016-12-24 08:36:21-05:00,farsightxr20,,"University of Washington is very well-regarded for CS. OP will likely have more opportunities if he can get into UW, but there are still plenty of well known companies that hire from all schools.",5k2fr2,t1_dbksxvs,PM_ME_YOUR_SHELLCODE,,Reply,5,0,5
dbl4ced,2016-12-24 12:59:22-05:00,UncleMeat,,Not just very well regarded. They have skyrocketed to prominence. I'd place them in top 10 in the US.,5k2fr2,t1_dbkw34a,farsightxr20,,Reply,3,0,3
dbkxflm,2016-12-24 09:32:06-05:00,panderingPenguin,,"UW is a top 10 CS program. That's essentially like going to an ivy in a lot of ways. Not saying OP is screwed if they go elsewhere, but UW has the kind of name recognition that will make things easier.",5k2fr2,t1_dbksxvs,PM_ME_YOUR_SHELLCODE,,Reply,4,0,4
dbl7jle,2016-12-24 14:15:33-05:00,PM_ME_YOUR_SHELLCODE,,"Ah, I didn't know that. I'm not from the US originally so I'm familiar with Harvard, MIT, Princeton and stuff but UW didn't stand out to me.   ",5k2fr2,t1_dbkxflm,panderingPenguin,,Reply,2,0,2
dbktig7,2016-12-24 06:11:19-05:00,brennanfee,,"Came here to say basically this... unless you went to MIT or CalTech it doesn't matter.

> what matters is just that you do have a degree.

Not as much as you might think.  It is just sort of a check-mark for most interviews\applications.  Skill is always the most important thing.",5k2fr2,t1_dbksxvs,PM_ME_YOUR_SHELLCODE,,Reply,3,0,3
dbkw4iq,2016-12-24 08:38:08-05:00,CaptainBland,,"Sure, just take whatever opportunities you can to get experience while you're taking your course - this might be things like internships, undergrad research assistant, teaching assistant, taking part in events or even organising them yourself. ",5k2fr2,t3_5k2fr2,forgive2forget,,Comment,1,0,1
dblhw47,2016-12-24 18:35:45-05:00,systemnate,,"You will absolutely be able to find a job. Don't even worry about it. Study hard, do some projects on your own, and try to land an internship or two.",5k2fr2,t3_5k2fr2,forgive2forget,,Comment,1,0,1
5jz071,2016-12-23 14:50:36-05:00,BrokeDiamond,"How are extremely important passwords, such as national security logins, generated/determined?",,,,,,Submission,13,0,13
dbk2u3c,2016-12-23 15:27:45-05:00,matt_bishop,,"Nice try, Putin! :)",5jz071,t3_5jz071,BrokeDiamond,,Comment,16,0,16
dbkfi32,2016-12-23 21:01:08-05:00,Microwave_on_HIGH,,"In Soviet Russia, passwords generate you.",5jz071,t1_dbk2u3c,matt_bishop,,Reply,3,0,3
dbk5pd7,2016-12-23 16:39:18-05:00,0x68656c6c6f,,NIST releases guidelines for this kind of thing. For example see https://pages.nist.gov/800-63-3/sp800-63-3.html,5jz071,t3_5jz071,BrokeDiamond,,Comment,5,0,5
dbk2c3e,2016-12-23 15:15:32-05:00,brennanfee,,"You do know that the password for our nukes for the first 20 years was ""00000000"".  I believe you may be giving those in the government a bit too much credit.

Perhaps the question should be:  How should important passwords, such as national security logins, be generated/determined?",5jz071,t3_5jz071,BrokeDiamond,,Comment,10,0,10
dbktupm,2016-12-24 06:33:01-05:00,killerstorm,,"I'm not sure if this is what you're asking about, but it's related:https://en.wikipedia.org/wiki/Key_ceremony

One example: https://petertodd.org/2016/cypherpunk-desert-bus-zcash-trusted-setup-ceremony

Pretty interesting read.",5jz071,t3_5jz071,BrokeDiamond,,Comment,1,0,1
dbkepwr,2016-12-23 20:39:03-05:00,cravenspoon,,"I had a good write up, then realized I was *totally* violating an NDA at a minimum so I'm going to keep it simple:  

It depends. It depends on what platform they're using, if they're even using the ""computer"" you think of, who is setting it up, access rules, on and on and on.  

Obviously, having a set way of generating passwords probably isn't the best idea in case that ever became compromised. Instead there will be password rules, access rules and methods, psychical security considerations, etc. ",5jz071,t3_5jz071,BrokeDiamond,,Comment,1,0,1
5jtk7a,2016-12-22 17:38:39-05:00,Jean-Alphonse,Minimum number of moves to solve a puzzle game? (x-post /r/algorithms),"Hi everyone!

I made a simple puzzle game and i'm struggling to find an algorithm to solve it... here's the game:  

On a N*N grid, each cell is initialized with a random value. Adjacent cells of same value form a group, and clicking on a group increments the value of each of its cells. Similarly, right-clicking decrements a group. Increment/decrementing costs 1 move.  
The goal is to have only one group left on the board (ie. all the cells have the same value), with the minimum number of moves.

[Here](http://entibo.fr/lvlr) is the prototype.  
So here's the question: how to determine the minimum number of moves required to solve a level ?

I feel like there exists an algorithm to properly find it but i can't figure it out.  
I tried a shortest-path algorithm on the graph of game states but of course starting at N = 4 or 5 it grinds to a halt. Am i doing it wrong ?  

Any help would be much appreciated, even pointers to maybe similar games and how they are solved!

thanks!",,,,,Submission,17,0,17
dbizedi,2016-12-22 19:45:05-05:00,okiyama,,"I'm going to toy for a little while but I'm having good luck not with choosing the minimum tile, raising it to the largest value on the board then reducing that same tile back down until I get a win.

I suspect that only works because of the small grid size though. It's definitely not optimal, but it feels somewhat close.

EDIT: Nah, not even close. It fails bad when there's one high square surrounded in the corner by lower squares. Interesting problem...",5jtk7a,t3_5jtk7a,Jean-Alphonse,,Comment,3,0,3
dbj75ks,2016-12-22 22:53:44-05:00,ImTheTechn0mancer,,"I'm having fun with your game, and it seems like starting with the smallest tile, raising it to the biggest tilt, and then decrementing it yields 8 moves almost every time. I have yet to fail getting it in 8.

edit:
I got one that took me more: [oh well](http://i.imgur.com/q9c9e8h.png)
",5jtk7a,t3_5jtk7a,Jean-Alphonse,,Comment,2,0,2
dbiyfdp,2016-12-22 19:20:50-05:00,justlikestoargue,,"My thinking is the level you want to reach is whatever the highest number on the board is from there, go to the smallest number and increment it until they're all the same no.

May not be /perfect/ but it should be pretty one for a basic algorithm.",5jtk7a,t3_5jtk7a,Jean-Alphonse,,Comment,1,0,1
dbiyx4r,2016-12-22 19:33:14-05:00,okiyama,,"This would work fine if you could only increment, however that would be very inefficient for a board similar to this:

    1 1 1 1 1
    1 2 2 2 1
    1 2 3 2 1
    1 2 2 2 1
    1 1 1 1 1

Two moves if you decrement the 3, then the 2s. Many moves if you increment the ones.",5jtk7a,t1_dbiyfdp,justlikestoargue,,Reply,2,0,2
dbj5q18,2016-12-22 22:18:03-05:00,justlikestoargue,,"Ah, I didn't realize you could decrement. I played on mobile so I didn't have that option",5jtk7a,t1_dbiyx4r,okiyama,,Reply,2,0,2
dbjx6ah,2016-12-23 13:19:25-05:00,xocaxola,,"I may be interpreting this wrong, but wouldn't it be the same number of moves either way? Incrementing a 1 would raise all the 1s to 2s, then incrementing a 2 would raise all the 2s to 3s.

edit: Of course there are boards where decrementing is more efficient, just referring to the example",5jtk7a,t1_dbiyx4r,okiyama,,Reply,2,0,2
dbjxslz,2016-12-23 13:32:32-05:00,okiyama,,Oh yeah you're right. It's surprisingly difficult to think of edge cases for this game.,5jtk7a,t1_dbjx6ah,xocaxola,,Reply,1,0,1
dbjjvps,2016-12-23 07:26:40-05:00,singham,,"Your game is quite similar to color filling games. 
See this : http://www.flashbynight.com/drench/

You will need some clever data structure to play the game. Brute force won't cut it. Look at [Kruskal's algorithm](https://en.wikipedia.org/wiki/Kruskal%27s_algorithm) to get an idea. ",5jtk7a,t3_5jtk7a,Jean-Alphonse,,Comment,1,0,1
5jqoio,2016-12-22 08:57:22-05:00,seabee494,Book publisher suggestion,"So I am a subscriber to Safari Books Online, and one thing I've noticed is that the Packt publisher tends to produce content that is with a litany of errors. To me it seems like anything published by O'Reilly media is better in quality, as well as Apress. I have also had a good experience with No Starch Press. Are there any other high quality book publishers / learning resources that I'm not aware of? ",,,,,Submission,4,0,4
5jom1b,2016-12-21 23:25:32-05:00,misdry,Double majoring in Computer Science and Informatics,"Hey guys. I am a student at the University of Michigan and am trying to decide on my major. I know I want to do computer science, but for some reason, I feel like I also need to do something else on top of that. Informatics seems very interesting and it seems like there is a decent amount of overlap with cs that might make it more possible to do. I just wanted to ask, do you think doing both majors is a waste of time because of the overlap?",,,,,Submission,8,0,8
dbhuswn,2016-12-22 00:49:50-05:00,lordvadr,,"""Informatics"" wasn't a serious degree when I was in school. Double majored in electrical engineering... But it's worth mentioning that at my alma mater, CS was in the college of engineering. Didn't finish the double, but got close enough that the knowledge has been outrageously useful, both on and off the job.

Wish I had a better answer for you.",5jom1b,t3_5jom1b,misdry,,Comment,3,0,3
dbhyfas,2016-12-22 03:10:02-05:00,litepotion,,I originally double majored in electrical engineering and computer science. There was a lot of overlap with programming. Despite EE being lower level and CS being more upper level programming there was a lot of overlap in courses and theory. I ended up dropping the CS at the equivalent of junior status due to so much overlap and money. I graduated in May 2016 and I can land jobs in CS with my EE degree as employers would still take you for their interviews. My upper electives was a mixture of RF hardware design and CS programming courses.,5jom1b,t3_5jom1b,misdry,,Comment,3,0,3
dbib8x8,2016-12-22 11:02:17-05:00,wasimwesley,,how tedious is learning professional practice for engineers and physics on top of computer engineering?,5jom1b,t1_dbhyfas,litepotion,,Reply,1,0,1
dbj7sxn,2016-12-22 23:10:25-05:00,litepotion,,"I'd say extremely tedious. As you lean towards more hardware the CS courses no longer apply. Such as power and circuit design. Even if it's digital circuit design it's very different, this is why you have compes do fpga and not CS majors so to speak.",5jom1b,t1_dbib8x8,wasimwesley,,Reply,1,0,1
dbihkhb,2016-12-22 13:12:01-05:00,CelticJoe,,"Heya - extensive work history in Healthcare IT here. Informatics, while it can be intriguing to some, is kind of the ""library science"" of computer degrees. There are a handful of good paying positions or research opportunities if you're lucky, good at making connections and (most likely) pursue a higher level degree, but honestly it's a pretty low reward field. In healthcare for instance it would only really qualify you for a HIM position which, while expanding, is still very low paying for computer based degree (35k avg I think). No one I know who got CS or IT degree in this field bothered to go the informatics route as well and had no problem with getting hired.

That being said, if you have the money time and the interest, it can't hurt, and there probably are some employers who will simply be impressed by the double major itself.",5jom1b,t3_5jom1b,misdry,,Comment,1,0,1
dbij6pj,2016-12-22 13:44:08-05:00,misdry,,"Thanks for the response!
Yeah that was my worry.. do you have any other suggestions regarding a second field of study that would go really well with computer science? Ideally i'd like to do something similar as opposed to something entirely different w entirely different classes but i can't decide what... i know data science is another option but not so sure what exactly that is",5jom1b,t1_dbihkhb,CelticJoe,,Reply,1,0,1
dbinu2l,2016-12-22 15:19:07-05:00,CelticJoe,,"This really is a very broad question and one I can't really answer well without knowing what your interests and goals are. A few pieces of general advice I can offer:

1. If you're only getting two degrees so you can say you have two degrees, I strongly recommend against it. It's better to excel in one area then do okay in two, all the good money to be made is in specialization. Plus when you get to your JR or SR year and the courses start get particularly in depth and intense you may regret the time sink required to double. But again that's all you, you know yourself better than anyone else and especially me.

2. If you do go the double route, you may actually want to consider doing a different field rather than ""stacking"" the degrees with something similar. It's a much bigger time investment right now but may save time later if you get into the Tech industry and realize it's not for you. Business, Liberal Arts type degrees, or a pure science usually are good options for this.

3. Speaking of business, management type degrees are pretty much a must if you want to advance really far. Like I said before specialization is the best bet for highest paying pure tech jobs but also will take extensive experience in addition to education and certifications to make the big bucks and there will likely be a lot of competition from those who have been in the field 20+ years even if your pure technical knowledge and expertise is better. Real world job market is less of a meritocracy than we're raised to believe. The advantage of business or management degrees is that it will help prepare you and (more importantly) be a good resume boost for someone to take a chance on offering you a management position earlier in your career. It's a radically different skillset than tech itself and my personal experience has been that those with straightup technical training and skills make terrible managers. 

e: Pure pure pure pure pure pure pure",5jom1b,t1_dbij6pj,misdry,,Reply,2,0,2
5jolv5,2016-12-21 23:24:25-05:00,asdff01,"How realistic is relocating after getting a degree in CompSci to Manhattan, New York, and succeeding?",,,,,,Submission,11,0,11
dbhspc4,2016-12-21 23:46:13-05:00,lugoues,,Find a job first and have them pay to relocate you. It is much safer and pretty easy to find a company willing to pay relocation costs.,5jolv5,t3_5jolv5,asdff01,,Comment,15,0,15
dbi4ws3,2016-12-22 08:18:08-05:00,umib0zu,,"I was going to say this, but you also need to be good enough to negotiate this in your compensation package. It's totally easy to find a job somewhere, but many companies try to sell you on the fact that you ""get to live in an awesome city"" and ""free beer"", so they exclude some basic requirements and basically undercut you in benefits while giving you a great salary. 

Just be weary of this and don't limit your options to NYC. The best negotiation strategy is being willing to walk away if you don't get what you want, and you shouldn't need to settle for any company.",5jolv5,t1_dbhspc4,lugoues,,Reply,4,0,4
dbhth5y,2016-12-22 00:08:57-05:00,jmonty42,,"/r/cscareerquestions is a better place to ask this. Generally, yes, it's possible, I don't know how common it is for that particular city, but lots of companies hire new grads and relocate them.",5jolv5,t3_5jolv5,asdff01,,Comment,6,0,6
dbi68og,2016-12-22 09:00:33-05:00,Pardomatas,,"I went to school in Manhattan, it's very very expensive, but otherwise amazing.",5jolv5,t3_5jolv5,asdff01,,Comment,0,0,0
5jj1m1,2016-12-21 04:28:46-05:00,Canary_Touch,Booking System is NP Complete?,"I want to show that the following problem is NP-Complete and need a bit of guidance as I think I might be going at it wrong. 

The problem:

We're looking at a meeting booking system. The input is a list n of possible times as well as m lists (where m <= n), one list per person containing their choice of possible meeting times. For each possible time, a priority number is also given. For each reservation time in the list of n, a cost is also given. (Cost of booking the room). The algorithm should assign times so that the combined priority for those who have booked should be as small as possible while the total cost of booking should not be higher than M.

The Decision problem would be something like ""Is it possible to to verify if that the total cost of meeting booking does not exceed K and that the total priority is lower than V?"" 

NP

So first to show that it's in NP we should show that given a correct solution it can be verified that it is indeed correct. I guess it should verify that that the cost is below the threshold of K and that the priority of the correct solution is indeed the minimum - both of which can be done in Polynomial time I assume. We traverse through the lists of people, assert that each one has a time granted to them, add up the cost in a variable and at the end of this list assert that the cost is below K. The priority can be dealt with in similar fashion I suppose?

NP Hard

Then to show it's NP Hard I can use the Knapsack Problem since they're rather similar. With input S, size of bag, a list of items with weight w and value v as well as the goal W which is the goal-value. I guess it's clear that S can correlate to cost and that W correlate to the priority? So we want S, the size, to be below S i.e we have the similar condition for the problem above where the cost has to be below K. Then W, the total value should generally exceed W, but in our case we want it to be as low as possible which seems doable.

The reduction to show it's NP Hard is perhaps not thought out all the way. Some pointers would be very helpful! ",,,,,Submission,2,0,2
dbgwqob,2016-12-21 11:51:18-05:00,victorsenam,,"When you were proving NP, you said  
>  that the priority of the correct solution is indeed the minimum 

you should prove for the decision problem, so you don't need to show that it's minimum, just that it's less than V, which is, indeed easy.  

I'll say something about the reduction to NP-Hard later, but I've got to go now, sorry (hehe)",5jj1m1,t3_5jj1m1,Canary_Touch,,Comment,1,0,1
dbh10ez,2016-12-21 13:17:20-05:00,victorsenam,,"**EDIT (begin)**  
I've later realised that you asked for a tip, not the answer. Below is a proof of the NP-Completeness of the problem you've stated. Try not going to that directly, might spoil the problem.    
So I'll give you some tips. When you are trying to prove NP-Completeness, try formalizing the definitions of your problems (I've done that below, If you read everything to that part I think the problem will not be spoiled yet). You are trying to reduce problem A to problem B (which means, solve problem B and you can solve problem A). In your case B is Booking and A is Knapsack (pay attention to this fact, don't get it backwards).  
To reduce, take any instance of B and transform it (polynomially) into an instance of A, then solve that instance of A and transform that solution into a solution for B. Now, prove that the answer for B is true if and only if some solution will be found by this method.  
I think that's the thinking path. And, yeah, you've correlated stuff the right way on your post. On Knapsack you have a restriction of values >= K, you want to make something like: (some function of values) <= (some function of K), because you want to transform one problem into another? What is a good way to do that? That is, how do you transform a >= into a <=?  
I hope I could help :)  
**EDIT (end)**  

I'm quite confused about the statement, actually.  I'll assume that you need both costs and priorities to be positive, otherwise it's actually much easier to prove NP-hardness, and I'll formally define the problem in the future, if it's wrong, please say.  
Ok, so, to prove it's NP-Hard, you have to show that solving this solves another NP-Hard problem. The Knapsack seems like a good one, but I think you're trying to go the other way around (solving this by solving the Knapsack). So, take an instance of the Knapsack. An instance has a value S (size of bag), a value W (goal) and n elements, the i-th of which has a weight w\_i and a value v\_i. We want to know if we can choose a subset of the elements such that the sum of weights is at most S and the sum of values at least W.   
An instance of the Booking problem can be described as a value K (cost threshold), a value V (priority threshold), n people (i'm reusing a variable, they'll be equal) and m times, the i-th of which has a cost c\_i. For each pair of time i and person j, there's a priority p\_{i,j}. We want to assign each person to a time such that the sum of priorities of each assigned pair is less than V and the sum of costs of times chosen is less than W.  
Let T be the maximum value on the Knapsack. For each element in the Knapsack problem, create a person (therefore, n elements generate n people). Now, for each person, create two times, with indices 2i and 2i+1, the priority for those to every other person is infinite (if you're not comfortable with using infinity, complain). Set c\_{2i} = w\_i and c\_{2i+1} = 0 (the assigning the first one is equivalent to taking the element on the knapsack, assigning the second is equivalent to not taking it), now, set p\_{2i,i} = T - v\_i and p\_{2i+1,i} = T. Lastly set V = nT - K (if this is negative, the answer to the knapsack is false).   
Now we have to prove that this solves the knapsack problem. Each person i will be assigned either to 2i or 2i+1, because the other times have infinite priority. If a person is assigned to 2i, take the corresponding item on the knapsack, otherwise, don't. The sum of times chosen in the Booking is exactly equal to the sum of weights chosen in the Knapsack. Now, we have to prove that, if P is the sum of priorities of the Booking and R is the sum of values on the Knapsack, P <= V if and only if R >= K.  
We know that P = nT - R (prove it), also   
R >= K if  <=>  -R <= -K   <=> P = nT - R <= nT - K = V   <=>  P <= V.  
This proves that a valid solution on Booking generates a valid solution on Knapsack and, also, any valid solution on Knapsack is a valid solution on this instance of Booking.  

Hope it was not too confusing.

EDITS: formatting, Markdown is not very good for this :/",5jj1m1,t1_dbgwqob,victorsenam,,Reply,2,0,2
dbi97os,2016-12-22 10:17:19-05:00,Canary_Touch,,Thank you for this reply! I think my only confusion is as to why two times per person are needed. I.e the 2i and 2i + 1. ,5jj1m1,t1_dbh10ez,victorsenam,,Reply,2,0,2
dbia9jz,2016-12-22 10:40:55-05:00,victorsenam,,"We are trying to model the Knapsack as the Booking. For each element on the Knapsack, we have two possible choices: ""take"" and ""don't take"". My goal was to translate both those choices into two different choices on the Booking problem. That was one way of doing it. Don't know if there's a better way, one that uses less times per element, but I'd guess there's not (maybe I can get a constant number of those out, but that's irrelevant).  
Does that explain?",5jj1m1,t1_dbi97os,Canary_Touch,,Reply,1,0,1
dbh00a4,2016-12-21 12:57:21-05:00,roflberry_pwncakes,,"I am not an expert in this area but there might be a different way of looking at this. This seems to be similar to the stable matching problem. On one side we have rooms that prefer the lowest cost and people that prefer the highest priority. I haven't worked through it fully but I think you can derive the optimal cost by having the rooms initiate and the people review. This potentially runs in O(n^2).

Probably way off base but there you go. ",5jj1m1,t3_5jj1m1,Canary_Touch,,Comment,1,0,1
5jj0u8,2016-12-21 04:21:45-05:00,mynameisforgotten,Does a greedy strategy exist for this problem,"I was wondering whether i can solve the following problem by using a greedy strategy: Let's say i have a set of containers with 2 dimensions and a set of items with 2 dimensions. Can a solution be found where each item is assigned to a container so that it fits inside. I can have containers left over, however all items must be assigned. Assuming a solution exist, is there a greedy choice I can make for assigning an item to a container?",,,,,Submission,12,0,12
dbgkk63,2016-12-21 05:55:54-05:00,Madsy9,,"This sounds like a packing problem. But since you're not trying to minimize the number of containers used, could one simplify the problem specification into using one larger container than many small ones without losing generality? Also, are holes allowed or do items need to be tightly packed? And is anything more known about the shapes to be packed? Like are they always squares/rectangles, spheres, or completely arbitrary?

For an introduction to these kinds of problems, see here https://en.wikipedia.org/wiki/Packing_problems and here https://en.wikipedia.org/wiki/Bin_packing_problem",5jj0u8,t3_5jj0u8,mynameisforgotten,,Comment,5,0,5
dbgwdbg,2016-12-21 11:44:00-05:00,victorsenam,,"That's the decision version of the bin-packing problem.  
Imagine that you know how to solve this problem in polynomial time. And you are given an instance of bin packing, which is something like ""Given these 2-dimensional items and this 2-dimensional boxes, how many of those boxes do you need to fit these items"".  
You can try solving this problem for 1 box, 2 boxes, 3 boxes and so on, until you can solve it. If n is the number of items and you can't solve in n boxes, you'll never solve, therefore, you've got a polynomial algorithm for bin-packing.  
This means that this problem is NP-Complete. And you can't solve it with a polynomial algorithm.

EDIT: typos",5jj0u8,t1_dbgkk63,Madsy9,,Reply,5,0,5
dbgmhus,2016-12-21 07:28:36-05:00,mynameisforgotten,,"The shapes of containers and items are both rectangular of arbitrary size. I'm not sure if simplifying to one large container is easier, since ""glueing"" the containers together can lead to non rectangular shapes.",5jj0u8,t1_dbgkk63,Madsy9,,Reply,3,0,3
dbgosw1,2016-12-21 08:49:39-05:00,MudkipGuy,,"It feels like we will have to deal with non-rectangular shapes either way, since placing an item into a container will usually leave a non-rectangular region of unused space.",5jj0u8,t1_dbgmhus,mynameisforgotten,,Reply,5,0,5
5ji9o8,2016-12-21 00:48:33-05:00,finessseee,Reccomended laptop for CS student?,Looking for something around $1400~ (I know I can get by with a lot cheaper) I'm most likely going to install Linux on it too. Any recommendations?  ,,,,,Submission,1,0,1
dbgen2f,2016-12-21 01:22:39-05:00,thrway_1231,,IMO Dell already sells preconfigured Linux laptops. If you want something sturdier go for a Thinkpad.,5ji9o8,t3_5ji9o8,finessseee,,Comment,5,0,5
dbgx88w,2016-12-21 12:01:06-05:00,finessseee,,Sturdier?,5ji9o8,t1_dbgen2f,thrway_1231,,Reply,1,0,1
dblhetc,2016-12-24 18:22:19-05:00,Treyzania,,"Thinkpads tend to be on the more rugged end, and Linux environments tends to run on them unusually well.  XPS laptops I've heard also handle it pretty well.",5ji9o8,t1_dbgx88w,finessseee,,Reply,1,0,1
dbgeemq,2016-12-21 01:14:47-05:00,None,,"I can't think of anything you would need to do as a computer science student that you couldn't do on the cheapest computers offered 10 years ago. Obviously more power is better, but you could get by with just a command prompt & vim. Anything on the market today should be more than enough.",5ji9o8,t3_5ji9o8,finessseee,,Comment,4,0,4
dbgfcb4,2016-12-21 01:47:42-05:00,theacorneater,,"you could get one for $200, second hand, that would do the job.",5ji9o8,t3_5ji9o8,finessseee,,Comment,3,0,3
dbgg3sw,2016-12-21 02:17:37-05:00,swilliamseu,,Literally anything is good enough. Spend it on whatever beast you want,5ji9o8,t3_5ji9o8,finessseee,,Comment,3,0,3
dbghya2,2016-12-21 03:40:10-05:00,randomfield,,Get something with a good display!,5ji9o8,t3_5ji9o8,finessseee,,Comment,2,0,2
dbglls7,2016-12-21 06:48:28-05:00,NewToBikes,,This is very important. Good advice.,5ji9o8,t1_dbghya2,randomfield,,Reply,1,0,1
dbh3mj5,2016-12-21 14:09:14-05:00,jzarob,,"I personally like MacBook Air, OS X is POSIX compatible, comes with a great warranty, can run Linux in a VM, and can run Windows with boot camp or in a VM. Battery life is great, as is the weight and size. ",5ji9o8,t3_5ji9o8,finessseee,,Comment,2,0,2
dbgklhy,2016-12-21 05:57:59-05:00,qwertwerker,,"For that much I'd recommend a Dell XPS 13, a few of my friends have them and they can't vouch enough for them. Really good battery life and great build quality, pretty dainty and light too. ",5ji9o8,t3_5ji9o8,finessseee,,Comment,1,0,1
dbgxa5y,2016-12-21 12:02:11-05:00,finessseee,,Would you recommend the i5 with Ubuntu pre installed or the i7 with Windows pre installed then dual booting Ubuntu from there? (Same price),5ji9o8,t1_dbgklhy,qwertwerker,,Reply,1,0,1
dbh1fek,2016-12-21 13:25:30-05:00,albatrek,,"Not the person you asked, but having Windows is really nice. I dual boot Windows and Ubuntu. I don't need the Windows side often, but when I do, it's nice to have a real Windows machine instead of a VM.

I'd go for the i7 with Windows. ",5ji9o8,t1_dbgxa5y,finessseee,,Reply,1,0,1
dbh3rwg,2016-12-21 14:12:15-05:00,qwertwerker,,"Just get the i7 if it's the same price! Check your course handbook or whatever resource you have to see if you'll need windows-only software at any point (we needed Keil Microvision for ARM assembly, visual studio for some other machine language etc). 

If you'll need windows for anything college related or there's video games you'd like to play that aren't on Linux then you can dual boot it, otherwise you can just nuke it and install whatever you'd like. I find that other than those modules I've never found myself needing windows. ",5ji9o8,t1_dbgxa5y,finessseee,,Reply,1,0,1
5jewr6,2016-12-20 13:38:59-05:00,apocalypsedg,Does a secure way of verifying attendance exist?,"For example a lecturer wants to check attendance, an insecure example of what I am looking for would be if you had a code displayed on the overhead projector and you had to enter it online. The flaw is that people could leak the code to people outside by texting, allowing them to mark themselves present without being there.

A code unique to everyone suffers from the same problem. You could text friends their code without them being present.


If everyone had a secret function which took as input a code that varied every lecture, and outputted a verification code, that would work, but there is an incentive to leak your secret function allowing people inside to use it and text you back your output.

Any ideas?",,,,,Submission,9,0,9
dbfpakh,2016-12-20 15:03:27-05:00,SUsudo,,You shoot everyone in lecture and who ever attends the next lecture wasn't there previously. ,5jewr6,t3_5jewr6,apocalypsedg,,Comment,8,0,8
dbfy3y1,2016-12-20 18:02:08-05:00,apocalypsedg,,genius,5jewr6,t1_dbfpakh,SUsudo,,Reply,2,0,2
dbfni07,2016-12-20 14:28:50-05:00,Tinamil,,"Yes, but you can't rely on information as your verification method if the students have unlimited ability to transmit information via their cell phones.

The 3 usual forms of verification are something you know, which doesn't work if people are sharing information, something you have, and something you are.

The easiest way to verify attendance is to just count people, and match names to faces.  That's something you are.  You could also use fingerprints or retinal scans for a high tech version.

If the room is too large to count, then use a smart card (something you have) that you swipe when you enter the room, don't let anyone swipe more than one card, and put photo ID on the card to verify the person doing the swipe is the owner of the card.  Edit: Which I just realized goes back to something you are.  

Essentially, what you are is the only thing that you can't give someone else.  ",5jewr6,t3_5jewr6,apocalypsedg,,Comment,3,0,3
dbfxxxx,2016-12-20 17:58:20-05:00,apocalypsedg,,"This is the type of answer I was looking for. So theoretically any cryptographic methods of authenticating people for attendance are void if *something you know* information is shared? Is there really no way to encrypt the relevant info? Is there a (simple) proof that if *something you know* information can be shared it cannot be relied on? 

*something you are* type information with biometrics seems like the way to go I guess. Not very sophisticated/satisfying from a cryptography/authentication angle, I don't know why...It works though. 

 *Something you have* type information is indeed useless if you assume everyone works together against the system. No inspecting people, else you could use that person to just tick names of a list haha.",5jewr6,t1_dbfni07,Tinamil,,Reply,1,0,1
dbg4mnq,2016-12-20 20:34:00-05:00,Tinamil,,"The closest thing to what you're describing is non-repudiation, which allows you to assure me that you are who you say you are, but it doesn't help if you attempt to deceive me by just giving away your key.  It only protects against someone else attempting to deceive me by pretending to be you.

I can guarantee (with extremely high probability at least) that our communication will not be able to be read by anyone else, will not be tampered with by anyone else, and that no one else is pretending to be me.  But I have to trust you, and you have to trust me.  

There's simply no way to provide me information that I can read but can't pass onto someone else unless you control my information flow into and out of the room.  

The only way I can see to (not really) make it work is to give each person a unique code that only that person can receive, so that the only way to get your code was to show up to class.  Which means you had to authenticate who I was to give me my code anyway at the beginning of each class.",5jewr6,t1_dbfxxxx,apocalypsedg,,Reply,1,0,1
dbfpguk,2016-12-20 15:06:52-05:00,drobilla,,"> For example a lecturer wants to check attendance

I'm going to play devil's advocate here and ask: why?

One of the things I liked about CS here back when I did my undergrad was that none of this hand-holding participation stuff happened.  You could go to class, or you couldn't.  You could go to tutorial, if there even was one, or you couldn't.  We weren't treated like children: assignment marks and exam  marks did a perfectly good job of filtering out the students who didn't bother to learn the content of the course.  It's pretty easy to design lectures such that students who don't attend will have a very hard time doing well, if that's your aim.  Frankly, I see this as a (non-)solution to the problem ""lectures are not worth attending"".

Now, at least here, things have changed considerably, and there's lots of mandatory attendance marks and other such things.  This came in lock-step with an ever-declining failure rate (and, let's be honest, a tuition money grab), which might sound like a good thing until you realize the quality of student that makes it to upper year and even through the program has definitely gone *down* considerably.  I don't think that is a coincidence.

(Apologies for going on a tangent and not answering the theoretical problem directly, but not all problems should be solved as stated).",5jewr6,t3_5jewr6,apocalypsedg,,Comment,7,0,7
dbfxj7b,2016-12-20 17:49:13-05:00,apocalypsedg,,"I only picked lectures because it was something we were all familiar with, the theoretical problem of verifying attendance was what I was wondering about.

I'm actually an engineering student who takes CS electives. In pretty much every engineering module we are punished for not attending through missing notes, examples, slides, etc. If you miss one it's gone forever so you really can't afford to. In my CS modules I have to sign in on a sheet. It's way better from an educational point of view, you can give your full attention, and you have complete information the way the lecturer intended by the end of the course which isn't always the case in engineering. Taking crucial notes in a lecture often ends up littered with errors because you don't have a clue what's going on and you are just copying symbols. I don't miss lectures regardless though :)",5jewr6,t1_dbfpguk,drobilla,,Reply,5,0,5
dbfntt1,2016-12-20 14:35:09-05:00,fladam123,,"Our school of computer science at University began using a system a few years ago, we all have key fobs that we can hang on our key rings, about every 15 minutes during the lecture there is some sort of box I guess that checks for the current keyfobs in the room and marks your attendance online, its not exactly what you are looking for but you may find it interesting, their website is here: http://www.activcampus.com/

From what I have been told they were actually students at the University and they had the idea while in their final year and managed to get funding for it.

Of course sometimes people who do not want to attend a lecture will give their key fob to another person within the class, but it is about as close as you can get, no one wants to have 4 or 5 fobs. The other problem is that sometimes people stand outside of the lecture and still get their attendance registered and once they see that online they can leave.

Great question I just can't think of any way to prevent people from contacting those within lectures etc

",5jewr6,t3_5jewr6,apocalypsedg,,Comment,2,0,2
dbfx75z,2016-12-20 17:41:51-05:00,apocalypsedg,,"This is the practical solution I think. I was hoping for some sort of theoretically secure, (possibly cryptographic) approach.",5jewr6,t1_dbfntt1,fladam123,,Reply,1,0,1
dbgp67f,2016-12-21 09:00:33-05:00,Captain_Reid,,Queen's University Belfast? We were told the same story.,5jewr6,t1_dbfntt1,fladam123,,Reply,1,0,1
dbh3t37,2016-12-21 14:12:55-05:00,fladam123,,That's the one,5jewr6,t1_dbgp67f,Captain_Reid,,Reply,1,0,1
dbfsy5b,2016-12-20 16:15:11-05:00,zorkmids,,"You can lead a horse to water....
",5jewr6,t3_5jewr6,apocalypsedg,,Comment,1,0,1
dbgb5xm,2016-12-20 23:32:18-05:00,Chandon,,"Unfortunately, there's no cryptographic way to identify humans.

This is really annoying. It means digital signatures, account authentication, and other similar things are impossible to build with the properties that you actually want.",5jewr6,t3_5jewr6,apocalypsedg,,Comment,1,0,1
dbgmqxt,2016-12-21 07:39:00-05:00,jamesk93,,"My university uses Bluetooth beacons to register attendance for lectures. You download the university app and turn it on every time you're in lecture and it logs if you were there or not. 

Problem is, students can just say something like they forgot to turn their Bluetooth on it it wasn't working on their phone. Either that or you can go to the clusters and sign in manually.",5jewr6,t3_5jewr6,apocalypsedg,,Comment,1,0,1
5jcvl9,2016-12-20 06:52:43-05:00,LucasvRLoL,Grammar to Chomsky Normal Form,"Given is the following grammar
S -> SS | SaSb | lambda  
I have to convert it into Chomsky Normal Form. I have no idea how to do this since I can only substitute lambda. Can anyone help me out? I have made a start but am not sure if I am doing this correctly. We started out with this.  
S -> SS  
S -> SaSb  
S -> lambda  
I added a new start symbol, so now we have:  
S0 -> S  
S -> SS  
S -> SaSb  
S -> lambda  
I then substituted S with lambda  
S0 -> S  
S -> SS | S | lambda  
S -> SaSb | aSb | Sab | ab  
S -> lambda  
I don't know how to continue",,,,,Submission,6,0,6
dbf5eqb,2016-12-20 07:38:11-05:00,Chappit,,"Introduce additional nonterminals such as X1 like so:

A => B | C | D

Becomes 

A => B | X1

X1 => C | D",5jcvl9,t3_5jcvl9,LucasvRLoL,,Comment,3,0,3
5ja8sd,2016-12-19 19:53:56-05:00,Annoyed_NYC,Curious CompSci Student,"Going into my 4th semester in computer science and was hoping reddit could answer some questions for me. First I was curious as to how much math you should know for CompSci. Second, does anyone use assembly language in computer science anymore? Because my school makes me take a required semester and it really is just boring to me.",,,,,Submission,1,0,1
dbemq8o,2016-12-19 20:58:27-05:00,nbp615,,"It's your forth semester? Haven't they given you a degree audit of some kind with math reqs? Computer science is some ways could be considered applied math, depending on what you do with it. 

Assembly is what your compiled languages get, well, compiled into. Obviously it's pretty relevant, and someone has to work with it (writing compilers and low level systems etc). They wouldn't teach you it if it was irrelevant.",5ja8sd,t3_5ja8sd,Annoyed_NYC,,Comment,2,0,2
dbepa57,2016-12-19 21:55:28-05:00,Annoyed_NYC,,"The audit they give only says math electives, but they prefer I take high level courses like linear algebra and classes on that level",5ja8sd,t1_dbemq8o,nbp615,,Reply,1,0,1
dbemqxp,2016-12-19 20:58:53-05:00,tomek142,,"1. Yes, you will have to take couple of math courses and some of them are not easy. If I remember correctly, I had to take around 6-7 courses.
2. Not really but it's the most low level language and you will understand how registers, etc work.",5ja8sd,t3_5ja8sd,Annoyed_NYC,,Comment,2,0,2
dbeqikx,2016-12-19 22:24:40-05:00,SakishimaHabu,,"Assembler is used pretty frequently in imbedded systems. And it's also used often inside c and c++ programs for scientific computing and game design to speed up programs.

 It's very important for reverse engineering software, because as they say if you can read assembler everything is open source. 

As for math, the more math you know the more opportunities you have to see solutions to your problems,  so it's always a good idea to get better at and learn more math.",5ja8sd,t3_5ja8sd,Annoyed_NYC,,Comment,1,0,1
dbesvyq,2016-12-19 23:21:04-05:00,ChaoticxSerenity,,"1. Had to take Calc I, Linear Algebra I and Discrete Math 
2. Learning Assembly teaches you about hardware and sort of how computers actually ""work"", which is important if you're into that sort of thing. That's why in my school, they make Assembly a pre-req to Operating Systems. ",5ja8sd,t3_5ja8sd,Annoyed_NYC,,Comment,1,0,1
dbeuuns,2016-12-20 00:12:33-05:00,UncleMeat,,"The amount of math really depends. People here will say it is absolutely the core of computer science. Some people on hackernews will says it is worthless overhead. In truth it is somewhere in the middle. It is possible to get a degree and become a software engineer without really internalizing math but there are a fair number of areas of CS where knowing a lot of math is essential (ML being the hot one right now). 

Very few people code in assembly today. But it teaches you the low level behavior of programs, which is essential for writing performant code and is absolutely essential if you want to know anything about system security.",5ja8sd,t3_5ja8sd,Annoyed_NYC,,Comment,1,0,1
dbf7n0u,2016-12-20 08:54:08-05:00,andybmcc,,">First I was curious as to how much math you should know for CompSci

Computer science is essentially a branch of mathematics.  You should have an understanding of basic logic, graph theory, number theory, combinatorics, calculus, numerical methods, linear systems, statistics, computational theory, language and grammar constructs, etc.

>Second, does anyone use assembly language in computer science anymore

Assembly language is still used directly in some instances (legacy, constrained 8-bit MCUs, performant systems).  It is very useful to be able to view and understand disassembly.",5ja8sd,t3_5ja8sd,Annoyed_NYC,,Comment,1,0,1
5j939n,2016-12-19 16:28:21-05:00,suddenintent,MSc Student of Computer Software Engineering-Need to choose my Supervisor & Thesis Area,"Hi!

TL;DR: I'm looking for a thesis area which isn't CS-ish! (don't like CS)

I'm a computer software engineering master student. Our college required us to choose our supervisor professor in next few days. So I'm seeking an Area, to choose a supervisor who is interested in it.

Honestly I don't like CS and I'm looking for a subject which is related to other areas such as biology and medicine (an interdisciplinary maybe). To date I found data mining, bioinformatics and machine learning.

What do you think? Looking for your suggestions and ideas!

Thank you
",,,,,Submission,5,0,5
dbecgym,2016-12-19 17:12:26-05:00,tanenbaum,,"What don't you like about CS? Internet of Things is hot right now and there's a lot of things to address, such as security, and new uses to uncover. HCI is also different from 'classical' CS - you could study how doctors work and examine how to improve the work flow. As an example, IoT has been used in some hospitals where  all ressources - instruments, beds, etc - are tagged and can be easily localized, as a lot of time is actually wasted on hospitals trying to find things.",5j939n,t3_5j939n,suddenintent,,Comment,2,0,2
dbeuiq8,2016-12-20 00:03:16-05:00,tRfalcore,,"some would call it wasted, some would call it ""the best way to charge this person $40k dollars for a one night stay in a not even comfortable one bed hotel room"".",5j939n,t1_dbecgym,tanenbaum,,Reply,1,0,1
dbl7949,2016-12-24 14:08:46-05:00,suddenintent,,"But I'm more interested in its science, i.e. biology.",5j939n,t1_dbecgym,tanenbaum,,Reply,1,0,1
dbeuc9t,2016-12-19 23:58:30-05:00,tRfalcore,,"you can do your thesis on anything you want, so long as you narrow the topic down through all the shitty weeds.  Just pick whatever you want to do then search search until you find something super specific.  If you're a CS student and you solve cancer it'll be accepted.  It's an incredible area of research-- you can do anything and just use computing's incredible power to solve your problem.

research why crabs have detachable hands, or why puffer fish can explode.  doesn't matter, just leverage computing's incredible power to solve your ""query"".",5j939n,t3_5j939n,suddenintent,,Comment,1,0,1
5j69ic,2016-12-19 08:02:11-05:00,treyvonbooke,Help Please!Python Range function.,"can someone explain the time complexities of this function?
seen here... http://imgur.com/a/QkSOj
like tight bounds and worst case best case? thanks!
Also if you could give a general proof that would help also..
sorry if this is basic btw I am a beginner",,,,,Submission,0,0,0
dbdoydi,2016-12-19 09:00:39-05:00,treyvonbooke,,I was thinking maybe O(1) or n... does the function depend on n? or maybe nothing lol?,5j69ic,t3_5j69ic,treyvonbooke,,Comment,1,0,1
dbdrk13,2016-12-19 10:13:11-05:00,i_am_suicidal,,"This seems to be homework so I'm just gonna write generally. 

Think about what the function is doing when creating the list of numbers. Write it yourself to practice.

Here's a start:

    def range(start, stop):
        step = 1 # start with one, add step to params later
        numbers = []
        # put some code that fills numbers and returns it

How many things do you have to do to create the list? Use that knowledge to determine the big O value.",5j69ic,t3_5j69ic,treyvonbooke,,Comment,1,0,1
5j4pey,2016-12-19 00:24:57-05:00,ramosaleonel,I got my computer science AA.... now what?,"I still feel like I do not know shit. I will be working towards my B.S (B.A? idk the difference) for Computer Science and software development but until I finish that, now what? Like I said, I still feel I don't know shit. Not for an internship not for a job in computer science... not for anything really. I am lost, I feel like if I were to get hired for a programming job, I would hide in the corner until I eventually got fired. 

I know how to make data structures and implement algortihms and I know how they work but that's all I know (aside from recursion and some threads), can I actually get any job with my limited knowledge? People always say BUILD SHIT! and I say WHAT SHIT! I would not even know where to begin, even if I were to write a program with a shit GUI in eclipse...what then?

Should I pursue doing a project or getting an internship or should I just keep studying until I feel like an actual programmer?",,,,,Submission,3,0,3
dbdfzlr,2016-12-19 01:54:28-05:00,panderingPenguin,,"Well there are two possibilities: you actually know nothing, or you're suffering from imposter syndrome, which is very common in this field. You sound like you know enough to land an internship to me. Most interns only have a vague idea of what they're doing anyways (trust me, I was one). There's nothing to lose by applying to some internships and see if you get anything. Absolute worst case scenario is that you get some interview practice. ",5j4pey,t3_5j4pey,ramosaleonel,,Comment,2,0,2
dbdtdk3,2016-12-19 10:55:33-05:00,andybmcc,,"Look for internships.  Any place worth working at will not expect you to hit the ground running, they'll bring you up to speed.  Depending on the place, you'll often just be paired off with an experienced developer who will walk you through some things.  Our desktop software guys here don't expect meaningful work from fresh B.S. graduates within the first 4-6 months.  Continue your education, but realize that actual experience is infinitely more valuable, unless you never plan on leaving academia.",5j4pey,t3_5j4pey,ramosaleonel,,Comment,1,0,1
5j4a1o,2016-12-18 22:43:31-05:00,winter_mutant,"What's the smallest program that can be called ""Linux"" and what does it do?",,,,,,Submission,22,0,22
dbdaeym,2016-12-18 22:59:46-05:00,tyggerjai,,"The core kernel with no modules.

Practically, it would do nothing. You might get some diagnostics to the serial console or framebuffer while it boots, but then to even accept input, you need an init process - something outside the kernel. 
",5j4a1o,t3_5j4a1o,winter_mutant,,Comment,30,0,30
dbdcbmp,2016-12-18 23:49:41-05:00,HatchCannon,,"""I'd just like to interject for a moment. What you’re referring to as Linux, is in fact, GNU/Lin- wait a minute....""",5j4a1o,t1_dbdaeym,tyggerjai,,Reply,38,0,38
dbdnr52,2016-12-19 08:19:07-05:00,bsdnoob,,Not today,5j4a1o,t1_dbdcbmp,HatchCannon,,Reply,14,0,14
dbdimac,2016-12-19 03:56:29-05:00,Treyzania,,It'd boot up really fast though.,5j4a1o,t1_dbdaeym,tyggerjai,,Reply,8,0,8
dbddyq3,2016-12-19 00:40:04-05:00,jajajajaj,,"In addition to that, I guess you'd have to compare architectures and compiler settings, and figure which one makes the smallest binary. 

Probably a x386 Linux version from 1992 would be it. ",5j4a1o,t1_dbdaeym,tyggerjai,,Reply,4,0,4
dbdim3c,2016-12-19 03:56:11-05:00,Treyzania,,"I think the OP was referring to smallest in terms of complexity and components.

You're not really wrong, though.",5j4a1o,t1_dbddyq3,jajajajaj,,Reply,6,0,6
dbdmkdz,2016-12-19 07:29:19-05:00,james4765,,"Back in the day, there were bootable Linux floppy disks. Intended for rescue / installation on machines that couldn't boot from CDROM, it gave you a shell and not much more.

The 2.6 kernels got too big to fit on a 1.44MB floppy, but one of my first Linux workstations used a boot floppy with a 2.4 kernel because the IDE channels on the motherboard were shot and the salvaged SCSI parts I dumpstered wouldn't play nice with the PC BIOS (I think they were out of a AIX system).",5j4a1o,t3_5j4a1o,winter_mutant,,Comment,4,0,4
dbe5fy1,2016-12-19 14:59:37-05:00,fadumpt,,I still have 3 floppy disks that have the basic Slackware install on them that I would carry for times when I needed gparted on an old system.,5j4a1o,t1_dbdmkdz,james4765,,Reply,3,0,3
dbdq41a,2016-12-19 09:35:11-05:00,ldpreload,,"""Linux"" is the proper name for a particular piece of software, a mostly-UNIX-compatible kernel: https://github.com/torvalds/linux

You can download the source (with git clone) and run `make menuconfig` and go through the menu options and see what you can do, or more specifically, what all you can disable. Or even better, `make allnoconfig`. Then build it and see what you get.

If you're on an x86 (or x86-64) machine, your kernel will end up in arch/x86/boot/bzImage. You can run it in a VM by installing qemu and running `qemu-system-x86_64 -kernel arch/x86/boot/bzImage`. No filesystem, so it won't get very far.

Depending on how you want to interpret the name ""Linux"", grabbing the very first version of Linux from https://www.kernel.org/pub/linux/kernel/Historic/ might get you an even smaller kernel.",5j4a1o,t3_5j4a1o,winter_mutant,,Comment,4,0,4
dbdw4pr,2016-12-19 11:53:54-05:00,JesusFappedForMySins,,"Hey guys please forgive me I'm a guy trying to learn CS, What does Linux mean?? I thought Linux was an OS..",5j4a1o,t3_5j4a1o,winter_mutant,,Comment,3,0,3
dbe035f,2016-12-19 13:14:06-05:00,dragonnyxx,,"It is.

An operating system is a program running on the bare metal of the computer (i.e. directly controlling the hardware). Its job is to control the overall state of the computer, provide basic features like filesystems, multitasking, and memory management, and allow the user to start and stop other programs.

But, crucially, an operating system is still a program. It's a low level one that only has access to features directly provided by the hardware and firmware, but it's just a program. 

""Back in the day"", games on floppy disks sometimes ran directly on the bare metal of the computer - you would boot the computer up with that disk in it, and it would skip DOS or whatever the computer's normal OS was altogether. The game was, in effect, its own operating system, having to provide all of its own device support, memory management, and so forth. Operating systems aren't magic, just programs that run without any help.",5j4a1o,t1_dbdw4pr,JesusFappedForMySins,,Reply,5,0,5
dbdm1gu,2016-12-19 07:04:08-05:00,fadumpt,,"A usable Linux that takes up almost no space would probably be one of the router distros like pfsense.
edit: instead of pfsense, insert Linux based router distro instead.",5j4a1o,t3_5j4a1o,winter_mutant,,Comment,-2,0,-2
dbdszo3,2016-12-19 10:46:59-05:00,verandaguy,,Isn't pfSense software BSD-based?,5j4a1o,t1_dbdm1gu,fadumpt,,Reply,2,0,2
dbdtkqk,2016-12-19 10:59:52-05:00,4z01235,,"Yup, so definitely not something that can be called ""Linux"" no matter how small.",5j4a1o,t1_dbdszo3,verandaguy,,Reply,4,0,4
dbe5b6m,2016-12-19 14:57:07-05:00,fadumpt,,"There are Linux based router distros though as well and they are tiny.
So pick one of them :)
ipcop maybe?  
",5j4a1o,t1_dbdtkqk,4z01235,,Reply,0,0,0
5j3ziz,2016-12-18 21:38:15-05:00,ksr_is_back,Do you really need a laptop for computer science?,"I'm asking because the program of my future university doesn't specify it.

I was thinking to build a desktop pc with two screens but like I just said I don't really know if I'm going to need a laptop.

Sorry for my english",,,,,Submission,11,0,11
dbd7m0g,2016-12-18 21:53:03-05:00,tyggerjai,,"Some kind of laptop is probably handy, yes, but it doesn't have to be pricey. The main advantage is a consistent environment that you control. That way, it doesn't matter if you're in a lab, lecture hall or library, if you need to do something you can focus on doing it, rather than setting up or fighting with infrastructure. 

That's tied in with remote storage. You can solve the storage with git or Dropbox or a thumb drive, but that doesn't help if you're trying to develop on a machine that won't let you install the relevant libraries. ",5j3ziz,t3_5j3ziz,ksr_is_back,,Comment,12,0,12
dbd9khh,2016-12-18 22:39:22-05:00,ksr_is_back,,"Here the university is different than in america.

You only go to university to get classes, so I think that the friends rooms, campus, etc is out of the question.

I read some opinions from redditors and they said that the fact that you can use more than one monitor and a keyboard, make coding easier, It's this true? btw I always felt that laptops were akward to write for long periods of time.

The university also has public computer labs with remote storage ids.

",5j3ziz,t1_dbd7m0g,tyggerjai,,Reply,3,0,3
dbdi99m,2016-12-19 03:37:32-05:00,ukkoylijumala,,"> Here the university is different than in america.
> You only go to university to get classes, so I think that the friends rooms, campus, etc is out of the question.

So, where will you go? Maybe you will meet some future co-students or alumni here. I certainly did :D

I have done both my degrees in different European countries. While you go only to university for classes, almost every university I know has some sort of assignments. Some of these assignments will have to be done in a group. For this purpose I found that working with your group in the same room can have some incredible advantages and a laptop is a must in this case.

> I read some opinions from redditors and they said that the fact that you can use more than one monitor and a keyboard, make coding easier, It's this true?

Personally, I think 2 screens are very convenient, for example you can simply code on one screen and have documentations open on the other. A keyboard is nice, but after hours of coding on a laptop keyboard you will be just as much used to it than a real keyboard.
",5j3ziz,t1_dbd9khh,ksr_is_back,,Reply,2,0,2
dbd9pdi,2016-12-18 22:42:43-05:00,tyggerjai,,"If you will never need to write code outside your bedroom, then yes, a laptop is probably not necessary. You can always leave it and see - if you need one later, buy it when you need it. 

Some people do indeed add a proper keyboard and monitor to their laptops for coding.",5j3ziz,t1_dbd9khh,ksr_is_back,,Reply,1,0,1
dbdz94w,2016-12-19 12:57:05-05:00,anamorphism,,there's nothing preventing you from attaching a keyboard and second monitor to a laptop if you really can't get used to the laptop's built-in keyboard.,5j3ziz,t1_dbd9khh,ksr_is_back,,Reply,1,0,1
dbdg3b0,2016-12-19 01:58:40-05:00,ACoderGirl,,"Probably not, but depends on school. My university had everything you'd need on school computers, so you'd never need a personal computer of any kind. Notes in lectures wouldn't need a computer and sometimes are best written, anyway.

Laptops can come in handy if you'd rather type notes. Sometimes that is the better idea, such as if the teacher uploads outlines and you can just add to that. Laptops could in theory be useful for group projects, but I've never had anything that I can think of where it'd be necessary. Group meetings don't necessarily need a computer. Or can get by with just one computer. Or can use a school computer. Most group projects I did were more along the lines of meet up to plan things out, but do all the coding and all alone. Thus, no computer really needed.

A laptop could also be useful for simply having more customization than school computers. Also, if your university sucks (particularly if it's in a developing country), your school computers might not be good enough or there might not be enough of them.

And in case you're unaware, you'll probably never need to care about the difference in speed between a laptop or desktop for a typical CS program. That is, you'll likely never make anything so intensive that you'd need a desktop to run it in a timely fashion. It could maybe speed things up a bit, but never be strictly necessary. For most programmers, the difference is only really for PC gaming. Also, dual monitors is an excellent idea. Should get that even if you don't go the desktop route.",5j3ziz,t3_5j3ziz,ksr_is_back,,Comment,3,0,3
dbdaul7,2016-12-18 23:10:38-05:00,Ipswitch84,,"I did 6 years of CS schoolwork (undergrad + grad) without a laptop.  Having one in class would have been more of a distraction then useful.  I had a workstation at home and I could use a campus computer lap to SSH into various machines to complete homework (personal and through the university).  So, no, I would say that you don't need a laptop.",5j3ziz,t3_5j3ziz,ksr_is_back,,Comment,5,0,5
dbdaymo,2016-12-18 23:13:20-05:00,Nadrieras,,"Yes, because group projects and added mobility.",5j3ziz,t3_5j3ziz,ksr_is_back,,Comment,2,0,2
dbdi6c9,2016-12-19 03:33:18-05:00,luv0,,"If you work well with both your pc and your university one, its ok. I find easy to have everything in on my laptop and don't need to transfer from the university to my pc at home. ",5j3ziz,t3_5j3ziz,ksr_is_back,,Comment,1,0,1
dbdp8rt,2016-12-19 09:09:49-05:00,falafel_eater,,"I did my entire undergrad without a laptop. It's convenient but not necessary.  
No need to get a high-spec laptop.",5j3ziz,t3_5j3ziz,ksr_is_back,,Comment,1,0,1
dbdtg14,2016-12-19 10:57:02-05:00,KronktheKronk,,"A desktop is fine.  You just want to be able to do work from wherever is comfortable for you. without a laptop this means ""comfortable for you"" is going to be your room or office, but if you're happy with that that's fine.
",5j3ziz,t3_5j3ziz,ksr_is_back,,Comment,1,0,1
dbe6e51,2016-12-19 15:18:04-05:00,MenWithCandy,,"I compromised and got a slightly cheaper desktop and a dirt cheap laptop. I do most of my work on the desktop, but the laptop comes in handy if you need to go to the library or want to study somewhere else.",5j3ziz,t3_5j3ziz,ksr_is_back,,Comment,1,0,1
dbel0hd,2016-12-19 20:19:16-05:00,flopperr999,,Short answer: yup!,5j3ziz,t3_5j3ziz,ksr_is_back,,Comment,1,0,1
5j3x9f,2016-12-18 21:24:39-05:00,I_love_seaweed,Proofs and theoretical computer science,"I don't know what's wrong with me.

I am currently a Junior in College, and I've aced every single calculus class from Calculus I all the way to Calculus III, with ease, Linear Algebra, which was a bit harder to do so, but I can't for the sake of my life come up with proofs for my computer science classes. It's not even the fact I don't know how to write a proper proof, it's that if I had to proof something, I don't even know where to begin with, or even come up with a coherent solution.

I've had 2 courses that required me to proof things, the first one being the introduction to computation which introduced me to all sorts of proofs such as induction, contradiction, as well as other things such as conditional statements. The second course is Algorithms. For this course I pretty much have to proof that my solutions are correct, that the run time is correct, etc. 

I don't know how to do most of the proofs in the assignments. For most of my answers, I've literally put down things that did not make any sense to me, but when the solutions are posted, and I read through them, I understand most of them completely, and I just keep wondering to myself why I can't come up with these proofs.

I've somehow made it pass the first course with a B, and possibly a B+ or A- in algorithms. This sucks because I feel like I have to have a good understanding of how to do this in my future classes, and possibly career.

I guess my real question is: Why do I do well in my first few math courses, but is absolutely horrid when it comes to theories and proofs, and how do I improve?",,,,,Submission,5,0,5
dbdnt2x,2016-12-19 08:21:07-05:00,albatrek,,"I found proofs like these to be a completely different style of math from anything I had done in my calculus of linear algebra classes. For me, the thing that helped was tons of exposure - I read through all the proofs from class, things in the textbook, proofs from friends, Youtube videos, and I went to talk to professors outside of class. 

Fwiw, the way I start is really clearly stating what I'm trying to prove and what I have to work with. It sounds silly - like, of course you know what you're trying to prove - but seeing the definitions unwound and written out in mathematical terms really helps me.",5j3x9f,t3_5j3x9f,I_love_seaweed,,Comment,1,0,1
dbegi8u,2016-12-19 18:36:59-05:00,wasimwesley,,"Same here: I'm internalizing a style of reasoning that's so self-referentially confusing that I don't recognize my own proofs once I've finished one. I know something is true, and then I have to ask myself ""wait, why do I know this? do I know anything?!""

my point is, you've gotten either a B+ or an A-, I think you're doing well enough and you just need to force your brain to get in the habit of using this straightjacket-style of logic which is required for Discrete Mathematics so that you're confident in your approach",5j3x9f,t3_5j3x9f,I_love_seaweed,,Comment,1,0,1
5j296x,2016-12-18 15:43:49-05:00,cmpscistudent,How do you prove an algorithm is not fast by contradiction?,I have a specific algorithm im trying to prove isnt fast by using contradiction? can anyone help me please I cant get myself to know how to start this,,,,,Submission,10,0,10
dbcs9wn,2016-12-18 16:03:28-05:00,IgnorantPlatypus,,If you wanted to prove e.g. that quicksort isn't O(n lg n) you can do this by contradiction: you pick a set of input that the algorithm runs in O(n^(2)). Since there exists some input that is O(n^(2)) the algorithm cannot be O(n lg n).,5j296x,t3_5j296x,cmpscistudent,,Comment,9,0,9
dbcwfuz,2016-12-18 17:30:50-05:00,Devvils,,"Perfectly correct but why would anybody do that.

Just show the asymtotic time complexity [Big O](https://en.wikipedia.org/wiki/Big_O_notation).",5j296x,t1_dbcs9wn,IgnorantPlatypus,,Reply,-7,0,-7
dbd0912,2016-12-18 18:59:27-05:00,INCOMPLETE_USERNAM,,Maybe because that's what the homework asked for?,5j296x,t1_dbcwfuz,Devvils,,Reply,7,0,7
dbcs5jp,2016-12-18 16:00:57-05:00,danltn,,"What does fast mean?  Give an input, show by worked example the running time and show this isn't fast. ",5j296x,t3_5j296x,cmpscistudent,,Comment,3,0,3
dbcz13m,2016-12-18 18:30:23-05:00,theobromus,,"As other commenters note it's not clear what you mean.

One interpretation that makes sense to me: showing that an algorithm is NP-Hard. This can often be accomplished by showing that you can construct inputs to the algorithm that would solve a known NP-Complete problem (most often boolean satisfiability or 3SAT). To cite an example from [wikipedia](https://en.wikipedia.org/wiki/NP-hardness):

""For example, the Boolean satisfiability problem can be reduced to the halting problem by transforming it to the description of a Turing machine that tries all truth value assignments and when it finds one that satisfies the formula it halts and otherwise it goes into an infinite loop.""

At least to my thinking, this is a way to prove by contradiction that the problem can't be solved quickly.",5j296x,t3_5j296x,cmpscistudent,,Comment,2,0,2
5iyfb8,2016-12-17 22:48:09-05:00,finessseee,Can someone explain to me how exactly Linux makes everything so much easier versus Windows?,How does it work? I'm just curious since I've never used Linux before.. ,,,,,Submission,18,0,18
dbc5bj4,2016-12-18 04:10:30-05:00,PastyPilgrim,,"There's the Unix terminal for one, which makes it easy to do anything you could want to do quickly and efficiently from the terminal while you're working. No only is it more efficient, but most Linux distros come with command-line utilities like ssh that require third-party software on Windows.

The biggest difference, however, is the package managers. If you want to install a new language, compiler, library, etc. etc. etc. you can do it in one step. If you want to install a Python library on Windows, you've gotta search the internet for an installer. You need to make sure that the installer matches your version of Windows and your CPU's architecture. You need to actually download and install that software. Then you scratch your head over why you're unable to use it before you realize that you changed your Python's install directory and your new library installed somewhere else and wasn't configured properly with Python. Or, god forbid you need to fuck with your path variable or something. Finally you get it installed correctly but when you import it Python exits with an error, citing some nondescript exception. After an hour of web searching and posting your problem on stackoverflow you learn that you're missing a dependency needed for this library. Go back to step one to install that dependency.

If you want to fuck around with Numpy on Linux and see if it fits with a project that you're doing, you type ""apt-get install python-numpy"" and you're done. That's it. One simple command that installs your software, ensures compatibility with your machine, configures it to work with existing software/packages, and allows you to easily remove it when you no longer need it (""apt-get purge python-numpy"").",5iyfb8,t3_5iyfb8,finessseee,,Comment,28,0,28
dbc0kto,2016-12-18 00:46:37-05:00,veeberz,,"I don't know who you heard that from or the context of the conversation, but here are a few possibilities:

 - Licensing. I can download a Linux distro and run a Linux-based server without paying for a license (not true for all distros however)
 - [POSIX-compliant](https://en.m.wikipedia.org/wiki/POSIX)-ish if you need it (for systems programming).
 - For my personal web development projects, I found that Linux is friendlier to use. Many languages and platforms, while often being cross-platform, seems to have more extensive documentation for Linux users. This is just my experience, however. ",5iyfb8,t3_5iyfb8,finessseee,,Comment,13,0,13
dbc6ttc,2016-12-18 05:37:32-05:00,daymi,,"It depends on what you want to do. Since you ask in AskComputerScience, I'll assume that you also write programs.

Then Linux is much much easier to use since everything you need to compile, debug, edit etc has available source code at no charge (that you are allowed to change, too). That means if you hunt for a bug you can step right into the system libraries when you need to! *All* the libraries have source code. Hell, the system kernel has source code available to you right now!

Also, there are ""distributions"" which select libraries that are compatible with each other so you can just install a library by literally typing ""apt install *library*"". That's it.

I've been working with closed source compilers before (Watcom, Delphi, ...) and it's *really* annoying in that there is a limit somewhere which you can't step over. Bug in there? Too bad. Bug at your side causing symptoms only in there? Too bad, you won't find it.

Let's say your program written using a closed source compiler and with closed source libraries works A-OK. You ship it and then one of the libraries gets updated (by Microsoft or whoever). That means you (or worse, only the customer) gets a new black box to replace the previous black box with. How do you know that your program will still work? The vendor says he will tell you about breaking changes. But he doesn't really care about your program...

Let's say a Free Software library gets updated. Wanna find out what changed? Just read the source code and find the lines that were changed right there.

Nowadays I'm in the nice situation that I get paid to program Free Software and I'm not going back to black boxes doing god knows what. Eeew.

On the other hand, if you are a regular user, Linux still has some catching-up to do. Some things are straight up unusable. Even when there are GUI applications, half of the time they don't work or break in mysterious ways. Some programs have weird user interfaces that are unlike anything anyone has ever seen before (which is bad from a usability standpoint). In general there's a emphasis on the commandline to the detriment of graphical user interfaces.",5iyfb8,t3_5iyfb8,finessseee,,Comment,6,0,6
dbc8v2e,2016-12-18 07:31:11-05:00,MyKingdomForAShip,,"For me, terminal and the package managers are what make it so easy to do everything. A lot of that comes down to a system that was built giving the user power over their computer and accessing a huge variety of free and/or open-source software. 

The biggest insight I have though, is the way system calls work. Working in assembly, the kernel provides you a lot of functionality (so you don't have to implement it yourself or run your code at a privilege level above where it should be). A simple assembly program on Linux, using system calls could be done with say 100-200 instructions. The equivalent program on Windows would be 500-800 instructions.  To me, this kind of representative of everything with the 2 OSes, one built by a huge bureaucracy and the other by small group ruled closely by one person. ",5iyfb8,t3_5iyfb8,finessseee,,Comment,5,0,5
dbc9nkx,2016-12-18 08:12:45-05:00,KronktheKronk,,"The one and only thing Linux has going for it from an ease of use perspective is apt-get (or yum).

But since I still use my personal machine for dicking around on the internet and playing video games Windows is better.",5iyfb8,t3_5iyfb8,finessseee,,Comment,10,0,10
dbcb5j3,2016-12-18 09:16:41-05:00,None,,[deleted],5iyfb8,t1_dbc9nkx,KronktheKronk,,Reply,-8,0,-8
dbcc9jq,2016-12-18 09:56:04-05:00,KronktheKronk,,"Dissenting opinions aren't valueless.

Don't be so full of your own opinion",5iyfb8,t1_dbcb5j3,None,,Reply,9,0,9
dbco0lm,2016-12-18 14:35:24-05:00,Madsy9,,"There are many reasons, from POSIX compliance to filed-mapped devices to how incredibly useful the terminal is.

Some examples:

 * I have the name of a process, and I want to find its pid.
   * Linux: ps aux | grep 'name-of-my-process'
   * Windows: Open the task manager and read the process list until you find the name. If the process is even listed there. It might be hidden behind an svchost instance.

 * Given a process' pid, I want to figure out which files a process has open.
   * Linux: ls /proc/<pid>/fd
   * Windows: No easy out of the box solution. You could download SysInternals and use the Process Explorer GUI.

 * I want to upload a single file to my server box
   * Linux: scp myfile madsy9\@myserver.net:/home/madsy9/
   * Windows: Download an ftp client, have to use the ftp client's GUI, whether it's 1000 files or just one file

 * I want to make a simple script which recursively finds all my videos under a directory and displays their sizes in sorted order
   * Linux: Create a simple shell script which uses `sort`, `locate` or `find` and `xargs`.
   * Windows: No out of the box solution. You might get something similar to work with Powershell (maybe), or you could download Python. Still way more work than using bash/dash together with coreutils.

 * When people build my C++ projects, I want the makefile to check if the user has installed the required libraries already, and tell them if they are missing any.
   * Linux: Use pkg-config and/or check standard directories for this defined by the Linux Standard Base, such as /usr/lib and /usr/include
  * Windows: Windows have no specified standard directories for libraries or headers. Users who build other people's code need to specify such a path, or Windows projects have to bundle all their third-party dependencies.

I could make plenty more examples, but I think the ones above genuinely reflect my everyday tasks I need to get done. For people who never mess around with software development, server hosting or similar, the stuff above might seem irrelevant and maybe it is. Everything else being equal, what one requires  from the operating system and its tools depends on what you use your computer for.",5iyfb8,t3_5iyfb8,finessseee,,Comment,3,0,3
dbcwlr3,2016-12-18 17:34:29-05:00,njaard,,"The filesystem makes more sense on linux:

* symbolic links *just work* (Windows has them too, but they just don't work well, they're not universal, and they're not application transparent, often enough).
* Linux uses UTF-8 normally, so unicode filenames ""just work"". In windows, there's no way for a standards compliant C++ program to open Unicode filenames 
* Linux doesn't have the confusion of if the path separator is / or \\. 
* Filenames don't have weird restrictions in linux (You can use every character except / and the 0 byte).
* You can delete a file even if it's open: this might seem weird, but this comes in handy for me about 10 times a day: the Windows behavior just seems broken by comparison.",5iyfb8,t3_5iyfb8,finessseee,,Comment,2,0,2
dbd24ns,2016-12-18 19:43:29-05:00,trucekill,,"Windows and Apple put the layman consumer first. The consumers of desktop Linux have often been the developers themselves.

Even going back to UNIX in the 70's, 80's and 90's, the evolution of the OS and ecosystem has been driven by developers and the software industry first, and for regular users second.",5iyfb8,t3_5iyfb8,finessseee,,Comment,1,0,1
5iwvvz,2016-12-17 17:00:53-05:00,herejust4this,Using LC-3 simulator to learn about assembly language and I'm very confused about a couple things.,"I've been taking a computer organization class at school and we are using the LC-3 simulator to learn assembly language and I'm having a real hard time grasping a few simple concepts. 

What's mainly confusing to me is how PCoffset & Base+PCoffset really work. I understand how the PCcounter ticks one line at a time through the code and can jump to different addresses and read/write data, and that instructions with PCoffset allow you to read/write instruction that are ""further away"" than you would get from using a regular LD or ST command. 

However, I just cannot figure out how to use them correctly so right now I'm stuck with just using LD and ST to grab my data from my data fields, yet I still cannot print out the stored data using PUTS or OUT and I don't understand why.  

Here's a sample of my crowning achievement in LC-3 so far.
The addition is working and it appears that the values are being stored & loaded in the correct registers but it does not print. I would really like to be able to utilize the PCOffset & Base+PCOffset capabilities of certain Load and Store instructions but right now I can't even get a simple print routine to work.

	.ORIG	x3000

	AND	R0, R0, #0	; clear R0
	AND	R1, R1, #0	; clear R1
	ADD	R2, R2, #5	; R2 = 5
	ADD	R3, R3, #2	; R3 = 2

	ADD	R0, R3, R2	; R0 now is 2+5=7
	ST	R0, MEM1	; Store 7 inside MEM1
	LD	R1, MEM1	; Load 7 into R1
	
	AND	R0, R0, #0	; clear R0 again
	ADD	R0, R0, R1	; Add 7 into R0
	OUT			; print 7 to console

	     HALT

     ;*******data************
      MEM1	.BLKW	1	

	     .END",,,,,Submission,13,0,13
dbbvqga,2016-12-17 22:21:14-05:00,hljklbo,,"The 7 is in ASCII. It's not a decimal number, that's why it's not printing out a ""7"". You need to convert it before you can print out the value you're looking for. ",5iwvvz,t3_5iwvvz,herejust4this,,Comment,3,0,3
dbe6dzj,2016-12-19 15:17:59-05:00,herejust4this,,"Yup, that was the problem. Makes sense because it was printing out weird characters but not the correct number. I just needed to add 48 to it to get the correct value printed out.",5iwvvz,t1_dbbvqga,hljklbo,,Reply,1,0,1
dbby9r9,2016-12-17 23:33:12-05:00,Graysless,,"I actually just finished taking the class at the University of Texas where the LC-3 was invented by Yale Patt! Id be happy to help, and feel free to ask any more questions as you learn.  The PC Offset is the last 9 bits of the instruction that allows you to select an operand from a memory address that is about 2^8 memory addresses away from the original instruction. When the instruction is fetched, the PC (program counter) is incremented by 1, so when you use a PC offset, the operand will be fetched from a memory location that is 'Location of instruction' + 1 +PC offset away from the instruction. During the fetch operands phase, the PC offset will be sign extended (it is a signed 2's complement integer) and then added to the PC to get the location of the operand. The Base+offset is similar to PC offset except instead of using the PC, it uses a register (Base register + base offset) the only difference in the actual offset is (if I remember correctly) it is only a 6 bit offset. When using assembly language, you can use labels just like you did in the code that you wrote, the assembler will compute the offset and include it in the object code. You dont have to worry about PC offset in assembly. You do have to use the base offset in assembly though. And lastly, the above comment is correct that the 7 must be converted to ASCII before printed. You should be able to add like 30 or something, I cant remember exactly what the number is, but I think the ascii digits are somewhere in the 30's. Let me know if you have any questions or need more clarification! ",5iwvvz,t3_5iwvvz,herejust4this,,Comment,2,0,2
dbe6lev,2016-12-19 15:21:56-05:00,herejust4this,,"> When using assembly language, you can use labels just like you did in the code that you wrote, the assembler will compute the offset and include it in the object code. You dont have to worry about PC offset in assembly.

This makes so much sense. I was thinking it was a little more complicated than this. I just need to put the label of the memory position I want and it will compute the PCoffset for me. Brilliant!

I do have another question if you don't mind. How does one go about computing offsets or memory addresses? For instance, above you said that 9 bits would be able to ""reach"" 2^8 memory addresses. How do you know it would be 2^8? Do you just subtract 1 from 9 to get the exponent?",5iwvvz,t1_dbby9r9,Graysless,,Reply,1,0,1
dberrrw,2016-12-19 22:53:56-05:00,Graysless,,"With a N-bit signed 2's compliment integer you can represent numbers from -2^(N-1) through 2^(N-1) -1. So with 9 bits, we can represent -2^8 through -2^8 -1. But keep in mind that the PC will be pointing to the location directly following the location of the instruction, so you have to take that into account in the PC-offset. For example, if you have a LD instruction at location x3003, and you want to load the data from x3005, you would have a PC Offset of x01 because the PC will be x3004 when the offset is added, so in order to get to x3005, you only have to add one. Also, if you were trying to load something from location x3002, the PC offset would be x-02. This also means that your code will not assemble if you have a label that is greater that -2^8 or 2^8 -1 away from the location after the instruction that is trying to access it.",5iwvvz,t3_5iwvvz,herejust4this,,Comment,2,0,2
5iwfdo,2016-12-17 15:27:32-05:00,finessseee,Best recommended laptop to run Linux for coding?,"So I think I'm sold on going with Linux over Windows. I don't know much about it, but I'm gonna learn since it seems like the best way to grasp programming and setting things up the way you want. Does anyone have any Linux advice or a recommended laptop to get that runs Linux? I'm looking to spend roughly $1400~ max. ",,,,,Submission,2,0,2
dbbi34b,2016-12-17 16:22:27-05:00,dirac_majorana_weyl,,Any of the thinkpads work well. ,5iwfdo,t3_5iwfdo,finessseee,,Comment,4,0,4
dbbgn53,2016-12-17 15:45:01-05:00,RozenKristal,,"Anything work, but if u can have an ssd, it will be better.",5iwfdo,t3_5iwfdo,finessseee,,Comment,3,0,3
dbbrm49,2016-12-17 20:33:25-05:00,justlikestoargue,,"Can't second the SSD comment enough. Writing code isn't resource intensive, but after using an SSD I will never go back to an HDD.",5iwfdo,t3_5iwfdo,finessseee,,Comment,2,0,2
dbbk26b,2016-12-17 17:14:03-05:00,cravenspoon,,"Is this for college? If so, $1400 is going to be overkill for anything you need to do. But, you can always dual boot and run Windows for gaming or whatever.  

I agree with RozenKristal. Get an SSD, makes life easier.  

Also keep in mind that laptop drivers can be an issue (or were the last time I put a linux distro on my laptop) so ideally you'd want to check and make sure the distro+your laptop have decent support.",5iwfdo,t3_5iwfdo,finessseee,,Comment,2,0,2
5ittis,2016-12-17 04:49:53-05:00,inquisitive_idgit,How do I detect when one text roughly quotes or paraphrases another?,"Student reads a text and during the course of the year writes a (book-length) thesis on it.  
We want to semi-automatically flag all instances of quotation or paraphrase of the original text.   Then we're going to try to do statistics to see if quotations correlate with overall course grade, etc.  

But how do we semi-automatically find paraphrases and quotations?   I've seen algorithms to detect plagarism, but they generally provide only an overall scalar score of text similarity;  for our purposes, we need to detect sentence-level paraphrases or rough quotations.    How?? 
",,,,,Submission,1,0,1
5itc2a,2016-12-17 02:00:02-05:00,finessseee,Do you prefer coding on a Mac/PC/Linux?,,,,,,Submission,21,0,21
dbaw6zp,2016-12-17 03:58:18-05:00,5225225,,"Linux, personally. My experience of coding on Windows was painful.",5itc2a,t3_5itc2a,finessseee,,Comment,28,0,28
dbb1fkd,2016-12-17 08:54:00-05:00,Sonder777,,How so?,5itc2a,t1_dbaw6zp,5225225,,Reply,1,0,1
dbb1xq4,2016-12-17 09:13:44-05:00,5225225,,"I like having a proper terminal with nice fonts and transparency. Ideally a tiling window manager.

The default terminal in Windows is shit. I think they *just* allowed it to be fullscreen rather than just taking up half the screen in Windows 10, which is nice of them. You probably could hack support on using third party tools, but then you're reliant on them working. Also Windows itself apparently likes to force updates on you, and actively fights methods to disable that. I'd rather update when *I* want to.",5itc2a,t1_dbb1fkd,Sonder777,,Reply,10,0,10
dbb77oj,2016-12-17 11:50:44-05:00,KronktheKronk,,Yeah you're supposed to use an IDE with Windows,5itc2a,t1_dbb1xq4,5225225,,Reply,4,0,4
dbb7lss,2016-12-17 12:00:36-05:00,Madsy9,,"But the thing is that software development can't be fully contained in a handy IDE. At least not without the operating system making it possible. And Windows makes it impossible.

How? For one, Windows' POSIX compliance is far from stellar. Devices are not mapped as files, the file system does not have a single root like '/' and other partitions can't be mapped on the root. Windows does not emulate proper terminals, nor does it support proper shell scripts out of the box. All the handy tools you have in OSX and Linux like cut, more, less, xargs and sed aren't there either. And there is no packet manager and no default directory to look for headers and libraries.

Now, IDE's like MS Visual Studio are great, no doubt about it, and MS have one of the best debuggers experiences around. But it's still not enough when their OS insists on doing everything differently than everybody else. And Visual Studio solutions will never be a full replacement for make.

When I have to do development on a Windows box, the first thing I do is to install MSYS and GNU Coreutils for Windows. And sometimes MinGW. At least then I got make, coreutils, a semi-working bash shell and a workaround for the stupid file system.",5itc2a,t1_dbb77oj,KronktheKronk,,Reply,7,0,7
dbb8djl,2016-12-17 12:20:02-05:00,drummyfish,,"I switched from Windows IDEs to Linux and I have to agree. IMO IDEs are good if you're working on a big project for a long time in your company. For the fun, fast and flexible programming there's nothing like a good Linux shell. So to answer OP's question: Linux.",5itc2a,t1_dbb7lss,Madsy9,,Reply,3,0,3
dbbke57,2016-12-17 17:22:46-05:00,PM_UR_B00BS_4SCIENCE,,"This is all true, but as of a few months ago Windows 10 includes a complete Ubuntu Bash shell, making most of the above moot.",5itc2a,t1_dbb7lss,Madsy9,,Reply,2,0,2
dbbnw0k,2016-12-17 18:55:00-05:00,Madsy9,,"That sounds great. I won't touch Windows 10 with a ten foot pole, but that's for completely different reasons. For people who have no qualms regarding Windows 10, that sounds like quite the upgrade.",5itc2a,t1_dbbke57,PM_UR_B00BS_4SCIENCE,,Reply,1,0,1
dbbpjw6,2016-12-17 19:38:41-05:00,PM_UR_B00BS_4SCIENCE,,Windows 10 as of the last update is pretty damn good. It has only one glaring flaw now and that would be the way it forces updates. In many ways it's the best OS now. I still like the window gui of MacOS *a little* better but it's pretty close. Meanwhile the touchscreen aspects are clearly superior to apples stupid touch bar.,5itc2a,t1_dbbnw0k,Madsy9,,Reply,1,0,1
dbbci7d,2016-12-17 14:02:04-05:00,HenryJonesJunior,,"> How? For one, Windows' POSIX compliance is far from stellar. Devices are not mapped as files, the file system does not have a single root like '/' and other partitions can't be mapped on the root. Windows does not emulate proper terminals, nor does it support proper shell scripts out of the box.

Please elaborate how any of those are required for development to be contained in an IDE.  Visual Studio with F5 deployment gets along just fine, as do many other IDEs.

> And Visual Studio solutions will never be a full replacement for make.

Funny, a number of serious projects seem to build just fine with solution files.",5itc2a,t1_dbb7lss,Madsy9,,Reply,0,0,0
dbayeeg,2016-12-17 06:14:01-05:00,HelloYesThisIsDuck,,"Linux. Virtual desktops make keeping different programs organized, Unix tools, Guake terminal at the touch of a button, etc. Plus you can personalize your environment the way you like it, and that makes it most efficient for you.

Can't stand Windows and all its limitations.

Never tried OSX. It's probably got a few of the same advantages as Linux, but their hardware is way too overpriced (and extremely dated, nowadays).",5itc2a,t3_5itc2a,finessseee,,Comment,10,0,10
dbb0lpj,2016-12-17 08:17:57-05:00,watsreddit,,"Linux. There's no substitute for a Unix CLI. OSX would be fine, but then you have to use Apple hardware, which has been declining pretty badly in quality. Hackintosh requires as much work as Linux for less customization, and even though you can get homebrew on osx, my impression is that regular Linux package managers have more packages or more easily compiled from source.",5itc2a,t3_5itc2a,finessseee,,Comment,8,0,8
dbaznz9,2016-12-17 07:30:25-05:00,litepotion,,"Depends. But for almost everything I would say OSX which would be my MacBook. Then Linux cuz it's easy to load up on my desktop (hackintosh hardware compatibility make it impossible for my desktop) then lastly windows. I hate coding on windows it's horrible... But I'm on win7 still. I'm considering on upgrading to windows 10 for the native Unix support.

As for programming languages. I'm all over the place depending on my work. My recent work was with Java. In the past I worked on embedded c projects, web languages, and some r search with Python.",5itc2a,t3_5itc2a,finessseee,,Comment,5,0,5
dbbctfm,2016-12-17 14:09:40-05:00,anamorphism,,"you use whatever operating system you're targeting, if you happen to be targeting one. if you want your code to be cross-platform, you should be testing on all platforms anyway and be comfortable with all of them.

next in line is that you use whatever operating system the tech you want to use targets, if applicable. obviously if you want to use directx, you're using windows. if you want to use full-fat .net, windows. if you want to check out swift, probably want os x still. this is less of a restriction than the first one.

third, you use whatever operating system everyone else you're working with is using. the first two are going to dictate this for the most part, but not always. there's not much worse than someone trying to stick to using linux in a windows shop. basically you want to make helping each other and collaboration as easy as possible. the last thing you want there is people being uncomfortable with the operating system you are using.

last and, in my opinion, least, you use whatever you prefer.

as a side note, at some point you're going to run into someone that only develops in a terminal-based text editor like vim or emacs. they'll preach to you about how much better the experience is or about how only people that don't know how to program use ides. don't get caught up in it. in my 10 years of professional coding experience, those people tend to be a hassle for everyone else and are almost always less productive. of course, if you end up working for a company that only uses vim or emacs or whatever, i'd still recommend you do the same.",5itc2a,t3_5itc2a,finessseee,,Comment,5,0,5
dbbhm7b,2016-12-17 16:10:12-05:00,bot_bot_bot,,This is the correct answer! :-),5itc2a,t1_dbbctfm,anamorphism,,Reply,2,0,2
dbax8ac,2016-12-17 04:59:57-05:00,zetas2k,,I prefer OSX. It's got the best of both worlds imo. Unix command line and better-than-windows window manager. ,5itc2a,t3_5itc2a,finessseee,,Comment,16,0,16
dbb81qv,2016-12-17 12:11:48-05:00,artillery129,,"I personally use OS X, having said that, the WM in OS X leaves a lot to be desired next to my favorite WM: i3",5itc2a,t1_dbax8ac,zetas2k,,Reply,1,0,1
dbb1i2e,2016-12-17 08:56:49-05:00,james4765,,"OSX for the day job - there's some closed source software we use that won't run on Linux, although our sysadmins and DBAs use it.

For home use? Linux all the way. I love my OSX desktop, but dealing with Arduino serial links and imaging SD cards is more of a hassle than it needs to be.",5itc2a,t3_5itc2a,finessseee,,Comment,4,0,4
dbbvgxs,2016-12-17 22:14:01-05:00,None,,[deleted],5itc2a,t1_dbb1i2e,james4765,,Reply,1,0,1
dbbxyd9,2016-12-17 23:23:49-05:00,james4765,,"I use dd as well, I'm just more used to the Linux command line version of the tools. OSX also gets grumbly about dealing with NTFS, without jumping through some hoops.",5itc2a,t1_dbbvgxs,None,,Reply,1,0,1
dbav6fa,2016-12-17 03:03:12-05:00,high_side,,Matlab is the One True OS.,5itc2a,t3_5itc2a,finessseee,,Comment,10,0,10
dbb4p5r,2016-12-17 10:43:47-05:00,tyggerjai,,Emacs!,5itc2a,t1_dbav6fa,high_side,,Reply,2,0,2
dbbcnnm,2016-12-17 14:05:42-05:00,HelloYesThisIsDuck,,It's a pretty good OS. I wish it had a good IDE though.,5itc2a,t1_dbb4p5r,tyggerjai,,Reply,3,0,3
dbb2hhm,2016-12-17 09:33:30-05:00,roflberry_pwncakes,,I prefer to set up my coding environment to match the production OS. I haven't found the OS to be a limiting factor for development. ,5itc2a,t3_5itc2a,finessseee,,Comment,2,0,2
dbb7p44,2016-12-17 12:02:54-05:00,screamconjoiner,,"At first I was all about Linux with Emacs. Now I write in Windows using Emacs and Powershell. Casualty of working at a company that religiously uses Microsoft (SharePoint, Azure VMs, etc). Powershell is really the difference maker for me on Windows, it makes my life tolerable.",5itc2a,t3_5itc2a,finessseee,,Comment,2,0,2
dbb7rk2,2016-12-17 12:04:38-05:00,finessseee,,What kind of laptop do you have? ,5itc2a,t1_dbb7p44,screamconjoiner,,Reply,1,0,1
dbb9f26,2016-12-17 12:45:41-05:00,screamconjoiner,,"A Dell 15inch touchscreen, an Inspiron 5559. I resurrected a 5 year old dell for my Linux box at first, but had too many problems trying to RDP into the VMs and I was never able to get SharePoint mapped correctly so I went back to the newer (and much nicer) Windows machine. 

Also I was having a number of issues with getting various Selenium drivers working on Linux. Windows really isn't as bad of a development platform as I initially thought but in a perfect world I'd probably prefer to work on Linux.",5itc2a,t1_dbb7rk2,finessseee,,Reply,1,0,1
dbbaeew,2016-12-17 13:10:02-05:00,finessseee,,Does it take awhile to learn how to use Linux? Right now I'm just starting out in CS and don't wanna confuse myself as I learn languages. I really wanna switch to windows after having a Mac for 10 years. I'm just bored of the software honestly. Any advice? ,5itc2a,t1_dbb9f26,screamconjoiner,,Reply,1,0,1
dbbbfbe,2016-12-17 13:35:28-05:00,screamconjoiner,,"Well I've used all of them and there are trade offs to all. I did some development on Mac using NetBeans, and thought it was great. I think it depends on what languages you want to learn. 

Linux requires a bit of time to get used to, but the terminal/command prompt/shell experience is by far the best. It's what I used all throughout undergrad and I really enjoyed it.

I think the best thing would be to pick something early on and stick with it. I chose Linux, Emacs, and Git and used them religiously. A lot of my friends use Sublime text editor and swear by it.

At the same time it's hard if you're expected to use Microsoft products like Visual Studio for class and you're on a different operating system.

I think the easiest approach to learn Linux is to run whatever operating system you're used to and use something like VirtualBox (or pay out for better virtualization software if you can afford it) to run Linux. Linux Mint or Ubuntu worked really well for me.

Edit: VirtualBox, not OpenBox",5itc2a,t1_dbbaeew,finessseee,,Reply,2,0,2
dbbi1q1,2016-12-17 16:21:25-05:00,dirac_majorana_weyl,,If you've been programming on a Mac then linux will be a piece of cake for you. Personally I would recommend not using Windows if you want to get into tech when you graduate. I've interviewed lots of candidates and have seen that people who use Windows over OSX or Linux get pigeonholed into jobs at Windows-only companies. Want to work at a big Silicon Valley company or a Silicon Valley startup? You gotta be familiar with Linux and OSX. Most companies won't give you anything but a MacBook when you work there. ,5itc2a,t1_dbbaeew,finessseee,,Reply,1,0,1
dbbfcg9,2016-12-17 15:12:31-05:00,Treyzania,,"ITT: Don't use Windows.

In all seriousness, the only thing that Windows is good for as far as development goes is Unity3D.  I've been using Linux as my daily driver since August this year and have not had any major problems (aside from Nvidia drivers), but I had to work on a small project for a class in Unity3D and it was incredibly annoying.  It *worked* yes, but there are still things that need to be worked on to make it as painless as it is on Windows.

So if the only videogames you play also run well on Linux (Minecraft, Dwarf Fortress, etc.), and you don't use specialized software that won't run well in WINE then there's really no reason not to switch.",5itc2a,t3_5itc2a,finessseee,,Comment,2,0,2
dbbfjdj,2016-12-17 15:17:12-05:00,tyteen4a03,,"Mac.

It's UNIX and it's got an UI that's actually eye-pleasing and intuitive to use. (I can never find a Linux UI that's quite what I want)",5itc2a,t3_5itc2a,finessseee,,Comment,2,0,2
dbbyczw,2016-12-17 23:35:55-05:00,the_one_hit_wonder,,Check out elementaryOS,5itc2a,t1_dbbfjdj,tyteen4a03,,Reply,1,0,1
dbbj65z,2016-12-17 16:50:34-05:00,drobilla,,"Linux, or any suitable free n*x, by a wide margin.
Macs can be made tolerable.
Windows is a torture session.",5itc2a,t3_5itc2a,finessseee,,Comment,2,0,2
dbb5ym0,2016-12-17 11:18:24-05:00,huck_cussler,,Linux.  OSX if you like spending more money than necessary.  Windows iff you are doing anything .NET.,5itc2a,t3_5itc2a,finessseee,,Comment,2,0,2
dbaytkk,2016-12-17 06:40:16-05:00,tiddeltiddel,,"I think it would be helpful to include your programming language(s) maybe.  
I am relatively new Java developer and I have only ever programmed on Windows except for a bit of bash scripting, so I don't really have an educated preference in this regard.",5itc2a,t3_5itc2a,finessseee,,Comment,1,0,1
dbbc60e,2016-12-17 13:53:40-05:00,dexterandd,,"Linux. 

Though bash on windows subsystem is quite decent if you have to work on Windows. At which point not you are basically working on Linux.",5itc2a,t3_5itc2a,finessseee,,Comment,1,0,1
dbbchva,2016-12-17 14:01:50-05:00,finessseee,,Would I be fine to get thru my CS courses with windows? ,5itc2a,t1_dbbc60e,dexterandd,,Reply,1,0,1
dbbhdcp,2016-12-17 16:04:00-05:00,bot_bot_bot,,"yes, I did most of my university stuff on windows, and TBH most of the instructions provided by the university regarding project setup, etc... were usually windows oriented (unless it was a linux specific project). I used windows for day to day college stuff, along with eclipse, GIT bash for git/ssh/general terminal work. I ran debian and ubuntu on VM's using virtual box to play around with them, and ubuntu on AWS too when doing cloud stuff.   
Check out [Vagrant](https://www.vagrantup.com/) it's an amazing bit of kit for all your VM needs. Well worth a look.   

Also, I think the best attitude with regards to working environment is to go with whatever is best suited for the project. I'd make a point of setting up my environment to go with the default for whatever it is I'm working on.   

Edit: I always use GIT bash instead of windows terminal, so my command line is Linux. ",5itc2a,t1_dbbchva,finessseee,,Reply,2,0,2
dbbivk6,2016-12-17 16:42:53-05:00,dexterandd,,"As u/bot_bot_bot said, using windows is quite possible. It is mainly just the ease of setting up the environment etc. that, at least for me is better in linux or mac. 

Worst case, you can run stuff easily in a linux virtual machine or dual boot later if you want to. I would advise you to familiarize with the linux command line though as depending on your job, you may be working through ssh on some linux environment. ",5itc2a,t1_dbbchva,finessseee,,Reply,1,0,1
dbbes2o,2016-12-17 14:58:21-05:00,idoescompooters,,"First Mac, then Linux. Never Windows.

I don't code much right now though.",5itc2a,t3_5itc2a,finessseee,,Comment,1,0,1
dbbf5kq,2016-12-17 15:07:34-05:00,achNichtSoWichtig,,"I see I preach into the choire, but one point wasn't mentioned so far, which for me weighs very heavy:

Linux is kind of built for coders:
 
1. The design principle *everything is a file* is extremly handy for programmers, because in coding this is called ""string manipulation"" and every basic language has huge standard library support for those kind of things. 

2. Also Linux is by nature more heavily based on being controll by command line interface then windows, which is also very nice for coders. A clean terminal is way more easy to use, then programs that are mainly ment to be used with a GUI.
",5itc2a,t3_5itc2a,finessseee,,Comment,1,0,1
dbbln4s,2016-12-17 17:55:41-05:00,codythecoder,,"I use widows for everything, including coding. However I have occasionally used Ubuntu, and frankly it's been the best coding experience I've ever had. Missing a dependency in Ubuntu? sudo apt-get. Missing a dependency in Windows? Find a binary online, download it, install it. And that's if you can find a binary, I've had to compile programs myself and it always makes me wish for Ubuntu.",5itc2a,t3_5itc2a,finessseee,,Comment,1,0,1
dbbmilr,2016-12-17 18:18:40-05:00,Devvils,,"Prefer linux BUT Windows 10 has some new features that make it much more useable these days.  

- virtual desktops

- native [Ubuntu/bash on windows](http://www.howtogeek.com/265900/everything-you-can-do-with-windows-10s-new-bash-shell/).  It replaces putty & cygwin and is better than both. I tried apt-get imagemagick and was blown away when it actually worked! Only annoying factor is there is no native support for windows shares but apparently you get around that with samba.",5itc2a,t3_5itc2a,finessseee,,Comment,1,0,1
dbdtkmo,2016-12-19 10:59:48-05:00,andybmcc,,"A wise man once told me ""The best OS is the one they pay you to develop on.""",5itc2a,t3_5itc2a,finessseee,,Comment,1,0,1
5isu6s,2016-12-16 23:47:28-05:00,Superinduction,Doing horrible in ComSci. Need advice,"Freshmen college student here. Took three courses in first semester of ComSci and absolutely screwed up big time. Currently hovering around 1.8 GPA which is considered as academic probation in my school. The three courses I took were intro programming, discrete math 1 and Calc 1.

Discrete math was hell. I couldn't fully grasp the concept that was taught in class. Propositional logic, Predicate logic and binary relations were fairly straight forward. Everything after that such as proofs and combinatorics was when I got lost. I spent majority of my time studying discrete math rather than Calc 1 and programming. Went to office hour EVERY WEEK to ask for help. Ended up with 70% before finals then failed the whole course because I didn't pass the final exam.

Calc 1 wasn't as brutal as discrete math. I think I would've done better in Calc if I have done more practice. Ended up with a C in that class.

Intro programming was simple, only learned intro python. Ended up with a B even though I didn't do much studying

Is there any advice that can help me improve my letter grade for next semester? Thanks.
",,,,,Submission,4,0,4
dbaqkh9,2016-12-16 23:59:32-05:00,znick5,,Better prioritize your time. It seems like with only three classes you should of had plenty of time to study and do better. You also might be a little weak in math. Not everyone can be a math genius. I struggle in it also. Next semester don't double load yourself with math. Finish the calc progression and then do the discrete progression.,5isu6s,t3_5isu6s,Superinduction,,Comment,5,0,5
dbartu1,2016-12-17 00:43:14-05:00,RHC1,,"If you are not strong in math I would take 1 math course a semester until you start to feel comfortable. I thought I wanted to be a psychologist most of high school so I goofed off in math. Wasn't until senior year I changed my mind and tried to get serious. I had a lot of catching up to do when in college because my math skills were underdeveloped. The key to getting well at math is to practice, practice, practice until you feel comfortable. I started with one math course per semester for my first year. After that I felt more confident in my math abilities and moved to do 2 per semester if I had to. Overall I was a B student in math and I was proud of that considering I was a C- student in high school in my math subjects. I would practice constantly for calc, discrete, linear, prob & stats, and calc 2 and it took me sometimes a while to grasp concepts. I went to to tutoring and office hours when I deemed necessary. Don't overload
Yourself if you're weak in a subject instead do small increments and study hard in them and you should do okay at the very least. Hope this helps. ",5isu6s,t3_5isu6s,Superinduction,,Comment,1,0,1
dbbc13d,2016-12-17 13:50:15-05:00,CaptainBland,,"With maths the trick is just to practice a lot. Find problems similar to those on your course and do them all. It can be kind of painful and time consuming, but if you're serious about doing better in your courses it's well worth doing. ",5isu6s,t3_5isu6s,Superinduction,,Comment,1,0,1
dbbhz8b,2016-12-17 16:19:35-05:00,Nattfrosten,,Khanacademy.org,5isu6s,t3_5isu6s,Superinduction,,Comment,1,0,1
dbavztr,2016-12-17 03:47:15-05:00,one_creed,,"That's a heavy load for your first semester. Always try to mix it up with some GEs. But nonetheless, it's your first, start allocating more time to your weak spots and you'll do just fine.",5isu6s,t3_5isu6s,Superinduction,,Comment,1,0,1
5is1ye,2016-12-16 20:37:28-05:00,goblyn7,IT Certification Questions,"I was thinking about going for a certification or two, cause why not. So could I have a relatively compact list of what I would generally need to know something about for either A+ or Network+? ",,,,,Submission,0,0,0
dbalqzs,2016-12-16 21:35:01-05:00,dxk3355,,"Are you going to computer science like programming or going for IT like networking?  In the programming world A+ is pointless since it's hardware related.  Network+ I don't know about and I think I might only know a single person that has it (he has like 30 certs).  Again, the only people that would have it are IT people.    

In the computer science world certifications aren't worth the paper they are printed on.  People really want to see open source contributions or project history.  

The computer security and IT fields really like certifications.",5is1ye,t3_5is1ye,goblyn7,,Comment,1,0,1
5ir2dx,2016-12-16 17:10:25-05:00,The_Code_Runner,"Friend offered to trade my Dell XPS 15 512ssd 16gigs of ram for a 2014 Macbook pro i7, 8gigs, 256gigs","Hi everyone,
I am a student about to go to college and purchased this dell xps 15. I like ecerythign about it, however, i would like to be able to publish ios apps, which you need a mac for. My friend offered to trade, do you guys think im getting ripped off? 

Thanks!",,,,,Submission,0,0,0
dbac5y7,2016-12-16 17:25:27-05:00,RHC1,,"Well, you're trading your newer model laptop for an older model. I mean I would just ask for some cash with it too. Macs are pretty reliable from what I've seen and the XPS is a solid laptop, but be aware it's $100 a year to be a developer for the App Store and the fact you're trading for less hardware and having the added expense of getting the privilege to publish apps to the App Store I would want a little extra money. ",5ir2dx,t3_5ir2dx,The_Code_Runner,,Comment,3,0,3
dbae8xu,2016-12-16 18:15:49-05:00,The_Code_Runner,,"Hmm thanks for your advice! I'm going into software engineering, and I've heard that a Linux based os is essential",5ir2dx,t1_dbac5y7,RHC1,,Reply,2,0,2
dbai1v5,2016-12-16 19:53:11-05:00,RHC1,,I used a windows laptop and put Linux OS on it. You can dual boot ubuntu or use virtual box. Macs are great though since it's unix based and you can do iOS development. Doesn't matter though if you don't have Linux. At my school we just used putty and connected to a unix server and I did all my programming on their when I had to. I wanted a mac for the same thing but now that the news are so much and they apparently have issues the only other choice is the older air. I say if you really want the mac do the trade just make sure you get some money with it too I think that's a fair deal.,5ir2dx,t1_dbae8xu,The_Code_Runner,,Reply,2,0,2
dbakd0u,2016-12-16 20:56:16-05:00,The_Code_Runner,,Hmm thanks for the advice!,5ir2dx,t1_dbai1v5,RHC1,,Reply,2,0,2
dbakfow,2016-12-16 20:58:16-05:00,RHC1,,No problem! Good luck!,5ir2dx,t1_dbakd0u,The_Code_Runner,,Reply,1,0,1
dbajwiz,2016-12-16 20:43:40-05:00,Wartz,,"3 years from now you'll get 800 plus for that MacBook, while the dell will struggle to sell for 300",5ir2dx,t3_5ir2dx,The_Code_Runner,,Comment,0,0,0
5iqi6r,2016-12-16 15:26:18-05:00,animagraffs,Question on basic computer processing,"Animagraffs research team here. We're learning how microprocessors work and we think we have the steps/ path of information down but would like some feedback. Here's what we have:

1. Fetch - The input (from a user device like a keyboard or mouse) is sent through the input/ output (I/O) bus to the control unit. The control unit retrieves an instruction via the address bus from the memory (either RAM or ROM).

2. Decode - The control unit ""decodes"" the instruction into commands that the ALU can understand and sends them to the ALU. The control unit also sends the input data to be processed to the ALU via the data bus.

3. Execute - The ALU, with the use of registers (which act as small, temporary storage space), executes those commands on the given data and produces and sends output to the control unit via the data bus.

4. Store - If the output needs to be stored, the control unit sends the output data back to the memory (RAM, in this case) via the address and data bus. The output data is also sent to a user device (like a monitor) via the I/O bus.

A few questions: 
1. How does the control unit know which instruction it needs?
2. What does the ""decoding"" process look like? The instruction from memory is in bytes, right? What form (or language) does the ALU need to understand the instruction?
3. What sort of computation, if any, is performed by the ALU for basic functions (like typing)?
4. In what situations would the output need to be stored?

Any help would be appreciated. If we're off on any of this or if you have any great resources to help point us in the right direction, let us know! Thanks!",,,,,Submission,1,0,1
dba8nde,2016-12-16 16:07:56-05:00,pencan,,"This is all dependent on the type of processor you're talking about. Pipelined, in order, out of order, support for VM, support for interrupts, etc. 

Can I ask where you're getting this information? 


1. Fetch has nothing to do with user input. It simply loads the instruction at the current PC (program counter) and increments the PC. The PC is simply an internal counter which determines the next instruction to fetch. Usually, this is from the icache. 


2. This is implemented as a control ROM where bits from the instruction index into the ROM and it spits out control signals that tell the hardware how to deal with an instruction.  The processor implements a micro architecture which understands the ISA that the program is written with (x86, ARM, ALPHA, etc). 


3. A basic instruction is not something like typing. A basic instruction is something like ADD R1, R2, R3. (Add the contents of register 2 and 3 and store in R1).  There are also loads and stores to memory. 


4. You need to store to memory very often. Any non-trivial program will need more than the limited number of registers you have and then the data 'spills' to memory. For example, storing an array of 100 integers when you have 8 registers.


Let me know if you'd like any help with this project. 


Source: Working on MS is computer architecture ",5iqi6r,t3_5iqi6r,animagraffs,,Comment,4,0,4
dbaccj1,2016-12-16 17:29:41-05:00,PastyPilgrim,,"It's really important to note that processors (especially those with different architecture philosophies) work *incredibly* different from one another. The science behind computer architecture is far more complex than you'll be able to sum up in 4 simple steps. To get you started, I'll summarize the five-stage, single-cycle, MIPS architecture, which is often the architecture that students will learn first when studying this in college.

The five stages are: Fetch, Decode, Execute, Memory, Writeback.

Fetch: Simply read an instruction from memory that is stored at the address indicated by the Program Counter. This instruction is fed to the Decode phase.

Decode: This phase, which starts with just an instruction (e.g. 00000000001000010001100000000111) will determine what to do with that instruction. MIPS has several instruction types (R, I, and J). I've just given an R type, which contains an opcode, 3 register addresses (1 output and 2 input), a small integer used for some operations, and a function code that tells the ALU which mathematical operation to perform. This phase will send along the data from the two input registers (after reading them from the register file) to the execute phase.

Execute: This phase will perform any math/transformations that need to be done. It has an ALU, which uses a ton of circuits to perform math. For example, adding two bits (0 and 1) uses what is called an adder. An adder xors together the two bits to determine their sum. 0 + 0 = 0 ^ 0 = 0: 0 + 1 = 0 ^ 1 = 1: 1 + 1 = 1 ^ 1 = 0. In that third case, you would have circuitry in place to detect an overflow. 1 + 1 really equals 10, but the bit in the place of those original 1s would be 0, so that's what that adder unit detects. True addition would need a chain of adders to determine the new value of each digit in the binary number, with the ability to pass along overflow to the next adder. All ALU operations work like this, where you have circuits in place that take 1s and 0s over wires and transform them according to some pattern. The results of these operations get sent to the memory phase.

Memory: In this phase, if your instruction was intending to store information from the registers in memory, that write is now performed. If it intended to load something from memory, then that read is now performed and passed along to the writeback phase.

Writeback: Finally, the results of your instruction, whether that be math, or memory loading, etc. are written back to the registers to be used in a later instruction. At this point, you increment the program counter so that when you next fetch, it will be the next instruction in your program.

-------------------------

That is the simplest you can possibly explain one kind of CPU. Your current understanding has some obsession with user input which really has nothing to do with how the CPU functions. User input is handled by the programs that are being run on the CPU.

The science of computer architecture comes into play when you want to make better CPUs then what I've described. For example, there's no reason for Fetch, Decode, and Execute to just be sitting there doing nothing when an instruction is currently in the Memory phase. Because of that, there's a concept called pipelining where you perform operations in parallel, with each instruction utilizing a different function of the chip at the same time. Another example is with what are called conditional branches. Programs contain *tons* of conditional actions (e.g. if this happens, do this, if it doesn't happen, do this other thing). When those conditional actions come up, you want to be able to make a decision about whether or not the branch is going to happen or not going to happen before you waste the time waiting for the instruction to get all the way through your CPU. This topic is called branch prediction. Then there's the study of whether or not you want to have tons of very specific instructions that do complex things, or if you want just a few simple instructions that get combined together to perform complex tasks. That is the CISC vs. RISC debate. Then you have the topic of how decoding and controlling the machine should be done. One way to do is with a chain of circuits, but another is with what's called a micro-programmed CPU, which is a smaller/simpler CPU that exists in the chip of another CPU that has its own set of operations that it reads from a program to control the outer machine. In fact, that's almost certainly what the computer you're using is doing.

Anyway, computer architecture is unbelievably complicated. There's a reason that it's usually something reserved for graduate students.",5iqi6r,t3_5iqi6r,animagraffs,,Comment,2,0,2
dbam10j,2016-12-16 21:43:02-05:00,dxk3355,,"Like the others said this is super complicated, in college you usually take a class called 'computer organization' and end up white boarding something like this which is a very simple CPU which I think would help with questions 2 or 3 http://www.cs.carleton.edu/faculty/jondich/courses/cs208_w15/assignments/fig417.png


",5iqi6r,t3_5iqi6r,animagraffs,,Comment,1,0,1
dbbdskb,2016-12-17 14:33:41-05:00,xerxesbeat,,"^(Ack...  Okay, well.. having looked through some of the animagraffs work linked...  some ()^(hopefully constructive)^) ^criticism:

*Assuming ""microprocessor"" is used in the context of a home computer, I'll be addressing x86 and amd64 architectures*

> What sort of computation, if any, is performed by the ALU for basic functions (like typing)?

The ~~ALU~~* CPU will generally be performing a buttload of operations for programs running in the background.  *Which* operations are performed as a result of user input is entirely dependent on how the OS and programs running decide to handle a given input.

* Instructions that perform math use the Arithmetic Logic Unit in the CPU, but not all instructions are math operations, in the standard sense.

> What does the ""decoding"" process look like?  [...]  What form (or language) does the ALU need to understand the instruction?

The simplest way to make this level of the computer human readable is to read up on assembly languages.  Typically, one statement of assembler will [translate](http://ref.x86asm.net/coder64-abc.html) 1:1 for an instruction in machine code.

The ALU itself doesn't need to *understand* any language, the machine code is representative of the specific hardware io required to trigger the processor to perform the instruction it represents.  (e.g. the binary value for an ""add"" instruction will be the electric signal that switches the addition circuit to input/output values, directly)

> How does the control unit know which instruction it needs?  [...]  The instruction from memory is in bytes, right?

Yes, but not always the same number of bytes.  One of the ""register""s in the processor will be the IP (instruction pointer) / PC (program counter), which counts in integer numbers of bytes.

> In what situations would the output need to be stored?

Almost always.  If the CPU performs an instruction that, for example, multiplies two numbers, the result will typically be stored temporarily in a register.  If a later instruction does not use this value directly and immediately, any instruction that outputs to that register will *usually* modify or overwrite the value (sometimes, only partially)  ^(note: it is *sometimes* faster to perform the calculation again than it would be to store elsewhere:  furthermore, some output ()^(such as the carry flag for addition^)) ^(is usually intentionally discarded ()^(to the extent that there are actually two separate forms of addition in x86/x64, one that skips calculating a carry entirely)^)

> Fetch... Decode... Execute... Store...

No, no, no, no...  (This one is a personal opinion, but) with respect to the other material at animagraffs, this model would be bloat in a perspective of layman/technical content.  It's a long winded way to draw out ""The computer can input data.  The computer can compute with data.  The computer can output data.""  (again, personal opinion, but:)  This wouldn't be info, it would be fluff.  The reader already knows that the computer computes.

Perhaps...

&nbsp:

The microprocessor is an essential component of any modern home computer.  The term ""microprocessor"" itself simply refers to the concept of a CPU, or central processing unit, as implemented on a single or small set of integrated circuits.  ^(as opposed to one the size of a building, lol.  hence ""micro"")

1. The CPU interfaces with the computer primarily through two ""controller hub""s, commonly referred to as the north bridge and south bridge.  The south bridge is responsible for providing access to the CPU for ""slower"" features such as USB and networking, whereas the north bridge provides access to ""faster"" features such as RAM, graphics cards, and the south bridge itself.  (The north bridge has, in many processors, been integrated into the CPU chip itself, for performance reasons)

2. The CPU needs memory to function.  This memory is inside the CPU chip itself (caches), or can be externally connected to the CPU.  Within this (binary) memory, sequences of bytes can represent either *instructions* or *data*.  The CPU performs instructions -- or ""op""s -- sequentially, with the exception of special (""jump"", ""call"") instructions that provide which alternate instruction to perform next.  Data is accessible to the instructions, either in the form of variables the program generates while running, or constants that are stored alongside the program and immediately available when the program starts.  (see: statically vs dynamically allocated memory)

3.  A typical microprocessor design includes a central ""clock"".  Rather than keeping time, this ""clock"" simply cycles on and off, and serves to sequence instructions performed by the CPU.  ""Clock speed"" was once a primary source of increases in microprocessor performance as newer chips were designed

4. Due to physical limits of the materials common in CPU manufacture (namely heat dissipation), a typical consumer microprocessor now contains several ""cores"".  These cores are, essentially, duplicates of circuits within the CPU that, while not able to perform faster sequentially, allow the computer to perform several tasks simultaneously, using a concept known as ""hardware threading"".  (""software threading"" was already in use, allowing multiple programs to run at a time, but is a feature of the operating system rather than the microprocessor itself)

*anyway, it's just my two cents, but there's a lot of other introductory information out there on microprocessors than just ""it inputs, it outputs, and it computes"" :)*",5iqi6r,t3_5iqi6r,animagraffs,,Comment,1,0,1
5ipxxn,2016-12-16 13:47:00-05:00,pookei_,I need help studying for my Theoretical CS final.,"Hi reddit, I'm studying Computer Engineering in India and I have about 4 days to study for my Theoretical CS final. At this point all I can say is I'm barely acquainted with the subject and I need to finish studying for it so that I can at least pass the exam. I'd like to know about any resources where I can learn the basics so I can figure out wtf is going on in my textbooks so if anyone has any ideas as to where I can get a proper explanation for the basics, please post about it here. I'm sorry in advance if this is not the place for such a question; if it isn't then please let me know where I can go for some help with this.  
I'm listing the syllabus in case anyone needs a more in-depth idea about the stuff I need to know.  

&nbsp;


Module 1-Introduction:


* Alphabets, Strings and Languages  
* Chomskey hierarchy and Grammars.  
* Finite Automata (FA) and Finite State machine (FSM).  


Module 2-Regular Grammar (RG):


* Regular Grammar and Regular Expression (RE): Definition, Equivalence and Conversion from RE to RG and RG to RE.  
* Equivalence of RG and FA, Converting RG to FA and FA to RG. *Equivalence of RE and FA, Converting RE to FA and FA to RE.  


Module 3-Finite Automata:


* Deterministic and Nondeterministic Finite Automata ( DFA and NFA ): Definitions, Languages, Transitions ( Diagrams, Functions and Tables).  
* Eliminating epsilon-transitions from NFA.  
* DFA, NFA: Reductions and Equivalence.  
* FSM with output: Moore and Mealy machines.  


Module 4-Regular Language (RL):


* Decision properties: Emptiness, Finiteness and Membership.  
* Pumping lemma for regular languages and its applications.  
* Closure properties.  
* Myhill-Nerode Theorem and An application: Text Search.  


Module 5-Context Free Grammars (CFG):


* Definition, Sentential forms, Leftmost and Rightmost derivations.  
* Context Free languages (CFL): Parsing and Ambiguity.  
* CFLs: Simplification and Applications.  
* Normal Forms: CNF and GNF.  
* Pumping lemma for CFLs and its applications.  
* Closure properties and Kleene’s closure.  


Module 6-Pushdown Automata(PDA):


* Definition, Transitions ( Diagrams, Functions and Tables), Graphical Notation and Instantaneous Descriptions.  
* Language of PDA, Pushdown Stack Machine ( PSM ) as a machine with stack, Start and Final state of PSM.  
* PDA/PSM as generator, decider and acceptor of CFG Deterministic *PDA (DPDA) and Multi-stack DPDA.  


Module 7-Turing Machine (TM):


* Definition, Transitions ( Diagrams, Functions and Tables).  
* Design of TM as generator, decider and acceptor.  
* Variants of TM: Multitrack, Multitape and Universal TM.  
* Equivalence of Single and Multi Tape TMs.  
* Power and Limitations of TMs.  
* Design of Single and Multi Tape TMs as a computer of simple functions: Unary, Binary ( Logical and Arithmetic ), String operations ( Length, Concat, Match, Substring Check, etc )  


Module 8-Undecidability and Recursively Enumerable Languages:


* Recursive and Recursively Enumerable Languages.  
Properties of Recursive and Recursively Enumerable Languages.  
* Decidability and Undecidability, Halting Problem, Rice’s Theorem, Grebach’s Theorem, Post Correspondence Problem  
* Context Sensitivity and Linear Bound Automata.  


Module 9-Comparison of scope of languages and machines:


* Subset and Superset relation between FSM, PSM and TM.  
* Subset and Superset relation between RL, CFL and Context Sensitive Language.

&nbsp;

I understand that this may seem stupid to some of you and you might feel that I'd be better off studying rather than wasting my time on reddit but I could really use all the help I can get. Thanks!  

*edit: formatting

",,,,,Submission,0,0,0
dbbht9r,2016-12-17 16:15:17-05:00,achNichtSoWichtig,,"/u/lmbro has it formulated pretty harshly, but really covering all this topics in such short amount of time is near impossible.

I would suggest starting with the less mighty (dunno if its the right word in english) regular grammars, DFA and NFA. All this stuff is hiearchy based (check out chomsky hierarchy) and when you understand the basic idea of simple formal languages, you maybe can start to get the underlying idea without looking into the details (which you definetly won't make) of turing machines and more complex type of languaes to much. But really do this. You have to understand what languages are, you have to understand what a grammar is, what an automata is, and so on. This concepts are core. If you don't understand those, you will understand nothing.

For all this stuff you should easily find material on youtube. I would suggest to start there. Theoretical CS has scrict formalisms, so I would also suggest sticking with your textbook for your studies. The formalisms are not optional.",5ipxxn,t3_5ipxxn,pookei_,,Comment,2,0,2
dbaofrj,2016-12-16 22:52:47-05:00,None,,"You're fucked, sorry.",5ipxxn,t3_5ipxxn,pookei_,,Comment,1,0,1
5ipdde,2016-12-16 12:07:49-05:00,zSilverFox,Is there viable encryption that doesn't rely on the factoring and discrete logarithm problems?,,,,,,Submission,1,0,1
dbadci2,2016-12-16 17:53:34-05:00,thatwasntababyruth,,"Did you mean to specify 'viable asymmetric encryption'? As far as I know, most symmetric algorithms in use today (AES, usually) are not based around factoring or discrete logs.",5ipdde,t3_5ipdde,zSilverFox,,Comment,2,0,2
dbamcsr,2016-12-16 21:52:32-05:00,panker,,The one time pad. ,5ipdde,t3_5ipdde,zSilverFox,,Comment,2,0,2
db9zxff,2016-12-16 13:07:26-05:00,G3n3ralSh3rman,,"There's a category of cryptography [lattice based cryptography](https://en.wikipedia.org/wiki/Lattice-based_cryptography). Unlike DLP and factoring problems, it is quantum safe, but is not used that much yet, partly due to powerful quantum computers not being around, and partly due to patents that make them more expensive to use (specifically NTRUEncrypt). The security of these cryptosystems relies on the hardness of breaking lattice problems.",5ipdde,t3_5ipdde,zSilverFox,,Comment,1,0,1
dbabzrv,2016-12-16 17:21:33-05:00,kepael,,You're link is broken: https://en.wikipedia.org/wiki/Lattice-based_cryptography,5ipdde,t1_db9zxff,G3n3ralSh3rman,,Reply,2,0,2
dbaezif,2016-12-16 18:34:03-05:00,G3n3ralSh3rman,,Oops! Thanks for letting me know. ,5ipdde,t1_dbabzrv,kepael,,Reply,1,0,1
dba2mbb,2016-12-16 14:01:48-05:00,gman2093,,See ecliptic curve crypto,5ipdde,t3_5ipdde,zSilverFox,,Comment,0,0,0
dba4gkz,2016-12-16 14:39:30-05:00,Steve132,,"Elliptic curve crypto relies on the elliptic curve discrete logarithm problem, which obviously maps to the regular discrete logarithm problem over finite fields.",5ipdde,t1_dba2mbb,gman2093,,Reply,2,0,2
5ioqds,2016-12-16 10:11:54-05:00,adirondacktraveler,A question about difficulties of facial recognition software,"I'm a very entry level programmer and I'm curious about how difficult something like this would be to take on. This is really a satirical example but it still gets to the point I want. Say you want to make a program that will look through photographs and recognize the face of a certain person consistently from all different angles and lighting conditions. How difficult is this? Also go one step further and when that person is recognized by the program, replace their hands with little tiny baby hands automatically so that every picture becomes them with baby hands. Trump hands, if you will. Seriously though, how difficult is it for a program to recognize certain aspects of a picture and then edit or categorize based on those aspects? ",,,,,Submission,3,0,3
dbad908,2016-12-16 17:51:09-05:00,urielsalis,,"OpenCV has much of the work done for you, but yeah ots pretty hard to be accurate",5ioqds,t3_5ioqds,adirondacktraveler,,Comment,2,0,2
dba4uye,2016-12-16 14:47:46-05:00,BBQspaceflight,,"In contrast with the other poster, I think it is feasible to do _something_ with this. It might not be as good as any commercial image recognition software, and it won't be anywhere near the filters you have in snapchat these days, but if you find a good dataset you can get somewhere.

A good tool in image recognition are neural networks, especially convolutional neural networks for a job such as facial recognition. I would suggest reading up on this if you are interested in giving it a try yourself. Getting started on doing this yourself is not as hard as it once was: with the TensorFlow library in Python you can build a network without having to write the backpropagation (the math-heavy stuff) yourself.

To give you some perspective, with a fairly simple neural network you can categorize images in 10 categories (CIFAR dataset) with an accuracy of roughly 50%. Localizing things in images is a step above this, but it is definitely possible with deep neural networks.

Source: writing this while procrastinating on my convolutional neural network assignment.",5ioqds,t3_5ioqds,adirondacktraveler,,Comment,1,0,1
db9tq5n,2016-12-16 10:59:58-05:00,JudasRose,,"Not a super programmer or even highly educated on the topic but from what i know even the most complex ones are only ok at it whether its made made by the governments or companies that specialize in it. I would say extremely hard with the need of some new technique or technology to come about to make a radical difference in detection. It also depends on how you do it, like if you take a pic and then another pic to match at the same angle or if you're doing it at different angles, different lighting, wearing a hat? Lots of variables and the more you throw in the less accurate it becomes.

http://www.pbs.org/wgbh/nova/next/tech/the-limits-of-facial-recognition/",5ioqds,t3_5ioqds,adirondacktraveler,,Comment,1,0,1
5in760,2016-12-16 03:22:47-05:00,codythecoder,I can pause browser music on my phone but not computer. Why is that?,"When I play music through the chrome app on my phone, I get a push notification that lets me pause it from anywhere in my phone. I find it a little odd that there's no similar thing for desktop. It seems to me like the technologies wouldn't be too different, and computers often have a play/pause music button, so why can't they do the same as phones?",,,,,Submission,0,0,0
db9i47s,2016-12-16 04:03:12-05:00,i_am_suicidal,,"Most likely people just haven't written the code for it/deemed it unnecessary for PC use. 

It's definitely possible to do, as it is the same underlying technologies as you suggested.

Chromium is open source, so you could add it yourself if you know how to code it.",5in760,t3_5in760,codythecoder,,Comment,3,0,3
dbi8f4t,2016-12-22 09:58:14-05:00,saruko27,,You can right-click a tab in chrome and mute all sounds from it.,5in760,t3_5in760,codythecoder,,Comment,2,0,2
dbipy7d,2016-12-22 16:03:12-05:00,codythecoder,,"Yeah I know, but my problem is that when you un-mute it, the song doesn't start back where you left off.",5in760,t1_dbi8f4t,saruko27,,Reply,1,0,1
5im5w1,2016-12-15 22:46:36-05:00,Murikk,An optimization problem!,"Hey everyone,  

I am working on a project that requires an optimization algorithm and I'm looking for some input on where to start. If anyone knows the name of this problem (something like the _travelling salesman problem_, for example), please let me know. Any input is greatly appreciated. The problem goes like this:   

- Every week, a delivery person drops new packages off to 10 points across town and picks up any packages left from the previous week. This is a “return""
- Each drop off point has a variable maximum number of packages that it can hold
- Throughout each week, the packages are picked up somewhat randomly (perhaps assume exponential distribution?)
- This process repeats. The delivery person picks up the old packages and puts down new (different) ones each week. This is a “pickup""
- Given all the history of how many packages were picked up from where each week, we need to recommend how many to put down each week
- The delivery person needs to know how many to put down before leaving to drop packages off, so the current week’s return count is unknown
- The number recommended must be less than the maximum, and aim to minimize the number returned, while maximizing the number picked up randomly",,,,,Submission,1,0,1
db9j9yn,2016-12-16 05:03:21-05:00,chico_science,,"From your description it seems that the 10 points are independent? And thus each dropoff point could be treated as a separate problem?

If so take a look at the *news vendor problem*. It is not exactly the same (no prices involved) but seems similar, without definition of a sell price then this problem would have an easy analytical solution. If the 10 points are not independent you would have something like a capacitated news vendor problem and then it starts to look like traditional optimisation. You would have to have some assumption regarding the distribution, and depending if you discretise it or not, you might have to formulate the problem with different techniques.
",5im5w1,t3_5im5w1,Murikk,,Comment,3,0,3
dba1pow,2016-12-16 13:43:38-05:00,Murikk,,"Thanks! I missed the point that they are independent, so that clears it up a lot. I'll check out that link too!",5im5w1,t1_db9j9yn,chico_science,,Reply,1,0,1
5im2i8,2016-12-15 22:27:01-05:00,finessseee,I need help getting started with CS,"So this past semester I took Pre Calc and Programming 1 (java based). I failed pre calc and got an 88 in Programming, but I didn't really learn as much as I wanted to. I kind of went into it expecting to learn everything about programming that I need to know to get started when in reality it was just basic java assignments.. I could have learned more on the side, but I didn't have a laptop (my old white MacBook died out on me) so this made things a bit more difficult to use my free time learning how to code on the side. So this upcoming semester I'm gonna have to retake Pre Calc to fix my grade and then take Programming 2.. the thing is.. I feel like I don't know enough just yet to take that step forward with such limited knowledge. 

Any advice? 

Also, I'm gonna be purchasing a laptop for Christmas.. any suggestions? $1400-",,,,,Submission,1,0,1
db9a1jg,2016-12-15 23:17:35-05:00,rewardiflost,,"CS isn't just programming.   

It is a huge field that can get into computational theory,  electronics and architecture, human-machine interfaces, networking, security, or a dozen other directions.   

Practice and read.    Get some library books, or read from the material available online.   Learn a few programming languages - Java is a good, basic object-oriented language:  look at others like C, C++, Fortran, Basic, as well as compilers and Assembly.   
If you aren't doing well in math (Calculus), you may have trouble with theory and computing efficiency like ""Big-O"" calculations.   
Practice your math, even if you just want to program. 

Get any laptop (or computer) at all.    Programming doesn't require a huge investment.  Most languages can be learned on a very basic machine.   Personally, I prefer a desktop because I want a keyboard I can comfortably type on, and a screen big enough to scour code without straining my eyes.   

Study, practice, read, and ask questions.    Just keep going forward.    Eventually, you'll want to narrow down your interests - but don't hurry.  Try different things out.  
",5im2i8,t3_5im2i8,finessseee,,Comment,7,0,7
dba1l1p,2016-12-16 13:41:04-05:00,andybmcc,,">I could have learned more on the side, but I didn't have a laptop

You don't have open computer labs?",5im2i8,t3_5im2i8,finessseee,,Comment,1,0,1
5iie8y,2016-12-15 11:21:47-05:00,Vic_Vega_,Can/Do VPNs protect you from malicious code?,,,,,,Submission,1,0,1
db8dqnw,2016-12-15 11:47:03-05:00,Vic_Vega_,,Thanks for the responses.,5iie8y,t3_5iie8y,Vic_Vega_,,Comment,0,0,0
db8d7nd,2016-12-15 11:36:33-05:00,brennanfee,,"No.

EDIT: The only way that would be possible is if they provided their own DNS that would filter out known ""bad"" sites.",5iie8y,t3_5iie8y,Vic_Vega_,,Comment,0,0,0
db8z20u,2016-12-15 18:58:45-05:00,oneonetwooneonetwo,,"Well, depends on what the malicious code does and how and where it runs. Malicious code like viruses? Nup. Malicious code that tracks your location or intercepts sensitive communications? Potentially quite effective.",5iie8y,t1_db8d7nd,brennanfee,,Reply,0,0,0
db8d8e0,2016-12-15 11:36:57-05:00,BonzoESC,,"No.

A properly set up VPN protects you from malicious actors in privileged network positions (i.e. someone at Starbucks) between you and the other side of the VPN, and serve as a way to allow participation in a private network remotely.

For example, I have a small cluster of computers at home that I frequently want to SSH to from outside the house. Using a VPN, I can only open the VPN server port to the world, connect with a VPN client on my computer or phone, and get an IP address that routes inside my home network. I can either set up my VPN client to only send traffic to the home network through the tunnel, or to send any non-VPN traffic through the tunnel.

The second case *can* be a way to avoid some malicious code: a malicious barista (good band name) is in a position to modify any unencrypted HTTP traffic with something like a [WiFi Pineapple](https://www.wifipineapple.com), including but not limited to replacing every image with goatse, inserting malicious JavaScript into pages, rewriting links or forms, and tampering with any other file (including executables) served over an unencrypted protocol.

However, it's not complete protection, and don't use one just to avoid malicious code. An up-to-date web browser with aggressive sandboxing policies is much much much more useful protection against malicious JS. Sticking to well-curated app stores and a well-developed sense of trustworthiness for non-app-store software is more useful against malicious native applications.",5iie8y,t3_5iie8y,Vic_Vega_,,Comment,0,0,0
db8db81,2016-12-15 11:38:30-05:00,jorgejams88,,"Not at all, VPNs help you anonymize yourself when you go around the internet, so your traffic appears to come from somewhere else.

If you want to defend yourself from malware, some basic steps are:

* Get an adblocker.
* Turn off javascript. 
*  Basic common sense (Avoid clicking sketchy banners and going to strange sites).
* If there's an SSL error when you go to a website (the little lock on the address bar gets red), get out of that site immediately.

I hope this helps. ",5iie8y,t3_5iie8y,Vic_Vega_,,Comment,0,0,0
db8qdu4,2016-12-15 15:53:22-05:00,5225225,,"> * Get an adblocker.
> * Turn off javascript.
> * Basic common sense (Avoid clicking sketchy banners and going to strange sites).
> * If there's an SSL error when you go to a website (the little lock on the address bar gets red), get out of that site immediately.

One of these things just doesn't belong here. Turning off JS breaks a *lot* of websites. Noscript
is great and all, but it's high maintenance for the first week or so that you have it, where you
have to select what to allow to make a site actually work.
",5iie8y,t1_db8db81,jorgejams88,,Reply,2,0,2
db8giwb,2016-12-15 12:41:28-05:00,NonsenseInc,,"""No."" is not entierly true. Of course it does not imply protection, but it can be used to protect from malicious code by using software that does protect you on the vpn's gateway to the internet.",5iie8y,t3_5iie8y,Vic_Vega_,,Comment,0,0,0
db8y02n,2016-12-15 18:34:07-05:00,oneonetwooneonetwo,,Malicious code is such a broad term.,5iie8y,t1_db8giwb,NonsenseInc,,Reply,2,0,2
db8yc18,2016-12-15 18:41:49-05:00,BonzoESC,,"Yup! One person's malicious code is so often another person's test input or proof-of-concept.

Possibly nsfw, although no images are loading for me: https://web-beta.archive.org/web/20121215191658/http://www.evilscheme.org:80/defcon/ 

This is a hilarious prank at DEF CON (where it was used,) but could be seen as malicious and criminal in other contexts.",5iie8y,t1_db8y02n,oneonetwooneonetwo,,Reply,0,0,0
db9fxk5,2016-12-16 02:24:41-05:00,NonsenseInc,,"While it was quoted from the question I have to agree that it is a broad term. Still, while keeping to the intended question, a false positive is better then a missed one. A plain ""No."" discourages research and is too definitive, because while no 100% protection is possible without unplugging your internet an burrying yourself, some form of protection is possible utilizing a vpn.",5iie8y,t1_db8y02n,oneonetwooneonetwo,,Reply,0,0,0
5ieu3r,2016-12-14 21:06:58-05:00,TheMajesticArtichoke,Designing UI for children.,"So I'm building a math game for kids in Primary School (ages 7-11).

I know of so many resources that talk about designing the UI and UX etc.. however I've not been able to find any resources that are specific to designing the UI for children.

If anyone has any I'd welcome it greatly. Or if you have any opinions/question please post then. ",,,,,Submission,2,0,2
db7o12m,2016-12-14 21:48:38-05:00,fatzombi,,google gave me https://www.google.com/search?q=child+friendly+ui+pattern,5ieu3r,t3_5ieu3r,TheMajesticArtichoke,,Comment,2,0,2
db7zden,2016-12-15 03:41:11-05:00,INCOMPLETE_USERNAM,,"This falls under the subfield of Human-Computer Interaction, so try searching things like ""HCI children"" or ""IDEO children UI"". The latter gave me [this design blog article](https://uxdesign.cc/design-considerations-for-little-fingers-ad2a19ed3816#.gnjakbw96).",5ieu3r,t3_5ieu3r,TheMajesticArtichoke,,Comment,2,0,2
5icoi7,2016-12-14 14:41:20-05:00,CptCoolguy,what happens if you have a hdd from another PC with windows 10 in it then you move that hdd to a new pc would the OS work or do you need a new licence?,,,,,,Submission,0,0,0
db74hzj,2016-12-14 14:57:22-05:00,None,,[deleted],5icoi7,t3_5icoi7,CptCoolguy,,Comment,2,0,2
db74o4a,2016-12-14 15:00:28-05:00,CptCoolguy,,it's hardware componant thought that was something to do with computer science? ,5icoi7,t1_db74hzj,None,,Reply,1,0,1
db78zgk,2016-12-14 16:20:42-05:00,videoj,,You want /r/techsupport,5icoi7,t1_db74o4a,CptCoolguy,,Reply,3,0,3
db78ufc,2016-12-14 16:18:03-05:00,None,,[deleted],5icoi7,t1_db74o4a,CptCoolguy,,Reply,1,0,1
db79ysa,2016-12-14 16:38:53-05:00,CptCoolguy,,kool didn't know thanks man,5icoi7,t1_db78ufc,None,,Reply,1,0,1
db7j21v,2016-12-14 19:53:50-05:00,soggybiscuit93,,"The license for Windows 10 authenticates against the motherboard. 

Any different motherboard would require a new license, so it would not work.",5icoi7,t3_5icoi7,CptCoolguy,,Comment,2,0,2
db7jrta,2016-12-14 20:10:20-05:00,CptCoolguy,,thanks man,5icoi7,t1_db7j21v,soggybiscuit93,,Reply,1,0,1
5ic9zd,2016-12-14 13:34:25-05:00,finessseee,Mac/Windows/Linux for CS major?,,,,,,Submission,1,0,1
db71il9,2016-12-14 14:01:28-05:00,justlikestoargue,,Whatever you prefer.,5ic9zd,t3_5ic9zd,finessseee,,Comment,8,0,8
db86307,2016-12-15 08:50:18-05:00,i_takes,,"Right? The debate in which OS os better for development is pretty much over. I used windows and linux whilst at OSU and I use OSX at my job now. Nothing is superior, it just comes down to whichever you like more as your going to be using it a lot. You can set up a quality dev environment on any OS these days. 

That being said I am really starting to like the Mac for development, just because how easy the rest of the environment is to use. It allows me to just focus on my work and not have to worry about the little things. That and I also prefer to develop in a Unix based environment which pushes me towards linux and OSX. ",5ic9zd,t1_db71il9,justlikestoargue,,Reply,2,0,2
db7p3nz,2016-12-14 22:12:36-05:00,UrbanPizzaWizard,,"There's a good chance your teachers will personally own macs with OSX, but classes will most likely be taught using Windows for earlier classes and some universities teach Unix/Linux either as a class or using it or expect you to be familiar with it. I personally prefer using GNU/Linux as my personal OS, but I often have to switch to Windows for classes with strict requirements, but in my case, that changes as I go up the years to higher classes.


If we're ignoring software and just being practical using Windows and understanding Linux is probably your best bet. I'm the inverse of that and it makes my degree a hell of a lot easier, but most people prefer Windows for it's obvious advantages over GNU.",5ic9zd,t3_5ic9zd,finessseee,,Comment,1,0,1
db7tklp,2016-12-15 00:06:25-05:00,None,,"It doesn't really matter anymore. If you require a Unix based OS for an assignment/class, just boot it up in a virtual machine. ",5ic9zd,t3_5ic9zd,finessseee,,Comment,1,0,1
db84d40,2016-12-15 07:52:16-05:00,TashanValiant,,"Probably a bit late to the game.

My suggestion is always don't get a computer for your focus in CS, get something that will help you do your other duties as a student. Fact is, any OS is going to work for programming. And now with Unix is accessible in some supported form on all major OSs. Its a complete wash for CS. For development, use the OS that you are most comfortable with.

However, you will have your gen eds. You will have your physics and math classes. You will have courses that don't need a compiler or Unix. Buy the PC that will help you with that. It still goes down to personal preference, but look at the hardware available and see if it helps. Do you want a touch screen? Do you want to write notes? How portable do you want? How heavy? How large a screen? Do you potentially want to game in the off hours?",5ic9zd,t3_5ic9zd,finessseee,,Comment,1,0,1
db75hfz,2016-12-14 15:15:52-05:00,UncleAlanC,,"Since you might be working a lot with both word editing and compiling code, using OS X may appear as the better choice. You might take classes in development with iOS, so having a Mac as an OS is optimal for both code editing/compiling and regular school-related duties",5ic9zd,t3_5ic9zd,finessseee,,Comment,1,0,1
5ic85i,2016-12-14 13:26:16-05:00,CptCoolguy,what is the name given for the numbers that corresponds to the domain name it looks like a IP address?,,,,,,Submission,8,0,8
db71huy,2016-12-14 14:01:06-05:00,justlikestoargue,,It's an IP address.,5ic85i,t3_5ic85i,CptCoolguy,,Comment,17,0,17
db7401p,2016-12-14 14:48:04-05:00,CometThunder,,"You say it looks like an IP address, and that's because it is.  Why would you not think it's an IP address?",5ic85i,t3_5ic85i,CptCoolguy,,Comment,11,0,11
db72fd5,2016-12-14 14:18:31-05:00,wafflestealer654,,I'm confused. Can you give an example of what you are referring to?,5ic85i,t3_5ic85i,CptCoolguy,,Comment,5,0,5
db73jq3,2016-12-14 14:39:31-05:00,CptCoolguy,,"it's like aset off numbers you type in. It kind of looks like an IP address ie 17.9.263 etc. It's like a domain name but numbers, if you have the number you can access the website. Like 12.24.89 could be the number for reddit. I wanted to know what the name was for such a thing",5ic85i,t1_db72fd5,wafflestealer654,,Reply,3,0,3
db7442i,2016-12-14 14:50:11-05:00,wafflestealer654,,What you're seeing are IP addresses. IP addresses are the actual addresses of the computers on the internet. And a domain is just a way of eliminating the need to type in IP addresses.,5ic85i,t1_db73jq3,CptCoolguy,,Reply,11,0,11
db72jcp,2016-12-14 14:20:36-05:00,iknide,,"Domain name is just a prettier, human readable name that maps directly to an IP Address (or another domain)

So the numbers you're referring to are most likely the IP Address that correspond to that domain name

The job of DNS is to translate that name to the Ip Address, and there are entire dedicated servers that do this translation (like googles at 8.8.8.8)",5ic85i,t3_5ic85i,CptCoolguy,,Comment,3,0,3
db7bj9a,2016-12-14 17:08:31-05:00,AFineTapestry,,"Are you thinking of [Reverse DNS](https://en.wikipedia.org/wiki/Reverse_DNS_lookup) addresses?

`4.4.8.8.in-addr.arpa` for example.",5ic85i,t3_5ic85i,CptCoolguy,,Comment,3,0,3
db7oqdv,2016-12-14 22:04:22-05:00,cravenspoon,,"That's what I thought he was talking about.  

Otherwise it's an IP address or a domain name/FQDN. Perhaps a reverse/forward lookup zone in DNS if he just caught a glimpse and didn't know what he was seeing.",5ic85i,t1_db7bj9a,AFineTapestry,,Reply,2,0,2
db89r8a,2016-12-15 10:24:13-05:00,acromulent,,All of the questions here serve as proof that people still think CS is IT.,5ic85i,t3_5ic85i,CptCoolguy,,Comment,2,0,2
5ibrq8,2016-12-14 12:12:18-05:00,finessseee,Recommended laptop for CS major?,"I know that I can get by with a basic cheap laptop, but I'm looking to spend around $1300~ 
Any suggestions?",,,,,Submission,0,0,0
db6vzl1,2016-12-14 12:14:42-05:00,The_Code_Runner,,"I've got the Dell XPS 15, with the 512gig hardrive love it",5ibrq8,t3_5ibrq8,finessseee,,Comment,2,0,2
db6x08c,2016-12-14 12:34:44-05:00,finessseee,,"I was thinking about getting the XPS13, since it has Kaby Lake.. the 15 doesn't have KL.. do you recommend a 15 inch laptop over a 13?",5ibrq8,t1_db6vzl1,The_Code_Runner,,Reply,1,0,1
db6xqmu,2016-12-14 12:49:10-05:00,The_Code_Runner,,"Meh kl is a pretty marginal improvement mostly on the graphics side, but the 15 has an external GPU which is waaay better than kl. Plus the larger screen is nicer paired with cheaper storage, something that if you work on your computer as much as I do, will need a lot of",5ibrq8,t1_db6x08c,finessseee,,Reply,1,0,1
db6y795,2016-12-14 12:58:05-05:00,finessseee,,Hmm interesting. So do you think my best bet would by the XPS 15 then? ,5ibrq8,t1_db6xqmu,The_Code_Runner,,Reply,1,0,1
db6z0j9,2016-12-14 13:13:57-05:00,The_Code_Runner,,"In my personal opinion, yes. I did a lot of research on it and couldn't be happier",5ibrq8,t1_db6y795,finessseee,,Reply,1,0,1
db6z82f,2016-12-14 13:18:00-05:00,zanidor,,"A laptop with early 2000's specs is probably more than enough compute power to handle a full CS curriculum. Above that, get a laptop for reasons that make you happy rather than CS student reasons.

I will, however, +1 keeping an eye on Linux compatibility. I'd be surprised if you don't want to at least dual boot by the end of a CS degree.",5ibrq8,t3_5ibrq8,finessseee,,Comment,2,0,2
db6w4rv,2016-12-14 12:17:34-05:00,bluesnowman77,,"Whatever laptop you get, I'd recommend running OS X or Linux.",5ibrq8,t3_5ibrq8,finessseee,,Comment,3,0,3
db6wmoe,2016-12-14 12:27:20-05:00,finessseee,,Why not windows? ,5ibrq8,t1_db6w4rv,bluesnowman77,,Reply,1,0,1
db6x593,2016-12-14 12:37:30-05:00,bluesnowman77,,Stuff I need for classes is all written assuming i have a Linux/OS X environment. C/Clojure/Ocaml and others are much easier to work with. If you need windows for gaming I'd recommend dual booting. ,5ibrq8,t1_db6wmoe,finessseee,,Reply,3,0,3
db7canp,2016-12-14 17:23:49-05:00,BrokeDiamond,,"There's only one real requirement for getting a laptop for CS: it must run, or be able to run, some form of Unix/Linux. However, it is not recommended to get a pure Linux laptop quite yet because a lot of standard software (ex. Microsoft Office) is not supported on Linux.

OS X is based on Unix so any Macbook fulfills this requirement.

Windows does not, but can be configured to dual-boot Linux as long as there is enough hard drive space. Therefore if you like Windows you should only consider laptops with, at the bare minimum, 128 GB of storage. This should be easy though since I think only netbooks come with less storage than that. If you're okay with the extra cost, go with an SSD to increase the performance and durability of your laptop.

Also, no matter what you choose you should make sure you have at least 8 GB of RAM, but most laptops have that by default anyway.

As for specific choices, if you're looking at Macbooks I would stay away from the latest generation - early users are saying they're underperforming their specs, which were already meh to begin with. If you can find a Macbook Pro from 2014 or 2015, that would a good balance between cost and performance. If you're looking at PCs, I hear good things about the Dell XPS.",5ibrq8,t3_5ibrq8,finessseee,,Comment,1,0,1
db7e8sy,2016-12-14 18:04:27-05:00,finessseee,,Do you recommend me getting a 15 inch over 13 for the screen size?  ,5ibrq8,t1_db7canp,BrokeDiamond,,Reply,1,0,1
db7f40s,2016-12-14 18:23:34-05:00,BrokeDiamond,,"That's up to personal preference, so no strong recommendations either way.",5ibrq8,t1_db7e8sy,finessseee,,Reply,1,0,1
db6yf08,2016-12-14 13:02:13-05:00,987f,,"Get a Macbook (not pro) and save your money. It's strong enough for coding and not strong enough to waste your time with games. Spend the extra money on conferences, books, and other self-enrichment.",5ibrq8,t3_5ibrq8,finessseee,,Comment,-2,0,-2
5ia7zc,2016-12-14 07:05:45-05:00,NF95,Evaluating CTO,"I have in mind an idea for a fintech startup, but I do not have programming knowledge. How do I understand if the candidate has the ability to build such an app/website?",,,,,Submission,0,0,0
db6o1hu,2016-12-14 09:24:53-05:00,ixampl,,If your CTO is supposed to be building the app/website himself you are not actually looking for a CTO.,5ia7zc,t3_5ia7zc,NF95,,Comment,5,0,5
db6p6hw,2016-12-14 09:52:42-05:00,NF95,,He would in the future,5ia7zc,t1_db6o1hu,ixampl,,Reply,1,0,1
db6qpw6,2016-12-14 10:27:30-05:00,maq0r,,"That's not a CTO. A CTO isn't the first dev you hire for a startup or similar, a CTO is someone with the ability to set, track and manage the technical strategy for the product(s) and the organization. Devs and just devs are usually poor at that. ",5ia7zc,t1_db6p6hw,NF95,,Reply,3,0,3
db6rjm2,2016-12-14 10:45:07-05:00,NF95,,Ok. So how do I evaluate such a developer?,5ia7zc,t1_db6qpw6,maq0r,,Reply,2,0,2
db6rq1c,2016-12-14 10:48:50-05:00,LudoA,,"Some options:


* you ask a tech friend to evaluate them


* you base your opinion on a lot of prior work they did


* you google around for advice -- many startup founders have this problem, there's plenty of advice to be found on this subject



There are subreddits that are more relevant for this type of questions BTW (like /r/Entrepreneur), this one is about questions on CS subjects. You'll get better suggestions in other subs.",5ia7zc,t1_db6rjm2,NF95,,Reply,2,0,2
db6vl5x,2016-12-14 12:06:48-05:00,NF95,,"Thank you, I thought that this was the best place because I could get the point of view of developers :)",5ia7zc,t1_db6rq1c,LudoA,,Reply,1,0,1
db6wtne,2016-12-14 12:31:07-05:00,maq0r,,"If all fails and you just need an app made, plenty of freelancers on upwork that can help with that. ",5ia7zc,t1_db6vl5x,NF95,,Reply,1,0,1
5i4ycx,2016-12-13 12:44:05-05:00,amenhofis,I updated my AMD R10 Driver(help),"Yesterday I updated my laptop processor driver, I did it by the Radeon Setting App, When it was finish the whole screen turn black and it wasn't responding. It's an HP laptop with 12gb of ram and an AMD R10 2.6ghz I think ",,,,,Submission,0,0,0
5i4fkk,2016-12-13 11:15:18-05:00,Leeds1234,Compare SQL queries,"I need to correct queries against a correct array of queries.. I'm wondering how would I go about making it possible for e.g SELECT A,B FROM table, to be the exact same as SELECT B,A FROM Table(because these give the same answers only outputted differently) or other similar possible answers that may have the same answer just not in order, the problem here is if theres gonna be a limit or order on a query entered.. Like I was thinking of running the queries and comparing the results of the queries, then compare the data as sets and try get an intersection point, I'm not sure what way should I go about this.. If anyone has any recommendations that would be great, thanks.. I'm using php pdo to do this by the way",,,,,Submission,0,0,0
db5xp7a,2016-12-13 19:18:27-05:00,ostracize,,"AFAIK, you cannot do this in straight SQL. SQL is not a perfect implementation of sets and set theory. Those two queries are literally different queries with literally different results. Nothing you can do about that. 

If you load the results in any programming language that supports operations on sets, you can do it. ",5i4fkk,t3_5i4fkk,Leeds1234,,Comment,1,0,1
db5z8qv,2016-12-13 19:52:49-05:00,Leeds1234,,"Sorry I should've mentioned that, I'm using PDO PHP, do you think this would be difficult in PHP to do? Or do you have any other ideas that might help?",5i4fkk,t1_db5xp7a,ostracize,,Reply,1,0,1
db65gfa,2016-12-13 22:10:27-05:00,Pseudofailure,,"In php, you could just use `fetchAll(PDO::FETCH_CLASS, ""YourClass"")` to map the results to an object, then just write a comparison function for that object that can be used to test equivalency of the result set. Pretty brute-force, but it would work. 

Would probably be worth either sorting in the query, or sorting the result sets so that you only need to compare results one-to-one in order, rather than one-to-every-other. ",5i4fkk,t1_db5z8qv,Leeds1234,,Reply,1,0,1
db6kli8,2016-12-14 07:34:01-05:00,Leeds1234,,"Ye I was thinking of ordering the query by the answer I'm looking fors columns, if you understand me, then compare them as sets using a function.. This would probably work good wouldn't it, any other suggestions in how I could implement this?",5i4fkk,t1_db65gfa,Pseudofailure,,Reply,1,0,1
db632mt,2016-12-13 21:17:19-05:00,proskillz,,"It's not a very elegant solution, but you could keep lists of valid fields by table. I don't know PHP syntax off the top of my head, but here's how you could implement it in Java:

    Map<String, List<String>> map = new HashMap<String, List<String>>():
    
    List<String> personTableFields = new ArrayList<String>():
    personTableFields.add(""a""):
    personTableFields.add(""b""):
    
    map.add(""person_table"", personTable):
    
    // to test for each field being passed in
    
    if(map.get(""person_table"").contains(fieldName)) //valid",5i4fkk,t3_5i4fkk,Leeds1234,,Comment,1,0,1
db6kmjl,2016-12-14 07:35:10-05:00,Leeds1234,,"Thanks for your help, i'm make sure to try this out when I'm working on the project later on,this could be useful, thanks for the advice!",5i4fkk,t1_db632mt,proskillz,,Reply,1,0,1
5i12th,2016-12-12 21:42:05-05:00,ZakDarkElf,Not quite sure what laptop to get,"I'm planning to get a laptop for when I start Computer Science next year, with a rather small budget to work with. I've been trying to decide between the 

Dell Inspiron 13 (cheapest option) https://www.microsoftstore.com/store/msca/en_CA/pdp/Dell-Inspiron-13-i5368-Signature-Edition-2-in-1-PC/productID.5057684700


or the 

Asus Vivobook http://www.thesource.ca/en-ca/computers-and-tablets/laptops/all-laptops/asus-vivobook-e403sa-sb91-cb-14%E2%80%9D-laptop-with-intel%C2%AE-n3700%2c-128gb-emmc%2c-4gb-ram-and-windows-10---grey/p/108050220


In your guys professional opinion, which should I choose? If you think they both suck, what would be good within a budget of around $500?",,,,,Submission,1,0,1
db4kpys,2016-12-12 22:13:06-05:00,Badgrs,,"For the specific two you linked to, I'd probably lean toward the Dell over the Asus, but that's mostly just personal preference.

In general though, the truth is it doesn't matter too much. As a university student, the biggest thing you're looking for is a solidly constructed machine. This thing is going to get tossed around in your backpack. No matter how carefully you plan on treating it… you _will_ throw it in a bag and then later throw that bag on the floor. I guarantee it.

Favor an SSD (safer to toss around). Feel free to cut back on the HD size and get an external hard drive for media. You can make 128 GB work if you _really_ need to, but I recommend at least 256 GB. For RAM, aim for 16 GB. 8 GB will be _okay_, 4 will crush your soul, more than 16 will be unnecessary for the average day. Spend the rest of your budget on getting the best CPU for the price. Integrated vs dedicated graphics is completely up to you and really just depends on if you want to do any gaming or graphically heavy stuff (e.g. CAD, movie editing).

I have a lot of family and friends ask me for this advice, and I _always_ tell them to find a physical store that has the model your looking at on display or in stock. Go there and touch it or a similar model. Feel it. How's the keyboard feel? The trackpad? Does the hinge feel sturdy? Is the display good enough for you?

Don't actually buy the thing at the store, though, unless you want to support a local business or something like that.",5i12th,t3_5i12th,ZakDarkElf,,Comment,4,0,4
db4lz1i,2016-12-12 22:40:59-05:00,ZakDarkElf,,"Alright, thanks a lot! I guess it would be better if I worked on saving more money then, in order to increase my budget and be able to afford a more powerful laptop. Although, most likely to stay within a smaller budget I'll aim for 8GB RAM, with the HDD Vs SSD choice depending on how much I can fit into my budget. ",5i12th,t1_db4kpys,Badgrs,,Reply,1,0,1
db4m9sr,2016-12-12 22:47:26-05:00,Badgrs,,"Yeah, being tightly constrained by a budget makes it tough.

One option I've known some people to do is to get a _dirt cheap_ machine, or even a used one, and make it last as long as they can, usually just the first year, maybe a decent chunk of the second. The idea being that you'll be able to get a decent-paying job or internship during the summer after your first year and have enough money to splurge on something nicer. I'm not a huge fan of this approach, but it definitely can work.

Another option I've heard of (that's relevant for this gift-giving time of year) is people will ask for gifts of cash to go towards pre-college expenses. My parents were gracious enough to help fill in the gaps between scholarships and loans for my first semester. (I realize not everyone has this luxury, but it's potentially worth considering or pursuing).

Or just save more aggressively.",5i12th,t1_db4lz1i,ZakDarkElf,,Reply,1,0,1
5i0shc,2016-12-12 20:43:41-05:00,sadboys0987654321,Got a 2.77 after my first semester. Is it still possible to get into a good CS grad school program?,"So I'm a freshmen in college. I study at a top 5 CS program. Everyone here is so much more hardworking than me; I feel like I don't belong. The thing is, I'm really smart, I was just very depressed and lonely this semster. I could have gotten a 4.0 if I wanted to. I was really looking into doing grad school for either math or CS but I'm not sure if ill be able to do it. Should I be worried about my 2.77 if I'm considering grad school? I worked at an engineering firm before and I didn't like it, I really want to do research instead. Is it even possible for me? I heard CS is REALLLY competitive for grad school. I was really looking to go to one of the top programs. I really have a lot of crazy ideas I wish to expand on but I'm starting to realize maybe this ""professional"" life isn't for me. I had all this ambition when I got here... but I guess I just got too comfortable. 

Like this is honestly the most depressed I've ever been in my life. I'm scared about my future. I spend most of time now feeling anxious and sick. I failed my parents. I failed everybody and I'm don't know how to deal with this. Even if I end up not doing research, I'm not sure if I can even get an internship with this horrible GPA. ",,,,,Submission,1,0,1
db4iqbl,2016-12-12 21:29:59-05:00,TheCommador,,"I'm willing to bet that your school offers counseling to students. Visit them. Even if you don't think you're clinically depressed, visit them. 

One bad semester will never be your undoing. One bad semester is nothing. Your mental health will be your undoing. Your mental health is everything. ",5i0shc,t3_5i0shc,sadboys0987654321,,Comment,8,0,8
db4l94t,2016-12-12 22:24:51-05:00,Badgrs,,"What u/TheCommador said is _insanely_ important. Mental health is infinitely more valuable than grad school.

Grad school is still a ways off for you. I know it's easy for me to say this and likely hard for you to hear this, but don't sweat it too much right now. A _lot_ can happen in the next 3-4 years. Having a rocky start to college is not that uncommon.

It's important that you don't let this ""failure"" ruin you emotionally. I know this sounds cheesy, but failures are learning opportunities. Spend some time and think about _why_ this semester went poorly. There are likely some smaller, underlying, addressable issues that you can individually focus on and tackle.

Improvement doesn't happen all at once, and sometimes the process is painful. But you can't let a failure get you down to the point where you give up on improving yourself.

A tip on goal setting: your end goal of undergrad might be to get into a grad school. Great. That's awesome. But you need to break that down into smaller steps. Focus on each semester at a time. Start sketching out a course schedule so you can balance your course load and find room for things like interesting electives, undergrad TA-ing, and/or helping with research. Work with your advisors and department resources. They want you to succeed.

",5i0shc,t3_5i0shc,sadboys0987654321,,Comment,2,0,2
5i0jk4,2016-12-12 19:54:25-05:00,Stephen_Rothstein,What is the difference between object code and target code?,,,,,,Submission,0,0,0
db4g05c,2016-12-12 20:32:11-05:00,lordvadr,,"We're not here to do your homework for you. Tell us what you think the answer is and why, or what your thoughts are.  Tell us what you know and don't know.",5i0jk4,t3_5i0jk4,Stephen_Rothstein,,Comment,3,0,3
db4qam2,2016-12-13 00:26:58-05:00,Stephen_Rothstein,,"Your comment is very hurtful. I hope you're just having a bad day.

spez: Target code is just a way to label to resulting code of a compiler. It could even be a conversion to a high level language, whatever. If we have a source code, and we want to convert it to any other language, low or high level, we call the compilled source code target code.

Object code is a machine language/binary (are they the same thing?) target code.

tl:dr: ""target code"" is just a way to refer to the code after that was compilled. ""Object code"" refers to binary target code that wasn't yet linked (linked being something which which I'm now afraid to ask about), specifically.",5i0jk4,t1_db4g05c,lordvadr,,Reply,1,0,1
db52ef8,2016-12-13 08:50:56-05:00,lordvadr,,"I wasn't having a bad day.  This sub very frequently, gets a homework question lobbed at it--and they can be pretty obvious.  Judging by the number of upvotes your question got, it appears others agree with me.

If this wasn't such a question, I apologize, but you have to admit that it sure looks like one...especially given that ""target code"" isn't really a term--it sounds like someone's term for some kind of theoretical-but-not-in-the-real-world thing.  The entire gcc man page doesn't mention the term.  A google search for it brings up little more than some wishy-washy stuff...oh yeah, and a power-point from a university.",5i0jk4,t1_db4qam2,Stephen_Rothstein,,Reply,3,0,3
db56zw3,2016-12-13 10:40:34-05:00,saruko27,,"I think what he's getting at, and you will 100% see this in every tech subreddit maybe minus techsupport, but you can't just come onto the various tech reddits and ask a blatant question with no proof that you tried to resolve the issue yourself. It's not just lordvadr. And if anything, he was extremely light about it. 



It comes down to the idea that these subreddits are considered to be professional subreddits, and thus are expected to see professional/well educated questions being asked.",5i0jk4,t1_db54png,Stephen_Rothstein,,Reply,2,0,2
db54png,2016-12-13 09:51:20-05:00,Stephen_Rothstein,,"I've read this ""target code"" term in some site (I could take the time to look for it in my browsing history if you want), but couldn't find anything more, so having few resources available I had to make Reddit a visit and hope for a clarification.

I've been reading about these things out of my own volition, it's not some school homework. Most often I have to read several texts explaining the same thing because the language isn't clear. Sometimes the doubts that arise in my mind aren't directly verifiable (how many sites dealing with the ""target code vs object code"" theme will you find online?) so I have to take it into my own hands and ask these things.

If I was really asking for an answer to be served on a silver plate, I would ask ""how is source code turned into machine code?"". Instead I'm just reading about the steps. It led me to the compiler, then I got confused with the difference between interpreters and compilers. Then I found out about object code, and then I've gotten confused about it, because of such weird name, ""OBJECT code"". And so on. Sometimes I find a 'definitive definition' that I can carry around being sure it's the right one, and can proceed to the next step without being afraid of finding mutiple different answers on stackexchange or whatever.",5i0jk4,t1_db52ef8,lordvadr,,Reply,1,0,1
db57fo8,2016-12-13 10:49:36-05:00,Stephen_Rothstein,,"Wow. Ok, I won't let you people hurt me any further. You're all just enjoying being mean. Keep your ""professionalism"" and ""well educated"" questions to yourselves. I'll go back to the books to get my answers, they're safer to deal with. Have a nice day.",5i0jk4,t1_db56zw3,saruko27,,Reply,1,0,1
db58kv0,2016-12-13 11:12:23-05:00,saruko27,,"No one's trying to hurt you. This is pretty standard stuff no matter where you go. I'd like to offer genuine advice that if you are unable to deal with situations like this and this is too ""hurtful"" then i'm afraid you're not ready for the real world yet (not sure what to say if you already have a full time job with a company more than just 50 people).



Prepare to be rejected a lot in the workplace. It's happened to me, and everyone else a million times.",5i0jk4,t1_db57fo8,Stephen_Rothstein,,Reply,3,0,3
db59cuy,2016-12-13 11:27:10-05:00,Stephen_Rothstein,,">No one's trying to hurt you.

If that's the case, then I'm fine, kind of. Have an upvote.

I'll still lurk inside the books and sites though, as unclear as they may be (they sound like they were written for people who already know about this, it's annoying).",5i0jk4,t1_db58kv0,saruko27,,Reply,1,0,1
5i0gxj,2016-12-12 19:40:42-05:00,space__sloth,Is this example interview from Google an accurate representation of format and difficulty?,,,,,,Submission,32,0,32
db4jz8s,2016-12-12 21:56:24-05:00,Badgrs,,"_Disclaimer: this is based on my experience with the final, on-site interview for an entry level software engineer position between 1 and 3 years ago. Interviews earlier in the process were easier and more behavioral, in my experience._ 

For an _entry level_ software engineer position, it's fairly accurate. The question itself (find two values in a list that satisfy a condition) is definitely on the easier end of the spectrum, but _most_ technical interview questions will follow a similar format, typically along the lines of

> Given [some data] represented in [some fairly straightforward data structure] and [some logical condition], determine if [said condition] is true for [some data].

Or very similar variants like

> Given [some data] represented in [some data structure], perform [some operation] and return [some sample of the data or condition about the data].

The questions are typically set up so that there are multiple ""valid"" solutions, e.g:

* a naive, inefficient one (i.e. the n^2 approach briefly mentioned as he got started)
* a reasonable, but still inefficient one (i.e. the log(n)-ish approach with binary searches)
* a good, potentially clever approach (i.e. the solution he landed on when the list was sorted)
* a crazy, over-optimized approach that no sane interviewer would expect you to know/find, but may exist so that you can impress them

The questions will also scale to multiple ""difficulties"", e.g.:

* with lots of simplifying assumptions (the data can assumed to be sorted, all integers, valid, etc.)
* with realistic expectations (usually discarding some previous simplifying assumptions, e.g. the data can't be guaranteed to be sorted)
* with ""Google scale"" (e.g. the data is astronomical in size and is impossible to fit in memory)
* occasionally… they'll add an interesting or confusing requirement or constraint, sometimes this is to force you to find a more generic solution, other times it's to see if you can find a clever optimization with new assumptions.


However… and this is probably the most important bit… getting the question _right_ is not the ultimate goal. Communicating how you think through the question is the most important thing that the interviewer wants to see. Second to that is familiarity with data structures, control flow, and a really solid grasp of complexity analysis.

Also, not all of the questions will be like this. It varies on who your interviewers are and what mood they're in (on-site interviews are typically several 1-on-1 interviews throughout the day, and a ""non-interview interview"" over lunch). There will likely be some ""brain teaser-y"" questions. There will likely be some stupidly specific questions that will completely stump you. The goal for you is to demonstrate your technical knowledge and critical thinking and communication skills. If you make it to the final interviews, your recruiter wants you to get hired.

Interviewing at Google is intense. Fun… but intense.",5i0gxj,t3_5i0gxj,space__sloth,,Comment,22,0,22
db4v3a1,2016-12-13 03:22:29-05:00,watsreddit,,This does seem to me to be really easy. I would think the actual interviews would be a lot more difficult.,5i0gxj,t3_5i0gxj,space__sloth,,Comment,3,0,3
db614o2,2016-12-13 20:34:35-05:00,newdingodog,,Just a wild guess?,5i0gxj,t1_db4v3a1,watsreddit,,Reply,3,0,3
db67eiz,2016-12-13 22:55:45-05:00,watsreddit,,"Yeah I'm just guessing. I don't really know. It just seems surprisingly easy, which makes sense since they would want a simple example to demonstrate the approach one should take to technical interviews.",5i0gxj,t1_db614o2,newdingodog,,Reply,1,0,1
dbawve5,2016-12-17 04:38:06-05:00,BenMQ,,"From my personal interview experience this definitely CAN be a real question. it's an easy one that's certainly below average in terms of algorithm difficulty, but definitely possible. ",5i0gxj,t1_db4v3a1,watsreddit,,Reply,1,0,1
5hyzcr,2016-12-12 15:23:58-05:00,wait_watchers,Questions regarding future interviews,"Hello all! I wasn't sure if this was the right sub, but it seemed the best fit. I am a computer science senior at university, and will finish my undergrad next spring and finish Master's next spring. Obviously, this means preparing for and (hopefully) acquiring interviews are looming ahead. I had a question regarding projects to show employers. I know a it is common and helpful to work on side projects apart from school and contribute to open source. I have not done a lot of either. How will this affect me, if I have had previous internships, have good grades in the subject and can answer technical questions well? Also, I do have several projects that I have done for school that I am very proud of and would love to show an employer, but many of them use files and libraries that have high cost licenses and as such I had to run them on the school's cluster, not my personal computer. How would I go about trying to show these off to an employer? Sorry if  these questions sound naive.",,,,,Submission,2,0,2
db45tfp,2016-12-12 16:52:31-05:00,KronktheKronk,,">How will this affect me, if I have had previous internships, have good grades in the subject and can answer technical questions well?

Almost not at all.  I think a lot of people are coming around on the idea that professional developers develop for a living and so want to do other things in their spare time.  Experience is 100x more valuable to them and the ability to answer technical questions and interview well will put you ahead of most other applicants.

If a job comes down to you and one other person the open source contributions and github portfolio may make a difference, but that's not likely to matter.",5hyzcr,t3_5hyzcr,wait_watchers,,Comment,3,0,3
db4hdde,2016-12-12 21:01:06-05:00,wait_watchers,,Ok. Well that is reassuring to hear. Thank you for your input!,5hyzcr,t1_db45tfp,KronktheKronk,,Reply,1,0,1
db4ljtg,2016-12-12 22:31:29-05:00,Badgrs,,"> Also, I do have several projects that I have done for school that I am very proud of and would love to show an employer, but many of them use files and libraries that have high cost licenses and as such I had to run them on the school's cluster, not my personal computer. How would I go about trying to show these off to an employer?

Talk about them. 

Even if you _could_ demo them to a potential employer or even send them the source code, that's not what they're going to care about. They're going to care about things like… 

* what did you learn from these projects?
* what sort of languages and technologies did you work with?
* was it a group project? how did you handle team coordination and communication?
* how much of the project was your original work?

They want to see a passion for problem solving and solid technical knowledge. Demoing projects or sharing source code don't accomplish either of those things very well. Discussing them on your resumé (briefly), or in a portfolio or personal website (extensively), or in person during an interview are much better.",5hyzcr,t3_5hyzcr,wait_watchers,,Comment,2,0,2
db4pzf4,2016-12-13 00:18:02-05:00,wait_watchers,,"Ok that makes sense. I knew that I need to talk about them, but I didn't make the connection of why, from the interviewer's point of view. You made that more clear, thanks!",5hyzcr,t1_db4ljtg,Badgrs,,Reply,1,0,1
5hwk7z,2016-12-12 08:21:14-05:00,KaanFresh,How to stay motivated all the time ?,"Hey guys

I'm in my freshman year at university. I've a background on programming so I'm familiar with most of the thing they should show us at school. The thing is the thing they teach us like we're 5 years old. 2 weeks ago they've showed us how to copy text and paste it on MS Word in the Introduction to Computer Engineering class. All right I'm not a senior software developer with 30+ years experience but I know how to copy a text... Also every example we solve in Programming Language class is super easy. They show algorithms in first 6 week and it's been 1 month since we started to C Language, still our associate professor show us the basic printf-scanf functions...

I ain't learning anything. My motivation going down pretty fast. How do I prevent this ?",,,,,Submission,13,0,13
db3hnj0,2016-12-12 08:23:35-05:00,BraveZA,,"The first year or two is pretty boring, so keep yourself busy with some online courses and/or find as many alternative solutions to the course work you're doing to diversify your approach to problem solving which is really what it is all about.",5hwk7z,t3_5hwk7z,KaanFresh,,Comment,5,0,5
db3knth,2016-12-12 09:51:36-05:00,IgnorantPlatypus,,This sounds a bit dumbed down compared to what I had in college. What courses are these? Are they core CS classes or are the CS overview classes for non-majors?,5hwk7z,t3_5hwk7z,KaanFresh,,Comment,4,0,4
db3l9cl,2016-12-12 10:06:30-05:00,KaanFresh,,"I'm studying Computer Engineering cause we don't have Computer Science as individual program. Unfortunately these are the core classes for Computer Science/Engineering major.

Edit : In first semester I take 7 courses. 5 of them are non-major, 2 of them are core courses. Physics, Chemistry, Calculus, History, Turkish Language courses. It's mandatory to take these. I don't know what I'll do with history and Turkish Language... The other two are Introduction to Computer Engineering and Programming Languages I. 

In introduction course's syllabus :
- Research and Application Areas of Computer Engineering
- History of the Computer
- Basic Process of Computers
- Basic Components of a Computer
- Software Hardware
- DOS Commands
- MS Office Programs


For ""Programming Languages I"" we saw algorithms and basic functions of C Language. Most complex example we had was the program that finds every armstrong number between 1-1000...
",5hwk7z,t1_db3knth,IgnorantPlatypus,,Reply,3,0,3
db4crhz,2016-12-12 19:20:06-05:00,theGentlemanInWhite,,"Personal projects/start a club. It was like this for me then and, tbh, it hasn't really gotten much better as I finish my junior year. I mean the content is harder but I worked ahead so it stayed easy. You're ahead now, take advantage and stay ahead. You might be bored in class but it will pay off when you start looking for work/internships. ",5hwk7z,t3_5hwk7z,KaanFresh,,Comment,2,0,2
db4jbg1,2016-12-12 21:42:18-05:00,KronktheKronk,,no one is motivated ALL the time.  I think you'll be more motivated when you need to be if you fill your leisure time with fulfilling activities.,5hwk7z,t3_5hwk7z,KaanFresh,,Comment,1,0,1
db4lene,2016-12-12 22:28:15-05:00,exneo002,,"The big thing I did was find books that would keep me excited about cs. Stuff like snow crash, little brother etc. Worked wonders staying engaged when I first started harder courses.",5hwk7z,t3_5hwk7z,KaanFresh,,Comment,1,0,1
db4uvmn,2016-12-13 03:12:36-05:00,space__sloth,,I wish my courses were this easy so I had more spare time for side-projects and self-studying machine learning. This is the best time for you to teach yourself the skills you need (ideally by learning through work on a side-project). ,5hwk7z,t3_5hwk7z,KaanFresh,,Comment,1,0,1
db5x2x2,2016-12-13 19:04:48-05:00,uber_kerbonaut,,"Making things that you can actually show off. Like web games, Processing scripts, art, music etc.",5hwk7z,t3_5hwk7z,KaanFresh,,Comment,1,0,1
5hv8wb,2016-12-12 01:55:48-05:00,timmah612,Simple AI for basic games.,"Ive learned a tiny bit about ai problem solving for things and was wondering how difficult it would be to make a program to find the best solution to something like candy crush or even more basic, games like poppit, where you just remove clusters of baloons or whatever.
Just an idle thought, feel free to ignore it haha.",,,,,Submission,5,0,5
db3a99k,2016-12-12 02:26:20-05:00,justlikestoargue,,"The more complex the game, the more complex the AI. Depending on the game, in many cases it's easier to just program an algorithm than an actually AI (where the program learns and creates and refines it's own algorithm over time.)

If you're new to AI, what I believe is a good starting point is checking out Bayes Theorem. It's pretty abstract and tough to find good information online for the layman, so my simple synopsis of it is this: it's a way to mathematically quantify beliefs, and update them according to new information. You should be able to see how this is useful in regard to AI- the AI has a set of options it can take in any given scenario, and if it can assign a numerical value to those options and use those numbers to pick the best choice or use new information to change those numbers in a way that improves it's AI, you've got a bit of a foothold into creating AI.

Sorry if even that's a bit confusing, but do some Google searches for basic AI and/or Bayes Theorem. If you get through all the basics, look into neural networks and machine learning.",5hv8wb,t3_5hv8wb,timmah612,,Comment,3,0,3
db3ekwr,2016-12-12 06:08:25-05:00,mehum,,"Without wanting to be nitpicking, what you describe as AI sounds like machine learning, which is a category of AI.  A chess program is still usually classified as AI, even if it isn't capable of improving itself.  Even something like A* search is part of AI, even though it is a fairly compact algorithm.  ",5hv8wb,t1_db3a99k,justlikestoargue,,Reply,2,0,2
db3i8hc,2016-12-12 08:43:08-05:00,andybmcc,,"The only way to find a ""best"" solution for games like that is to exhaustively search.  Some search branches can just be pruned early, depending on the problem.  For intractable problems, usually a ""good"" solution will suffice.  An easy first step would be coming up with some heuristic weighting with regards to board state and score, look a few levels down, and choose the optimal path for that subset of the problem, then repeat.  I'm guessing this is meant as a learning experience.  See how changes to the heuristics and maximum depth affect your solutions.  Once you have this basic understanding, you can move on to more complex algorithms.",5hv8wb,t3_5hv8wb,timmah612,,Comment,1,0,1
5huhcj,2016-12-11 22:53:32-05:00,TommySparkle,[Question/Help] Is there a way to see a page's edit history or the times a page was edited?,"I'm currently in a situation where me and my friend are being penalized for typing an essay response late. We both uploaded it on time but after uploading it we went back on it the next day to fix some stuff, nothing major just some grammar and spelling with permission from our teacher. But now there is a time stamp on it that says it was last edited on 12/8/16 which would be considered late according to my teacher because she believes that's when we uploaded it instead.

Is there any way we can find an edit history or prove to our teacher that we uploaded it on time? Like seeing when the page was first edited so that our projects are still valid.

I have no knowledge of computer science but I've been scouring the page's source and inspecting the page so that I can find any evidence of us changing the page before the due date, proving that we actually did upload our project on time.",,,,,Submission,2,0,2
db39kq6,2016-12-12 01:59:06-05:00,marco262,,"What web framework does your school use for tracking essays? 

At the most basic, computers don't have some inherent way of tracking when a file was updated. That's all stored in what's called ""metadata"", and the amount of information you get about the history of a file depends entirely on the framework being used. For example, Windows provides metadata of when a file was created, last accessed, and last modified. Wikipedia, by comparison, provides a detailed revision history of any page with dates and times. 

Unfortunately, if the web tool your school is using doesn't support revision history, you're SOL. If that's the case, just take this as a lesson learned, warn your other classmates, and take solace in knowing that once you graduate college, your GPA is going to mean jack squat. ",5huhcj,t3_5huhcj,TommySparkle,,Comment,1,0,1
5httnb,2016-12-11 20:36:07-05:00,UmamiSalami,What are some universities with a research focus in multi-agent systems?,So far I've found CMU and Duke.,,,,,Submission,3,0,3
db4mgjg,2016-12-12 22:51:16-05:00,Badgrs,,"It's a specific enough topic that it's going to be hard to find universities that focus on it at the department level.

You might be better off looking for individual professors and find a university that has a handful of professors that research it.

For example, at my university, there was one prominent professor that researched MASs (as one of a few research areas of his) and some other professors occasionally assisted. With only one tenured faculty member researching the topic, we definitely were _not_ ""focused"" on it, lol.",5httnb,t3_5httnb,UmamiSalami,,Comment,1,0,1
5hsa7l,2016-12-11 15:46:20-05:00,TOS_Statistics,ARPANET IMPs and TCP/IP,"I'm pretty ignorant of computer hardware and how the internet works, but in reading about the history of the internet the following jumped out to me as contradictory (b/c of my poor understanding):

If the ARPANET could already, in the late 1960s/early 1970s, through the use of IMPs, facilitate communication between different host computers regardless of the variations and constraints of those host computers, then why was it necessary to develop TCP/IP?  

I'm probably just confused about the function of TCP/IP, but if host computers could already communicate with other host computers through IMPs, why was a TCP/IP protocol necessary?",,,,,Submission,4,0,4
db2shuc,2016-12-11 18:54:26-05:00,adbJ114,,"[Disclaimer: I am not an expert in this field, just an enthusiast, and welcome additions and corrections to the following explanation]
____________________________________________

TCP/IP didn't exactly replace IMPs, as the story goes. The following provides the relevant context:

IMPs, or Interface Message Processors, were packet switching hardware with their own special software and protocol. IMPs handled the Physical Layer, the Data Link Layer, and the Network Layer of the Protocol Stack, and starting in 1969, essentially formed the primitive first infrastructure of ARPANET.

Then in late 1970 ARPANET introduced the **Network Control Program** (not to be confused with the Network Control Protocol). The Network Control Program handled the Transport Layer of the Protocol Stack. One of the benefits of implementing NCP was that on top of the NCP Transport Layer, an Application Layer could reside, handling things like Telnet, email, and file transfers. Here are a couple excerpts from articles describing the significance of the addition of NCP to ARPANET (Note that the IMP protocol is known as 1822):

> The 1822 protocol proved inadequate for handling multiple connections among different applications residing in a host computer. This problem was addressed with the Network Control Program (NCP), which provided a standard method to establish reliable, flow-controlled, bidirectional communications links among different processes in different host computers. The NCP interface allowed application software to connect across the ARPANET by implementing higher-level communication protocols, an early example of the protocol layering concept incorporated to the OSI model.

Source : https://en.wikipedia.org/wiki/ARPANET#Software_and_protocols
__________
> The Network Control Protocol (NCP) was the first standard networking protocol on the ARPANET. NCP was finalized and deployed in December 1970 by the Network Working Group (NWG), led by Steve Crocker, also the inventor of the Request For Comments.
> 
> NCP standardized the ARPANET network interface, making it easier to establish, and enabling more and more DARPA sites to join the network. 

Source : http://www.livinginternet.com/i/ii_ncp.htm

_____
So at this point in the story, ARPANET has developed a modern approximation of the Internet Protocol Stack:

    Application layer [On top of NCP (e.g. email and file transfer)] 
    Transport layer [NCP]
    Network layer [IMP]
    Data link layer [IMP]
    Physical layer [IMP]

This all worked out fine for the next twelve years or so, until TCP/IP was implemented as a replacement to NCP:

> On January 1, 1983, known as flag day, NCP was officially rendered obsolete when the ARPANET changed its core networking protocols from NCP to the more flexible and powerful TCP/IP protocol suite, marking the start of the modern Internet. However, NCP provided a slightly different service than TCP/IP by forming a network of networks. Hence, the transition meant a loss of functionality and increased complexity in TCP/IP.

Source : https://en.wikipedia.org/wiki/Network_Control_Program#Transition_to_TCP.2FIP
______
Interestingly, IMPs were still in use for the next six years or so:

> IMPs were at the heart of the ARPANET until DARPA decommissioned ARPANET in 1989. Most IMPs were either taken apart, junked or transferred to MILNET. Some became artifacts in museums: Kleinrock placed IMP Number One on public view at UCLA. The last IMP on the ARPANET was the one at the University of Maryland.

Source : https://en.wikipedia.org/wiki/Interface_Message_Processor#History
_______
So in short, IMPs were the foundation of the internet, starting in 1969. In 1970, the Network Control Program was built ""on top"" of IMPs. In 1983, ARPANET replaced NCP with TCP/IP. Finally, in 1989, IMPs were phased out for good by DARPA.

Edit: Formatting, typos",5hsa7l,t3_5hsa7l,TOS_Statistics,,Comment,5,0,5
5hs3zv,2016-12-11 15:15:27-05:00,asurarusa,Good books/textbooks for CS subjects?,"Does anyone have any recommendations for books about the following subjects?

	* Introduction to Computer Science
	* Discrete Math
	* Data Structures
	* Algorithms
	* Linear Algebra
	* Discrete Math
	* Software Engineering
	* Computer Organization
	* Artificial Intelligence
	* Computer Vision
	* Computer Graphics
	* Computer Networks
	* Distributed Computing
	* Real-Time Computing Systems
	* Introduction to Database Systems

I've tried the usual google the subject/check book lists for schools and the only clear recommendation I've found is that CLRS is the gold standard when it comes to Algorithms textbooks.
",,,,,Submission,5,0,5
db2ixke,2016-12-11 15:41:29-05:00,ProfessorAlgorithm,,"Introduction to Computer Science:  https://mitpress.mit.edu/books/introduction-computation-and-programming-using-python-1 (basic programming and problem solving)

Algorithms:  https://mitpress.mit.edu/books/introduction-algorithms (as you have heard)  It is not very easy to read, but the content is on point.

Artificial Intelligence:  https://www.amazon.ca/Artificial-Intelligence-Modern-Approach-3rd/dp/0136042597 (as above, this is a seminal book, but not the most approachable)",5hs3zv,t3_5hs3zv,asurarusa,,Comment,2,0,2
db30jwx,2016-12-11 21:54:33-05:00,arichi,,"* Discrete Math

Rosen's textbook is probably the most used.  Not bad, lots of problems to work, but not as much application to Computer Science as I'd like.  

Maurer and Ralston's _Discrete Algorithmic Mathematics_ is pretty good too.

* Data Structures

Goodrich and Tamassia have three books:  they're more or less the same, except for choice of language (and 1-2 language specific chapters).  _Data Structures and Algorithms in_ {C++, Java, Python}.  Pick your favorite language for this.

* Algorithms

CLRS is very dense and doesn't have a lot of problems to solve to reinforce concepts -- and few of those ask you to come up with an algorithm to solve a problem.  Kleinberg and Tardos have ""Algorithm Design"" that is the opposite:  very verbose but plenty of problems to solve.   Goodrich and Tamassia have a book as well that fits nicely in between (there's an older version in Java, but the newer one is language agnostic).

* Discrete Math

See previous entry ""Discrete Math""

* Computer Organization

Not strictly what you asked for, but read the CMU book: [Computer Systems: A Programmer's Perspective](http://csapp.cs.cmu.edu/).   If you want to see it from a quasi-EE point of view, I can suggest those, but if you're looking for this _as a computer scientist_, the CMU book is the one you want.

* Artificial Intelligence

Norvig's book, _Artificial Intelligence:  A Modern Approach_.  I'm not sure there are any other general AI books I'd put in the same category.

* Distributed Computing

Nancy Lynch's book is still pretty good, although it won't have more recent advancements.  

* Introduction to Database Systems

[Database Management Systems by Raghu Ramakrishnan and Johannes Gehrke](http://pages.cs.wisc.edu/~dbbook/) is my favorite.   There's also Ullman et al's book and  Silberschatz et al's that are worth reading.",5hs3zv,t3_5hs3zv,asurarusa,,Comment,2,0,2
db31l6i,2016-12-11 22:18:31-05:00,asurarusa,,Thank you for this list! This is really helpful.,5hs3zv,t1_db30jwx,arichi,,Reply,2,0,2
db4lzfd,2016-12-12 22:41:14-05:00,Badgrs,,"Computer Organization: I'm a fan of Operating System Concepts, 9th ed.

Discrete Math: I used Rosen's. It's okay.

Software Engineering: I used Essentials of Software Engineering (Tsui, et al.) It was okay, there are probably a lot better books out there.",5hs3zv,t3_5hs3zv,asurarusa,,Comment,2,0,2
5hpwv7,2016-12-11 07:44:50-05:00,Leeds1234,"Define a Prolog predicate noah/3 which is true of three lists when corresponding elements of the first two lists, which are of the same length are lined up two-by-two on the third list","new to Prolog, just want to understand it better, using this example

 noah([ ],[ ],[ ]).
noah([a,b,c,d],[aye,bee,sea,dee],[a,aya,b,bee,c,sea,d,dee]).",,,,,Submission,0,0,0
db248sp,2016-12-11 10:22:15-05:00,arichi,,"Look at it this way:

it's clearly true in the case case.  So you can write noah([], [], []).  

It's also true if the first two elements of the third list are  the first elements of the first two lists, and also if noah/3 applies to the rest of those lists.  

Try to write it based on that last paragraph.  ",5hpwv7,t3_5hpwv7,Leeds1234,,Comment,3,0,3
dbdutse,2016-12-19 11:26:51-05:00,Leeds1234,,"So it's gonna be sort of like    
noah([a,b,c,d],[aye,bee,sea,dee],[a,aya,b,bee,c,sea,d,dee]):-    
    noah([head],[head],[head, 1]), noah([head| tail],[head| tail],[head| tail])

Is it something like this? Sorry I'm new to Prolog",5hpwv7,t1_db248sp,arichi,,Reply,1,0,1
dbigjsg,2016-12-22 12:51:25-05:00,arichi,,"No.  There are two sets of rules you need to do:

Let's look at the first:

    noah([], [], []).  

That's just a fact:  the predicate noah/3 is true when all three lists are empty.

Now we need to introduce rules to figure out what else could be true:

      noah([H1|T1], [H2|T2], [H1,H2|T3]) :- noah(T1,T2,T3).

This says that noah/3 is true when it has three non-empty lists, the first two have at least one element, and those elements are the first two (in that order) of the third list.  In addition to that needing to be true, noah/3 must be true for the ""tails"" of those lists  -- everything after the first element of the first two, and the first two of the third.

Capital letters in ""variables"" (as you are probably thinking of them) indicate that they can be bound to various values, but are unbound.  Lower-case is used for ""constants"" (really bound terms).

They aren't really variables and constants, but that may be close enough for you to think of them for now.

You can then _ask_ questions of Prolog.

    ?- noah([a], [b], [a,b]).

It will respond yes if you did this right.

So will

    ?- noah([a,b,c,d],[aye,bee,sea,dee],[a,aya,b,bee,c,sea,d,dee]).

You can also ask questions with a ""no"" answer.

Try this and see how it works.  Then ask yourself what you think will happen if you put an unbound variable in a question in one or more positions.",5hpwv7,t1_dbdutse,Leeds1234,,Reply,2,0,2
dbitn3d,2016-12-22 17:25:00-05:00,Leeds1234,,"Thanks a million man I'll look into all of this tomorrow, I appreciate the help!",5hpwv7,t3_5hpwv7,Leeds1234,,Comment,1,0,1
5hourd,2016-12-11 01:34:56-05:00,xZoks,Best way to structure code for others' use?,"I have a C++ algorithm that I'd like to be able to share with some other people with ease (ideally they just include my header and can call the function). What should I do? Write a header file with a cpp file implementing the declarations in the header?

Thanks :)",,,,,Submission,12,0,12
db26mla,2016-12-11 11:26:03-05:00,ukkoylijumala,,"> Write a header file with a cpp file implementing the declarations in the header?

I'd say yes, but also write a documentation about what your function does. Be as concise as possible, but don't leave out details that may be interesting to someone that uses your function.

What goes in? What comes out? Side-effects? Time complexity? etc.",5hourd,t3_5hourd,xZoks,,Comment,6,0,6
db296qz,2016-12-11 12:27:18-05:00,xZoks,,Thanks! :),5hourd,t1_db26mla,ukkoylijumala,,Reply,2,0,2
db2fj0q,2016-12-11 14:35:42-05:00,Chappit,,"Easiest, just write a header file. 

Second easiest, header and cpp though that's a bit awkward. 

Hardest but what most people are doing, compile it to a DLL and have them include that.",5hourd,t3_5hourd,xZoks,,Comment,1,0,1
5hn2l3,2016-12-10 18:34:25-05:00,CancerBabyJokes,Anyone wanna help me write a program in C# to display news and weather via serial to my TRS Model 200??,"Was wanting a program that simply parses information like latest news and weather and outputs it via RS-232 serial (Null modem) at 1200 buad for my old TRS model 200 to display in the native Tandy telcom program.


Here is what I'm looking for

* It would have to conform to a 40x16 (Would need to be ajustable) text only LCD.

* Basic ASCII data only, no broken XML or HTML output. 

* Would need to be able to send CLS commands

* Have have the source code files with basic //instructions listed in the code for customization.

* Would want the code to be open source for the most part and allow other users with serial capable Vintage Comps To use it as well.


Can anyone help me? This is outside of my coding ability. And I know a few subs that would shit bricks for this code.

",,,,,Submission,0,0,0
db1erfu,2016-12-10 19:00:42-05:00,None,,[deleted],5hn2l3,t3_5hn2l3,CancerBabyJokes,,Comment,1,0,1
db1f7dj,2016-12-10 19:12:19-05:00,CancerBabyJokes,,"Indeed. But I would be willing to test and report back, I'm no coder, but I'm no idiot with computers. Not to mention its all one way data with standard serial communication protocols.",5hn2l3,t1_db1erfu,None,,Reply,1,0,1
db1fg9m,2016-12-10 19:18:38-05:00,None,,[deleted],5hn2l3,t1_db1f7dj,CancerBabyJokes,,Reply,1,0,1
db1fv24,2016-12-10 19:29:10-05:00,CancerBabyJokes,,"Well that's where I come in, I know all there is to know about the Hardware, and I have already used serial terminal emulators to send text manually, Just need a program to do it now :) 

Plus I know exactly how the code needs to work and understand the communication protocols, so know i just need a kind person to help me with my weak point, code.

The HtmlAgilityPack would be perfect for phrasing the web pages from what I read, and I'm sure its not that hard to output char data to COM1. For an experienced coder that is. Who is not me, I do hardware real well lol.

Just think of it as a dummy terminal from the mainframe days :)",5hn2l3,t1_db1fg9m,None,,Reply,1,0,1
db1m7aa,2016-12-10 22:13:26-05:00,ldpreload,,"This is standard RS-232 serial, I'd hope&mdash:it's easy to test in a terminal emulator, or even with another PC with a null modem cable. There shouldn't be anything specific to this particular terminal.",5hn2l3,t1_db1erfu,None,,Reply,1,0,1
5hlqf5,2016-12-10 14:12:35-05:00,noobeeee,Real time data analytics without using any open source library.,"Hi all,

I need to process millions of web events - clicks, views, time-spent in real time (maximum 10 minutes delay).

I'm using rabbitmq and python to do the job. However, I'm a bit worried that my solution might not be scalable and of professional standard.

Please tell me what you think of my solution. 

Thanks.

---

1. My php web server received web events, and then they are put into rabbitmq queue. 
2. I have python consumers which basically aggregate stats like total views on a web page. The consumers just grab messages from rabbitmq queue in batch, and then update the total stats to the final postgre database. Then delete the processed messages from rabbitmq. Consumers are run in supervisor and they keep checking the queue if there were any messages. If not, they will sleep for 30 seconds before checking again.
3. I have a watchdog process that looks out for number of messages on the queue and expand the consumers based on the number of messages.

What do you think of this solutions? Am I supposed to be looking at better open sources solutions such as twitter storm or hadoop? Will my current solution be able to handle 5 millions events a day?

Thanks so much.",,,,,Submission,8,0,8
db1d7a0,2016-12-10 18:20:29-05:00,teraflop,,"> Will my current solution be able to handle 5 millions events a day?

This depends on so many different factors that the only real answer is ""test it and find out"".

But in particular, I'm skeptical that scaling up your Python aggregators will do what you want. Each process just reads messages off the queue and submits queries to Postgres, which means your database server is actually doing almost all the work. Running more aggregators in parallel will be able to keep your database busier up to a point, but at some point the DB server will run out of available CPU or IOPS, and throwing more load at it won't help.

5 million events per day is about 60 events per second, assuming no spikes or diurnal variations. If your working set doesn't fit in RAM then each operation will require *at least* 1 read + 1 write, plus overhead to update indexes etc. So you're looking at 120 IOPS as an absolute lower bound, which means you probably want SSD storage for your database.

Another point to bear in mind: if your aggregator updates a database row, and then crashes before acknowledging the message, that message will end up being processed twice. Similarly, you may lose or reprocess events if your RabbitMQ server dies. You can work around this by carefully using a distributed consistent log such as Kafka.",5hlqf5,t3_5hlqf5,noobeeee,,Comment,2,0,2
db1nn3r,2016-12-10 22:52:59-05:00,moeseth,,"My aggregator collects 200 messages batch from rabbitmq at a time and aggregate the stats. So, the load on database is greatly reduced. 

For the fault-tolerance, I will have a look at  Kafka.

Currently, I will grab the messages, aggregates, updates the table and then delete from the queue. As you say, it could fail at the last stage. However, we can allow 200 messages to fail without effecting anything much.",5hlqf5,t1_db1d7a0,teraflop,,Reply,1,0,1
db1tl05,2016-12-11 01:53:37-05:00,teraflop,,"OK. I would suggest that you verify that assumption, by benchmarking your aggregator and seeing how much time it spends doing CPU-bound work vs. waiting for the database query to complete.

Good luck with your project!",5hlqf5,t1_db1nn3r,moeseth,,Reply,2,0,2
db28fbv,2016-12-11 12:09:29-05:00,noobeeee,,Thanks. So I believe there is nothing wrong with my architecture. Just need to make it scalable. :),5hlqf5,t1_db1tl05,teraflop,,Reply,0,0,0
5hff92,2016-12-09 13:32:10-05:00,finessseee,Do you smoke weed?,"Just a general question for CS students and employees. Do you smoke? Do you get drug tested? Did you smoke while studying CS in college? I'm at a point in my life where all I'm doing is smoking weed and playing video games. I keep skipping class and exercising. All I really have going for me rn is a job as a food runner... I find myself a lot less motivated while on weed. Idk how to handle it. I want to quit, but I don't.. I like getting high.. I like the way it makes me feel. I just don't like being stupid, lazy and unconcerned about my priorities. Any advice/suggestions/criticism is appreciated.

Thanks.",,,,,Submission,11,0,11
dazvn0z,2016-12-09 15:04:04-05:00,moaf,,"Weed can be a lot of fun. I smoked daily for ~20 years. However if it's interfering with your day to day life then it might be a good time to reconsider your choices. Don't sell yourself and your future short for a bit of fun and relaxation. If you're struggling with quitting or cutting back, check out /r/leaves. It helped me out a lot when I was quitting.  Now that I'm not smoking daily, I have much more time for side projects and learning new things.",5hff92,t3_5hff92,finessseee,,Comment,19,0,19
dazscjm,2016-12-09 13:59:21-05:00,omg_bbq_brb,,"[deleted]  
 ^^^^^^^^^^^^^^^^0.3013 
 > [What is this?](https://pastebin.com/64GuVi2F/42256)",5hff92,t3_5hff92,finessseee,,Comment,13,0,13
db0qdvp,2016-12-10 07:51:34-05:00,gas3872,,"> If we only did what feels good we'd never leave our beds and just masturbate all day.

That's gold",5hff92,t1_dazscjm,omg_bbq_brb,,Reply,3,0,3
daztqmx,2016-12-09 14:26:39-05:00,dragonnyxx,,"Drug-free myself, but in over twenty years of professional programming I have never once been drug tested.

Seriously, though, while recreational drugs are certainly not uncommon among programmers, ""unmotivated and stupid"" is not going to get you far in this career. If you're going to smoke, you need to ensure that it doesn't interfere with your work performance. There are plenty of organizations to help you quit if that's what you need to do.",5hff92,t3_5hff92,finessseee,,Comment,10,0,10
db0p9k2,2016-12-10 06:47:13-05:00,ConceptOfWuv,,"There are a lot of people who will tell you they are very productive when they smoke. I'm not one of those people. When I was stressed out about projects/tests in college, I would smoke to calm my nerves. I didn't write good code while high, I didn't produce quality projects while high, I didn't learn anything while high. I ended up dropping out. 

&nbsp:

Eventually I took classes again to complete my degree. I stopped with the weed and worked my ass off. Ended up graduating and getting a really great job. **Now** I smoke again because I find it very therapeutic, but I *only do it at the end of the day when I've taken care of all of my chores/errands/etc.*

&nbsp:

I'm not even close to productive while high, so I will never make it a priority. I highly suggest you follow that same mindset. Please prioritize school and work! And if you really can't put that pipe down, at least do it *after* you've finished your homework/projects, and never do it prior to studying or taking tests.

&nbsp:

Also, upvote for the Frank Ocean reference :)",5hff92,t3_5hff92,finessseee,,Comment,3,0,3
dazvfza,2016-12-09 15:00:07-05:00,no_flex_zone,,"I do 

I feel like you only blaze and code if you really know your shit which I don't

so I'm in the process of stopping...


sooooo you should definitely stop too, it's not helping us man, I'm in the same boat as you are. I like it cause a lot of my habits have formed around it. That's why they call it the Devils Lettuce",5hff92,t3_5hff92,finessseee,,Comment,6,0,6
dazscz3,2016-12-09 13:59:36-05:00,TheMonsterCat,,"If you want to quit, stop yourself. It's kinda like ripping off a bandaid from a hairy arm. It'll hurt at first but then the pain will go away. 

At the end of the day the choice is yours. Be strong, my friend.",5hff92,t3_5hff92,finessseee,,Comment,5,0,5
db052gz,2016-12-09 18:27:33-05:00,autoshag,,"Yes i smoke. No i don't get drug tested. I have a strange relationship with marijuana, where i'll often get MORE motivated to get stuff done when I'm high. Days when i seriously have NO motivation to do anything, sometimes i'll have a bowl and then not be able to stop coding. Now it takes me LONGER to get stuff done when I'm high, but to me thats better than nothing getting done at all.

As for smoking during school, in my first few years i did. I didn't get great grades those years. In my last few years of school I only smoked like once a week. I just didn't have the time to be ""not sober"" if you know what i mean. I pretty much just ran on pure stress.

TL:DR Yes i smoke. Weed affects everyone differently though. Whats important is that it isn't negatively affecting your life or your work.",5hff92,t3_5hff92,finessseee,,Comment,2,0,2
db07uyy,2016-12-09 19:39:08-05:00,Bernie29UK,,"Do it after you have finished all the serious stuff for the day.

I've been smoking almost daily over more than 40 years, with a 10 year break in the middle. I can't work efficiently when I'm stoned, but it's because my mind wanders, not because I lack motivation. I notice you are motivated to exercise. That's because it gives you pleasure I guess. Reward yourself for work with pleasure. ",5hff92,t3_5hff92,finessseee,,Comment,2,0,2
dazwzbz,2016-12-09 15:31:07-05:00,andybmcc,,"If you're in the US, a lot of the larger companies will test upon hire to receive Drug-Free Workplace tax credits.  They mostly don't perform random tests, as that's just a cost for at-will employment.  If there is any kind of workplace accident, insurance will require a test.  If you're not performing and skipping class, I suggest you consider taking a break.",5hff92,t3_5hff92,finessseee,,Comment,1,0,1
db00zsv,2016-12-09 16:54:06-05:00,rfinger1337,,"Not a lot of workplace accidents in software development, unless you mean breaking a coffee pot over the head of the asshole who left it with a 6th of a cup and the burner on... but I digress.

And he totally deserved it.",5hff92,t1_dazwzbz,andybmcc,,Reply,-1,0,-1
db0iufj,2016-12-10 01:07:18-05:00,Syde80,,"It sounds like you want to quit.  So you shouldn't care if anybody else is doing it or not.  It's not a like a secret handshake to get into the club.  Be your own person.

Sounds like you are bored with your job, might be time to set your sights to something new.",5hff92,t3_5hff92,finessseee,,Comment,1,0,1
dazw32q,2016-12-09 15:13:00-05:00,uber_kerbonaut,,"No. But I did for a while, because it was just a normal pastime where I grew up. But I gradually realized that it pushed me in the **exact opposite** direction of where I wanted to be mentally. I want to win, I want to focus all the time, and want to be the most perceptive person in the room. Pot made me less of all of those things. Yet people just said ""practice and fight it, and you'll get better at doing those things while high"" what stupid advice. Caffeine is the drug for me. Stronger stimulants are probably good too, but illegal.  ",5hff92,t3_5hff92,finessseee,,Comment,1,0,1
dazxwrs,2016-12-09 15:50:11-05:00,nwertman,,"It sounds like you need some goals my friend.  Once you have goals you really care about, discipline will drive you to make better choices.

Take up Brazilian Ju-Jitsu, CrossFit, a running group, checkers...  Hell, *anything* that has a community around it.  Anything that you can't do well while high.  Once you fall in love with something, it will be more important to you than weed.

Basically addiction is a very ugly beast it doesn't matter if it is porn, weed, gambling or something more socially acceptable like doughnuts.  You have two options: a) hit rock bottom and have no choice but to quit, b) find something you value more than getting high.  Personally, I'd opt for b) in that situation.

Once you reestablish balance in your life, you can decide how much weed you want to smoke from a clearer perspective on your priorities.

JockoPodcast.com worked well for me recently.  It may not be a good fit for you, but it really helped to focus my discipline and my focus.

edit: 20 years in the industry.  Never been drug tested.  I did pot once in college and I didn't enjoy it.  I love the *high* of having a very clear and focused mind way to much to intentionally cloud it.  I would suspect adderall is a more common addiction in this industry than weed.",5hff92,t3_5hff92,finessseee,,Comment,1,0,1
dazy4jx,2016-12-09 15:54:28-05:00,combuchan,,"Of hundreds of places I've been considered at across California and a red state, I know of perhaps only two that did drug test and I declined them for other reasons.  I've never been drug tested for employment, I don't have an interest in the companies that do really.  

If they drugtested, we wouldn't have an Internet or computers as far as I could tell.  Marijuana use is wildly common in the Bay Area among tech people.  

I've find alcohol significantly impacts my productivity and mood more than marijuana.  I've given that up for the most part (only drink on vacations not in my home metropolitan area), replaced it with exercise and a job I like, and have never felt better and able to get stuff done.  

To your other question, pot doesn't necessarily make you lazy, it makes you complacent.  Sometimes this is good because reality can be terrible and it's otherwise fun to smoke, but you're being complacent about things you shouldn't be.  There is *zero* reason to skip class.  Stopping pot won't make you go to school or be motivated if you haven't felt the motivation first.  ",5hff92,t3_5hff92,finessseee,,Comment,1,0,1
db00pig,2016-12-09 16:47:54-05:00,proskillz,,I don't. I never have and I'm fairly sure it kept me ahead in life. I have been drug tested for one or two of my CS related jobs.,5hff92,t3_5hff92,finessseee,,Comment,1,0,1
db00wg2,2016-12-09 16:52:05-05:00,rfinger1337,,Nope.,5hff92,t3_5hff92,finessseee,,Comment,1,0,1
dazrsxr,2016-12-09 13:48:47-05:00,Unconsciousrage,,"I feel like this is the wrong sub for this question.  I knew a few CS majors who liked to smoke much.  Those that did ended up compensating with amphetamines in their jr and sr year.  Much more than ocasional use during midterms and finals.  Personally I would find a vice that does not have the lasting effect that weed has, like alcohol.  However I am slightly biased since I only enjoyed the social aspects of weed.  Other than that I was not a big fan of the high.",5hff92,t3_5hff92,finessseee,,Comment,-1,0,-1
dazx2lu,2016-12-09 15:32:57-05:00,uber_kerbonaut,,I welcome this question. Its an issue that many students face and it's great to have some professionals share their experiences.,5hff92,t1_dazrsxr,Unconsciousrage,,Reply,3,0,3
db08gjv,2016-12-09 19:55:06-05:00,None,,The broader question is does consumption affect computer science?,5hff92,t3_5hff92,finessseee,,Comment,0,0,0
5hfcl9,2016-12-09 13:19:45-05:00,iEmerald,Facing Trouble Understanding the Number System,"Hi,  
  
So I'm a CS major freshman and we're currently studying number systems using this book ( https://drive.google.com/file/d/0B4AWDPXCbgauSThDQVAwU3plVUU/view?usp=sharing )  
  
The problem is I'm facing trouble understanding the concepts covered there, is it just me or the book is too difficult for a beginner?! Are there any resources that I can use to understand how binary works and how to convert between numbers and what signed overflow and such means besides relying on that specific .pdf ??  
  
Thanks in advance",,,,,Submission,3,0,3
db054y2,2016-12-09 18:29:14-05:00,Cland,,"Yeah, I wouldn't have chosen that for an intro to number systems.  Try [this](http://www.math.wichita.edu/history/topics/num-sys.html) and then [this](https://code.tutsplus.com/articles/number-systems-an-introduction-to-binary-hexadecimal-and-more--active-10848).  They're quick and let us know if that helps.

Edit*: The first one was just interesting to me, not really relevant to what you're doing.",5hfcl9,t3_5hfcl9,iEmerald,,Comment,3,0,3
db0nnqu,2016-12-10 05:05:47-05:00,iEmerald,,"The second one is actually pretty helpful, thank you!",5hfcl9,t1_db054y2,Cland,,Reply,1,0,1
db05pdw,2016-12-09 18:43:17-05:00,anamorphism,,"so, let's take the binary number 10.

in decimal, that's 1 * 2^1 + 0 * 2^0 or 2.

binary 100 would be 1 * 2^2 + 0 * 2^1 + 0 * 2^0 or 4.

binary is base 2. you can follow this pattern to have numbers with any base you want.

decimal is base 10, which is why 987 is 987. :)

9 * 10^2 + 8 * 10^1 + 7 * 10^0 or 987.

overflow has to do with how numbers are stored in memory. you only have a certain number of bits allocated for you to store a value.

so, let's say you have an unsigned 4-bit number: 1111 = 1 * 2^3 + 1 * 2^2 + 1 * 2^1 + 1 * 2^0 = 15.

what happens when you add 1 to that number is you do the same thing you do when adding 1 + 9 in decimal. that is, you reset the first digit and add 1 to the next digit place.

in binary, 1 + 1111 = 10000. unfortunately, we need 5 bits to represent that number and we only have 4 bits allocated. so, what ends up happening is that 1 + 1111 = 0000. welcome to overflow. our number has overflowed the bounds of the memory allocated for it.

for signed numbers, the most significant bit (the bit furthest to the left in our number) represents whether the number is positive or negative.

in our example of 1111. that'd be +7 (1 to represent the number is positive, 1 * 2^2 + 1 * 2^1 + 1 * 2^0 = 7). so we add 1 and get 0000 like before, only that changes our number to a negative number. also, negative numbers are represented a bit differently where you get the equivalent of -1 * 2^3 + 0 * 2^2 + 0 * 2^1 + 0 * 2^0 which turns out to be -8.",5hfcl9,t3_5hfcl9,iEmerald,,Comment,3,0,3
db0nnvh,2016-12-10 05:05:57-05:00,iEmerald,,Thanks!,5hfcl9,t1_db05pdw,anamorphism,,Reply,1,0,1
db0opix,2016-12-10 06:12:08-05:00,wasimwesley,,"For any courses that I want better textbooks about, I read up on MIT open courseware, their equivalents are usually very accessible",5hfcl9,t3_5hfcl9,iEmerald,,Comment,1,0,1
5hepmn,2016-12-09 11:33:38-05:00,Tostino,Algorithm to find all combinations of non-overlapping ranges?,"I have a problem i'm working on which I haven't been able to solve on my own, and I haven't found any good resources online (possibly because i'm searching for the wrong thing?).

My problem in the abstract is: I have a set of objects with ranges in which a bunch overlap each other. I need to take that set and get every combination of non-overlapping objects.  

In concrete terms: I'm working on a password strength estimation library. It has a bunch of different methods (`Matcher`) of looking at a password, and finding a `Match`.  The `Matcher` will split the password into parts and try and find patterns, words, repeats, sequences, etc on each part of the password.  Any time a `Match` is found, it's added to a set.  The `Match` contains a starting and ending index, and a weight.  After all `Match`'s have been found, I need to find the combination of them that doesn't overlap and has the lowest weight.",,,,,Submission,2,0,2
dazsszr,2016-12-09 14:08:17-05:00,thewataru,,"Seems like you need a recursive generation algorithm here. Understand what is the recursion on example of generating combinations. You will do similar thing. You will have some ranges already taken into current set, the id of the first unused yet range is the only argument to the function. Then you either take it (if it doesn't intersect with any other range) or skip it. In either way you update the current set and call with argument id+1.

You can speed-up this algorithm a little, if you sort your ranges by beginning and keep together with the set the maximum end of any taken range. Then you can very quickly decide if you can take any given range (it has to begin later than current maximum end). 

Edit:  
words",5hepmn,t3_5hepmn,Tostino,,Comment,2,0,2
db1wjo4,2016-12-11 04:11:49-05:00,Tostino,,"Thank you, I appreciate the reply. Even with your explanation I am still not getting it though sadly. Any chance you could elaborate with some pseudo code?",5hepmn,t1_dazsszr,thewataru,,Reply,1,0,1
db1xhu5,2016-12-11 05:05:59-05:00,thewataru,,"Suppose you have n segments. i-th segment is from lo[i] to hi[i] inclusive. Already sorted in ascended order by lo[]. Indexed from 0 (as in c++). I treat touching segments as intersecting (easy to change the code). Now, for simplicity I will use global variables, but you can just pass them as references to each function call.


    int taken:
    int segments[]:
    
    void Generate(int i, int mx) {
      if (i == n) {
        // Output/process segments[0...taken-1] as a one set of
        // non-intersecting intervals.
        return
      }
      if (lo[i] > mx) { // if we can take i-th segment
        segments[taken++] = i:
        Generate(i+1, hi[i]):
        taken--: //revert changes to global state
      }
      //always can skip current segment
      Generate(i+1, mx)
    }
  

Please note, that it will generate an empty set (no segments taken at all). You can filter it out by checking for taken>0 where you process/output current set. Segments are stored by their number in array segments[]. There are 'taken' of them.

First, in this recursive function we have a terminating condition. i is the index of first not considered yet segment. mx is the highest end of all segments (since they are non-intersecting, it is hi[] of the last segment). Therefore, if i==n, we considered all segments and have to terminate. 

Next, we can either take or not current segment. We can take it only if it is not intersecting with any previous segments. Since they are sorted we only have to check that it is hi enough (therefore, lo[i]>mx). If we take current segment, we add it to segments[], continue recursively, and after that we have to backtrack all changes to global state. Here we can simply decrease 'taken' variable. There will be some garbage left in the array, but it will be ignored or overwritten, so no troubles here. If we don't take current segment, we simply skip it and call Generate function with i+1 as an index of first not considered segment.

To get answer to you problem, you have to call Generate(0, lo[0]-1). Instead of lo[0]-1 you can put there any small enough number. It has to be smaller than all segments lowest ends can be.",5hepmn,t1_db1wjo4,Tostino,,Reply,2,0,2
db3aeg5,2016-12-12 02:32:22-05:00,Tostino,,"Appreciate the help, I did actually get everything working.

If interested, code is in this commit: https://github.com/GoSimpleLLC/nbvcxz/commit/dbe4ecb6a5d04e240538577ea320409756829de5",5hepmn,t1_db1xhu5,thewataru,,Reply,1,0,1
db05sty,2016-12-09 18:45:42-05:00,thatwasntababyruth,,"I may be misunderstanding, but it sounds like you have a set of sets, and you want to find all sets for which their intersection with all other sets is the empty set. The naive solution would be to loop over your sets, and for each one, calculate the intersection of it with the union of all other sets. Intersection and union are available in most programming languages. If the output is empty, you have a valid result.",5hepmn,t3_5hepmn,Tostino,,Comment,2,0,2
dazpap9,2016-12-09 13:00:21-05:00,randomfield,,"Answering this quickly while entertaining my daughter - after a quick read, it sounds like you want to look for a dynamic programming solution to this. Smells like something that can be reduced to subset sum or something :) ",5hepmn,t3_5hepmn,Tostino,,Comment,1,0,1
5hdagi,2016-12-09 06:32:30-05:00,Okmanl,What determines a hit or a miss for direct mapped cache?,"Suppose we have a 8KB direct-mapped data cache with 64-byte blocks.

Offset =log2(64)=6

bits

number of blocks =8k/64=125

Index=7

bits

Tag=32–13=19

bits

Then how do I tell whether or not I have a hit or a miss given an address?

https://i.stack.imgur.com/c2SUW.png

Is it (Block address) modulo (Number of blocks in the cache)

Where block address = byte address / bytes per blocks?

For example if (block address) mod (number of blocks in cache) = 2, then does that mean row 2 of my picture is a hit?",,,,,Submission,2,0,2
dbcrstr,2016-12-18 15:53:39-05:00,pencan,,"Offset is the byte in the cache line.  You can picture this selecting horizontally.

Index determines the cache line.  You can picture this selecting vertically.

Tag determines whether it's a hit or miss.  You can picture this as a either hash match or selecting depth-wise in a 3d array.

Basically, your index and offset select geometrically where to look.  Your tag at the index must match the tag at the address for it to be a hit",5hdagi,t3_5hdagi,Okmanl,,Comment,1,0,1
5hb2jl,2016-12-08 20:48:54-05:00,1621656,how does instruction level parallelism impact software performances?,I would like a rough idea on how does instruction level parallelism impact software performances so I know a bit about the topic and them I can research the right things in order to know what it is.,,,,,Submission,4,0,4
dazerhf,2016-12-09 09:21:10-05:00,andybmcc,,"There is a theoretical limit to the impact of parallelism: https://en.wikipedia.org/wiki/Amdahl's_law

Is this what you're looking for, or do you have questions about specific mechanisms (e.g. pipelining, out of order execution, register re-mapping, etc?)",5hb2jl,t3_5hb2jl,1621656,,Comment,1,0,1
5h9u78,2016-12-08 16:57:42-05:00,TholomewP,What's your favorite algorithm?,,,,,,Submission,22,0,22
dayq5z0,2016-12-08 19:46:47-05:00,SayYesToBacon,,Bogosort. Randomize the list until it is sorted. I find its stupidity endearing,5h9u78,t3_5h9u78,TholomewP,,Comment,28,0,28
daytqbr,2016-12-08 21:07:46-05:00,chummer7,,"I like this, going to use it :)",5h9u78,t1_dayq5z0,SayYesToBacon,,Reply,2,0,2
dazfu80,2016-12-09 09:48:28-05:00,PM_ME_YOUR_SUBARU,,I prefer Bogobogosort personally.,5h9u78,t1_dayq5z0,SayYesToBacon,,Reply,2,0,2
daynobl,2016-12-08 18:49:10-05:00,fredspipa,,"ACO (Ant Colony Optimization), because it's really simple and sexy and ants are just the greatest thing ever.",5h9u78,t3_5h9u78,TholomewP,,Comment,4,0,4
days8dz,2016-12-08 20:34:29-05:00,brendanrivers,,FFT,5h9u78,t3_5h9u78,TholomewP,,Comment,3,0,3
daz4ca6,2016-12-09 01:46:31-05:00,arichi,,FFTs are all fun and games until someone applies it to your cat...,5h9u78,t1_days8dz,brendanrivers,,Reply,4,0,4
dazbsjm,2016-12-09 07:45:30-05:00,dk-,,Fun fact: what an MRI machine does is take an FFT of a person (or a cat!),5h9u78,t1_daz4ca6,arichi,,Reply,5,0,5
daym8qz,2016-12-08 18:16:53-05:00,pencan,,Simulated Annealing blew my mind the first time I was taught it in VLSI,5h9u78,t3_5h9u78,TholomewP,,Comment,3,0,3
dazc2ho,2016-12-09 07:56:06-05:00,thechao,,Knuth-Plass line breaking.,5h9u78,t3_5h9u78,TholomewP,,Comment,3,0,3
dazb69m,2016-12-09 07:19:47-05:00,besirk,,PCA.,5h9u78,t3_5h9u78,TholomewP,,Comment,2,0,2
dazsutp,2016-12-09 14:09:16-05:00,Rangi42,,"Simple search algorithms: BFS, DFS, A\*. I like how abstract the idea of a ""search space"" can be, so they end up being applicable for all kinds of non-geometric problems.",5h9u78,t3_5h9u78,TholomewP,,Comment,2,0,2
daym8e3,2016-12-08 18:16:39-05:00,Bone_Machine,,Dinic's algorithm for computing maximum flows!,5h9u78,t3_5h9u78,TholomewP,,Comment,1,0,1
daz6bc5,2016-12-09 03:09:00-05:00,JustGozu,,CYK. Good algorithm to explain dynamic programming to students.,5h9u78,t3_5h9u78,TholomewP,,Comment,1,0,1
dazd3bo,2016-12-09 08:32:03-05:00,Wallblacksheep,,[Jaccard's Index](https://en.wikipedia.org/wiki/Jaccard_index)- utilized it to compare image similarities.,5h9u78,t3_5h9u78,TholomewP,,Comment,1,0,1
5h8x9f,2016-12-08 14:26:33-05:00,1621656,ILP's impact on software performance?,"how does instruction level parallelism impact software performances?
",,,,,Submission,1,0,1
daybwor,2016-12-08 14:49:56-05:00,JonL13,,"This is kind of a broad question, but I can try to give a simple answer. 

With ILP, multiple instructions that usually happen sequentially happen in parallel: for example if multiple iterations of a for loop are independent, the for loop could have several iterations of the loop happening at once. This parallelism allows the different instructions to go to different processors, increasing the overall runtime. 

Does this help at all? If you have any more specific questions, let me know. I'm not an expert but I just finished my graduate level Computer Architecture class so I may be able to help. ",5h8x9f,t3_5h8x9f,1621656,,Comment,1,0,1
dayczko,2016-12-08 15:10:28-05:00,dhobsd,,"> increasing the overall runtime

Increasing?",5h8x9f,t1_daybwor,JonL13,,Reply,1,0,1
daymtet,2016-12-08 18:29:41-05:00,JonL13,,"Decreasing, my bad on that typo. ",5h8x9f,t1_dayczko,dhobsd,,Reply,1,0,1
5h5w24,2016-12-08 03:36:12-05:00,winter_mutant,What language(s) were used for some of the older video game consoles?,"So obviously NES, SNES, and Genesis games were written in pure assembly, because computer technology was still in its infancy at the time. But how about the Nintendo 64 and Playstation 1? Or even the Gamecube, Xbox, and Playstation 2?

At what point were console games written in languages other than assembly, and what did those languages tend to be?

And as a little add-on question: what language was Ocarina of Time written in? I've always wondered that.",,,,,Submission,25,0,25
daxpyj3,2016-12-08 05:54:53-05:00,boo_ood,,"It was basically all assembly, C or C++.
The Nintendo 64's devkit was no exception, and included a C compiler and libraries.",5h5w24,t3_5h5w24,winter_mutant,,Comment,15,0,15
dayacj4,2016-12-08 14:20:13-05:00,adbJ114,,Ocarina of Time was written in Elvish,5h5w24,t1_daxpyj3,boo_ood,,Reply,3,0,3
daxwwdu,2016-12-08 09:57:01-05:00,ObeselyMorbid,,I thought n64 was programmed in MIPS32? ,5h5w24,t1_daxpyj3,boo_ood,,Reply,0,0,0
daxx4cn,2016-12-08 10:02:09-05:00,boo_ood,,"MIPS32 isn't a programming language, it's an instruction set.",5h5w24,t1_daxwwdu,ObeselyMorbid,,Reply,10,0,10
daxxqyn,2016-12-08 10:16:22-05:00,ObeselyMorbid,,"Well it uses MIPS 32 architecture, but I was under the impression it was coded in assembly. I'm realizing now how stupid that would have been for game dev",5h5w24,t1_daxx4cn,boo_ood,,Reply,4,0,4
day2g5t,2016-12-08 11:50:04-05:00,james4765,,"There was a fair bit of hand optimized assembler in the Nintendo dev tools. Day to day developers wouldn't have touched it, but the people writing the rendering engines would drop back to assembler if necessary.",5h5w24,t1_daxxqyn,ObeselyMorbid,,Reply,6,0,6
daxqk0h,2016-12-08 06:25:46-05:00,retop56,,"[Here's a great series of blogs from one of the creators of Crash Bandicoot.](http://all-things-andy-gavin.com/2011/02/02/making-crash-bandicoot-part-1/) He goes over some of the programming challenges they faced and how they overcame them, if you're interested in that.",5h5w24,t3_5h5w24,winter_mutant,,Comment,8,0,8
day76xu,2016-12-08 13:20:21-05:00,francispoop,,I remember that article! Such a good read. Thanks dude.,5h5w24,t1_daxqk0h,retop56,,Reply,2,0,2
dayl1cn,2016-12-08 17:50:20-05:00,dizzydizzy,,"I made games for Jaguar,ps1,  Xbox and ps2. (At Bullfrog and MuckyFoot)

All were written in C with assembly for a few small places where hand optimization was required.

Some consoles came with no standard libs, so you had to write your own sprintf etc.

I wrote a few 100% assembly games for amiga and atari ST, but it was at that 16 bit era that C started to take over from 100% assembly.

Jaguar had a funky co processor that had no compiler so anything for that had to be hand written, it also had a dsp you had to hand code for.",5h5w24,t3_5h5w24,winter_mutant,,Comment,5,0,5
daz090e,2016-12-08 23:39:41-05:00,MCPtz,,"That's interesting on the Jaguar. Little support for programmers means projects take too long.

That explains a lot about that console.

Aside,
When programming for those consoles, what was your favorite bug",5h5w24,t1_dayl1cn,dizzydizzy,,Reply,3,0,3
daz5979,2016-12-09 02:23:12-05:00,dizzydizzy,,">what was your favorite bug

Probably when the Jaguar build of Theme Park came back from atari as failed compliance testing.

Apparently they take the cartridge out and reinsert and check the game reboots, they repeat this about 30 times and we failed to boot a couple of times in those 30 tries.

Turned out it never even reached our own code, it was crashing in Atari's own boot code.

That aside the jaguar was a really fun console to code for, lots of interesting hardware in it.. Like an amiga on steroids.
",5h5w24,t1_daz090e,MCPtz,,Reply,3,0,3
daxpqzr,2016-12-08 05:43:24-05:00,james4765,,"On the Playstation 2: here's an article on the Emotion Engine DSP / coprocessor's architecture:

http://archive.arstechnica.com/reviews/1q00/playstation2/m-ee-1.html

IIRC, the dev kit for the PS1 and PS2 was C++ (MIPS CPU), with a lot of custom libraries to interface with the hardware. Third party 3D engines had just started to be a thing at that point.",5h5w24,t3_5h5w24,winter_mutant,,Comment,3,0,3
daxpgse,2016-12-08 05:27:43-05:00,randomredditor87,,"I'm on mobile right now so I can't link it, but there are some really good wikipedia articles that talk about video game development during the 5 and 6th generation, with the n64 developing games in 3d was  very new and challenging with games like Mario 64 paving the way for most developers who wanted to work on the n64. ",5h5w24,t3_5h5w24,winter_mutant,,Comment,2,0,2
5h3egv,2016-12-07 18:31:57-05:00,CEP2,"MatLab/DipImage | Image Processing - Segmentation on Images, tutorials/resources?","I've got a project coming up in one of my classes having to do with automatic segmentation (no user input) of 8-bit (grayscale) images.

Although I do have notes from lecture that are pretty nice, if anyone has any additional resources for Matlab | DipImage that might be able to assist in my learning that'd be awesome!

Also, if you've any grayscale images that might make for good practice or an interesting challenge, I definitely appreciate those as well! 

Many thanks in advance to those who are able to lend a hand! ",,,,,Submission,1,0,1
5h0hus,2016-12-07 10:31:32-05:00,Splurgejan,SebPearce's BS,Hello Reddit! First post but an active reader here. Just wondering if you guys could provide some insight as to how you think http://sebpearce.com/bullshit/ works from a programming perspective! Thanks!,,,,,Submission,6,0,6
dax6ljq,2016-12-07 19:46:03-05:00,qwerty_danny,,Maybe [Markov Chains](https://blog.codinghorror.com/markov-and-you/)?,5h0hus,t3_5h0hus,Splurgejan,,Comment,2,0,2
dawqp4e,2016-12-07 14:28:04-05:00,dhobsd,,"""I don't understand this thing. It must explain this other thing I don't understand!""",5h0hus,t3_5h0hus,Splurgejan,,Comment,1,0,1
5gyeb2,2016-12-07 01:24:37-05:00,karmicnerd,"Employees/ex-employees at Google/Apple, What was you awestruck moment in the company?",,,,,,Submission,0,0,0
dawcviy,2016-12-07 09:53:16-05:00,CuntFaggotAssRape,,"Don't work there but have friends that do. 

Did you know that every toilet at google headquarters has every toilet bidet?!?!",5gyeb2,t3_5gyeb2,karmicnerd,,Comment,2,0,2
dawegkh,2016-12-07 10:28:33-05:00,lbrwvs,,"This isn't just limited to the headquarters :)
Source : I work at Google",5gyeb2,t1_dawcviy,CuntFaggotAssRape,,Reply,1,0,1
5gwyov,2016-12-06 20:25:30-05:00,Robotigan,What's an expression that requires more than two registers to evaluate?,"I'm trying to think of one, but it seems like every expression can be created from binary subexpressions.  I'm probably missing something obvious.",,,,,Submission,5,0,5
davsnbi,2016-12-06 21:44:17-05:00,teraflop,,"This depends on what operations you allow. Assuming a RISC architecture (so arithmetic operations can only take registers as operands, not memory locations) then I don't think you can evaluate

    A*B+C*D

with only two registers.",5gwyov,t3_5gwyov,Robotigan,,Comment,2,0,2
davv2aq,2016-12-06 22:41:45-05:00,None,,[deleted],5gwyov,t1_davsnbi,teraflop,,Reply,1,0,1
davvbwt,2016-12-06 22:48:03-05:00,teraflop,,"Well, yeah, I assumed OP wasn't interested in cases where you spill temporaries to memory, because that makes the problem trivial.",5gwyov,t1_davv2aq,None,,Reply,4,0,4
davw89t,2016-12-06 23:10:58-05:00,Robotigan,,Correct.,5gwyov,t1_davvbwt,teraflop,,Reply,1,0,1
davwbfs,2016-12-06 23:13:14-05:00,Robotigan,,"Yeah, I felt stupid when I figured it out.  Quick question, assuming its not built into the hardware, would division require three registers?  For instance a/b, you'd have to increment a count every time you perform a = a - b.",5gwyov,t1_davsnbi,teraflop,,Reply,1,0,1
daw0lhl,2016-12-07 01:23:53-05:00,zorkmids,,"Yes, if you're dividing by repeated subtraction.  [There are other ways to do it](https://en.wikipedia.org/wiki/Division_algorithm), but pretty much any algorithm will involve a quotient, remainder, and loop index.
",5gwyov,t1_davwbfs,Robotigan,,Reply,2,0,2
davwbux,2016-12-06 23:13:32-05:00,ldpreload,,"Multiplying two 32-bit numbers gives you up to a 64-bit result, so the x86 (and I think many others) MUL operation takes two 32-bit input registers and gives you the output split across two 32-bit registers.

Many architectures, including x86, make lots of use of segments / selector registers, but I'm not sure if those really count. They're not general-purpose registers. x86 also does a lot of addressing modes which involve adding together multiple registers: e.g., you can ask it to load the data at address, say, ESI + EAX * 4, without having to spend two instructions constructing that address in some register, and then one more to do the actual load. But the mathematical operaiton doesn't _require_ more than two registers.",5gwyov,t3_5gwyov,Robotigan,,Comment,1,0,1
davwpxv,2016-12-06 23:23:49-05:00,Robotigan,,"This is more trivial than what I was looking for, but thanks anyways.",5gwyov,t1_davwbux,ldpreload,,Reply,1,0,1
dawaswn,2016-12-07 09:01:35-05:00,BonzoESC,,"Binary functions are a fundamental part of our system of mathematics, and using [partial application](https://en.wikipedia.org/wiki/Partial_application) to turn *n*-ary functions into binary or unary functions makes it possible to express most functions in terms of binary functions.",5gwyov,t3_5gwyov,Robotigan,,Comment,1,0,1
dawm84z,2016-12-07 13:03:38-05:00,tarballs_are_good,,"However, now you must support functions being outputs of other functions for this to be true. This isn't the case when talking about register machines, where effectively your only data type is a word. ",5gwyov,t1_dawaswn,BonzoESC,,Reply,1,0,1
daws168,2016-12-07 14:53:34-05:00,BonzoESC,,"You don't need the register machine itself to support functional programming concepts, it's just a useful way to think about how certain parts of the compiler work.

Consider the `A*B+C*D` function from above, compiled for x86_64: https://godbolt.org/g/Ga6qIn

Different parts of the `multiplication_sum` function can be interpreted as partial applications of the entire function: https://i.bf1c.us/asm%20partial%20application.png

There's more than one correct and useful way to think about terminology and how computers work. I find FP logic useful because it's easy to reason about and has lots of useful terminology, but ASM and processor bytecode are great too because they, well, run on computers.",5gwyov,t1_dawm84z,tarballs_are_good,,Reply,1,0,1
daxdkxu,2016-12-07 22:24:30-05:00,tarballs_are_good,,"I am not familiar with that interpretation of partial application.

What you've outlined looks like a series of evaluations which are being erroneously equated to partial application.

Generally partial application is the transformation of a function like f : X x Y -> Z to another function g : Y -> Z such that g = (lambda x . (lambda y . f(x, y))) n for some n in X. I would say that this transformation is distant to what you're showing. ",5gwyov,t1_daws168,BonzoESC,,Reply,1,0,1
5gegfs,2016-12-04 02:47:23-05:00,ThePrime7,Threads for General Purpose Operating System,"If you are designing a general purpose operating system for a common desktop user, which type of threads will you choose in your operating system?
User level threads or Kernel level threads?
common desktop user tasks might include: office tasks(word,excel,etc), music, videos, internet accessing, file storage.",,,,,Submission,0,0,0
darw7so,2016-12-04 10:52:32-05:00,IgnorantPlatypus,,Sounds like home work.,5gegfs,t3_5gegfs,ThePrime7,,Comment,5,0,5
datf30w,2016-12-05 11:30:45-05:00,ThePrime7,,is a question nonetheless. ,5gegfs,t1_darw7so,IgnorantPlatypus,,Reply,1,0,1
datigp6,2016-12-05 12:37:26-05:00,IgnorantPlatypus,,But we're not going to do your homework for you. What do you know about the question? What is your reasoning behind an attempt at an answer?,5gegfs,t1_datf30w,ThePrime7,,Reply,1,0,1
5gd05o,2016-12-03 20:27:10-05:00,EESTUFF,QUESTION: What is the minimal computational power required for a cluster to provide a fast fourier transform and then a hidden markov model on a ten second mp3?,"Given a 8kHz sample with 8bit precision on a sample of up 10(realistically 5) seconds, how much computational power would be necessary to churn out the speech-to-text output within 4 seconds from end-of-sample?

Any pointers to analyzing the problem would be appreciated, thanks

Edit3: I think the architecture of the problem would require the sample being fragmented into relatively smalls chunks, which would be individually handled by different processors, i.e. in parallel. Each data splice would have a packet marker that would allow a merge sort to maintain their context relative to other data, allowing for the HMM to increasingly make sense of context per merge.  Output to be a Speech-to-Text of the input",,,,,Submission,5,0,5
darmp1q,2016-12-04 03:30:58-05:00,icendoan,,"Surely the real question is how long you want this to take? A determined chimp with a very large abacus can theoretically compute the FFT and then a hidden Markov model, on an mp3 of arbitrary length, but you might be waiting a while.",5gd05o,t3_5gd05o,EESTUFF,,Comment,6,0,6
dary4bi,2016-12-04 11:42:52-05:00,EESTUFF,,"Yeah, the implicit question is, given real time response with tops a latency of 4 seconds, how much computational power would you need",5gd05o,t1_darmp1q,icendoan,,Reply,1,0,1
dardm71,2016-12-03 22:32:53-05:00,EESTUFF,,"I mean I understand that google and apple have their own facilities dedicated to them, for voice and siri respectively, but they are dealing with vast swaths of individual requests. By computationally powerful, I mean how many individual processors would need to be working parallel in order to efficiently output the correct STT?

EDIT : 8kHz, 8bit precision. these are some constraints I found in a research paper that seems to be effective at maintaining quality and content of the sample.",5gd05o,t3_5gd05o,EESTUFF,,Comment,2,0,2
daylweu,2016-12-08 18:09:15-05:00,EESTUFF,,"not that anyone is following this, but for posterity, I found that apple was able to write software of 1.2 GB to run the entire speech to text on a mac in about three seconds. ",5gd05o,t3_5gd05o,EESTUFF,,Comment,1,0,1
5gconu,2016-12-03 19:18:40-05:00,UnicornFukei42,Problem with Binary Switch Forwarding,"Right now I'm trying to figure out how to forward data through a binary switch in a tree topology. How do I ensure that I get from one specific host to the next?

I don't know if this helps but I'm using Mininet, we are required to use 2 windows, one to SSH into the other. ",,,,,Submission,3,0,3
5gbygj,2016-12-03 16:49:21-05:00,lordvadr,Looking for a specific programming teaching game played years ago,"So more than two decades ago, I was a camper at a summer camp where we learned to program.  There was this game we had that, if you were good enough at it, you could play.  Essentially, you wrote the ""software"" to control a little ""robot"" on the screen, and it they fought each other.  The ""robots"" looked more like tanks.

This is what I remember about it:

The language was BASIC-ey.  You had only a handful of things you could do...drive forwards, drive backwards, rotate (either direction) the radar, gun, or body of the tank, ""ping"" with the radar (which would give you distance if it came back), and fire the gun.

Each of the parts rotated at different speeds, with the radar being the fastest, and the body being the slowest...this was where a lot of the strategy came in, or at least what I remember spending the most time on.  You could turn the ""radar"" which was how you located your enemies, the gun, and the body of the tank independently, or you could lock any two (or all three) of them together, but they rotated at the slowest speed of whatever was locked together.  Basically, if you locked your gun and radar together, you could fire as soon as you got a ping back, but when you rotated them searching for enemies, both the radard and gun would rotate at the gun speed, which was slower than the radar could move.

The software supported a compile-like feature...evidently there were contests (how big, I don't know)...so that you could send your robot to someone but they couldn't see the source.  It came with, what was apparently the champion of one of these contests called ""R2D2"", that was VERY well written...in fact, that was the metric everybody compared their code too...how well it fared against R2D2.

It was otherwise very simple.  The graphics were not great; it had an editor and the arena and that's about it.

I have no idea what this program was called, and I've on and off looked for it on the internet for the better part of the last 10 years with no luck.  Robocode looks a lot like it (perhapse with graphics improvements), but it was definitely not in java, and this was around 1995 or so the last time I played it.

I would reward anybody who can help me find it.  The camp was called National Computer Camps (NCC) and was located in Connecticut.  I have considered getting in contact with the Camp, but about my 3rd or 4th year I want as a Counselor, I left on bad terms.",,,,,Submission,9,0,9
dar395x,2016-12-03 18:08:53-05:00,Whitegrlproblem,,I'll see if i can find it ,5gbygj,t3_5gbygj,lordvadr,,Comment,2,0,2
dar79hb,2016-12-03 19:49:29-05:00,pi_stuff,,"[Robot Battle](https://en.wikipedia.org/wiki/Robot_Battle)?
",5gbygj,t3_5gbygj,lordvadr,,Comment,2,0,2
darae1b,2016-12-03 21:10:13-05:00,lordvadr,,"Release date 2002, I'm guessing not unless we had some kind of beta copy 5-7 years prior although the wiki page says 1994-1995 as inital release.  I also don't remember ""sliding"" or ""radio"" methods, but that seems pretty close.  Now to see if I can hunt down a copy of it.",5gbygj,t1_dar79hb,pi_stuff,,Reply,1,0,1
darr5el,2016-12-04 07:43:08-05:00,avent606,,"It could have been based on Logo,  Ive heard people call it turtle ( the robot, turtle thing you moved). It ran on Apple and Atari. 

There is a modern version:
https://en.wikipedia.org/wiki/Logo_(programming_language)
  ",5gbygj,t3_5gbygj,lordvadr,,Comment,2,0,2
dasosy3,2016-12-04 21:12:37-05:00,lordvadr,,"It wasn't logo.  We had a Lego Logo set there as well, and this was something entirely different...language and everything.",5gbygj,t1_darr5el,avent606,,Reply,1,0,1
5ga2bi,2016-12-03 10:38:33-05:00,Tacobellington,Depth First Searching Order,"I'm curious if the **exact** order matters when referring to DFS. For example in [this](http://www.thecshandbook.com/public_html/img/uploads/graph.png) graph, could the order of traversal, starting from 2 be:

2,1,5,4,6,3

or

2,3,4,5,1,6

or a bunch of other ones? As long as I follow the rules of going as deep as possible and then backing up?

",,,,,Submission,1,0,1
daqncxk,2016-12-03 11:40:10-05:00,PEdrArthur,,"The exact order will matter only if you are halting the traversal based on some heuristical criteria. However, in a full traversal, order will be mostly irrelevant.",5ga2bi,t3_5ga2bi,Tacobellington,,Comment,1,0,1
5g9z7p,2016-12-03 10:18:09-05:00,tech6,Getting started with formal methods for embedded systems,Can you please recommend resources to get started with formal methods . The implementation language is C as I am constrained by compiler support for the embedded paltform. I am looking for resources from practitioners point of view (use of tools) . ,,,,,Submission,0,0,0
5g9q7a,2016-12-03 09:16:13-05:00,FOKvothe,[Discrete mathematics] x Ry -> x^2 = y^2 in Z,"Is this an antisymnetric relation? I dont think so, because:
x,y in Z: x R y AND y R x  -> x = y, because -2^2 is = to 2^2, but -2 != 2. Is this correct?

Any help is appreciated.",,,,,Submission,3,0,3
daqm7lb,2016-12-03 11:09:42-05:00,jxf,,"A relation `R` is antisymmetric on a set `S` if `∀ a, b ∈ S, R(a,b) & R(b,a) ⇒ a = b`. So if you can find a single counterexample where R(a,b) and R(b,a), but `a != b`, this proves the relation is _not_ antisymmetric.

Here the relation is `R(a,b): a^2 = b^2`. So if we can find an `(a, b)` such that:

* `R(a.b): a^2 = b^2` holds,
* `R(b,a): b^2 = a^2` holds,
* but `a != b`

then we'll have proven the relationship is not antisymmetric. Your example of choosing a = -2, b = 2 shows it's not antisymmetric, since:

* `R(-2, 2): (-2)^2 = (2)^2` holds
* `R(2, -2): (2)^2 = (-2)^2` holds
* but `-2 != 2`

Hope that helps!",5g9q7a,t3_5g9q7a,FOKvothe,,Comment,3,0,3
daqmyvf,2016-12-03 11:29:56-05:00,FOKvothe,,"Yes, that helps. Thank you! :)",5g9q7a,t1_daqm7lb,jxf,,Reply,1,0,1
5g5zv2,2016-12-02 16:46:25-05:00,ahhneil,c++ programming,"Bookmark
C++ programming

Use the C++ Standard Template Library’s stack class to write a program for processing a file of postfix expressions.

Recall that in a postfix expression, a binary operator comes after its two operands. Thus the infix expression (w + x) * (y – z) becomes w x + y z – * in postfix.

A postfix expression can be efficiently evaluated using a stack as follows.

Read the expression from the left to the right.

When an operand is encountered, push it on the stack.

When an operator is encountered, pop the top two elements from the stack, perform the operation and then push the result.

There should be exactly one element on the stack at the end and this element is the value of the expression.

For example, if the expression 20 5 – 3 * is entered, the evaluation should proceed as follows.

Symbol read

20

5

–

3

*

Actions taken

Push 20

Push 5

Pop 5, Pop 20, Push 20 – 5

Push 3

Pop 3, Pop 15, Push 15 * 3

The result of 45 should then be output.

Your program should ask the user for the name of an input file. The input file will contain postfix expressions one per line. The program should open the file and evaluate the postfix expressions in the file. Your program should give the user the option of writing the results to an output file or to the screen.

Your program should also check the entered postfix expression for validity. A postfix expression is invalid if there is more than one element on the stack at the end of the evaluation or if ever there are not enough operands on the stack when an operation is performed.

E.g. If the input file contained:

20 5 – 3 *

100 50 60 + +

3 4 5 +

3 4 5 +

3 4 + +

3 4 ?

The output would be:

45

210

Too few operators.

Too few operators.

Too many operators.

Illegal operation

Limit your postfix expression to the arithmetic operations of +, –, *, and / on integer values.",,,,,Submission,0,0,0
daptsne,2016-12-02 17:57:42-05:00,lneutral,,"I keep saying this:

This is not /r/domyprogramminghomework",5g5zv2,t3_5g5zv2,ahhneil,,Comment,6,0,6
dapr180,2016-12-02 16:56:17-05:00,scorpzrage,,You know... A question would've been useful here.,5g5zv2,t3_5g5zv2,ahhneil,,Comment,4,0,4
db08g8s,2016-12-09 19:54:52-05:00,KingTwix,,You just straight up copy pasted your assignment,5g5zv2,t3_5g5zv2,ahhneil,,Comment,1,0,1
5g56iv,2016-12-02 14:23:16-05:00,vpjrml_,"Pre, in and post order traversal","Can pre-order, in-order and post-order traversals be done on only trees or can it be done on graphs too? If it can be done on graphs, then how do i know what's left or right at each stage?

Thanks in advance.",,,,,Submission,5,0,5
dapllw4,2016-12-02 15:05:12-05:00,PEdrArthur,,"Pre-order and post-order can be done in graphs. For example, you may reach a graph node, evaluated some function in the nodes its edges point to, and afterwards perform the evaluation on the node itself. Likewise, you could evaluate node's value first and afterwards visit the nodes its edges point to. These graph traversal strategies are called Depth-first searches. Indeed, tree traversals are special cases of graph traversal.",5g56iv,t3_5g56iv,vpjrml_,,Comment,2,0,2
dapvpon,2016-12-02 18:43:44-05:00,vpjrml_,,"Thanks for your reply. :) 

I understood what you explained there. My query is - When I reach a graph node, how will I identify that an edge is left or right? Pre-order, in-order and post-order traversals are defined in terms of ""evaluate"", ""go left"" and ""go right"". How does that fit in the graph?",5g56iv,t1_dapllw4,PEdrArthur,,Reply,2,0,2
dapxx9x,2016-12-02 19:40:15-05:00,adribar,,"> Pre-order, in-order and post-order traversals are defined in terms of ""evaluate"", ""go left"" and ""go right"". 

Not quite.

Pre-order: process node, then recursion on each branch

Post-order: recursion on each branch, then process node

In-order: recursion on one branch, then process node, then process the other branch.

You can implement a pre order strategy where you do recursion on the left, then on the right, or the other way around. It's still pre order. Left and right are just names.

",5g56iv,t1_dapvpon,vpjrml_,,Reply,4,0,4
daqi564,2016-12-03 09:01:40-05:00,vpjrml_,,I get it! Thanks for explanation :),5g56iv,t1_dapxx9x,adribar,,Reply,1,0,1
5g4ldw,2016-12-02 12:43:05-05:00,herejust4this,Reliable 8bit signed decimal/binary converter with the ability to add & subtract 8bit binary values?,"I'm trying to create a binary calculator that I can add/subtract two 8bit *signed* values together which will give me an answer in the form of another 8bit signed value as well as the decimal value of the answer. 

Right now I'm trying to compare the answers that my program spits out to the correct answers using some browser based versions of this program. The problem is that none of the websites I'm using seem to have all the features I need. 

Some websites only add/subtract unsigned bits. Other sites only give me a binary answer but no decimal answer, while others seem to just give me a completely wrong answer.

I can do the calculation by hand with 2's compliment but I would much rather compare it to the correct answer quickly for debugging purposes.

Does anyone know a site (or even an Android app) that could do this cleanly for me?",,,,,Submission,5,0,5
dapkucj,2016-12-02 14:50:00-05:00,theobromus,,"Well, adding and subtracting signed 8bit numbers is actually the same as adding and subtracting unsigned 8bit numbers (this is the magic of the 2s complement representation). The difference comes when you're comparing numbers, or converting to decimal (by which I assume you mean a decimal string). And both are the same as adding normal numbers together except when you have overflow. 

Almost every programming language has some way of dealing with signed bytes (although the topic is somewhat confused - languages like Java have *only* signed bytes, while others like c++ don't actually specify the size of int types in the standard :().

If you can give a bit more indication of your total problem, perhaps I can help point out the simplest way to solve it?",5g4ldw,t3_5g4ldw,herejust4this,,Comment,3,0,3
dapni5x,2016-12-02 15:43:11-05:00,herejust4this,,"If it helps here's a link to the program written in Java on Github ....

I think I've pretty much figured out the math but I think there are still a few inputs that make it output some bad data. Instead of doing the calculation by hand every time to confirm both the string decimal and the 8bit binary output is correct I was hoping there is some reliable web site that I could compare my answers with so that I don't have to calculate it by hand every time.",5g4ldw,t1_dapkucj,theobromus,,Reply,1,0,1
daps3ci,2016-12-02 17:19:26-05:00,theobromus,,"Ok. Is this an assignment, or are you just learning on your own?

Some comments as I read:

* Overall, representing stuff as bit strings (with '0' or '1' for the bits) is quite inefficient (since each char is 8 bits, but you're only really using one of them). But for learning it's fine. The same comment applies to using an array of int where each one only has the value 0 or 1. These use 4 bytes of memory each. You could use java's byte type (8 bits), or BitSet (which stores arrays of bits efficiently). Again, if you're just learning how binary math works, this doesn't really matter. It may make a lot of sense to use bool actually instead of an int type, because you have boolean operators. For example, in bitFlip you have an if statement that would just be the ! operator if you had a boolean array (you would have to use a more complicated method of handling carries for the addition though).

* In general, it makes more sense to have your array be ""little-endian"" I think - so that intArray[0] would be the least significant bit. The nice thing about that is if the number gets bigger, you add bits to the end. You also always know that the place value of intArray[i] is 2^i , and your carry algorithm can go forward through the array. For this case, it doesn't matter so much I suppose.

* For BitMath.java, lines 33-40 and 44-51 have the same logic essentially twice. You can split this out into a separate method that takes a parameter of the bits String and intArray.

* On line 90, you print an error if you have a ""carry-out"" (which is normally called an overflow). An interesting point is that two's complement math essentially uses overflows to produce the correct answer. So for example, if you have 0xFF (-1) and add 0x01 (1) you will produce 0x100, but since you have only 8 bits, this will just become 0x00 (0), which is the answer you want. True ""overflows"" for signed bytes happen when you go from 127 (0x7F) to -128 (0x80). Additionally, it's usually discouraged to allow an exception to happen when the case is expected. What I mean is that you probably want to check if x=0 before you try to change sumArray[x-1].

* For the shiftBits and flipBits functions - when you pass an array in java, the array **reference** is passed by value. This is a confusing way of saying that when you change the values *in* the array, you're changing the original copy. So you don't really need to return sumArray - you're already changing it.

* I don't think your overflow check will work when subtract=true is specified. But I think it probably would if you also set negative2 = !negative2 after line 57.

Meta comment:

This seems like a great time to learn how to use unit tests. For java, I'd say you should learn a little bit of [junit](http://junit.org/junit4/). The basic idea of this is that you can write some code that creates data you pass into your functions, and then check that you get the answer you expect. This is better than testing by hand because you can run this test every time you change something and it will check that you didn't accidentally break something. If you use Eclipse, this is really easy to set up.

An example test might be (I haven't tried to build this):

    @Test
    public void bitFlipTest() {
         int[] intArray = new int[8]:
         for (int i = 0: i < intArray.length: ++i) {
             intArray[i] = i % 2:
         }
         BitMath.bitFlip(intArray):
         for (int i = 0: i < intArray.length: ++i) {
             assertSame(""should be same"", intArray[i], (i+1) % 2):
         }
    }",5g4ldw,t1_dapni5x,herejust4this,,Reply,2,0,2
dapyefb,2016-12-02 19:52:39-05:00,herejust4this,,"I'm not going to lie it's going to take me a little while to wrap my noodle around everything you just said. Thanks so much for typing that all out, I'm always astounded by how nice people can be sometimes. Do you mind if I ask a side question: how long have you been programming for? I've been doing it in school for about 2 years and feel like I don't know anything close to your understanding, it's really quite frustrating sometimes reading other people talk about code.

I'm going to spend the next couple days mulling over the gems you just handed me so that I can improve. Thanks so much and by the way this isn't for an assignment. It was an assignment I had to do this last September but now I'm trying to fix some bugs & add a GUI to put on my Github for a resume I'm trying to build.",5g4ldw,t1_daps3ci,theobromus,,Reply,1,0,1
daq5i0t,2016-12-02 23:11:12-05:00,theobromus,,"No worries. I've been programming for over 20 years now (crazy now to realize that) and for 8 years as a full time job (I also did some internships before that). It definitely takes some work to get good at programming. In school it can be pretty tough because you'll have some people in the class who have been coding for years and some people who are just starting out. It definitely takes some practice to learn how to do things really well. It also really helps to have somebody more experienced reviewing your code, which is unfortunately pretty rare in school. There's also **tons** of terminology in software, some of which can be pretty difficult to understand at first.

Anyhow, keep working at it. If you have any more questions I'm happy to help. Or if you have more code, I can review it. The best thing to do is just find some fun project and try to build it (start with something simple :)). Contributing to some open source projects is also great (but might be a little hard for you at this point until you get some more experience and knowledge).",5g4ldw,t1_dapyefb,herejust4this,,Reply,2,0,2
daqx4um,2016-12-03 15:37:47-05:00,herejust4this,,"Thanks so much for the kind words and the good advice, you really made my day.",5g4ldw,t1_daq5i0t,theobromus,,Reply,1,0,1
5fz28s,2016-12-01 15:49:05-05:00,MichelCamarillo,Need information about creating a specific software,"So for my service learning project in college my team was asked to create a software for a man who runs a non-profit institution for jobless people to get a job.

What this man needs is a software where his clients can put their resumees and the software is supposed to send this resumes to a list of websites where employers post job offers.

This class is an Inquiry class and this is also freshman class so we really have no idea on how to do it. 

What I need are concrete reasons to support our new proposal which is hiring a professionals. I need to know how complex would this software be, how much would it take to creating it and any other information that I could use as a reason of why hiring a professional to do it its the best option.

Thank you. Also if this the wrong subreddit please let me know.

Edit 1: I need the info asap so Ill give 20usd (or more, depending how complete the info is) to the person who gives me the most and valid information on this situation. Thanks",,,,,Submission,0,0,0
daqco06,2016-12-03 03:51:52-05:00,MichelCamarillo,,Bump,5fz28s,t3_5fz28s,MichelCamarillo,,Comment,0,0,0
5fxw41,2016-12-01 12:36:15-05:00,pm_me_gold_plz,How does searching simplified and traditional Chinese work?,"Let's take a database for example. I want to search the word ""from"" in Chinese. In traditional Chinese it's 從 and in simplified it's 从 . If I search the traditional 從 and the website is in simplified Chinese how does the website process my request? Does the website convert my query to simplified Chinese using some kind of database or do computers automatically know that 從 and 从 are the same ""word?""",,,,,Submission,3,0,3
dao11cm,2016-12-01 14:42:42-05:00,andybmcc,,"It depends on what it is you're searching.  The characters are obviously encoded differently, so strict textual searches will not relate them.  If you're talking about search engines, you'll likely get similar hits, just as you would for common spelling mistakes in a Latin alphabet that map to the proper word.",5fxw41,t3_5fxw41,pm_me_gold_plz,,Comment,2,0,2
5fx3yr,2016-12-01 10:23:43-05:00,hassan-6663,What is the most impressive piece of software that exists today?,What is the most impressive piece of software that exists today?,,,,,Submission,32,0,32
danqseu,2016-12-01 11:24:10-05:00,jollytopper,,"The original Rollar Coaster Tycoon is quite impressive
",5fx3yr,t3_5fx3yr,hassan-6663,,Comment,50,0,50
dao1hlp,2016-12-01 14:51:05-05:00,andybmcc,,"https://en.wikipedia.org/wiki/RollerCoaster_Tycoon#Development

It was written by one person in nearly all x86 assembly.",5fx3yr,t1_danqseu,jollytopper,,Reply,27,0,27
dao55o2,2016-12-01 16:00:17-05:00,jollytopper,,Exactly! Not sure why I was downvoted:(,5fx3yr,t1_dao1hlp,andybmcc,,Reply,8,0,8
daogewg,2016-12-01 20:30:53-05:00,coolthesejets,,I'd guess because without context it seems flippant.,5fx3yr,t1_dao55o2,jollytopper,,Reply,6,0,6
daobg9k,2016-12-01 18:06:46-05:00,j0hnl33,,I can't believe he got that done in two years. That is amazing. ,5fx3yr,t1_dao1hlp,andybmcc,,Reply,1,0,1
dao4iey,2016-12-01 15:48:10-05:00,dragonnyxx,,"I don't know if it's the *most* impressive, but Compcert is pretty mindblowing. It's a C compiler which is mathematically proven to be bug-free.",5fx3yr,t3_5fx3yr,hassan-6663,,Comment,22,0,22
dao9k03,2016-12-01 17:27:06-05:00,danltn,,"For others reading who may be unfamiliar, It's important to be clear what bug-free means here. They put it concisely themselves. 

> In other words, the executable code it produces is proved to behave exactly as specified by the semantics of the source C program. 

Learn more: http://compcert.inria.fr/compcert-C.html",5fx3yr,t1_dao4iey,dragonnyxx,,Reply,11,0,11
daoa5ba,2016-12-01 17:39:25-05:00,dragonnyxx,,"Just out of curiosity, what else would it mean?",5fx3yr,t1_dao9k03,danltn,,Reply,3,0,3
daoh709,2016-12-01 20:47:41-05:00,wescotte,,Somebody might get the impression that if you compile your program with it your program is guaranteed to be bug free. ,5fx3yr,t1_daoa5ba,dragonnyxx,,Reply,13,0,13
daoim9n,2016-12-01 21:18:19-05:00,Nadrieras,,"The Linux kernel.

",5fx3yr,t3_5fx3yr,hassan-6663,,Comment,19,0,19
daoi97d,2016-12-01 21:10:26-05:00,BonzoESC,,"The Chrome browser. The modern HTML stack is impressive: three standard languages (HTML, CSS, JS), optional additions like WebGL, several streaming video formats, raster and vector images, sounds, and so on. Chrome's a pretty decent implementation of it: works mostly the same in a bunch of environments, proactive use of OS security features, extensions smartly implemented, a good R&D platform for future web standards, and a top notch update system that means people actually update their browsers.",5fx3yr,t3_5fx3yr,hassan-6663,,Comment,14,0,14
daotoyy,2016-12-02 01:51:44-05:00,None,,Linux kernel or Chrome browser. ,5fx3yr,t3_5fx3yr,hassan-6663,,Comment,3,0,3
daowhey,2016-12-02 03:44:36-05:00,Yourothercat,,Google search engine or the Linux kernel,5fx3yr,t3_5fx3yr,hassan-6663,,Comment,3,0,3
dao9gi9,2016-12-01 17:25:05-05:00,None,,"Original release of Myst on CD (Compact Disc). Total nonsense made perfect.

Quake at low resolution (320x240 or lower) has no comparison.

Have you been in the arcade of arcades in your mind? That's a real place.",5fx3yr,t3_5fx3yr,hassan-6663,,Comment,5,0,5
daobstv,2016-12-01 18:14:18-05:00,j0hnl33,,Does Quake still look good at that resolution because of the art style or something? I'm not exactly sure what you mean. ,5fx3yr,t1_dao9gi9,None,,Reply,4,0,4
dap3zxv,2016-12-02 09:09:08-05:00,UniverseCity,,Excel.,5fx3yr,t3_5fx3yr,hassan-6663,,Comment,3,0,3
daow2ed,2016-12-02 03:24:54-05:00,rsmoz,,Clippy. It's widely regarded as the first AI to pass the Turing Test.,5fx3yr,t3_5fx3yr,hassan-6663,,Comment,2,0,2
daurcwr,2016-12-06 08:29:21-05:00,zxy_xyz,,Tay,5fx3yr,t3_5fx3yr,hassan-6663,,Comment,1,0,1
5fw5i4,2016-12-01 06:45:31-05:00,vehlad_durjan,Why is O(n) = 2^lg(n) ?,"I am following MIT's Intro to Algorithms course, and in an assignment they simplify following equations: http://i.imgur.com/6kntNdH.png

I am unable to understand how and why they are replacing n with 2^lg(n).

I'll very much appreciate if someone could help me understand these equations.

(Yes, my maths is piss poor)",,,,,Submission,12,0,12
danh4m1,2016-12-01 06:53:44-05:00,PEdrArthur,,"There is a log property that states b ^ log_b x = x. So, assuming that b is 2 in those equations, 2 ^ log_2 n = n.",5fw5i4,t3_5fw5i4,vehlad_durjan,,Comment,12,0,12
danhhnn,2016-12-01 07:10:51-05:00,vehlad_durjan,,"> b ^ log_b x = x

Darn it! All I had to do was see it as an equation and look at it with my eyes open 😅 

 It is so obvious, if `n` is `log_b x`, that means b^n is `x`. Replace `n` with `log_b x`, and we have the thing that has proven my silliness. 

Thank you for helping me.",5fw5i4,t1_danh4m1,PEdrArthur,,Reply,8,0,8
dar5nzk,2016-12-03 19:10:21-05:00,DeeJay250,,"As a side note, the logs in computer science tend to be base 2 even though we dont always write the base in


There are cases where it might be a different log base (e.g a tertiary tree instead of a binary one) but for big o notation log bases are considered something we dont have to worry about


",5fw5i4,t1_danhhnn,vehlad_durjan,,Reply,2,0,2
dase6wi,2016-12-04 17:16:01-05:00,fresapore,,"Beware, this equation is false in a technical sense.
The big-O notation expresses sets of functions. That is, the function f(n)=n^2 is an element of O(n^2 ). Also, f(n)=2^ln(n) is in O(n), but does not equal it.
This is mixed up sometimes, though.
",5fw5i4,t3_5fw5i4,vehlad_durjan,,Comment,1,0,1
5fvjhy,2016-12-01 03:32:16-05:00,vpjrml_,Graph algorithms complexities,Hi! I am taking the Algorithms course at the university. I'm unable to understand is how graph complexities are computed. Tips and tricks with some reasoning would be appreciated. Thank you :),,,,,Submission,1,0,1
5fuy9y,2016-12-01 00:46:23-05:00,NotASmurfAccount,"In C++, how does the eof function access the set bit that flags end of file?","From what I understand, the eof bit will only be changed after read has been attempted at the end of the file. So the eof() stream member function is accessing this flag bit from the stream. How can the function access that specific status bit? What sort of bitwise operations would need to be used, if any? Would it be using a bit mask to select out the relevant bit?

e: Really appreciate everyone's insight! Thanks :)",,,,,Submission,5,0,5
danawt1,2016-12-01 01:46:59-05:00,lordvadr,,"Without going and reading the source code to be perfectly specific...

The standard library (in this case at least) is a wrapper for the operating system's normal open/read/write/close routines.  Each OS (and in some cases filesystems) indicate the end of a file a little differently, and with varying levels of ""certainty""--for example, where exactly is the end of a network stream?  In any case, it's stdlib's attempt, at least, to abstract that away from you with some predictability.

The answer to your question is that there's a little mini data structure that contains the info necessary for stdlib to tell the OS what you want to do, as well as keep track of (among other things), ""the OS already told me there was nothing more to read.""  That's the flag you're talking about.

But I suppose maybe I don't really understand your question as you seem to imply that it's a mystery how a routine that's part of a library has access to persistent information that library keeps.",5fuy9y,t3_5fuy9y,NotASmurfAccount,,Comment,2,0,2
danb6bp,2016-12-01 01:57:00-05:00,NotASmurfAccount,,"Thanks a bunch for the reply. I realized how cryptic the question was originally, so I edited the OP.

e: Am I way off base to say a bit mask would be used to select the pertinent eof status bit from the stream? Something like:
    
    if ( (incoming_stream & 0000 0001) == 0 ) {
            //bit isn't flipped, not eof
        }

    if ( (incoming_stream & 0000 0001) == 0000 0001) {
            //nonzero result, marks end of file
        }
    ",5fuy9y,t1_danawt1,lordvadr,,Reply,1,0,1
danfjgw,2016-12-01 05:27:26-05:00,taricorp,,"It doesn't necessarily need to be packed as bits, but yes. In general it would have a set of flags, and the actual storage isn't very important.

---

I looked at [what Clang's libcxx](http://llvm.org/viewvc/llvm-project/libcxx/trunk/include/ios?view=markup&pathrev=267076) does, and it does pack bit flags into a single field (pulling just the relevant bits out):

    typedef T2 iostate:
    static const iostate eofbit = 0x2:

    iostate __rdstate_:
    
    bool ios_base::eof() const {
        return (__rdstate_ & eofbit) != 0:
    }",5fuy9y,t1_danb6bp,NotASmurfAccount,,Reply,3,0,3
danje6c,2016-12-01 08:24:42-05:00,lordvadr,,"Nice, thanks for digging out the source.  I just want to add that packed-bits are pretty common because you can do things like this:

**Edit** Fixed bug

    #define FLAG_A 0x01
    #define FLAG_B 0x02
    #define FLAG_C 0x04
    #define FLAG_D 0x08
    #define FLAG_E 0x10 ... etc ...

    int flags:

    set_a () {
        flags |= FLAG_A: // OR-Equals
    }

    clear_b () {
        flags &= ~FLAG_B: // AND-equals inverse of B
    }

    toggle_c () {
        flags ^= FLAG_C: // XOR-Equals
    }

    has_d () {
        return flags & FLAG_D:
    
        // EDIT 3
        // This is probably better for consistency sake
        // return flags & FLAG_D ? 1 : 0:
    
        // Or maybe even this as it avoids a branch
        // return (flags & FLAG_D) >> (FLAG_D/2):
    }

**Edit 2**: Also any number of intersection, or difference and other such functions.",5fuy9y,t1_danfjgw,taricorp,,Reply,2,0,2
dannvlo,2016-12-01 10:22:48-05:00,IgnorantPlatypus,,That `clear_b()` function doesn't do what you'd want...,5fuy9y,t1_danje6c,lordvadr,,Reply,2,0,2
danp475,2016-12-01 10:49:30-05:00,lordvadr,,"You sir, are right.  Fixed.  Thank you.",5fuy9y,t1_dannvlo,IgnorantPlatypus,,Reply,1,0,1
danj93t,2016-12-01 08:20:06-05:00,lordvadr,,"Kind of, but not you're missing something.  It doesn't read ""end of file"" *from the file*...e.g. there's isn't data in the stream indicating the end of the file.",5fuy9y,t1_danb6bp,NotASmurfAccount,,Reply,2,0,2
5fs18p,2016-11-30 15:41:54-05:00,asdfqwertylol,"P Turing reducible to Q, but Q not Turing reducible to P.","I have been asked to prove that for every language P there exists a language Q such that P is Turing reducible to Q but Q is not Turing reducible to P.  
  
I came up with the following: Q={<L, x> | L is a language description and x is in L}.
It is clear that P is Turing reducible to Q, since we can build a TM with an oracle for Q that decides P. This machine simply queries <P, x> on the oracle for any input x it receives.  
  
I am almost certain that Q is not Turing reducible to P, but I don't know how to prove this.  
  
Any ideas? Thanks!",,,,,Submission,2,0,2
dan4yhx,2016-11-30 22:55:24-05:00,_--__,,"> Q={<L, x> | L is a language description and x is in L}

What exactly do you mean by ""language description""?",5fs18p,t3_5fs18p,asdfqwertylol,,Comment,1,0,1
dan5tpg,2016-11-30 23:16:24-05:00,asdfqwertylol,,"I was thinking something like ""L={x|x is a prime}"", so running <""L={x|x is a prime}"", 7> on the oracle for Q would return true. However I realise that not all languages can be described in such a way, so I likely need to find a different Q.",5fs18p,t1_dan4yhx,_--__,,Reply,1,0,1
dan6165,2016-11-30 23:21:35-05:00,_--__,,Yes.  Note that unless you have missed something in your problem description this is supposed to apply to *any* language P - including ones which do not admit a finite description.,5fs18p,t1_dan5tpg,asdfqwertylol,,Reply,2,0,2
dan6ajh,2016-11-30 23:28:05-05:00,asdfqwertylol,,"Yeah the problem stated that this should apply for any given P. If P was decidable for instance then the problem would be much easier, but as of right now I'm really clueless...",5fs18p,t1_dan6165,_--__,,Reply,1,0,1
dan83wg,2016-12-01 00:16:43-05:00,_--__,,Have a look at [Turing jumps](https://en.wikipedia.org/wiki/Turing_jump).,5fs18p,t1_dan6ajh,asdfqwertylol,,Reply,2,0,2
daoox2z,2016-12-01 23:36:04-05:00,asdfqwertylol,,"Someone has hinted at me that Q={<M,x>|M is an oracle TM with an oracle for P that accepts x} might work. I was able to prove that Q was not Turing reducible to P by using the same diagonalization proof as with the halting problem. However, I am faced with a new problem, as I can't figure out how to show P is Turing reducible to Q in this case. If I want to know if some string x is in P, I query Q with <M,x>, but what should M be here?",5fs18p,t1_dan83wg,_--__,,Reply,1,0,1
daowhfv,2016-12-02 03:44:38-05:00,_--__,,M is the oracle TM that queries its input directly to the oracle and then immediately returns the result of the oracle.,5fs18p,t1_daoox2z,asdfqwertylol,,Reply,1,0,1
5fs09n,2016-11-30 15:37:24-05:00,Matta174,"Hey guys, computer architecture questions here.","So I just missed two questions on a recent homework and I honestly don't know where to begin. I was wondering if someone could give a quick guide on how to answer these:

>Q1. Consider a 32-bit processor that has an on-chip 32-KByes four-way set-associative
>cache. Assume the cache has a line size of four(4) 32-bit words. How many sets does the cache have?


>Q2. Consider a direct-mapping cache of 128 lines and a main memory block size of 8
>bytes. What cache line number does byte address 120010 map to?
",,,,,Submission,2,0,2
damnzhn,2016-11-30 16:48:59-05:00,andybmcc,,"Probably start by reading some basic material on cache architecture.

>Q1. Consider a 32-bit processor that has an on-chip 32-KByes four-way set-associative cache. Assume the cache has a line size of four(4) 32-bit words. How many sets does the cache have?

So here, each set is 4 lines, and each line is 16B (128-bit).  That means each set is 4 * 16B = 64B.  We have a total cache size of 32KB as given in the problem, so the number of sets is 32KB / 64B = 512 sets.",5fs09n,t3_5fs09n,Matta174,,Comment,1,0,1
damtns4,2016-11-30 18:43:06-05:00,Matta174,,Thanks!,5fs09n,t1_damnzhn,andybmcc,,Reply,1,0,1
danjh7q,2016-12-01 08:27:28-05:00,andybmcc,,"Here's a decent whirlwind tour of a lot of the concepts you'll probably see. https://www.cs.umd.edu/class/fall2001/cmsc411/proj01/cache/cache.pdf

The wikipedia article isn't bad either: https://en.wikipedia.org/wiki/CPU_cache",5fs09n,t1_damtns4,Matta174,,Reply,1,0,1
5fragp,2016-11-30 13:38:53-05:00,SkyewardSword,How did you learn to think in such an abstract way?,"I'm a sixth form student taking computer science. It's a very enjoyable subject for me - I've loved it since I was very young. But in recent lessons we've started talking about abstract data types, like stacks, queues, and trees. My problem is trying to tie those abstract structures to a tangible thing - it's very difficult for me to grasp the concepts I'm taught about without being able to apply it to real life, not to mention seeing a bunch of code and immediately identify that that part of the code declares a stack, or that part of the code declares a tree. Hell, I even find pointers difficult to grasp. So how did you learn? Or was it just an intrinsic thing?",,,,,Submission,5,0,5
damdxz6,2016-11-30 13:44:49-05:00,None,,"I sat in a room with a class of fellow students that had been there for three years too while somebody who has a way gave a PowerPoint presentation, then did that for months with homework, as an adult.",5fragp,t3_5fragp,SkyewardSword,,Comment,3,0,3
damgs7b,2016-11-30 14:38:39-05:00,umib0zu,,"I think you want to look at [Ullman's Automata Theory, Languages, and Computation](http://infolab.stanford.edu/~ullman/ialc.html), which is readily available on the internet just by searching with a ""filetype:pdf"". The short answer is, you're asking two questions.
 Abstract data types, like stacks, queues, and trees could be well explained by Context-Free Grammars, and there's an [interesting bunch of theory relating how to analyze these data types](https://www.amazon.com/Introduction-Analysis-Algorithms-2nd/dp/032190575X/ref=sr_1_2?s=books&ie=UTF8&qid=1480534477&sr=1-2&keywords=introduction+to+algorithm+analysis). As for abstraction on the computation side, you'll eventually dive into [Type Theories](https://www.cs.kent.ac.uk/people/staff/sjt/TTFP/).

But a fair bit of warning, especially coming from someone who notices you also caught the real ""computer science"" bug by asking this kind of question in the first place: Programming/software engineering and computer sciece are not necessarily the same thing. Be ready for proofs and math, and be ready to balance the abstract with pragmatism.",5fragp,t3_5fragp,SkyewardSword,,Comment,3,0,3
damn2pv,2016-11-30 16:32:09-05:00,anamorphism,,"it came fairly natural to me. my first interaction with code was around the age of 7. i started really teaching myself how to code around 10. by the time i started learning about data structures in my late high school and early college years, i didn't really have any difficulty at all understanding what they were.

understanding why and how they're used took a bit more time, but that's just like being given a hammer for the first time. you learn how to use the hammer and then you try to use the hammer with everything until you figure out and remember what a hammer is actually good for.

think of a stack like a stack of papers. when you go to put a new piece on top of the stack it goes on top. when you go to remove a piece of paper from the stack you take it off the top. first in, last out (FILO). the first item you add (push) to the stack will be the last item that gets removed (popped) from the stack.

a queue is a ... queue, or i suppose 'line' if you're from the states. the customer that enters (enqueue) the line first is the one that gets served (dequeue) first. first in, first out (FIFO).

pointers are fairly simple on paper but a lot of people seem to have difficulty grasping the core abstraction that is happening. the most common problem i see is thinking of a variable as if it were the actual thing and not just some metadata about some chunk of memory.

everything is just bits stored in memory.

`int i:` is just you going ""i want 32-bits (in most cases) of memory that i'm going to use as if it were a signed, integer value."" a whole bunch of other programming is what results in you being able to use `i` in certain ways in your code.

`int* i:` is just you going ""i want some memory that i'm going to use to represent the starting location (address) of a chunk of memory that i'm using as an int.""

granted, full understanding of why pointers came about and why and how they're used takes quite a bit more learning.

---

anyway, as /u/umib0zu points out, if you're more interested in concrete application and real-world examples of computer science, you may want to look at a software engineering/programming/development program over a pure computer science program.

computer science should be more focused on the theory where the other program should be more focused on the application.

think of it like studying 'art' versus studying something like 'painting'.",5fragp,t3_5fragp,SkyewardSword,,Comment,2,0,2
5fradx,2016-11-30 13:38:25-05:00,kratFOZ,Best way to 'stretch' a list in Python3?,"Given = [1,2,3,4,5] and a stretch_factor 4

Want output:  [1,1,1,1,2,2,2,2,3,3,3,3,4,4,4,4,5,5,5,5]

I think looping through each item in Given then re-adding stretch_factor times is inefficient when working with gigantic lists. Anyone know of another more efficient way? Thanks.

EDIT: This is the algorithm I came up with, don't know if this is any better:


    for i in Given:
        output += [i]*stretch_factor    ",,,,,Submission,5,0,5
dammipt,2016-11-30 16:22:07-05:00,brtt3000,,"Do you really need this as a list or could you just iterate over values? 

Using a generator is efficient:

    def stretch(max, factor):
        for n in range(1, max + 1):
            for i in range(factor):
                yield n

    stretch(5, 4)

Can be even faster if you cache the inner range as a list or tuple. Or check some [itertools](https://docs.python.org/3/library/itertools.html), you can do this in a single (generator) expression I guess. Or make an infinite generator if you base it on a `itertools.count()` instead of a `range()`.",5fradx,t3_5fradx,kratFOZ,,Comment,3,0,3
damng06,2016-11-30 16:39:02-05:00,kratFOZ,,Thanks for your reply! Stretch should work on lists of all types of objects. ,5fradx,t1_dammipt,brtt3000,,Reply,1,0,1
damo6z7,2016-11-30 16:52:47-05:00,brtt3000,,"There are various ways you can bolt this together, kinda depends on input and output requirements. Probably involving range, count and repeat. 

Here is an itertools based infinite oneliner:
    
    from itertools import chain, repeat, count
    e = chain.from_iterable(repeat(n, 4) for n in count(1))

",5fradx,t1_damng06,kratFOZ,,Reply,1,0,1
dan92bc,2016-12-01 00:44:50-05:00,zorkmids,,"Is this a homework assignment?
",5fradx,t3_5fradx,kratFOZ,,Comment,2,0,2
danaw22,2016-12-01 01:46:12-05:00,i_takes,,"and if it is, why not just go to stack overflow like everyone else",5fradx,t1_dan92bc,zorkmids,,Reply,1,0,1
danqxls,2016-12-01 11:27:04-05:00,kratFOZ,,"it is for a project I'm working on. Truth be told, I do not know how to use stack overflow but I know how to use Reddit :).",5fradx,t1_danaw22,i_takes,,Reply,1,0,1
dan9u9z,2016-12-01 01:09:42-05:00,AustinCorgiBart,,    [i for i in given_list for repeats in range(stretch_factor)],5fradx,t3_5fradx,kratFOZ,,Comment,1,0,1
5fr882,2016-11-30 13:28:01-05:00,Rabrg,Time complexity confusion,"I have a [Java implementation of the count-min sketch data structure](https://gist.github.com/Rabrg/de57dae3f3815b67eb14fd8db04194a7) with an added heavy hitter algorithm and I'm trying to find out the time complexity of the heavy hitter algorithm.

I'm having a hard time figuring it out because the complexity depends on three different variables: *depth*, *min*, and *max*.

How do you find the Big-O notation for algorithms like this?",,,,,Submission,5,0,5
damyl0p,2016-11-30 20:33:09-05:00,Badgrs,,"I'm unfamiliar with count-min sketch, but I would imagine that when dealing with it in general, you would want complexity to be either in terms of the number of values added to the frequency table, or the size of the frequency table. Your heavy hitters depends on the latter, `depth`.

Your heavy hitters algorithm makes `max - min` iterations, calling `estimateCount(…)` each time, which internally makes `depth` iterations.

So, you're basically looking at `O(depth • (max - min))`. `depth` and `(max - min)` are completely independent of each other: you have to decide which you are using to base the complexity on. If you want complexity in terms of `depth`, `max - min` can be omitted leaving you with `O(depth)`. Similarly, if you want complexity in terms of `max - min`, you end up with `O(max - min)`. This can be done since each acts as a ""constant"" scalar on the other. Either way, this is _technically_ (and slightly counterintuitively) linear `O(n)`, though casually I would call it `O(nm)`.
",5fr882,t3_5fr882,Rabrg,,Comment,2,0,2
damzkzh,2016-11-30 20:54:39-05:00,Rabrg,,"Thank you very much, that makes sense!

One small follow up question: what if I changed `min` to the constant `1` and the `max` to `1000000`? Would the time complexity technically be constant `O(1)`?",5fr882,t1_damyl0p,Badgrs,,Reply,1,0,1
damzofn,2016-11-30 20:56:43-05:00,Badgrs,,"In terms of `max - min`, yes, you'd technically have constant complexity. In terms of `depth`, it'd be the same as before.",5fr882,t1_damzkzh,Rabrg,,Reply,2,0,2
5fnqnz,2016-11-29 23:52:01-05:00,trineroks,How do you contribute to OSS?,"Hi guys, question in title. I want to expand my portfolio more and I've heard multiple times that contributing to OSS is extremely helpful. However, I don't know how exactly to do this?

Almost all of my projects have been solo or done at hackathons with a group. How do I get around to contributing to OSS, and are there any popular repositories for this?",,,,,Submission,6,0,6
dam21du,2016-11-30 09:44:05-05:00,NoNotTheDuo,,http://www.firsttimersonly.com/,5fnqnz,t3_5fnqnz,trineroks,,Comment,2,0,2
dammdml,2016-11-30 16:19:36-05:00,trineroks,,I'll take a look at this. Thanks a bunch!,5fnqnz,t1_dam21du,NoNotTheDuo,,Reply,1,0,1
5fl6pt,2016-11-29 15:52:39-05:00,nmego12345,Can someone explain this diagram to me?,"[Link to the Diagram](http://i67.tinypic.com/2eksqqh.png)
Where the second diagram is the circle in the fist diagram (magnified though)

This diagram explains how does the RAM works. It’s taken from “But how do It know” book

I understand every part of the diagram individually, however, I still cannot understand the whole diagram, can someone explain",,,,,Submission,14,0,14
dalrc1o,2016-11-30 02:03:10-05:00,teraflop,,"At the risk of maybe being overly blunt: we can explain it to you, but we can't understand it for you. In order for us to properly help you, you'll need to tell us a bit more about what you're having difficulty with.

To start with: the diagram shows a 256-word RAM, organized as a grid of 16x16 words (of unknown size). Each word is represented by the block labeled R in the bottom right, and everything else is just the logic to select a particular word.

The decoding logic is made out of two basic kinds of components: decoders and AND gates. The 4-to-16 decoders (confusingly, also labeled ""R"") each take a 4-bit number and use it to decide which of the 16 outputs should be high. At any given time, exactly one of the row lines and one of the column lines is turned on.

The other important thing to understand is that an AND gate can be used to enable or disable a signal. You can arbitrarily label one of the inputs as ""signal"" and the other as ""enabled"". If ""enabled"" is high, the output of the gate is equal to ""signal"": otherwise, the output is zero.

The reason this is useful is that we want all of the memory words share a common set of data and control signals (""s"", ""e"", and ""bus"") so that we don't need hundreds of different pins on the memory module. (The diagram doesn't say what the control signals mean, but I'm assuming ""e"" means ""enable"" and ""s"" means ""set"", or something like that.) But we want to make sure that only a single word tries to use those signals at a time. So each word has an AND gate labeled X, which turns on if both the corresponding row and column lines are active. Then the output of X is used to enable ""s"" and ""e"". So only one of the 256 cells can be active at any given time.",5fl6pt,t3_5fl6pt,nmego12345,,Comment,7,0,7
dalwaa3,2016-11-30 06:14:25-05:00,nmego12345,,"So I understand it as a cupboard, with 16x16 holes in it, and we can somehow hide all but one hole (thereby selecting it and taking what's inside). We could store stuff into the holes and take stuff from the holes using the variables s and e we input there (set = set, e = enable)

Is the above right? If so, then:

1. Is this a 256-word RAM or 256-letter RAM? (Since each input is only a byte, not a word)

2. Is the box, where each input is stored, determined by the value of the input?

3. How about if we input 2 identical inputs? (They will take the same (box) right?)


",5fl6pt,t1_dalrc1o,teraflop,,Reply,3,0,3
dam0mkr,2016-11-30 09:06:46-05:00,teraflop,,"Question 1: Forget about letters: I was using ""word"" in the technical sense of [a group of bits](https://en.m.wikipedia.org/wiki/Word_(computer_architecture\)). A ""bus"" in this context means a group of related signals that are drawn as one for convenience (in this case, using a thicker line). So if the memory uses 8-bit words, then ""bus"" is actually 8 wires, one for each bit. There's no way of knowing how wide the bus is because the diagram doesn't say. But it's uncommon for actual memory devices to have a separate address for each bit. Normally, a word is at least 8 bits (1 byte) and often a larger power of 2.

Questions 2/3: no, read what I wrote again, and look at the diagram. The actual data that you read and write is passed along ""bus"". But a cell is active if its AND gate labeled ""X"" outputs a 1. Where do X's inputs come from?

Hope this helps!",5fl6pt,t1_dalwaa3,nmego12345,,Reply,3,0,3
damkc3j,2016-11-30 15:42:59-05:00,nmego12345,,"Thank you, this helped a lot",5fl6pt,t1_dam0mkr,teraflop,,Reply,2,0,2
dam8nf2,2016-11-30 12:03:08-05:00,kingkrruel,,"We select the ""hole"" using the horizontal and vertical grid wires, input values in the ""holes"" using s and e wires. so, the location of input is not determined by the value of input.",5fl6pt,t1_dalwaa3,nmego12345,,Reply,2,0,2
damkba4,2016-11-30 15:42:35-05:00,nmego12345,,"
Awesome, that made complete sense and gave me my aha moment, thank you!",5fl6pt,t1_dam8nf2,kingkrruel,,Reply,3,0,3
5fkoxr,2016-11-29 14:31:10-05:00,StupidWes,Stress testing a new computer?,"I got a great deal on a new desktop pretty recently. However, upon reading up on the people behind it (CyberPowerPC), I'm reading a lot of places that their components have a tendency to fail suddenly. The end of the return period is coming up pretty soon.

Is there a way to put a PC through its paces a little bit? Put a manageable amount of strain on the various components, make sure they all at least hold up?",,,,,Submission,0,0,0
dakzwty,2016-11-29 15:22:04-05:00,1stonepwn,,"There are a ton of good benchmarking/stress testing utilities out there. Also, this isn't really the right kind of question for this sub, you'd probably have more luck on /r/buildapc or a related sub.",5fkoxr,t3_5fkoxr,StupidWes,,Comment,5,0,5
dalo25r,2016-11-30 00:14:50-05:00,StupidWes,,I definitely had a hard time deciding what sub to post this in. I'll try there. Thank you,5fkoxr,t1_dakzwty,1stonepwn,,Reply,2,0,2
dalam7c,2016-11-29 19:04:25-05:00,dxk3355,,Used to be you run memtest for 24 hours and you run PCmark for 24 hours and that was the gold seal.  I'm not sure how current i am with that though.,5fkoxr,t3_5fkoxr,StupidWes,,Comment,2,0,2
dalo333,2016-11-30 00:15:36-05:00,StupidWes,,Awesome. I'll do some looking into those and see what I learn. I wasn't even really sure where to begin.,5fkoxr,t1_dalam7c,dxk3355,,Reply,2,0,2
5fj8o3,2016-11-29 10:33:11-05:00,DoneeH,Lambda Calculus help,"I've been stuck on the question below for a while and I'm not really making any progress with it. Would anyone with knowledge of Lambda Calculus be able to explain what the questions asking for a bit better.


-----

## Setup

Consider the simply typed lambda calculus, with added types t1, t2, ..., t9 and one basis object of type b:t8 and eight basis functions of types f4:t4→t5, f5:t5→t6, ..., f8:t8→t9.

Recall that in the simply typed lambda calculus, a variable after the symbol λ must be given a type, as in:

   λ x:t1 . b
   
This entire term would have type t1→t8, i.e.,

   (λ x:t1 . b) : t1→t8

### Problem 1

Define a term, using as short a definition as you can manage, of type (t1→t3)→(t2→t3→t5)→t2→t1→t7


---- 



",,,,,Submission,5,0,5
dakm5xp,2016-11-29 10:53:01-05:00,icendoan,,"It might be useful to try coming up with partial fragments, starting from the right hand side (remembering that the λ term `a → b → c` is the same as `a → (b → c)`).

So, the first term to look at is `t1 → t7`, and it's reasonably clear that if you're given a term `x : t1`, you can produce a term of type `t7` just by applying all of the basis functions until you get there: `(f6 f5 f4 f3 f2 f1 x) : f7`, so a term of type `f1 → f7` would look like:

    λ x : t1 . f6 (f5 (f4 (f3 (f2 (f1 x)))))

Then, with the above term in mind, how would you produce a term with the type `t2 → (t1 → t7)`?",5fj8o3,t3_5fj8o3,DoneeH,,Comment,3,0,3
dakn8qz,2016-11-29 11:15:13-05:00,DoneeH,,"So taking what you've said, would      t2 → (t1 → t7)    just be a basic function which takes the type t2 to t7, as in

      λ x : t2 . f6 (f5 (f4 (f3 (f2 x)))))


Edit: changed mistake of t1 to t2 in lambda function",5fj8o3,t1_dakm5xp,icendoan,,Reply,1,0,1
dakng5a,2016-11-29 11:19:19-05:00,icendoan,,"But surely that's of type `t2 → t7`, and not `t2 → t1 → t7`, which is actually what we're after?",5fj8o3,t1_dakn8qz,DoneeH,,Reply,2,0,2
dakno4f,2016-11-29 11:23:38-05:00,DoneeH,,"I'm not really sure how the basic functions would take t2 → t1 though in the line  t2 → t1 → t7. Is the basic function for t2 not just for taking a type t2 to t3 (f2:t2 → t3).

Sorry if that's a simple question, I'm not too confident with Lambda Calculus.",5fj8o3,t1_dakng5a,icendoan,,Reply,1,0,1
daknw0h,2016-11-29 11:27:57-05:00,icendoan,,"Well, what about invoking that `a → b → c` is the same as `a → (b → c)`, and yielding something of type `t1 → t7`? 

Remember the typing rules: if we have terms `x : S`, and `y : T`, then `λx. y : S → T`.",5fj8o3,t1_dakno4f,DoneeH,,Reply,2,0,2
dakoris,2016-11-29 11:45:26-05:00,DoneeH,,"So is it something along the lines of

     λ y x : t2 . (t1 . f6 (f5 (f4 (f3 (f2 (f1 x)))))

   ",5fj8o3,t1_daknw0h,icendoan,,Reply,1,0,1
dakrgq9,2016-11-29 12:38:04-05:00,icendoan,,"Yes, that looks like the right type to me. (Though I'd usually write it as `λy : t2, x : t1 . f...`)",5fj8o3,t1_dakoris,DoneeH,,Reply,2,0,2
daks6ii,2016-11-29 12:51:48-05:00,DoneeH,,"Ok nice, thanks a million for your help, honestly I really appreciate it. I'll have a go at getting the rest of the question out myself now.",5fj8o3,t1_dakrgq9,icendoan,,Reply,1,0,1
dam6e05,2016-11-30 11:18:46-05:00,Leeds1234,,"for t1->t3 would it be λx:t1 . f2(f1 x)
(t2->(t3->t5) would be λx:t2 . f4(f3(f2 x))
and for t2->(t1->t7) is it λx:t2, y:t1 . f6(f5(f4(f3(f2(f1 x)))))

I'm not sure how to from function-> function if you get me, for example how would the 3 lines connect?
is this right? 
/u/icendoan",5fj8o3,t1_daks6ii,DoneeH,,Reply,1,0,1
dam7c0y,2016-11-30 11:37:36-05:00,icendoan,,"You can take variables and just do nothing with them, even if they are functions.

If you have a term `c : t2 → t1 → t7`, then in some context `Γ,c ⊢ (λx : t1 → t3, y : t2 → t3 → t5 . c) : (t1→t3) → (t2→t3→t5) → (t2 → t1 → t7)`.
",5fj8o3,t1_dam6e05,Leeds1234,,Reply,1,0,1
5fj7vc,2016-11-29 10:29:08-05:00,Leeds1234,Lambda Calculus question,"I'm struggling doing Lambda Calculus here, is it really that hard?
Can anyone help me wth this question?

Prove (using the Curry-Howard Correspondence) that it is not possible to define a term of type (t7→t1)→t1→t7 but that it is possible to define a term of type (t7→t1)→t1→t8.",,,,,Submission,2,0,2
dakmsuc,2016-11-29 11:06:15-05:00,icendoan,,"What are `t1`, `t7`, `t8`, etc?",5fj7vc,t3_5fj7vc,Leeds1234,,Comment,1,0,1
dakx1q6,2016-11-29 14:25:59-05:00,Leeds1234,," added types t1, t2, ..., t9 and one basis object of type b:t8 and eight basis functions of types f4:t4→t5, f5:t5→t6, ..., f8:t8→t9.",5fj7vc,t1_dakmsuc,icendoan,,Reply,1,0,1
dakzhgb,2016-11-29 15:13:43-05:00,icendoan,,"Ah, I see. You are a colleague of the poster in the other thread.

Firstly, this is two questions: prove that there is no term with the type `(t7 → t1) → t1 → t7`, and then secondly to construct a term with the type `(t7 → t1) → t1 → t8`.

Let's look at the first part. Under the Curry-Howard correspondence, a function type `a → b` is associated with a *logical inference* ""A implies B"". So, then, what logical proposition is associated with `(t7 → t1) → t1 → t7`?

Also, you said there are 8 basic functions, going from `tN` → `t(N+1)`, but you started at `t4`: are there functions `f1 : t1 → t2`, etc? (And if so, what is wrong with the term `λf . λ x . f7 f6 f5 f4 f3 f2 f1 x`? You can never actually invoke such a function, since you can't construct anything of type `t7 → t1`.",5fj7vc,t1_dakx1q6,Leeds1234,,Reply,2,0,2
dalvncb,2016-11-30 05:39:15-05:00,Leeds1234,,"Thanks for your reply, I'm going to take your help on board, and try do out the question again today, so if I have any questions, I'll probably just give you a shout on this.. Cheers",5fj7vc,t1_dakzhgb,icendoan,,Reply,1,0,1
5fi5wf,2016-11-29 06:31:33-05:00,popfalushi,Why MSB's are discarded during 2's complement multiplication?,"Hello, everyone.

I have a trouble with understanding why we discard MSB when we do multiplication of 2's complements numbers.
Let's take `101` (decimal: -3) and `011` (decimal: 3) for example. Algorithm goes like this: first we double length of our numbers, than we do usual multiplication as we did in school with decimals, than we take (doubled length)*2=6 least significant bits.

1. Double lengths:

        101 -> 111101
        011 -> 000011

2. Do multiplication: `111101 * 000011 = 10110111` (decimal: -73). As we see, this result is wrong.

3. Take 6 least significant bits (drop 2 most significant bits): `10110111 -> 110111` (decimal: -9)

And so result became magically right. How this can be explained? I understand that MSB is special and the same rules I've used in school cannot be 100% suitable for 2's complements, but while I fully understand school rules of multiplication, I can't wrap my head around last step of 2's complement multiplication (first two steps I understand).",,,,,Submission,3,0,3
dan08hh,2016-11-30 21:08:57-05:00,Badgrs,,"[Wikipedia's explanation](https://en.wikipedia.org/wiki/Two's_complement#Multiplication) is actually pretty good, though a bit dense.

My tl:dr: basically you need `2N` bits to represent the possible products, which is why you do step 1. But this leads to 2 extra ""garbage"" bits that aren't actually relevant. The product is mathematically _guaranteed_ to fit in `2N` bits: the additional bits are just the padding from step 1 and any ""carrying"" fizzling out into irrelevancy.",5fi5wf,t3_5fi5wf,popfalushi,,Comment,1,0,1
dan7le8,2016-12-01 00:02:16-05:00,popfalushi,,"I've read it. I understand why we double the length. I still don't understand why someone who invented 2's complement arithmetic at some point said 'Well, we get 8 bits instead of 6.. let's drop 2 most significant because it is obviously garbage that does not mean anything and we get right answer'. It could not happened that way because otherwise mathematics would be really really not as rigorous and strict as I know.",5fi5wf,t1_dan08hh,Badgrs,,Reply,1,0,1
5fg62b,2016-11-28 21:52:05-05:00,cenaice,High School Senior looking for people in CS careers to take a survey for a school project. (2-3mins),,,,,,Submission,4,0,4
5ffk7y,2016-11-28 19:53:06-05:00,whathaveidone56,I have posted amateur porn videos without my face. Is there any chance of me ever getting caught?,"Yeah, so I have posted like 5 amateur videos on pornhub and shared the stuff on reddit.  I am genuinely concerned that because I didn't use a VPN and my voice can be heard that at some point in the future the videos can be connected back to me.

There is just something in the back of my mind that tells me one day our technology will have the capacity to do voice matches or literally just figure out who uploaded what video.  

Are my concerns just ignorant, or is there an actual basis? Thanks in advance.",,,,,Submission,5,0,5
dajuz3q,2016-11-28 20:22:50-05:00,mrmnder,,"It's possible, beyond the voice, are there any distinctive body markings or attributes?

I wouldn't worry about the network stuff.

However, you really shouldn't post things online that you don't want seen.",5ffk7y,t3_5ffk7y,whathaveidone56,,Comment,9,0,9
dak4kms,2016-11-28 23:58:00-05:00,cravenspoon,,"It's certainly possible future technology will get there. Keep in mind the [Streisand effect](https://en.wikipedia.org/wiki/Streisand_effect), though.  

I find it doubtful that there will be bots crawling pornhub to identify people. If it gets that far, everyone who's ever sent nudes will also have issues. ",5ffk7y,t3_5ffk7y,whathaveidone56,,Comment,3,0,3
dak9xxj,2016-11-29 03:11:05-05:00,watsreddit,,"While I suppose it's possible computer vision techniques could be used to compare your body in the videos to your body in some other image/video, it's certainly a non-trivial task and would probably require a concentrated effort by a person using research software to do. As such, I think the risk is very minimal here.

That being said, you might be identifiable by your voice. Voice recognition is a potentially easier task than visual recognition for the simple fact that the range of possible input for comparison is smaller.

Regardless, I don't think you actually have anything to worry about. You'd need to have someone have a recording of your image/voice and be comparing that to videos on pornhub. I don't really see that happening.",5ffk7y,t3_5ffk7y,whathaveidone56,,Comment,3,0,3
dak0wxa,2016-11-28 22:31:02-05:00,thatguywhorows,,"the only way that it could be traced to you would be through either someone recognizing you or through the metadata of the videos you posted (which I have no clue how pornhub deals with). 

in the future you really shouldn't post things you think you'll regret.",5ffk7y,t3_5ffk7y,whathaveidone56,,Comment,2,0,2
dak2b81,2016-11-28 23:02:16-05:00,whathaveidone56,,You are right.  I'm freaked out so I'm gonna look into this whole pornhub metadata situation.,5ffk7y,t1_dak0wxa,thatguywhorows,,Reply,5,0,5
dakd305,2016-11-29 05:57:10-05:00,None,,"In computer science I think your concern is definitely valid in the tradeoffs of possible network layouts. The current Internet relies on trusting the people owning the computer information storage holding your videos because the computer network architecture for these computer network applications (like pornhub and reddit) are behind this HTTP Internet website model, and since these people own the servers they can do anything with the information including selling it to accumulating database people. Other network information models may expose your information to unwanted connection in other ways.

Your concerns are mostly outside of Computer Science I think, or you may be interested in the subjects of computer security against enemies (such as governments that disagree with you).",5ffk7y,t3_5ffk7y,whathaveidone56,,Comment,1,0,1
dak4eo2,2016-11-28 23:53:26-05:00,RatherPleasent,,Sounds serious. You should post the videos for analysis and posterity's sake. ,5ffk7y,t3_5ffk7y,whathaveidone56,,Comment,0,0,0
dakbnk8,2016-11-29 04:37:50-05:00,eid-a,,"Can we see it ? Lol 

There is no way of knowing actually  , did u speak during it ? How about ur partner ? Where did u shoot it ? Where did u post it ?  .... 

My pet is on >> Probably not ! ",5ffk7y,t3_5ffk7y,whathaveidone56,,Comment,0,0,0
dak1q27,2016-11-28 22:48:59-05:00,theroyalham,,Is this seriously a question on here?,5ffk7y,t3_5ffk7y,whathaveidone56,,Comment,-7,0,-7
dak2a1u,2016-11-28 23:01:28-05:00,whathaveidone56,,I thought it was the best place to ask :/ ,5ffk7y,t1_dak1q27,theroyalham,,Reply,2,0,2
5ff0ry,2016-11-28 18:14:40-05:00,adbJ114,"How do the font packages you download from Font Squirrel work, exactly? Why are they needed when we have Unicode?","Okay, so I'm an experienced web developer and I know in general what webfonts are and how a site like Font Squirrel basically works. You select a collection of icons and then download .eot, .woff, etc. files, and load them in a style sheet with @font-face.

What I'm wondering is why, for example, the ""content"" attribute is filled with a value like ""\e004"" for each icon. Is this hexadecimal? Why the backslash? How does the loaded font file then know to decode ""\e004"" into a specific icon?

Also, aren't these icons part of the Unicode charset? If so, why the need for separate font files in the first place? Are the font files using the Unicode charset, and using strange encoding like ""\e004"" to double translate them, for some reason?",,,,,Submission,5,0,5
dajrih4,2016-11-28 19:03:46-05:00,ldpreload,,"Unicode reserves codepoints E000 through F8FF as a [private use area](https://en.wikipedia.org/wiki/Private_Use_Areas), which means each font / application is free to use that for whatever purpose it wants, without standardization. So even though Unicode is being used, the codepoint E004 doesn't mean anything without reference to a specific font, or some other specific local definition of what that character is supposed to represent.

So yes, it is hex: the backslash is to indicate that you're meaning a Unicode character. You _could_ just type the actual E004 character there, it'd just be incredibly annoying to use with a text editor (especially if your text editor isn't using the webfont in question, in which case you'll see a � if you're lucky and a blank if you're not). If you leave off the backslash, you'd set the content to the specific characters uppercase E, zero, zero, and four.",5ff0ry,t3_5ff0ry,adbJ114,,Comment,4,0,4
dak5gpm,2016-11-29 00:23:49-05:00,adbJ114,,"This was super helpful, thanks!",5ff0ry,t1_dajrih4,ldpreload,,Reply,1,0,1
5fe2nz,2016-11-28 15:37:07-05:00,amicus_nintendi,Are the MiBench benchmarks compatible with the DineroIV cache simulator?,"I know that DineroIV, being trace driven, only runs trace (.din) files. At least I think that's the case anyway.  Do the MiBench benchmarks even come with trace files or is trying to get DineroIV to run them a big waste of time? Or am I completely mistaken about all of this? That's always a possibility.",,,,,Submission,0,0,0
5fa1wq,2016-11-28 00:48:08-05:00,zxy_xyz,What do you do?,"Trying to decide between Physics and CS. Curious what you guys do as careers.

Me:
Always toyed with tech and learned about it. Ended up spending hours daily and all of teens coding. Mostly video games. So my biggest skill is programming (from android to arduino) but I feel like CS isn't the best thing i should do. Physics compels me because it makes me feel im doing something important. I both love math and writting code but above all I need to have problems to solve that are interesting and exciting. 

So Im hoping you can give me some thoughts in what makes you passionate. I want the right major so I can study endlessly. I both want to try the hardest and interesting problems as well as feel important doing it.",,,,,Submission,3,0,3
daj1gen,2016-11-28 10:05:28-05:00,videoj,,Sounds like what you want to do is [Computational Physics](https://en.wikipedia.org/wiki/Computational_physics).  ,5fa1wq,t3_5fa1wq,zxy_xyz,,Comment,5,0,5
daiubv2,2016-11-28 05:29:54-05:00,Disloc8,,"I'm not who you are looking for but I'm endeavoring to major in CS or CE. I may love math and science but I don't have any experience in programming. How and where did you start learning if I may ask? And if at all possible could you point me to sites or ways to learn, it's been hard to find anything that teaches such things without having to pay.
",5fa1wq,t3_5fa1wq,zxy_xyz,,Comment,1,0,1
daiwk34,2016-11-28 07:24:46-05:00,zxy_xyz,,Sure thing. Thr best videos were thenewboston on youtube. They are short and effective. Whenever something breaks just googlr it lol. But Tutorialspoint.com has decent tutorials but ehat mostly worked was the videos and experimenting. It was a lot of painful trial and errot (was learning to make games) but it's really helpful ,5fa1wq,t1_daiubv2,Disloc8,,Reply,1,0,1
dak05r9,2016-11-28 22:13:58-05:00,Disloc8,,Thanks for the pointers!,5fa1wq,t1_daiwk34,zxy_xyz,,Reply,1,0,1
dajivdd,2016-11-28 16:04:55-05:00,ruslp11,,Also search for Derek Banas on youtube he has great tutorials.,5fa1wq,t1_daiubv2,Disloc8,,Reply,1,0,1
dak1mxr,2016-11-28 22:47:01-05:00,Disloc8,,"I'll check him out, thanks.",5fa1wq,t1_dajivdd,ruslp11,,Reply,1,0,1
daj6imp,2016-11-28 11:59:39-05:00,MCPtz,,"You can do, for example, B.S. in computer science to learn how to program well. Then do a graduate degree in, like another user said, computational physics.

Another path is to do physics and take some extra programming classes, even as much as a minor in CS. I was very busy with just one degree, but it's only four years to have a lifetime of fun problems to solve.

Look to ask departmental advisors for help in making this decision. They'll help you shape it for your school.",5fa1wq,t3_5fa1wq,zxy_xyz,,Comment,1,0,1
daj7l94,2016-11-28 12:21:47-05:00,zxy_xyz,,"Thanks. I think I can program fairly well, so for getting better I think a design book would do. I do have interest in AI and graphics though. Is there a detective kinda job that can mix this in?",5fa1wq,t1_daj6imp,MCPtz,,Reply,1,0,1
daj9ipc,2016-11-28 13:00:19-05:00,MCPtz,,"Almost all bugs in software required ""detective"" type gathering of evidence.",5fa1wq,t1_daj7l94,zxy_xyz,,Reply,1,0,1
dajayhk,2016-11-28 13:28:58-05:00,zxy_xyz,,They're usually more tedious than interesting. Id like some variety every now and then,5fa1wq,t1_daj9ipc,MCPtz,,Reply,0,0,0
5f8y0q,2016-11-27 20:41:56-05:00,jollytopper,At what point does loop unrolling no longer becomes useful?,"Most of the time, the unroll factor is 4. Is there a reason for this? Can I keep rolling and get more performance? Is there a memory limit?",,,,,Submission,2,0,2
daimzj5,2016-11-28 00:21:52-05:00,theobromus,,"There is a limit for a few reasons. Firstly the higher the unrolling factor the longer the total operation needs to be to make up for the overhead of running on the end (although that itself can be unrolled with a smaller factor). But more importantly, more unrolling results in more object code for the copies of the loop. In many cases, memory bandwidth becomes the ultimate constraint on performance, and smaller program code will run faster (because it can remain resident in cache).

Unfortunately, this depends on the specifics of the processor you're using, so really you have to measure. Although an optimizing compiler should do a reasonable job.",5f8y0q,t3_5f8y0q,jollytopper,,Comment,5,0,5
daipvxc,2016-11-28 01:54:52-05:00,jollytopper,,"Thank you for your response! By your second point, are you referring to instruction memory and cache misses caused by more code?",5f8y0q,t1_daimzj5,theobromus,,Reply,1,0,1
dair23c,2016-11-28 02:42:47-05:00,theobromus,,"Yeah, it depends on the architecture, but in general, the bigger your program, the more often you are going to have instruction cache misses. There's also the more general issue on most newer processors that just reading in the memory for a calculation (for example if you are computing the dot product of two large vectors or something) might be the slowest part.

And of course on a larger scale, if you make the program bigger, it will load slower from disk, be bigger to download, etc., etc.

It really depends on your app and you have to measure, but I've often found that aside from general algorithmic issues (i.e. having good big-O behavior) being aggressive about using less memory and smart about having memory locality (trying to keep related data together) has a bigger performance impact than things like loop unrolling except in some very limited cases (and in those cases, I probably should have been doing things like using gpu/cuda to really get the best performance).

Btw, Chapter 5 of ""Computer Systems: A Programmer's Perspective"" has a pretty clear explanation of how various of these types of optimizations work at a processor level (at least in the 2nd edition, which is the one I have) if your interested in that.",5f8y0q,t1_daipvxc,jollytopper,,Reply,1,0,1
daizbav,2016-11-28 09:05:44-05:00,thechao,,"Register pressure. Most loops that need automatic unrolling are too large to fit into the loop cache (or whatever we're calling it these days), so they never get predecoded, in the first place. The I$ is going to get hammered, too, but what stops the compiler well before any I$ analysis is the cost of spill-fill---especially for nested loops with hot outer variables.",5f8y0q,t3_5f8y0q,jollytopper,,Comment,1,0,1
dajgko2,2016-11-28 15:20:21-05:00,andybmcc,,"> Most of the time, the unroll factor is 4. Is there a reason for this?

Perhaps the SIMD register set size?  I know AVX uses 256 bit sets, which is four 64bit words.

> Can I keep rolling and get more performance?

There is a limit due to caching.  It really depends on the architecture and data locality during unrolling/fusion, as well as instruction caching.",5f8y0q,t3_5f8y0q,jollytopper,,Comment,1,0,1
5f7lqn,2016-11-27 16:20:16-05:00,Murikk,"How would you explain directories, files, and absolute vs. relative paths to a true beginner?","Hey /r/AskComputerScience,

I teach a web development course to classes of about 25 adults. The students come from all walks of life and have all different levels of understanding. I have been teaching programming and web development for 5 years and still have not come up with a good way to explain directories to students. For example, when explaining how to link a CSS file to an HTML file using a path that is relative from that HTML file. No matter what I do, I end up having to explain it many times and in different ways for different students. How would you explain it to beginners? What sort of visuals would you use? Just looking for inspiration to help me get this point across more clearly.",,,,,Submission,15,0,15
daiat8c,2016-11-27 19:33:09-05:00,BonzoESC,,"It's a navigation problem, just like driving or walking directions. Sometimes ""[go out the door, take an immediate left, and walk until you see the Star Wars mural, then it'll be on your right](https://goo.gl/maps/HRoNhiVwhPt)"" is the most useful way to get someone somewhere, other times ""[565 NW 24th Street, Miami, Florida](https://www.google.com/maps/@25.7999649,-80.2041685,3a,75y,150.15h,92.58t/am=t/data=!3m6!1e1!3m4!1s1ysrUGx4ZWThJ1bako0aMg!2e0!7i13312!8i6656)"" is more useful. 

Relative directions are quick to build if you're familiar with the locations of the origin and destination and easy to unspool even without familiarity. In filesystem terms, this is useful if you don't want to care where your program is. In a Rails app, you can use `Rails.root.join('config/boot.rb')` to turn a path relative to the app's root directory to an absolute path, which means you don't have to care whether it's installed locally in `~/Documents/app-name` or on the production platform at `/app`.

Absolute directories are useful for things with canonical locations, when there's the possibility of navigation error or not knowing where the navigator is starting from. For example, ""565 NW 24th Street, Miami, Florida"" gets you to the brewery whether you're coming from the coffee shop down the street or from Canada. On a filesystem, an absolute path can be passed from one program to another without concern for what their current directory is (i.e. where they'll be navigating from). In a Rails app, you might pass the absolute path to a file back to the front end web server so it can send that file to the client without tying up a relatively expensive Ruby thread.",5f7lqn,t3_5f7lqn,Murikk,,Comment,8,0,8
daj73gj,2016-11-28 12:11:48-05:00,Murikk,,Thanks! I do know that they often struggle with the difference between absolute and relative (think CSS positioning). Maybe I can kill two birds with one stone by starting here.,5f7lqn,t1_daiat8c,BonzoESC,,Reply,1,0,1
dai47in,2016-11-27 17:09:01-05:00,MakingMarconi,,"Perhaps you could use the [boxes from futurama](http://imgur.com/JR3lQ6X) (perhaps even play the part of the episode where they attach strings to themselves and jump into boxes over and over). The only modification you'd need to make is to introduce the idea of a ""parent"" (and ultimately, ""root"") box.",5f7lqn,t3_5f7lqn,Murikk,,Comment,3,0,3
daj71ju,2016-11-28 12:10:43-05:00,Murikk,,Love the idea of using a Futurama reference. Thanks!,5f7lqn,t1_dai47in,MakingMarconi,,Reply,2,0,2
dai5ggs,2016-11-27 17:35:58-05:00,albatrek,,"For absolute vs relative paths, I like to draw it out in a standard tree structure, then anthropomorphize the heck out of the files - ""this file can see these other files"", ""imagine this file walking along this path to find this other one"", ""imagine giving this file directions to go from here to where this other file is"", that kind of thing. I'll take it pretty far - ""it's confused because you told it to find this file, but it can't see that one"", ""it didn't know you wanted it to start up there, so it tried starting from here and got lost"", etc. 

I've only ever helped people who already understood the basics of files and folders, so I'm not sure how to explain that if you're working with people who don't have it down yet.

Interested to see what else people come up with.",5f7lqn,t3_5f7lqn,Murikk,,Comment,4,0,4
daj75ak,2016-11-28 12:12:49-05:00,Murikk,,"This is sort of what I've been doing. I think there's hope for this method with a bit of repetition, perhaps combined with another one of these recommendations.",5f7lqn,t1_dai5ggs,albatrek,,Reply,1,0,1
dairxnj,2016-11-28 03:22:56-05:00,awesomo_prime,,"If possible, try to learn what their backgrounds are. Whenever I'm explaining/teaching/tutoring students/people I take a few minutes to get to know what's their background. 

Often times I can come up with something to what they already know. Which makes it easier for them to grasp concepts and such. ",5f7lqn,t3_5f7lqn,Murikk,,Comment,3,0,3
daj77e3,2016-11-28 12:14:02-05:00,Murikk,,"Thanks! I'm currently approaching this in a similar way, but as the number of students grow, I'd like to find a solution that will work for a good portion of students right up front, then use this approach for any still confused.",5f7lqn,t1_dairxnj,awesomo_prime,,Reply,1,0,1
dak108g,2016-11-28 22:33:04-05:00,awesomo_prime,,">  as the number of students grow, I'd like to find a solution that will work for a good portion of students right up front,

If possible, try to get them to fill out a survey before the presentation and break them up into groups based on survey information. I tend to find that there's a one/few in a group that can translate what you have to say/teach into what everybody else can understand. Splitting people into groups and finding that one/few persons can really get things rolling.",5f7lqn,t1_daj77e3,Murikk,,Reply,2,0,2
dak5a2q,2016-11-29 00:18:23-05:00,Murikk,,Great idea. We are working on some sort of prep course so I'll include this.,5f7lqn,t1_dak108g,awesomo_prime,,Reply,1,0,1
daivkje,2016-11-28 06:37:24-05:00,ScienceAnTing,,"Your computer's storage is a building filled with floors, rooms, filing cabinets and drawers.

Your html file is located on Floor ""Internets"", Room ""Development"", Cabinet 4, Drawer 3.

Fortunately, your CSS files are also in your Internets/Development room, so you just point them to Cabinet 2, Drawer 1.

?",5f7lqn,t3_5f7lqn,Murikk,,Comment,3,0,3
daj5g91,2016-11-28 11:37:28-05:00,smeezy,,"Use the ""folders and documents"" metaphor that made the desktop GUI successful. Every document must go in a folder. A folder can go into another folder. There's one ""master"" folder that has to hold everything.",5f7lqn,t3_5f7lqn,Murikk,,Comment,3,0,3
daiys62,2016-11-28 08:48:59-05:00,ranci,,Filing cabinet.,5f7lqn,t3_5f7lqn,Murikk,,Comment,0,0,0
5f3zf1,2016-11-27 01:08:27-05:00,Optometrist__Prime,"Somebody explain to me how I can store ""trees"" of numbers or even ""graphs"" inside arrays or variables?","The graphs I refer to, are like webs, where there is a node at each intersection (used for pathfinding and that jazz)",,,,,Submission,2,0,2
dahb2gx,2016-11-27 01:17:59-05:00,ldpreload,,"There are multiple ways to store them, and which way is right depends a lot on how you want to use the structure.

Most programming languages have a way for objects to reference other objects. This is internally implemented with pointers, but only in a few languages (like C or C++) are those pointers visible to you. In others (like Java or Python or JavaScript), that's just how references to objects work.

So, you can have a ""node"" object that contains two references/pointers to two other ""node"" objects, or possibly an empty value (`None`, `null`, etc. as appropriate) if there's nothing to point to, as well as the actual number at that node. A collection of these nodes forms a tree.

At the raw memory level, the structure will consist of two memory addresses plus one integer. By convention, the address zero is unused and represents the absence of a value. But if there's something other than zero, it gives the location of some other structure in memory, which itself consists of two memory addresses and an integer. You can follow these pointers around until you hit a zero, and stop.

The other common way to represent a graph is with a matrix. Number the nodes, starting from zero, in some order you like. Then make a square matrix, and label the rows as starting points and the columns as ending points. If there's a path from, say, node 2 to node 5, put a 1 at row 2 column 5. Otherwise put a 0.

This is easy to look things up in, and has some nice mathematical properties (e.g., if you square the matrix, you get a graph representing which nodes can be reached in two hops instead of one hop), but for a matrix that has a large number of nodes and not that many edges, it's a pretty inefficient use of memory.",5f3zf1,t3_5f3zf1,Optometrist__Prime,,Comment,6,0,6
dahbzjx,2016-11-27 01:53:15-05:00,ILikeLeptons,,[Adjacency matrices](https://en.wikipedia.org/wiki/Adjacency_matrix) are commonly used to represent directed graphs in a computer usable form. They've got some interesting and useful properties to boot!,5f3zf1,t3_5f3zf1,Optometrist__Prime,,Comment,4,0,4
dahkjra,2016-11-27 09:16:03-05:00,devshady,,"http://nptel.ac.in/courses/106103069/51

I hope it'll help.",5f3zf1,t3_5f3zf1,Optometrist__Prime,,Comment,3,0,3
5f0zto,2016-11-26 14:09:04-05:00,balitimore,Preparation for Operating System course,"Hi everyone,
I am an undergrad with an extremely rigorous course load next semester. One of the courses I will be taking is Operating Systems, which is notoriously hard here, so I was wondering what I could do now that might help with the course or at least just save me time? 

Thank you",,,,,Submission,5,0,5
dagmk5k,2016-11-26 14:41:15-05:00,nbp615,,Read Operating Systems Concepts 9th ed,5f0zto,t3_5f0zto,balitimore,,Comment,5,0,5
dagmr8h,2016-11-26 14:45:32-05:00,balitimore,,I meant something not as intense as doing all of the course work. I was think more a long the lines of getting familiar with the command line. It's approach finals so I won't be having a lot of time to do something like that.,5f0zto,t1_dagmk5k,nbp615,,Reply,2,0,2
dan14qz,2016-11-30 21:28:48-05:00,Badgrs,,"No… seriously… it's an _incredibly_ good textbook: honestly my favorite from all of college. It's not too hard to find a (questionably legal) PDF of it online. Or I'm sure you could find a copy in a library somewhere: it's a pretty common textbook. Read the first few chapters.

But, if you're looking to just familiarize yourself a command line and messing around, there are a plethora of tutorials and educational videos out there. I don't have any specific ones to recommend, so go explore!",5f0zto,t1_dagmr8h,balitimore,,Reply,1,0,1
dagqrph,2016-11-26 16:16:57-05:00,None,,What's processor context switching? Don't ask about anything you knew before the class.,5f0zto,t3_5f0zto,balitimore,,Comment,3,0,3
dagt9rp,2016-11-26 17:16:06-05:00,4nexus,,What makes it hard if I may ask? I'm taking Operating systems and it doesn't seem that hard here.,5f0zto,t3_5f0zto,balitimore,,Comment,2,0,2
dagtze9,2016-11-26 17:34:09-05:00,pencan,,CMU has a notoriously hard OS course which has been adapted for many universities ,5f0zto,t1_dagt9rp,4nexus,,Reply,3,0,3
dah1yq9,2016-11-26 20:57:19-05:00,RosscoGiordano,,"a) https://youtu.be/9GDX-IyZ_C8

b) https://youtu.be/2i2N_Qo_FyM

There's a ton of videos like this all over youtube. ",5f0zto,t3_5f0zto,balitimore,,Comment,1,0,1
dahg0bh,2016-11-27 05:14:27-05:00,Gordon101,,Read about CPU scheduling.,5f0zto,t3_5f0zto,balitimore,,Comment,1,0,1
dahv2t6,2016-11-27 13:54:52-05:00,adamnemecek,,"https://news.ycombinator.com/item?id=11655472

Do you know how a queue/an event loop works? It turns out that all of OS is basically just a combination of these to a large extent.

Also check out these books http://pages.cs.wisc.edu/~remzi/OSTEP/ 

and https://www.amazon.com/Design-Implementation-FreeBSD-Operating-System/dp/0321968972/

Forget all the other OS books.",5f0zto,t3_5f0zto,balitimore,,Comment,1,0,1
5ewc11,2016-11-25 17:56:11-05:00,iamyou-whatiseeisme,"Is there a good reason to cover up your laptop webcam with a sticker or some tape, or is that just paranoid?","See a lot of people at uni with covered up webcams. I think it looks stupid, but I'm wondering if it's smart to do this or not.",,,,,Submission,24,0,24
dafvkpn,2016-11-25 22:27:44-05:00,BonzoESC,,"There are lots of good reasons:

* you don't trust that the entire software load on your machine has not been compromised in an undetectable way
* you work with material or in environments you don't want to show up on video accidentally
* you work remotely and prefer to not dress up because it lets you keep the thermostat at a higher temperature which saves money because air conditioning is expensive, but you also don't want to show up on a video conference without a shirt
* you want to signal socially that you Care about computer security
* your employer or customer requires it

All of these reasons are ones I've covered up cameras with stickers.",5ewc11,t3_5ewc11,iamyou-whatiseeisme,,Comment,11,0,11
dafq200,2016-11-25 19:45:12-05:00,w1282,,"I would like to kindly disagree with the other respondent here.

If your computer is connected to the Internet there is an implicit risk. Some people are targeted, others are just victims of opportunity.

If a botnet can exploit your machine it will: that's how they're programmed.

Is it a guarantee that if you are hacked you will be spied on? No, absolutely not, but you can't be certain you won't be hacked and you can't be certain you won't be spied on and blackmailed.

Is that paranoid? No, there is significant evidence supporting the fact that people are hacked all the time without being targeted.

I would suggest you cover your webcam until you need it, or use a machine with a mechanical aperture that lets you close it.",5ewc11,t3_5ewc11,iamyou-whatiseeisme,,Comment,26,0,26
dag71cx,2016-11-26 06:41:55-05:00,avaxzat,,"While I certainly agree with these sentiments, I think many people exaggerate this issue. It's always wise to minimize the attack surface and so covering up your webcam until you actually use it is a legitimate precaution. But I feel there are many much more effective ways to exploit a victim's machine than to spy on them via their webcam. For instance, I think keylogging would be much more profitable as this can be used to extract login credentials and credit card information, potentially without the victim ever realizing it. Of course, images captured via webcam can be used for extortion purposes, but these practices are way more risky than keylogging as they require more direct interaction which immediately alerts the victim to the attacker's presence.

On the whole, I think the risk of being spied on via webcam is overestimated, although it never hurts to eliminate as many attack vectors as possible. Especially when the solution is as cheap as covering a small part of your pc with duct tape.",5ewc11,t1_dafq200,w1282,,Reply,5,0,5
dag0zdx,2016-11-26 01:20:50-05:00,albatrek,,I cover mine because it's an incredibly simple fix for a potential problem. Do I think my computer is compromised? No. Do I think it's possible? Yeah. Do I want to be spied on? Hell nah. Do I regret the 15 seconds it took me to cut off the top of a sticky note and cover my webcam? Nope.,5ewc11,t3_5ewc11,iamyou-whatiseeisme,,Comment,14,0,14
dafqd9a,2016-11-25 19:54:14-05:00,artillery129,,"Depends, does your webcam have a hardware light that turns on as well? What OS are you running? Are there even drivers available for your webcam? There's a lot of questions, personally I think it's pretty dumb, and most of the time when I'm doing serious work, my computer is in clamshell mode",5ewc11,t3_5ewc11,iamyou-whatiseeisme,,Comment,1,0,1
dafvdlh,2016-11-25 22:21:48-05:00,okiyama,,The hardware lights have been bypassed before. ,5ewc11,t1_dafqd9a,artillery129,,Reply,10,0,10
dah2mv0,2016-11-26 21:15:28-05:00,wischichr,,Source pls.,5ewc11,t1_dafvdlh,okiyama,,Reply,3,0,3
dagq8rj,2016-11-26 16:05:33-05:00,KyleRochi,,"You can turn the light on an not the camera, or the camera without the light.",5ewc11,t1_dafqd9a,artillery129,,Reply,1,0,1
dagthrf,2016-11-26 17:21:44-05:00,artillery129,,"Depends on the camera, if you can turn on the camera without the light, then you don't have a hardware light by definition",5ewc11,t1_dagq8rj,KyleRochi,,Reply,1,0,1
dai0tki,2016-11-27 15:56:57-05:00,None,,"If the government (supposedly) can build a worm like STUXNET, then it can build something that your wonderful Norton or Windows Defender cannot detect. STUXNET was very specific, and very well built. They have real professionals working for them.

Why would they care about my camera? We know they have the means to collect and store all of this data, so why wouldn't they write code to collect it from everyone? Nobody is sitting there watching me, but the information can get collected and stored. I'd rather use a small piece of tape, thanks. Even if there's no real chance, I'm using a small piece of tape. It makes me happier.

https://en.wikipedia.org/wiki/Stuxnet",5ewc11,t3_5ewc11,iamyou-whatiseeisme,,Comment,1,0,1
daj9eiy,2016-11-28 12:58:03-05:00,swilliamseu,,"I have had mine covered ever since I found this subreddit: /r/controllablewebcams

There are people out there that will watch you for no reason. And that is horrifying.",5ewc11,t3_5ewc11,iamyou-whatiseeisme,,Comment,1,0,1
dafsqzb,2016-11-25 21:03:53-05:00,robot_one,,"I work in the security industry and think it is dumb. 

Botnets (generally) do not directly exploit end user systems, the commenter who said so is talking out of their ass. That would be considered a worm. Exploits are generally delivered through web browsers (exploit kits) and junkware that users download. 

Malware operators are concerned with making money. Pictures of you aren't very valuable. That is why ransomware is so popular. Also DDOS botnets. 

The biggest threat would be RATs (remote administration tools) that a stalker would install. It would need to be somewhat advanced to turn on your webcam without the light. 

The only time I have disabled a mic and webcam was on a secure system. I did this by opening the laptop and unplugging them. I'm not really afraid of someone taking pictures of me. The only time it would be a concern is if you want to hide your identity on a machine, completely. 

If you're going to video chat with someone, leave it off. If you won't be doing that, open up your computer and unplug the mic and webcam. The mic is probably the more useful of the two for an attacker.",5ewc11,t3_5ewc11,iamyou-whatiseeisme,,Comment,-3,0,-3
dafn2th,2016-11-25 18:22:32-05:00,ObeselyMorbid,,"Unless people have a reason to target you, you're probably OK. ",5ewc11,t3_5ewc11,iamyou-whatiseeisme,,Comment,-1,0,-1
dagodc2,2016-11-26 15:23:00-05:00,NeoKabuto,,"You're right, but ""a reason to target you"" could just be that you have any amount of money. Embarrassing webcam footage could easily be blackmail material.",5ewc11,t1_dafn2th,ObeselyMorbid,,Reply,1,0,1
dag61hn,2016-11-26 05:38:28-05:00,PmMeYourSexyShoulder,,"There are risks in every part of life. It's important to take precautions. But that shouldn't hinder how you live or you would never leave the house because  a plane might fall from the sky and land on your car.

If you have decent security and don't have dodgy software installed on your machine you are probably okay.

Don't live in fear.

That said, you mentioned university. There are way too many incidents of university students having computers compromised because they are ""desirable targets"" especially if you are deemed an attractive young lady. As unpleasant as it sounds there are people online who are really into webcam spying on unknown users. 

In a dorm environment odds are you are getting dressed or coming out of the shower or engaging in sexual activity within view of a camera if your laptop is open. Also being a university environment there are a lot of people who are of questionable maturity and think they have the world figured out, they could target whoever they want. It's not difficult to get access to someone's computer. People give all sorts of information to the IT guy and just hand over their machines.

This is just one example of one getting caught: http://www.nydailynews.com/news/crime/mastermind-teen-usa-sextortion-plot-18-months-prison-article-1.1724809
",5ewc11,t3_5ewc11,iamyou-whatiseeisme,,Comment,-1,0,-1
5evmqh,2016-11-25 15:33:28-05:00,boomertrooper,Is there a difference between turning my compuyer off then back on immediately and turning it off for the night and turning it back in the following morning?,My computer is getting pretty old and I end up rebooting after a download or something basic like that. When I turn it back on it runs okay for a couple hours then starts to struggle again. Would I solve this problem by leaving the computer off for longer periods of time?,,,,,Submission,1,0,1
daflfjk,2016-11-25 17:38:40-05:00,acromulent,,"Title Question:

Yes. About 8 hours.",5evmqh,t3_5evmqh,boomertrooper,,Comment,6,0,6
dafohrm,2016-11-25 19:00:50-05:00,notUrAvgITguy,,"Aside from allowing the heat sinks to fully cool off there would be no difference, and if that little bit is making a tangible difference you have some serious heat problems.",5evmqh,t3_5evmqh,boomertrooper,,Comment,5,0,5
dafvm5q,2016-11-25 22:28:56-05:00,BonzoESC,,"You probably wouldn't solve your particular problem but leaving it off longer saves power, so that's a thing. ",5evmqh,t3_5evmqh,boomertrooper,,Comment,2,0,2
dagd44f,2016-11-26 10:55:42-05:00,brainburger,,What OS are you running? Most flavours of Windows get bloaty and unstable after months of use. You could do a clean reinstall or factory reset. Also make sure there's no dust in the fans or vents.,5evmqh,t3_5evmqh,boomertrooper,,Comment,2,0,2
5ev7f7,2016-11-25 14:10:24-05:00,finessseee,[Beginner] what are some recommended languages to learn in order?,"Im new to programming and CS. I'm taking Prog1 (intro to object oriented programming).. but it's not very fun and I feel like having no coding background before is really making this a lot harder for me to learn than I imagined. All I know is some basic java and that's it. I feel like I need to take some steps back and learn the basics of other languages before I continue throwing myself into a sinking ship semester after semester. What languages do you recommend a newly hopeful programmer to start off with? Any recommended links or tutorials?

Thanks! ",,,,,Submission,1,0,1
dafem0a,2016-11-25 14:46:57-05:00,dorkus,,"I recommend Ruby or Python. Both excellent languages with wide adoption and you don't have to fuss with a bunch of arcane boilerplate as you would with Java

https://learncodethehardway.org/ruby/
https://learncodethehardway.org/python/
",5ev7f7,t3_5ev7f7,finessseee,,Comment,2,0,2
dafevgm,2016-11-25 14:53:26-05:00,finessseee,,Thank you! ,5ev7f7,t1_dafem0a,dorkus,,Reply,1,0,1
dam4bg8,2016-11-30 10:35:42-05:00,_HyDrAg_,,Learn python the hard way is a bit weird though. The writer tells you to spend a week memorizing tables for basic logical operations like or instead of actually understanding what they do. His article about how python 3 is evil also makes him pretty unreliable in my eyes.,5ev7f7,t1_dafem0a,dorkus,,Reply,1,0,1
daol68u,2016-12-01 22:14:21-05:00,dorkus,,"Zed is a bombastic SOB...but...I think the model he's put forward is pretty compelling, regardless of the other challenging content he throws out into the world.",5ev7f7,t1_dam4bg8,_HyDrAg_,,Reply,1,0,1
daffebn,2016-11-25 15:06:31-05:00,AgentZeusChops,,"HTML & CSS, Python || Swift, Java || C++, Objective-C, Shell (it's easier to understand once you finish learning about stacks and queues for Objective-C). HTML & CSS have immediate results while Python & Swift both have command-line interactions which don't always show issues up-front. Moving to Java & C++, those languages are considered ""low-level"" languages as you can start interacting with hardware using those languages, Objective-C runs Operating Systems but happens to also be a runner to launch Windows (7 & 8/8.1, not so sure about 10 yet). Shell is used for Terminal (Linux/Mac) and Powershell (Windows 10 only), which handles things in a very strange way, although it can be done prior to Java/C++ it just feels weird to write (IMO).

NOTE: "" || "" means ""or"" in this case.",5ev7f7,t3_5ev7f7,finessseee,,Comment,1,0,1
dafhox3,2016-11-25 16:03:37-05:00,esmith327,,"Another vote for Python. You can focus on the basics there, then move on to a ""bigger"" language like Java. It's not a bad idea to learn a shell like bash or Powershell as it'll come in handy for gluing stuff together, but bash won't necessarily aid you in learning other languages as it has some really arcane aspects and is not object-oriented. ",5ev7f7,t3_5ev7f7,finessseee,,Comment,1,0,1
daiti4j,2016-11-28 04:44:10-05:00,bbpgrs,,"I'd say start with Python then Java and C++. 

After that you can look into HTML, CSS, JavaScript, and PHP/C# if you're interested in web development. 

For desktop application development you should get comfortable with C++. I think C# and C++ are used for windows 10's ""universal platform"".

For mobile, the two main platforms are Android (mostly Java), and iOS (Objective-C/Swift)

",5ev7f7,t3_5ev7f7,finessseee,,Comment,1,0,1
5eu615,2016-11-25 10:49:39-05:00,brainburger,"What does your ISP, or a web proxy server know about web usage with HTTPS?","I have noticed that the net monitoring at my work would block some parts or reddit before reddit used https. Now it doesn't. Is that because it can't?

Friends and I were discussing what can middle-men read from https traffic? I imagine the data itself is encrypted, but how about the URL, and the domain being accessed by a user?

In the UK ISPs are to be requested to store urls visited for 1 year. What in practice does this mean they would know? For example, could a political dissident post on reddit or FB without being identified?",,,,,Submission,9,0,9
dafdces,2016-11-25 14:15:35-05:00,robot_one,,DNS. Using a VPN should get you sorted. ,5eu615,t3_5eu615,brainburger,,Comment,3,0,3
daficcv,2016-11-25 16:20:21-05:00,INCOMPLETE_USERNAM,,"Just a few additions to what others have said:

As /u/secureartisan said, it's entirely possible for a workplace to perform MITM to decrypt HTTPS traffic, though only on computers they can configure to allow their bogus certs. Ie., they couldn't decrypt your personal phone's wifi traffic. So, they could be continuing to inspect reddit traffic, but they clearly are not.

As /u/wischichr said, no, the domain in an HTTPS packet is not visible to an ISP. However, the DNS traffic is not encrypted (which obviously contains the domain), and even if it were, it would be trivial to simply resolve IPs themselves. ",5eu615,t3_5eu615,brainburger,,Comment,2,0,2
dag8l9i,2016-11-26 08:11:44-05:00,matthew5025,,"The domain in a TLS handshake IS VISIBLE if Server Name Indication is supported, which is basically every modern browser.",5eu615,t1_daficcv,INCOMPLETE_USERNAM,,Reply,1,0,1
dafg3ab,2016-11-25 15:23:38-05:00,secureartisan,,"HTTPS can be blocked/monitored - it's a more common practice in large workplaces that can facilitate the equipment.

They essentially deploy a man-in-the-middle device (you will see for example that the certificate for https Reddit is issued from your workplace (which your browser trusts) and not Reddit itself).",5eu615,t3_5eu615,brainburger,,Comment,1,0,1
dafckxa,2016-11-25 13:56:35-05:00,wischichr,,"* Target and source IP Adress (No URL / Domain)
* You can obviously see when the package was sent
* Package size (byte)

I'm pretty sure the ISP is saving all of those infos in the UK.

He monitoring tool you described can not inspect https requests (in a meaningfull way) the could block IP adresses or swap certificates (by using self signed certs - see fiddler2 for example)",5eu615,t3_5eu615,brainburger,,Comment,0,0,0
dafdblo,2016-11-25 14:15:02-05:00,okiyama,,Doesn't domain need to stay visible? Or is that fine to stay in the encrypted HTTP headers?,5eu615,t1_dafckxa,wischichr,,Reply,1,0,1
dafim0n,2016-11-25 16:27:02-05:00,INCOMPLETE_USERNAM,,"The domain is only needed when performing a DNS lookup, which your computer does itself. After which the Host and URL you are requesting is stored in the encrypted HTTP headers as you say.",5eu615,t1_dafdblo,okiyama,,Reply,1,0,1
dafojgl,2016-11-25 19:02:09-05:00,okiyama,,My computer does dns lookup? I thought it used an external dns server like the Google ones? ,5eu615,t1_dafim0n,INCOMPLETE_USERNAM,,Reply,1,0,1
dafpnxv,2016-11-25 19:33:55-05:00,INCOMPLETE_USERNAM,,Your computer performs a DNS lookup by querying external DNS servers like those run by Google (8.8.8.8). Your ISP can see these queries plain as day.,5eu615,t1_dafojgl,okiyama,,Reply,1,0,1
dafqkqv,2016-11-25 20:00:09-05:00,okiyama,,Ahh okay that's what I was remembering. So for practical purposes they do know the domains. Are dns results cached or do they happen for every request? ,5eu615,t1_dafpnxv,INCOMPLETE_USERNAM,,Reply,1,0,1
dafr67w,2016-11-25 20:17:27-05:00,INCOMPLETE_USERNAM,,Any good OS will cache DNS records.,5eu615,t1_dafqkqv,okiyama,,Reply,1,0,1
5etxib,2016-11-25 09:58:51-05:00,OptimisticElectron,Is it possible to get the running time of this algorithm?,"www.pastebin.com/rB5hZVY4

I know there's a O( 2^n ) time complexity solution to this problem, but what's would the running time be if I use the algorithm above instead?",,,,,Submission,0,0,0
daf62np,2016-11-25 11:15:08-05:00,Tylfin,,"Real bad, in the worst case the first iteration loops n-times recursively invoking itself looping (n-1) times etc. So probably like n^n.",5etxib,t3_5etxib,OptimisticElectron,,Comment,3,0,3
daiun8q,2016-11-28 05:47:48-05:00,OptimisticElectron,,Isn't it more like n!?,5etxib,t1_daf62np,Tylfin,,Reply,1,0,1
5erhz4,2016-11-24 22:27:01-05:00,adbJ114,How are emoticons encoded?,"Are they part of Unicode? Why are they not 100% compatible across Android, iOS, and Mac OS? Are there efforts to create standards for them? Are they in the UTF-8 character set?",,,,,Submission,5,0,5
daergk3,2016-11-25 00:50:32-05:00,PastyPilgrim,,"They are Unicode, but Unicode is used to encode and decode what a set of bits is *representing*, it isn't for storing visual information about a character. Like with Unicode 8, there are 8 bits used to reference a character such as the letter 't'. Those 8 bits aren't nearly enough information to store the information needed to visually draw the letter 't'. It's the same with emoticons: Unicode supports a specific set of emoticons (dog, American flag, winky face, etc) but it doesn't tell you how to render them (just what they are). If they did store what the character looked like, not only would each character take up a ridiculous amount of storage space, but it would be harder to have different fonts.

It's up to whoever is decoding the characters to have some data on how to render them. This results in platforms having varying renderings for the emoticons (just like the characters). Most of these platforms like to have their own emoticons because it allows them to fit their visual appearance with that of the platform. So Apple has a different approach visually and thematically to their platform than Google does.",5erhz4,t3_5erhz4,adbJ114,,Comment,8,0,8
daerox0,2016-11-25 00:59:06-05:00,adbJ114,,"Gotcha. I was hoping it was something this rational. Thanks for the info, I appreciate it.",5erhz4,t1_daergk3,PastyPilgrim,,Reply,2,0,2
daf4x48,2016-11-25 10:44:16-05:00,veltrop,,"Thought that the question was ""how are emotions encoded"" in which case this might not have been the best sub :)",5erhz4,t3_5erhz4,adbJ114,,Comment,2,0,2
5eqej4,2016-11-24 17:51:07-05:00,mucle6,Does using an adjacency list vs adjacency matrix change time complexity?,"Right now I'm learning about topological sort and Wikipedia [claims](https://en.wikipedia.org/wiki/Topological_sorting#Depth-first_search) that ""Since each edge and node is visited once, the algorithm runs in linear time"", but if one were to use an adjacency matrix then wouldn't the time complexity have to be O(n^(2)) to check all of the numbers in the NxN matrix.",,,,,Submission,0,0,0
daepecn,2016-11-24 23:39:54-05:00,Quintic,,Yes. The choice of data structure can change the run time of your algorithm. In this case you would get O(n^2) behavior if you used an adjacency matrix.,5eqej4,t3_5eqej4,mucle6,,Comment,2,0,2
daf5c8t,2016-11-25 10:55:43-05:00,_--__,,But an nxn matrix has size n^2 so the behaviour is still linear.,5eqej4,t1_daepecn,Quintic,,Reply,2,0,2
dafglu3,2016-11-25 15:36:19-05:00,Quintic,,"True. I added a reply to the OPs response to my comment to clarify that it depends on the parameters he uses for big O.

Its interesting to note I guess that for dense graphs, i.e. m = O(n^(2)), that both data structures lead to O(n^(2)) behavior anyway.",5eqej4,t1_daf5c8t,_--__,,Reply,2,0,2
daf2ze3,2016-11-25 09:46:03-05:00,mucle6,,As opposed to O(n) if I were to use an adjacency list?,5eqej4,t1_daepecn,Quintic,,Reply,1,0,1
dafgjh7,2016-11-25 15:34:41-05:00,Quintic,,"An adjacency list would be O(n + m) where n is the number of vertices and m is the number of edges.

An adjacency matrix would be O(n^(2)) where n is the number of vertices. So it is not linear if the parameter that you care about is number of vertices. However if you had a dense graph where m = O(n^(2)), you could say it is linear for the parameter n + m.

If the parameter you care about is simple the number of bits passed to the algorithm, then b = O(n + m) for an adjacency list and b = O(n^(2)) for an adjacency matrix. So in both cases the algorithm is linear with respect to the parameter b.

I would say in general algorithms you care mostly about the parameter b. Usually in graph algorithms you care about n + m.",5eqej4,t1_daf2ze3,mucle6,,Reply,2,0,2
daevec3,2016-11-25 03:38:12-05:00,_--__,,"Keep in mind that ""linear"" means linear in the number of vertices *and the number of edges*.  Since there are graphs where |E| = Ω(|V|^(2)), ""linear in the graph size"" can still be ""quadratic in the number of vertices"" (note the converse is not necessarily true).",5eqej4,t3_5eqej4,mucle6,,Comment,2,0,2
daf3cj4,2016-11-25 09:57:50-05:00,mucle6,,"My understanding is that best case for an adjacency matrix is linear where |E| = Ω(|V|^(2)), but worst case would be O(n^(2)) where |E|=Ω(|V|).

Wouldn't an adjacency list be linear regardless of the number of edges?",5eqej4,t1_daevec3,_--__,,Reply,1,0,1
daf56up,2016-11-25 10:51:36-05:00,_--__,,"It is linear in both cases, but you need to be careful by what exactly you mean by ""linear"".

Topological sort is linear in the number of vertices (usually n) and the number of edges (usually m).  The number of edges, however, *can* be quadratic in n, meaning in those cases topological sort is quadratic in n (but still linear in the graph size).  When we represent a graph with an adjacency list, the **input size** is n+m. 

Now, when you input the graph as a nxn adjacency matrix it is true that the algorithm will take Ω(n^(2)) time just to check all the entries in the matrix, but note that your **input size** is now n^2 - so the algorithm will *still be linear* (in the size of the input).
",5eqej4,t1_daf3cj4,mucle6,,Reply,2,0,2
5epcl7,2016-11-24 14:09:03-05:00,beatbrot,Is C# really that much better than Java?,"Hi reddit,

I am a CS student and one of my fellow students always keeps saying how much better C# is than Java and what a bad language Java is overall. 

Is he right? Are there any objective arguments/proofs that show that C# is superior to Java? What is your opinion?",,,,,Submission,30,0,30
daeesj1,2016-11-24 18:28:53-05:00,BonzoESC,,"Programming language slap fights aren't worth participating in. They're tools, and you don't see hammer people getting in fights with screwdriver or wrench people. 

If something you need to do is particularly well supported in C#, like making a Windows app or something use it. If you need to work with Cassandra or Zookeeper or something, Java is probably a better choice.

Java and C# are basically the same anyways, going from those to something farther out like Rust, Elm, or Erlang is going to be more fun and educational in the long term.",5epcl7,t3_5epcl7,beatbrot,,Comment,35,0,35
daet2ft,2016-11-25 01:52:19-05:00,glitchn,,"Obviously screwdrivers are superior to hammers. It can do the job of both, but let me see you try to screw in a flathead with a hammer!",5epcl7,t1_daeesj1,BonzoESC,,Reply,9,0,9
daezsoh,2016-11-25 07:39:40-05:00,Abysinian,,That's what the claw end of a hammer is for!,5epcl7,t1_daet2ft,glitchn,,Reply,5,0,5
daf6xxo,2016-11-25 11:37:36-05:00,BonzoESC,,"On the other hand, screwdrivers are of no particular importance in Norse or Motörhead mythology.",5epcl7,t1_daet2ft,glitchn,,Reply,2,0,2
dae6bic,2016-11-24 14:49:24-05:00,toast23,,"Environments aside, I prefer C#. I find that Java is too verbose and requires you to do a lot of unnecessary things. Obviously Java has the upside of running cross platform (readily).",5epcl7,t3_5epcl7,beatbrot,,Comment,15,0,15
daem1g8,2016-11-24 21:59:16-05:00,znick5,,I really wish Java had the implicit var declarations. With Java 8 the gap has gotten closer though. C# used to blow Java out with LINQ and lambdas.,5epcl7,t1_dae6bic,toast23,,Reply,3,0,3
dafb8rl,2016-11-25 13:23:39-05:00,exneo002,,"I've heard this is coming with Java 9. A quick Google search doesn't show anything definite but there are tons of proposals.

The only thing I don't like about c# is windows haha.",5epcl7,t1_daem1g8,znick5,,Reply,2,0,2
daff571,2016-11-25 15:00:08-05:00,AgentZeusChops,,"To Objective-C with that, then! lol",5epcl7,t1_dafb8rl,exneo002,,Reply,1,0,1
dag1nd5,2016-11-26 01:46:38-05:00,exneo002,,Jet brains could make it bearable,5epcl7,t1_daff571,AgentZeusChops,,Reply,1,0,1
dagg912,2016-11-26 12:15:40-05:00,AgentZeusChops,,"I'm personally not a fan of IntelliJ. Who knew? Lol I program in Java, Objective-C, and Swift most of the time. IntelliJ also forces me to deal with different contexts and a slightly more-noticeable lag than with Eclipse, whenever I write Java code. As for Objective-C and Swift, I am currently using Xcode (macOS-only, unfortunately) for those.",5epcl7,t1_dag1nd5,exneo002,,Reply,1,0,1
dae66zo,2016-11-24 14:46:22-05:00,PmMeYourSexyShoulder,,Better is relative. Always the right tool for the right job. ,5epcl7,t3_5epcl7,beatbrot,,Comment,26,0,26
daes7fo,2016-11-25 01:17:50-05:00,theobromus,,"Agreed. Hijacking to mention the relative advantages:

* C# came later and had the benefit of hindsight. It implemented many things in a cleaner way in my opinion (e.g. native types are boxed automatically so there's no int!=Integer). It's generics are arguably better as is Linq. The async/await features were integrated really well. Integrating with native (i.e. c++) code is *so* much easier with P/Invoke in C# compared to JNI in Java. If you're writing an app for Windows (or Azure, SqlServer, etc.), C# is usually the best way to go. For writing web apps Asp.net MVC is a solid choice, although there are so many competitors (python, ruby, node.js, and of course java jsp).

* Java has a larger development community. There are probably more companies with business apps written in Java. It was the de facto language taught in many schools and so there's tons of people who can write and maintain java apps (businesses like this). Java 8 has added a number of features to give parity with c# like lambda expressions. Java is also the language of Android app development, so if you're writing for Android it's basically your option. Because of it's large cross-platform development community, there are a lot of libraries and open source projects that use Java.

Overall, both languages are excellent. If you are writing code that doesn't have to be in a native language (i.e. non-garbage collected), I'd say the choice comes down to your target platforms, the available open source libraries related to your problem, and which you prefer.",5epcl7,t1_dae66zo,PmMeYourSexyShoulder,,Reply,7,0,7
daetjng,2016-11-25 02:12:29-05:00,Xander260,,"To add to the confusion, compiling a C# app to Android/iOS is pretty trivial now, thanks to Visual Studio taking on Xamarin, which in itself used C# as the language of choice. But again, native is always better (performance wise) and Java is the defacto standard at the moment for Android. ",5epcl7,t1_daes7fo,theobromus,,Reply,2,0,2
daefz7o,2016-11-24 19:03:05-05:00,falafel_eater,,"Generally speaking, the majority of ""Language X is better than Language Y"" arguments you'll hear are pretty much rubbish. Especially coming from someone still in their undergrad studies who might not be basing their assertion based on a very fine-grained and complex understanding of the design choices made when developing the languages/compilers and the history and reasons for them.

Does Java allow you to perform the tasks you need performed? Can you develop painlessly? Do you have debuggers and other tools you need? Documentation is available? The application has sufficiently good performance for what it's intended to do?  
If Java does all these things with respect to the task you assign to it, then Java is a fantastic tool for that task. If it only does some (or almost none) of the above, then it's probably poorly-suited.  

The same goes for C#, Python, Ruby on Rails, or any other language.  

> Are there any objective arguments/proofs that show that C# is superior to Java?

No. There are probably dozens (if not hundreds) of different metrics you could compare the two by, and get different results for each one.  
If you compare a hammer to a screwdriver, you get very different results if you compare their suitability for turning screws, driving in nails, making music, long-distance throwing or usability as a makeshift weapon.",5epcl7,t3_5epcl7,beatbrot,,Comment,8,0,8
dae7wwj,2016-11-24 15:29:01-05:00,tbrownaw,,"IIRC the CLR is better than the JVM, mostly due to being newer and having less historical baggage. But, this largely doesn't matter since most work is a couple levels of abstraction up from that.

Java as a language has had some pretty huge changes recently. Old versions kinda suck, new ones supposedly don't (but I don't get to work with them, so...).

C# has Visual Studio, which is frikken awesome. Java has Eclipse (which sucks), and IntelliJ (which doesn't).

Java has the problem that the language, standard library, and virtual machine are all the same thing. C# doesn't (the library/framework is .NET, the virtual machine is the CLR).

I hear a lot more about alternative languages running on the JVM than I do about them running on the CLR. This doesn't make sense to me, which makes me think I'm missing something.

C# works well with Powershell, since they're both CLR languages.

Java is a product of Oracle, ever since they bought out Sun. C# comes from Microsoft. As much as Microsoft has a known history of being evil, my understanding is that Oracle is generally considered worse (and Microsoft has realized lately that they need to do damage control on their reputation, and seems to have cleaned up their act a bit).",5epcl7,t3_5epcl7,beatbrot,,Comment,6,0,6
daegqx8,2016-11-24 19:24:33-05:00,adimit,,"> C# works well with Powershell, since they're both CLR languages.

That's really only relevant if you're running on Windows. Personally, I'd use both C# and Java mostly for web stuff, or enterprise-level stuff — so, on servers. Sure, you can have Windows servers. Though I like to keep my feet lead-free.

> Java is a product of Oracle, ever since they bought out Sun. C# comes from Microsoft.

They're both heartless megacorps. Yeah, it really sucks that I have to look at Oracles logo when browsing Java-anything, but, truth be told, the Java version that came *after* the buyout was very good (8). And the next two ones are shaping up to be not-too-shabby either.

To me, the C#/Java divide is this:

C# runs well only in Microsoft-environments. Yes, there's Mono, and maybe one day that'll change, but I have little hope that developing C# and running C# is at any point going to be equally painless on Linux as it is on Windows. Windows is always going to be the first-class citizen. To me, this kills C#.

C# *as a language* has two main things going for it: LINQ, and un-fucked generics. Or rather, C# has LINQ and Sun seriously fucked up Java generics through type erasure. Also, generics are nowhere near powerful enough to be used properly with the new Stream API, which makes me a sad panda. I'd love something like LINQ in Java-land, but it won't be happening any time soon.",5epcl7,t1_dae7wwj,tbrownaw,,Reply,2,0,2
dael37w,2016-11-24 21:31:37-05:00,ISvengali,,"And now there is .Net Core.  I havent done an analysis of it yet, but its an officially supported linux version of the CLR.  I really want to port our server over to it and try it out.  

*Mono* is unfortunately not good.  Certainly nothing I would ever do production work with.  If youre in a bind and people will kill you if you dont get their CLR program running on linux then yeah, use it otherwise, nope.  ",5epcl7,t1_daegqx8,adimit,,Reply,1,0,1
daesimj,2016-11-25 01:29:52-05:00,berlinbrown,,"But what cross platform systems does the CLR run on, effectively. Like JVM effective.  For example, million dollar IBM JVMs.  Or large scale clustered JVMs.",5epcl7,t1_dae7wwj,tbrownaw,,Reply,1,0,1
daeiodi,2016-11-24 20:19:57-05:00,shadowdude777,,C# has better syntax. Java has a better ecosystem and better tooling. Both are fantastic languages. ,5epcl7,t3_5epcl7,beatbrot,,Comment,7,0,7
dajh862,2016-11-28 15:33:18-05:00,andybmcc,,"As a general rule of thumb, a lot of your fellow students are full of shit and just want to get into a dick measuring contest over some completely asinine argument.  Somewhere down the line, you'll both realize that neither of you knew your asshole from a hole in the ground, have a good chuckle about how silly you were back in the day, and move on.  Languages are tools.  You should use the most appropriate tool for the context of the problem.",5epcl7,t3_5epcl7,beatbrot,,Comment,2,0,2
daefjo9,2016-11-24 18:50:45-05:00,wischichr,,"There is only a single pro for java and that is it's history of being platform independent (because C# and .NET started Windows only) and there are already many jvms for different machines.

So now the cons of java:
* no unsigned byte (and byte should IMHO always be unsigned)
* no unsigned types in general
* no decimal datatype
* override annotation is optional
* No real enums in java
* No delegates or function pointer, not sure if java has lambda expressions
* Insane implemenation of modulus (in fact java implemented a remainder function with the % (which is modulus in most languages) and implemented modulus later in Math.floorMod (WTF)
* No Generics with primitve datatypes - we have to use ugly wrapper classes. 
* No real setter and getter for properties - just (ugly) conventions
* No structs or custom value types
* No operator overloading.
* And the .NET Framework does not install the freakin Ask Toolbar.

Java started as a great idea, but Microsoft open sourced a lot .NET and C# stuff lately and passed java years ago. But Oracle is very busy suing other companies 😁",5epcl7,t3_5epcl7,beatbrot,,Comment,1,0,1
dae9exz,2016-11-24 16:06:21-05:00,Arsestolemyname,,"Java requires a paragraph to perform even the most basic tasks. Very verbose language. For a student, they're just looking at cobbling together a functional prototype as quickly as possible. As a student myself, I find python offers me the most time to drink myself into the floor, so I immediately think of it as the ""best"" language to know right now. While there is no objective good or bad, C# will let them do their tasks much more quickly/easily than  java. ",5epcl7,t3_5epcl7,beatbrot,,Comment,0,0,0
dae52ue,2016-11-24 14:19:06-05:00,BlueFootedBoobyBob,,"Not really. Of course on a Windows Maschine it is quite a bit faster and you have some nicer Features like the way that exeptions are handled and of course Linq. 

But cross platform development is still a nightmare, which might be better with Java. And have a look at the serial com under C# its a fkn joke. Unsecure code parts.

In my opinion there is very little difference. Use what you know, if you need to, you can easily relearn. Learn the concepts: garbagecollector, Lambda functions, Linq.

Be careful with heredity. If you are used to C++ everything goes, the limits of Java are hard and C# only allows one parent.

IDE: first time i saw Visual Studio i was shocked of the lack of features and instablility. Eclipse is a gigantic plus for Java.",5epcl7,t3_5epcl7,beatbrot,,Comment,-3,0,-3
dae5sf5,2016-11-24 14:36:21-05:00,rikbrown,,"> Eclipse is a gigantic plus for Java

Then IntelliJ is a gigantic multiplication sign.",5epcl7,t1_dae52ue,BlueFootedBoobyBob,,Reply,13,0,13
dae62k5,2016-11-24 14:43:20-05:00,urielsalis,,Intellij idea is awesome and a real step up from eclipse,5epcl7,t1_dae5sf5,rikbrown,,Reply,5,0,5
daewbzg,2016-11-25 04:26:51-05:00,PM-ME-YO-TITTAYS,,"It took me ages to switch to Intellij because I was happy with Eclipse. But now, trying to do anything on my coworker's computer is impossible because there are so many things I'm used to in Intellij that Eclipse just doesn't do.",5epcl7,t1_dae62k5,urielsalis,,Reply,2,0,2
dae5tz7,2016-11-24 14:37:24-05:00,cdlight62,,Really? I learned development in Java on Eclipse but once I got used to VS there wasn't even a contest.,5epcl7,t1_dae52ue,BlueFootedBoobyBob,,Reply,3,0,3
daeexov,2016-11-24 18:33:06-05:00,malsonjo,,Add Resharper if you want to go nuclear.,5epcl7,t1_dae5tz7,cdlight62,,Reply,2,0,2
dae4wcr,2016-11-24 14:14:47-05:00,None,,[deleted],5epcl7,t3_5epcl7,beatbrot,,Comment,-6,0,-6
dae5m9v,2016-11-24 14:32:13-05:00,adamkemp,,"Microsoft officially supports C# on Linux and Mac. They even just announced ""Visual Studio for Mac"" (basically rebranded Xamarin Studio). Since they open sourced their compiler/runtime and bought Xamarin they have been openly supporting cross-platform C# development. ",5epcl7,t1_dae4wcr,None,,Reply,6,0,6
dae615c,2016-11-24 14:42:21-05:00,None,,[deleted],5epcl7,t1_dae5m9v,adamkemp,,Reply,-5,0,-5
dae65ht,2016-11-24 14:45:19-05:00,adamkemp,,"With .Net Standard the same binary can be used on any platform. What is missing?

I'm not going to argue that it's perfect, but it sounds like your information is out of date. You might want to look at the current state of things. ",5epcl7,t1_dae615c,None,,Reply,5,0,5
daefmo0,2016-11-24 18:53:08-05:00,wischichr,,"Am I getting this right? WPF Apps run on Linux and Mac?
Edit: Googled it and nearly everbody suggest xamarin because WPF does heavily depend on windows.",5epcl7,t1_dae65ht,adamkemp,,Reply,1,0,1
daei5o3,2016-11-24 20:04:42-05:00,adamkemp,,"WPF is a UI framework. C# is a language. .Net is basically the common language runtime and non-UI framework (things like file I/O and networking).

The question was about C#, not WPF. Today you can write and run an ASP.Net web application in C# on a Mac. You can't run a WPF app, but you can write and run C# code. Visual Studio for Mac is itself written in C# (using GTK# as its UI framework). ",5epcl7,t1_daefmo0,wischichr,,Reply,1,0,1
5eljyh,2016-11-23 22:14:37-05:00,finalight,Need help deciding the ERD layout,"I have problem for the database design layout
right now I have 4 tables:

- company 
- branch 
- category 
- product


assumptions made: a company can have multiple branches a company can own a range of products. However some branches can only own certain specific products each product belongs to a category

I actually wanted to do this linkage:

company 1---* product 

company 1---* branch

product *---1 category

however, i cannot handle for the case that what if certain products belong to certain branch(s)?",,,,,Submission,2,0,2
dadeeta,2016-11-23 23:00:20-05:00,philimup,,"Hint: 
Each branch can have many products and
Each product can have many branches
...
Further hint below
...
You'll need another table
...
Further hint below
...
You'll need a junction table to relate the two tables",5eljyh,t3_5eljyh,finalight,,Comment,1,0,1
dadi2fh,2016-11-24 00:47:26-05:00,finalight,,"so given your suggestion

if I were to have a company to have all branches own the products: then I would need to have the junction table to be related to all the branches that are related to that specific company",5eljyh,t1_dadeeta,philimup,,Reply,1,0,1
dadkhsd,2016-11-24 02:15:02-05:00,finalight,,"with your suggestion
everytime i add a new branch to that company, I have to create records in the branchproduct table for every product the company owns (if the branch wants to sell all the product, which might be inefficient",5eljyh,t1_dadeeta,philimup,,Reply,1,0,1
dadv2ls,2016-11-24 10:11:28-05:00,PM-ME-YO-TITTAYS,,Yep. Not sure there's a way around that.,5eljyh,t1_dadkhsd,finalight,,Reply,1,0,1
dadvggv,2016-11-24 10:22:37-05:00,philimup,,"When you say inefficient, what do you mean? You realize that adding the branchproduct records could be done with a single insert query right?",5eljyh,t1_dadkhsd,finalight,,Reply,1,0,1
daek843,2016-11-24 21:05:56-05:00,finalight,,"you are not wrong, however, you still need to query out and retrieve all the products for that particular company and then insert into the branchproduct table.",5eljyh,t1_dadvggv,philimup,,Reply,1,0,1
dafthj9,2016-11-25 21:25:49-05:00,AgentZeusChops,,"This sounds like a problem for HashMap using the keys to identify the branch, product, and categories. (I am a Java programmer, probably same premise, just different wording.) Each query specifies the key data, which specifies which HashMap to identify with, so this would literally look like: HashMap<String key, HashMap<String key, HashMap <String key, byte[] data>>> depending on the 'level' of HashMaps. This actually would run around O(n) time due to hash map's ability to categorize data. I'll call it Hash-eption :)",5eljyh,t1_dadeeta,philimup,,Reply,1,0,1
5egyvk,2016-11-23 07:11:14-05:00,whiteguyinCS,Help me get out of a toxic relationship with C++,"So I'm in third year university, and most of my programming courses so far have used C or C++. This term I've finally realized how horrible a language C++ is, and I'm looking for a new language. I've used other platform-specific languages before (e.g.  swift when doing iOS development), but I want to become very good with a ""general"" OOP language like Java or Python. Any suggestions?",,,,,Submission,0,0,0
dacd73z,2016-11-23 08:23:00-05:00,quzR,,"C# .NET

But also,  c++ isn't awful ",5egyvk,t3_5egyvk,whiteguyinCS,,Comment,13,0,13
dacnymi,2016-11-23 12:40:44-05:00,panderingPenguin,,"Like all languages, C++ has its faults. But overall, it really is a great language that has enabled people to build some pretty phenomenal stuff.

That being said, it this point in your development it is very common to fixate on languages because they're fun and sexy and you probably enjoy learning new things. Trust me, I was guilty of the same. But core CS concepts are far, far, far more important than the language you choose to express them in.  My advice would be to not worry about languages so much right now, you'll learn the ones you need as you go. But if you really want to learn a new one anyways, I'd try something completely different than you're used to. Maybe give a functional language like Haskell or F# a shot.",5egyvk,t3_5egyvk,whiteguyinCS,,Comment,7,0,7
dadnn8u,2016-11-24 04:38:54-05:00,antiprosynthesis,,"C++ is the language that gives you complete freedom without performance compromise. It is not always 100% beginner friendly, but it is by far the most powerful language to know.",5egyvk,t3_5egyvk,whiteguyinCS,,Comment,3,0,3
daccdn2,2016-11-23 07:51:44-05:00,abyss344,,"First of all, C++ is not a bad language, it has its use cases.
I personally find Java to be closer to C++ than python is, so if you want to try something new (or newer) then go with python. If you want to try something that is way different then try doing something in Haskell or Prolog even, you will think in different patterns and might find it a rewarding experience.

If want to start with python, you can take a look at the ""Django"" web framework, find some tutorials on it, from Youtube maybe and start building an app with it (a todo app if you have no ideas).

But at the end of the day, don't forget that programming languages are only tools and it's a lot more important to learn new concepts and understand them well regardless of the programming language or the flavor of the day tech (unless you're learning programming languages theory, damn self reference!).",5egyvk,t3_5egyvk,whiteguyinCS,,Comment,3,0,3
dadmh8w,2016-11-24 03:40:57-05:00,zorkmids,,"Python is a good choice because it's very different from C++, so you'll learn a lot.  For example, it's an easy way to learn functional idioms like map, lambda, and list comprehensions.  

If you're really keen on OOP, Java might be a better choice.  But the conceptual distance between Java and C++ is not that great.
",5egyvk,t3_5egyvk,whiteguyinCS,,Comment,2,0,2
dackhjs,2016-11-23 11:28:57-05:00,ObeselyMorbid,,Learn java,5egyvk,t3_5egyvk,whiteguyinCS,,Comment,1,0,1
dacusiv,2016-11-23 15:00:54-05:00,brtt3000,,Or maybe python.,5egyvk,t1_dackhjs,ObeselyMorbid,,Reply,3,0,3
dajhbny,2016-11-28 15:35:11-05:00,andybmcc,,"C++  isn't a horrible language, there's just horrible C++ code.",5egyvk,t3_5egyvk,whiteguyinCS,,Comment,1,0,1
5eftwy,2016-11-23 01:15:51-05:00,plantice,Need advice from people in the industry,"Hello All,

So a little background before I ask my question (bear with me). I have a CS degree and have been working in the industry for 5 years now. I recently moved to a bigger city in the hopes of finding a new/better job but I am having troubles figuring out what to call myself. 

While I have a degree in CS, I have been basically a maintenance programmer/enhancement programmer (70% clients, 10% paperwork, 20% coding). After doing some interviews here, I noticed that I wasn't getting anything and I believe it is because: A) I don't code much anymore, B) I really enjoy the relationship building in my current role (weird I know). I am looking for a job in which I do something similar but without all the crazy interviews standard devs go through (they are a nightmare sometimes).

So the question is: What job title should I be using and what types of positions should I be looking for?

In college they didn't teach you about all the options you have as a CS major, I went through most of my career expecting to be put into new development sometime. I find I don't care too much about all the details of programming, I know how to read code and how to fix other peoples mistakes. I enjoy problem solving and customer relations much more than I do design methodologies and the like.

Thanks for any advice, it has been eating at me for a while now.",,,,,Submission,12,0,12
dac4i4p,2016-11-23 01:23:08-05:00,ashtar,,Sounds like sales engineer or technical pre-sales would be close to what you might want.,5eftwy,t3_5eftwy,plantice,,Comment,8,0,8
dac4l1h,2016-11-23 01:26:11-05:00,plantice,,"interesting, I never thought of myself as a salesman but I have worked with clients to figure out what they need and ways to make it more cost efficient. So maybe! I just wonder if I am still going to be able to code a bit. Thanks!
",5eftwy,t1_dac4i4p,ashtar,,Reply,1,0,1
dacfdqb,2016-11-23 09:30:15-05:00,ashtar,,I know several of these guys- they definitely still need to remain technical enough to POC product customizations and answer tougher tech questions. You'll need to be on point since you will be selling mainly to developers as time goes on.,5eftwy,t1_dac4l1h,plantice,,Reply,3,0,3
dace737,2016-11-23 08:55:48-05:00,jkudish,,Technical evangelist or developer relations  might be a good fit too ,5eftwy,t3_5eftwy,plantice,,Comment,3,0,3
daco9nc,2016-11-23 12:46:59-05:00,plantice,,"Just to give some diversity, anyone know any desk jobs that would fit into this?",5eftwy,t3_5eftwy,plantice,,Comment,1,0,1
dad0ipk,2016-11-23 17:03:12-05:00,dxk3355,,I would say 'technical analyst' is what you're describing.  At least that's the title I see from large companies with that break down of work. ,5eftwy,t3_5eftwy,plantice,,Comment,1,0,1
dadkmie,2016-11-24 02:20:18-05:00,plantice,,"> technical analyst

That is one of the ones I think I looked into, basically what I do is that but that title is so wide that it can go many ways.
",5eftwy,t1_dad0ipk,dxk3355,,Reply,1,0,1
dad2hg8,2016-11-23 17:49:39-05:00,xiongchiamiov,,See also r/cscareerquestions.,5eftwy,t3_5eftwy,plantice,,Comment,1,0,1
dad8sb8,2016-11-23 20:31:31-05:00,plantice,,"So many reddit fourms, I will post this on there too. Thanks",5eftwy,t1_dad2hg8,xiongchiamiov,,Reply,1,0,1
5eczpb,2016-11-22 15:29:31-05:00,LostInComputerSC,Need help,"I am so lost with trying to do my hw. It seems so simple but I have gone mad trying to figure it out. I need some guidance. 
I am doing work from David Reed ""Intro to computer science 3rd Ed"" and I am struggling with exercise 11.15 and 13.5.
11.15 I have written code but it doesn't seem to work. Can anyone help me? 

<!DOCTYPE html>
<classify.html>
<head>
<title>Page Title</title>
<script type=""text/javascript"">
function ShowMessage()
// Assumes: valueBox contains a number
// Results: displays whether number is positive, negative or zero
{
var values;
values=parseFloat(document.getElementById('valueBox').value;
document.getElementById('outputDiv').innerHTML='';
if (values>0) {
document.getElementById('outputDiv').innerHTML=
document.getElementById('outputDiv').innerHTML + 'Positive';
}
else if (values==0){
document.getElementById('outputDiv').innerHTML=
document.getElementById('outputDiv').innerHTML + 'Zero';
}
else if (values<0){
document.getElementById('outputDiv').innerHTML=
document.getElementById('outputDiv').innerHTML + 'Negative';
}
</script>
</head>
<body>

<p>
Your Number: <input type=""text"" id=""valueBox"" size=6 value="""">
</p>
<input type=""button"" value=""Initiate for Analysis"" onclick=""ShowMessage()"";>
<div id=""outputDiv""></div>
</body>
</html>
",,,,,Submission,0,0,0
dabjjk8,2016-11-22 16:29:55-05:00,ukkoylijumala,,"You should open the console in your browser (e.g. in Firefox it is SHIFT+CTRL+K), it will give you some information on what's wrong. In your case there is a parenthesis missing after `.value` where you parse the float, and a closing brace is missing at the end of your function.

You could also use proper indention to see these mistakes easily:

        <script type=""text/javascript"">
            function ShowMessage()
            // Assumes: valueBox contains a number
            // Results: displays whether number is positive, negative or zero
            {
                var values:
                values=parseFloat(document.getElementById('valueBox').value): // <-- this ) was missing
                document.getElementById('outputDiv').innerHTML='':
                if (values>0) {
                    document.getElementById('outputDiv').innerHTML= document.getElementById('outputDiv').innerHTML + 'Positive':
                }
                else if (values==0){
                    document.getElementById('outputDiv').innerHTML= document.getElementById('outputDiv').innerHTML + 'Zero': 
                }
                else if (values<0){
                    document.getElementById('outputDiv').innerHTML= document.getElementById('outputDiv').innerHTML + 'Negative':
                }
            } // <-- this } was missing
        </script>",5eczpb,t3_5eczpb,LostInComputerSC,,Comment,3,0,3
dac4ov0,2016-11-23 01:30:13-05:00,plantice,,"A tip would be look for tools on your IDE, and see if they tools for making sure to correct syntax. In your case, create the html file and run it in any browser. Using f12, the console section will show you the error and if you double click it: it will show you the line in code that has the problem. I believe notepad++ does this, a nice free tool for editing code if you dont have visual studio or any other more robust IDE",5eczpb,t3_5eczpb,LostInComputerSC,,Comment,1,0,1
5eb9kj,2016-11-22 10:28:36-05:00,stupidCSstudent,OS related question about scheduling algorithms for CPUs and disk queues,"Other than fcfs, and sjf, and scan, are there other algs for disk scheduling? Any without process starvation?  in general, how do these scheduling algorithms relate to priority and round robin scheduling?  I know the ready queue will most likely use round robin priority, but couldn't a disk queue?

",,,,,Submission,1,0,1
dab4kwp,2016-11-22 11:34:21-05:00,IgnorantPlatypus,,"Why would disk scheduling relate to process starvation?

There are as many disk scheduling algorithms as you can imagine, but a lot of them aren't good so they're not interesting.

Round-robin makes little sense for a disk scheduler because the things its scheduling happen once. A thread needs runtime until it exits: a disk access needs to happen once.",5eb9kj,t3_5eb9kj,stupidCSstudent,,Comment,1,0,1
dab4td5,2016-11-22 11:39:11-05:00,stupidCSstudent,,"the way my professor spoke about it, was that a process that needs to read/write from/to the disk could get starved if a bunch of processes writes/reads and a process never gets a chance to do its operation so it can go back to the CPU with new data.  or if there are a certain amount of requests that were a set priority, lower priorities might never get to do what they need to do. 

>There are as many disk scheduling algorithms as you can imagine, but a lot of them aren't good so they're not interesting.

So then how are disks scheduled? this is for a course which is why I asked. ",5eb9kj,t1_dab4kwp,IgnorantPlatypus,,Reply,1,0,1
dabeja3,2016-11-22 14:52:26-05:00,IgnorantPlatypus,,"Any scheduling algorithm with fairness will prevent a process from being starved, so first-come first-served works for that.

RedHat has a [small write-up on the Linux disk scheduling algorithms](
https://access.redhat.com/documentation/en-US/Red_Hat_Enterprise_Linux/5/html/Tuning_and_Optimizing_Red_Hat_Enterprise_Linux_for_Oracle_9i_and_10g_Databases/sect-Oracle_9i_and_10g_Tuning_Guide-Kernel_Boot_Parameters-The_IO_Scheduler.html).  The ""noop"" scheduler I believe would be equivalent to first-come first-served.",5eb9kj,t1_dab4td5,stupidCSstudent,,Reply,1,0,1
5eay6z,2016-11-22 09:24:17-05:00,Bernie29UK,"Can you understand this message, to be sent to an alien civilisation?","The question was raised ""assuming an alien civilisation could transmit binary strings, could we assign meaning to any given message received from them"". The following message was written by someone here on earth who says the aliens would understand it. It needs to be unzipped.

Can you assign meaning to it? If so, could you draft a response, using only Os and 1s? It would also be interesting if you try but can't assign a meaning to the message, if that is the case just say ""no"".

https://filenurse.com/download/5127810450986951d94222d986f38c5a.html
",,,,,Submission,5,0,5
dab0wh4,2016-11-22 10:14:06-05:00,KronktheKronk,,"No.  Even if I understood the necessary translations to turn that binary message into English, I could send it to a non-english speaking Chinese person and they'd have no idea what it meant.  Alternatively if I knew the necessary translations and the message came out in Chinese, I'd have no idea what it meant. Same thing with us and aliens.  

Hell, all the files on your computer are 0's and 1's and you couldn't get the wrong program to open them specifically because they have no idea how to translate random strings of 0's and 1's without context.
",5eay6z,t3_5eay6z,Bernie29UK,,Comment,2,0,2
dab635s,2016-11-22 12:04:56-05:00,xtyle,," Naa it's not that difficult.

You can define for example 8 bits in a prelude. For example:

00000000.00000001.00000010.00000011
And so on until you have 11111111.
I included the dots for you to see, they obviously wouldn't be there in a signal (because,say you would define signal =1, no signal =0 and even that is not necessary and can be done better.)
I assume aliens who have the technology to intercept a signal are intellectually capable of recognizing patterns. So now you have a basic number system with 256 possibilities. You have ""counted"" from zero to 256 so you can display numbers. Now you can display some kind of specific and unique mathematical sequence of patterns like a part of the Fibonacci sequence and from there on you can probably build more complex stuff.
",5eay6z,t1_dab0wh4,KronktheKronk,,Reply,4,0,4
dab8mx4,2016-11-22 12:56:26-05:00,Bernie29UK,,"The original post in /r/askphilosophy acknowledged that you could send recognisable ""mathematical"" patterns: but how do you get beyond that?

Apologies for not including that in my original question.",5eay6z,t1_dab635s,xtyle,,Reply,3,0,3
dabaovq,2016-11-22 13:37:23-05:00,xtyle,,"I can imagine a simple(and probably bad) system for basic mathematical operations.

You ""reserve"" some bytes. Say 11111100 is ""add"". In the counting part you could make it clear by only going up to 11111100. Now you have 4 reserved states that are not associated with a number. So after the counting part you show an example:
00000100.11111100.00000001.11111111.00000101
From this you can probably conclude that 11111100 is a combination of the two numbers and that 11111111 is the conclusion of this operation. So now you can add two 8 bit numbers. You could introduce higher bit numbers after a couple of examples. This example is bad by the way because I reserved 4 bytes but I have forgotten the =, but you get the idea",5eay6z,t1_dab8mx4,Bernie29UK,,Reply,1,0,1
dabb12o,2016-11-22 13:44:00-05:00,Bernie29UK,,"You may have misunderstood me xtyle: I know it's possible to send ""mathematical"" patterns, but how do you get beyond mathematics?",5eay6z,t1_dabaovq,xtyle,,Reply,1,0,1
dabblqg,2016-11-22 13:55:14-05:00,KronktheKronk,,"Xtyle suggests going the ""Contact"" (the movie) route.  Once you send the necessary baseline establishment of mathematics, you can then send geometric shapes, possibly instructions to fabricate some machine that would allow you to establish communications.

Just like in the movie Contact",5eay6z,t1_dabb12o,Bernie29UK,,Reply,1,0,1
dabcfme,2016-11-22 14:11:30-05:00,Bernie29UK,,I see. How would you send a shape? ,5eay6z,t1_dabblqg,KronktheKronk,,Reply,1,0,1
dabcizw,2016-11-22 14:13:21-05:00,KronktheKronk,,"If we can assume mathematics are universal, then binary mathematics should be discoverable.

Once you've used Xtyle's method above to demonstrate angles, line segments, and curves in 3 dimensional space you should have all you need to draw an object.

It's just a matter of finding an alien smart/diligent enough to figure it out.",5eay6z,t1_dabcfme,Bernie29UK,,Reply,2,0,2
dabfcyo,2016-11-22 15:08:22-05:00,xtyle,,"Exactly, thanks. Look below for a crude demonstration of drawing  a shape",5eay6z,t1_dabcizw,KronktheKronk,,Reply,1,0,1
dac2fxw,2016-11-23 00:13:25-05:00,not-just-yeti,,"In Contact, the clever idea was: send a bit-map whose dimensions are prime (e.g. 37x59), and repeat it a bunch. The receiver notices a repetition that's p*q, and realizes it must be a  rectangle of that size. (or at least, one of the two  -- row-major or column-major order.)",5eay6z,t1_dabcfme,Bernie29UK,,Reply,1,0,1
dabeq25,2016-11-22 14:55:58-05:00,xtyle,,"So you could probably expand the physics realm by doing the following, I will substitute bytes by letters and numbers for convenience:

510001N
401010N
300100N
201010N
110001N
012345N
Now this will probably be a challenge to figure out, but I have constructed a basic coordinate system:

510001N

401010N

300100N

201010N

110001N

012345N

I don't know if you can see it but I have drawn a cross with the 1s. It will be a challenge to figure out that a sequence of bytes stands for a line break,but if you scale this up you could draw more complex things. Now let's say that 0 is the big bang, that shouldn't be too difficult to figure out as it is the start of our universe and the start of the coordinate system. Then you can communicate our rough position in a two dimensional frame. I hope I have shown that you can communicate complex enough things that go a little bit beyond mathematics.
",5eay6z,t1_dabb12o,Bernie29UK,,Reply,1,0,1
dablh1o,2016-11-22 17:08:28-05:00,Bernie29UK,,"A line break? You're assuming a lot about the aliens there, they may not have passed through the typewriter stage of development, they may be nebulous gas beings who can transmit and receive binary signals telepathically but who don't read and write.

And you certainly can't assume that they would interpret one particular 0 as meaning ""The Big Bang"". I mean, if we got this message from an alien tomorrow: 

510001N
401010N
300000N
201010N
110001N
012345N

there'd be no reason for us to think the 0 in the middle was anything to do with the Big Bang. And that idea doesn't really stand up anyway, can you point to where the Big Bang was from here? 

So, thanks very much for the ingenious attempt, but I'm not persuaded.

",5eay6z,t1_dabeq25,xtyle,,Reply,1,0,1
dabndea,2016-11-22 17:49:25-05:00,xtyle,,"The Assumption is that they are intellectually capable of recognizing patterns. Have you ever seen a crazy conspiracy theory unfold? People make assumptions about so many things and try so many different solutions trying to recognize patterns where there are sometimes none, im pretty sure that with time the above line of code would be solved by humans, especially because the bit sequence N is returning after a set number of bytes and because a special kind of pattern emerges when solved correctly.
And even if I back down on the big bang is 0 point, you can still draw complex patterns with this and a pattern is more than mathematical information, which was your original question. I don't think that a line break is so far out nobody could ever think of it.


The 0= big bang thing was overly simplified. Still, you could encode locational data into this by going off the masses of pulsars in the galaxy. Those build a distinct pattern that one of those aliens will eventually figure out. 

Like I said it all depends on the aliens having a similar understanding of patterns as we do and some computational power after the introductory lines of code. If you Assume that they don't have the same understanding of patterns then there wouldn't be any kind of communication even if we faced them so what's the point. There has to be a lowest common denominator of understanding for communication to happen at all.


Edit: also consider the assumptions that the aliens can make from this type of encoding:


1: you can assume that code will get incrementally more difficult. It starts with basic counting.

2: some bytes are reserved for later use as they did not appear in counting. 

3: this data is one dimensional. As complexity of this code evidently grows there might arise a need for two dimensional data

So it will be up to the aliens to figure out that a byte that has never appeared before means something that they haven't tried  before.

",5eay6z,t1_dablh1o,Bernie29UK,,Reply,1,0,1
dabbhnc,2016-11-22 13:53:00-05:00,KronktheKronk,,"You have 256 unique symbols IFF the aliens/recipients can and do recognize the pattern in your message.

Once they recognize 256 symbols you still have no way of putting them together in understandable ways.  They have no way of knowing if each symbol has some unique meaning.  They have no way of knowing how many symbols they have to put together to make something of significance.  There's no way for the alien to know the symbol phrase 1,17,98,111 means ""hello"" or that 1,2,3,4 means ""happiness.""

It would be like someone using sign language at you.  You'll have no fuckin idea what they're talking about 99% the time.  the hand motions will just seem like noise and you'll only understand 1% because you have the context of human existence.",5eay6z,t1_dab635s,xtyle,,Reply,1,0,1
dabd7mt,2016-11-22 14:26:48-05:00,xtyle,,"We are at mathematics right now and I'm sure that  it would be possible to communicate a mathematical system. Now words and concepts are so abstract , that they are very difficult if not impossible to communicate",5eay6z,t1_dabbhnc,KronktheKronk,,Reply,1,0,1
dabe0p2,2016-11-22 14:42:24-05:00,KronktheKronk,,"I guess technically ""1+0=1, 1+1=2"" is a message.

You win this round.",5eay6z,t1_dabd7mt,xtyle,,Reply,1,0,1
5eapyh,2016-11-22 08:33:34-05:00,Twelve-Pound,Could someone ELI5 how a load balancer works?,"I'm trying to do some research for a project explaining load balancing, but I'm having trouble wrapping my brain around the concept. Any help?",,,,,Submission,1,0,1
daaxsob,2016-11-22 08:51:20-05:00,thatwasntababyruth,,"A load balancer works almost exactly like the host or hostess at a restaurant. When someone asks for a web page, the 'request' will either go straight to a web server (like a small restaurant with no host/hostess, where the wait staff will come right to you), or it will go to a load balancer (like at a very popular restaurant). The load balancer knows:

- which servers are online (which ones were scheduled tonight)
- which servers are 'healthy', and havn't had some kind of problem (which ones aren't on break)
- how much load each one has (how many tables is each staff member handling already)
- which 'region' each server is in (which block of tables they're handling tonight)

Based on where you are in the world, a load balancer will use those facts to assign you a server, then they'll send you that way for the rest of you meal/request. The next time you want a meal/web page you will need to go through the load balancer again, and unless the LB knows that you have a preference/favorite waiter, you may be assigned a different one that is faster. 

This is all very fast, because sending people to another server is not a very difficult task on it's own. A host/hostess is there because handling a single customer is pretty easy, but serving them is hard. Instead, the host/hostess will handle all of the customers and direct them for the wait staff, easy their burdens and making customers happier.",5eapyh,t3_5eapyh,Twelve-Pound,,Comment,8,0,8
daaxrns,2016-11-22 08:50:29-05:00,leftcurlybracket,,"Imagine you have like a million users for your web app at the same time. One computer probably can't handle processing all of these user's requests at once. What ends up happening is you get very high latency as all the requests get backed up. 

To solve this problem, people don't use just one computer, they use several (sometimes thousands). But how do you manage distributing all this work evenly to the various computers? This is what a load balancer does. It distributes all these millions of requests to your web app to different servers running an instance of your app.",5eapyh,t3_5eapyh,Twelve-Pound,,Comment,3,0,3
dab39lx,2016-11-22 11:07:16-05:00,dorkus,,"This happens even within a given server. A given process (depending on language/platform) can only server 1 request at a time (single-threaded). You'll have some software component on that server which distributes the load across all of the processes on said server so that you can adequately use all of the CPU cores on the machine.

If you didn't do this, that mighty 32 core machine would serve a lousy one request at a time, and likely only be using 1 core to do it.",5eapyh,t1_daaxrns,leftcurlybracket,,Reply,1,0,1
daaxxrn,2016-11-22 08:55:38-05:00,d0m58,,"Web servers work by taking incoming (typically HTTP) requests from the internet, processing the request, and formulating and sending a response. All of this processing requires the cpu doing work, whether its moving the incoming data from the network into memory or if its performing a database access or plugging the data into an algorithm, and all of that processing is said to be inducing ""load"" on the server, because a cpu is a limited resource. As the number of incoming requests increases, this ""load"" on your server increases, up to the point where your cpu is fully utilized, where it is literally constantly chugging away at processing data.

Load balancers work by taking incoming requests and distributing the processing of them to multiple servers. In the simplest case of load balancing, you have multiple copies of the same application server, and the load balancer sits ""in front of them"" on the network, taking in the requests directly from the internet, and then passing those requests off to one of the copies of your application server. This allows you to distribute the load of the incoming requests to different servers, and lets you ""horizontally scale"" your system architecture, letting you add more servers to handle more load if necessary.

EDIT: read over this and it seemed more like ELI15, so heres another shot at ELI5

Without load balancing, you have one computer on the internet to handle your internet traffic. With load balancing you have multiple computers to handle that traffic, and one computer, the ""load balancer"", to break up the incoming internet traffic and pass it off to your other computers so that no single computer gets too bogged down.",5eapyh,t3_5eapyh,Twelve-Pound,,Comment,3,0,3
daaxmac,2016-11-22 08:45:47-05:00,PmMeYourSexyShoulder,,"A load balancer's job job to direct traffic coming into your servers in such a way that the load is distributed evenly over your resources so  there isn't a dip in performance for any user.

Or if some resource  goes offline it will redirect the traffic heading towards to another one of your servers.

I tried to layout it out without getting too technical, hopefully that helps.",5eapyh,t3_5eapyh,Twelve-Pound,,Comment,2,0,2
dab3ami,2016-11-22 11:07:53-05:00,Fazaman,,"Imagine your old-school [scale](http://www.clipartkid.com/images/546/justice-20clipart-clipart-panda-free-clipart-images-o9pwPl-clipart.png). Each person using the system is a 'weight'. Your job as the load balancer is to put that weight on one side or the other such that the scale stays as balanced as possible.

Of course, there's usually more 'sides' to the scale, and the method you use to place the weights varies (balancing algorithms) but that's the gist.",5eapyh,t3_5eapyh,Twelve-Pound,,Comment,1,0,1
dab5tnh,2016-11-22 11:59:35-05:00,jajajajaj,,"Unless this project is for your kindergarten, an ELI5 probably won't help much",5eapyh,t3_5eapyh,Twelve-Pound,,Comment,1,0,1
dab6apx,2016-11-22 12:09:15-05:00,Twelve-Pound,,"Fair. I'm supposed to give an explanation on its purpose and how it works, but I'm barely understanding it without a lot of the technical background. Any advice?",5eapyh,t1_dab5tnh,jajajajaj,,Reply,2,0,2
dabxqi4,2016-11-22 21:59:20-05:00,dxk3355,,"If I have some information everyone is going to go to me for it.  With a load-balancer, I've told 10 people the same thing and we all get in a line.  Every time someone asks for it the next person in line steps forward to gives it.  That way when 10 people ask at once there are 10 others that can respond immediately.",5eapyh,t1_dab6apx,Twelve-Pound,,Reply,2,0,2
dabbo5d,2016-11-22 13:56:34-05:00,BetsyBest,,"A load balancer balances the demands you place upon your servers and network architecture. Here's more info I've found super helpful: https://rootleveltech.com/explaining-gslb-to-the-conscientious-administrator/
",5eapyh,t3_5eapyh,Twelve-Pound,,Comment,1,0,1
5e9vzz,2016-11-22 04:35:42-05:00,pgoodjohn,"Opaque, definition?","I was studying for my Systems class and I came across a sentence in which a type was defined as ""opaque"", I don't truly know what this could mean, can somebody explain it to me?
The full sentence reads: ""The type pthread_t is opaque"", the subject is POSIX Threads and the pthreads.h library.
Thanks!",,,,,Submission,2,0,2
daath8q,2016-11-22 05:38:16-05:00,Irony238,,"In the context I know it from ""opaque"" is the opposite of ""transparent"". If a type is defined as ""opaque"" the definition is hidden. This means that you can for example not prove equality by showing that the definition is identical to the definition of some other type because you cannot look at the definition. If the type is transparent you can unfold the definition and use this information.

It might, however, be the case that this word is used differently in your context.",5e9vzz,t3_5e9vzz,pgoodjohn,,Comment,7,0,7
dabh0r3,2016-11-22 15:40:44-05:00,pgoodjohn,,Thanks! ,5e9vzz,t1_daath8q,Irony238,,Reply,1,0,1
daawam0,2016-11-22 07:59:28-05:00,pi_stuff,,"It means you're not supposed to know what the type is. It might be an int, or a long unsigned, or a pointer to a structure, or anything else.

POSIX defines it this way so that it can be implemented differently on different systems. As long as your code is written assuming you don't know how pthread_t really is, then your code will work fine on different implementations.
",5e9vzz,t3_5e9vzz,pgoodjohn,,Comment,6,0,6
dabh0gf,2016-11-22 15:40:34-05:00,pgoodjohn,,Thank you very much! :) ,5e9vzz,t1_daawam0,pi_stuff,,Reply,1,0,1
5e8e4u,2016-11-21 21:57:43-05:00,deek1123,How to make a database front end application,"Hey guys. 

First time on this subreddit, hopefully i am in the right place for this. So I have a project in intro to databases course where I have to make a database, fill the tables and then create a front end application that allows users to edit, delete and add data form the tables as well as view the tables in a list or report . 

The course covered database design but never talked about how to create a front end application. I used MySQL to build and fill a database, now I need to create an application that connects to it and has a GUI. It is specified that I can use any language or tool.

So, my question is what do you think is that fastest, or easiest way to accomplish this. Ive only ever done GUI's in java, it was brief and im hoping theres an easier option. I also know C. If there is a easier language or tool to do this in then im willing to learn since i only need a very simple program. 

Also how do I connect an application and my database?
Thanks for the help, any information is greatly appreciated!",,,,,Submission,5,0,5
daamoij,2016-11-22 00:22:08-05:00,xiongchiamiov,,"You can do this in pretty much any modern programming language (search ""<language> mysql""), and it can be a desktop app or a web app (or I suppose an ncurses-type thing, if that's allowed by your professor). You probably want to use something you or your teammates (if you have a team) are most familiar with so that you don't spend all your time trying to learn a new tool. Or you can use this as an opportunity to learn something new, up to you.

Welcome to software engineering, where customers don't care *how* you solve their problems, just that you do.",5e8e4u,t3_5e8e4u,deek1123,,Comment,3,0,3
daakqm1,2016-11-21 23:27:59-05:00,None,,This is a relatively worn path in Go if you have access to Google searching.,5e8e4u,t3_5e8e4u,deek1123,,Comment,2,0,2
daamedv,2016-11-22 00:13:14-05:00,deek1123,,"oops, didnt realise go was a programming language. Ill check it out, thanks!",5e8e4u,t1_daakqm1,None,,Reply,2,0,2
daamabz,2016-11-22 00:09:47-05:00,deek1123,,"Yea, i googled it first. there seems to be a lot of tools that do this and more but there is a lot of terminology, benefits and what not for each one that go over my head so im having a hard time telling if the tools are what i need or not. Sorry for the stupid question, ill google it some more.",5e8e4u,t1_daakqm1,None,,Reply,1,0,1
dabeb7v,2016-11-22 14:48:07-05:00,None,,"I don't think Google is a trustworthy source of information and generally I feel dirty after Google holds onto my little ""search tweet"".

For Go the document ""Effective Go"" written by Google-employed language authors, plus talks they've given (available on the Internet, such as on YouTube), do wonders to narrow down programming technology in my opinion.",5e8e4u,t1_daamabz,deek1123,,Reply,2,0,2
dabguuv,2016-11-22 15:37:28-05:00,_open,,"If you're a little familiar with HTML / CSS, I'd suggest you building it with MySQL and PHP. In that way, you can come around learning OOP and MVC, which is about what you'd have to learn if you're doing it with Java or C#.

You also find a bunch of tutorials for it on the internet and it should be a little bit easier to understand as beginner than dealing with tutorials doing the MVC cycle in an OOP language.

edit: minor text fixes",5e8e4u,t3_5e8e4u,deek1123,,Comment,2,0,2
5e8cn1,2016-11-21 21:48:44-05:00,functionallycomplete,Understanding the run time of the Recursive Fibonacci Algorithm.,"This is not for homework, this is for understanding.

The professor states the run time for a recursive fibonacci algorithm is as [shown](http://i.imgur.com/4JDSDGZ.png) for the following [algorithm](http://i.imgur.com/kx4rRpV.png). He then [ask](http://i.imgur.com/FDdYDdw.png) how many digits T(100) has.

**Question 1:**
I do not understand what he means by how many digits T(100) has here, can someone explain?

He then states the [answer](http://i.imgur.com/ypKVmYx.png) as being ~1.77E21.

**Question 2:** How does he compute the answer ~1.77E21?  Is there an algorithm one can apply here to figure this out?

This must not be asking what the nth Fib number is, as F(100) is 2147483647 and it has 10 digits. 

I don't know what T(100) is suppose to represent nor how he derives this approx.

I didn't know where else to post this, so hopefully someone can enlighten me. 


Thanks!",,,,,Submission,4,0,4
daail5b,2016-11-21 22:35:13-05:00,_--__,,"I don't know where you get that F(100) is only 2147483647, as it is generally agreed to be 354224848179261915075.

Anyway, if we [solve the recurrence](http://nms.lu.lv/wp-content/uploads/2016/04/21-linear-recurrences.pdf) for T(n) we find that T(n) is [(approximately) 3.6\*1.61^(n)](http://www.wolframalpha.com/input/?i=t(n\)%3Dt(n-1\)%2Bt(n-2\)%2B3,+t(0\)%3Dt(1\)%3D2) [3.6=(5+sqrt(5))/2].

The number of digits in N (in base 10) is approximately log_10(n).

So all you have to do is compute log_10(3.6\*1.61^(100)), for which some basic log laws will tell you should be between 20 and 30.",5e8cn1,t3_5e8cn1,functionallycomplete,,Comment,2,0,2
daaj724,2016-11-21 22:49:29-05:00,functionallycomplete,,"I tried to use the binet formula to calculate F(100). What is wrong with the below code? Why is it not computing F(100) correctly?

	public static int countNums(double a) {
		double numerator1 = Math.pow(((1 + Math.sqrt(5.0)) / 2.0), a):
		double numerator2 = Math.pow(((1 - Math.sqrt(5.0)) / 2.0), a):
		double numerator = numerator1 - numerator2:
		double denominator = Math.sqrt(5.0):
		int result = (int) (numerator / denominator):
		int count = Integer.toString(result).length():
		return count:

	}

The professor never mentioned anything about recurrence. I don't know what a recurrence is & your link doesn't help. What is T(N) in the context of this question & how did he calculate T(100) is suppose to be between 20 and 30?

It looks like if I calculated F(100) correctly (which I didn't and I don't know whats wrong with the code) then all I need to do is count the number of digits and I'd get 21 exactly. ",5e8cn1,t1_daail5b,_--__,,Reply,2,0,2
daajras,2016-11-21 23:03:17-05:00,_--__,,"You have an overflow error - have a look at what your computation calculates for F(50)...  

T(n) is defined in the first link of your post.  If you haven't seen recurrences before, then my guess is that you are probably being asked to compute T(100), or at least a sensible approximation (e.g. 2^(100) which also has 20-30 digits).",5e8cn1,t1_daaj724,functionallycomplete,,Reply,2,0,2
daajxa2,2016-11-21 23:07:29-05:00,functionallycomplete,,"My program outputs:

[1, 1, 2, 3, 5, 8, 13, 21, 34, 55, 89, 144, 233, 377, 610, 987, 1597, 2584, 4181, 6765, 10946, 17711, 28657, 46368, 75025, 121393, 196418, 317811, 514229, 832040, 1346269, 2178309, 3524578, 5702887, 9227465, 14930352, 24157817, 39088169, 63245986, 102334155, 165580141, 267914296, 433494437, 701408733, 1134903170, 1836311903, -1323752223, 512559680, -811192543, -298632863]

The number is 2147483647 and the number of digits is 10.

I didn't realize that! How can I fix it?

I think my approach would work... calculate T(N) using the binet formula and then counting how many digits are there.

Have no idea how to fix my code 
",5e8cn1,t1_daajras,_--__,,Reply,1,0,1
daajzh8,2016-11-21 23:08:59-05:00,_--__,,"You can do slightly better if you replace all your ""int""s with ""long""s, but you still have an overflow.  Best is to keep everything as doubles (or have a look at the BigInteger class).",5e8cn1,t1_daajxa2,functionallycomplete,,Reply,2,0,2
daakbon,2016-11-21 23:17:16-05:00,functionallycomplete,,"Yes, you're right. We are to make a sensible approximation, but he never discussed how to do so. I don't think we will ever do such a problem again as he never discussed the approach and such a question hasn't shown up again and he never placed an emphasize on knowing how to derive it.

How do you make such a sensible approximation without using reoccurrences?

Even though this problem isn't important to my class it bugs me I don't have a general method to figure this out with.

Using double helped a bit. But for F(100) it says I have 20 digits now. How would I use BigInteger class? In the examples I've seen it looks like you create a BigInteger object and assign some number to a variable of that object. 

I'm doing computations and trying to assign those computations to a variable.

e.g. 

	public static double countNums(double a) {
		double numerator1 = Math.pow(((1 + Math.sqrt(5.0)) / 2.0), a):
		double numerator2 = Math.pow(((1 - Math.sqrt(5.0)) / 2.0), a):
		double numerator = numerator1 - numerator2:
		double denominator = Math.sqrt(5.0):
		double result = (numerator / denominator):
		double count = Double.toString(result).length():
		return count:

	}

Not sure how to use ""BigInteger"" here.  How can I use this in my program? ",5e8cn1,t1_daajzh8,_--__,,Reply,0,0,0
daakln1,2016-11-21 23:24:27-05:00,flebron,,"What you are missing is that your professor most likely wrote down that T(n) gives the number of operations required to compute the nth Fibonacci number, using that recursive function `FibRecurs`.

T(100) is a number. When he asks you how many digits T(100) has, he is asking you how many digits this number has. That is, if you wrote it out in base 10, how many places would you need?

You can just compute T(n) using the expansion the professor gave. Nothing wrong with that, except this will take a long while. For instance, writing a short Haskell program:

    let t 0 = 2
    t 1 = 2
    t n = t (n - 2) + t (n - 1) + 3

Evaluating t 30 already takes around 5 seconds on my computer. It'll only get worse from there. You can make this computation faster by using something called dynamic programming, where you re-use previously computed results. If, for example, I ask you to compute T(30), but I also give you the values of T(29) and T(28), computing this is very fast, you just add those two numbers I gave you, and add 3 to that. So the dynamic programming version of this would be to start computing T(1), T(2), T(3), T(4), ..., and every time you need to compute a value of T(n) for some n, you already have all the previous values of T (that is, you have T(n - 1), T(n - 2), T(n - 3), ...), because you computed them earlier.

So a short program in Python that computes this:

    T = [0] * 101   # This creates an array of 101 zeros.
    T[0] = T[1] = 2
    for i in range(2, 101):
      T[i] = T[i - 1] + T[i - 2] + 3
    print(T[100])

This prints out 2865739220069085420502L. When you want to fill in the value of T[i], you already know the values of T[i - 1] and T[i - 2], because you computed them in the previous iteration of the loop.

You can do the same with Fibonacci numbers, and I encourage you to try it out.",5e8cn1,t3_5e8cn1,functionallycomplete,,Comment,2,0,2
daakzp6,2016-11-21 23:34:06-05:00,functionallycomplete,,"So basically you are saying to compute T(100) you use the formula 3 + T(99) - T(98) and to compute T(99) you compute T(98) + T(97) and to compute T(98) you compute T(97) and T(96) and to compute T(97) you compute T(96) and T(95).... and so on... this is totally ignoring the other recursive calls and from this example alone I see how you are re-computing previous recursive calls.  So, what you're saying is you can use another trick called Dynamic Programming that 'remembers' previously computed values-- thus saving incredible time. 


Okay cool... that makes much more sense now what T(N) is. It's very similar to F(N) no? Only difference is you are taking the value of F(N) and adding 3 to it?

So, how are you getting the sensible estimation of it being between 20 and 30 digits (without reoccurrence)?  Is my idea of computing the nth value directly (using binet formula) and counting the digits good/correct?

I'm still trying to figure out how to get the precision correct... here is my code: http://pastebin.com/ZqX705Le

",5e8cn1,t1_daakln1,flebron,,Reply,1,0,1
daalkut,2016-11-21 23:49:43-05:00,flebron,,"One reasonable approximation is to see that the recursive tree for T is identical to the recursive tree for Fibonacci (i.e. the function calls are identical, just in one case you add 3 and in the other you don't, and the base cases are different). So, instead of annotating each leaf of the Fibonacci recursion tree with 0 or 1, depending on whether the leaf is F(0) or F(1), you annotate them all with 2, because T(0) = T(1) = 2. The value at the top of the tree will be the sum of those leaf values (because they propagate up when you say T(n) = T(n - 1) + T(n - 2) + 3, you sum the values of your two children, and then add 3), plus one copy of ""3"" for every internal node of the Fibonacci recursion tree (internal means it's not a leaf).

So you want to count the number of internal nodes of this tree, and the number of leaves of this tree.

How many leaves does a Fibonacci recursion tree have? The number of leaves of the tree rooted at T(n) is the number of leaves of a tree rooted at trees T(n - 1) (its left child) plus the number of leaves of a tree rooted at T(n - 2) (its right child). So If L(n) is the number of leaves of the recursive tree of T has, is actually Fib(n + 1) :) (It's shifted by 1 because L(0) = L(1) = 1, instead of Fib(0) = 0, Fib(1) = 1).

So we have Fib(n + 1) leaves. How many internal nodes? The same recursion applies, with a - 1. You can just draw these trees out and count: http://www.comscigate.com/cs/IntroSedgewick/20elements/27recursion/fig5-14.png

So 1, 2, 4, 7, 12, 20, and so on. This means if we call I(n) the number of internal nodes in a Fibonacci recursion tree rooted at Fib(n), we have Fib(n + 1) - 1 such nodes.

So for the top value in the tree T(n), we have 3 * L(n) + 2 * I(n) = 3 * Fib(n + 1) + 2 * (Fib(n + 1) - 1). 

To approximate this and get the number of digits, you can see that this is approximately 5 * Fib(n + 1), so the number of digits of this is ceil(log10(5 * Fib(n + 1))). You can now just approximate Fib(n + 1) by its usual Binet expansion, and you get http://www.wolframalpha.com/input/?i=log10(5)+%2B+101+*+log10(1.618033989). This is 22, which is indeed the number of digits of T(100) = 2865739220069085420502.",5e8cn1,t1_daakzp6,functionallycomplete,,Reply,1,0,1
daamvlw,2016-11-22 00:28:36-05:00,functionallycomplete,,"Sorry you lost me after the first paragraph. Maybe you can explain this and explain your terminology too? I'm new to CS.

I thought if I computed the Binet formula for F(N) I could get how many digits appear in T(N). Is that assumption right? How can I fix my code if so?

Also, can you reexplain everything after paragraph 1? Maybe an example or better illustration. I don't know what you mean by leaf, internal node, and all the calculations.

",5e8cn1,t1_daalkut,flebron,,Reply,1,0,1
daaom4f,2016-11-22 01:30:32-05:00,flebron,,"First, a note on Fibonacci, Binet, and digit count.

The Binet formula tells you that, if n is a natural number, then the nth Fibonacci number is exactly ((1 + sqrt(5))^n - (1 - sqrt(5))^n) / (2 sqrt(5)). Note sqrt(5) is irrational, however. Your computer, no matter how many digits of precision you use, cannot represent sqrt(5) as a decimal number of a finite number of digits. IF you could, THEN you could simply use that formula to compute the nth Fibonacci number.

IF you could compute the nth Fibonacci number X quickly, THEN the number of digits of X is easily obtainable as floor(log10(X)) + 1. This is true for any number. The number of digits of a number X is always floor(log10(X)) + 1.

Since this is true, and again assuming you had infinite precision real numbers in your computer (you don't), then you could use the fact that log(a * b) = log(a) +log(b), and log(a^b) = b * log(a). Since Fib(n) = (((1 + sqrt(5))/2)^n - ((1 - sqrt(5))/2)^n) / sqrt(5), then the number of digits of Fib(n) is floor(log10(Fib(n))) + 1 = floor(log10((((1 + sqrt(5))/2)^n - ((1 - sqrt(5))/2)^n) / sqrt(5))) + 1 = floor(log10(((1 + sqrt(5))/2)^n - ((1 - sqrt(5))/2)^n)) - log10( sqrt(5))) + 1. Since ((1 - sqrt(5))/2)^n is tiny, for large values of n it's even tinier (much less than 1), so you can mostly ignore it, as well as log10(sqrt(5)) which is around 0.34, and then use round instead of floor, dropping the + 1 (this is an approximation). So you get an approximation of round(log10(((1 + sqrt(5))/2)^n)). Using the rules for logarithms and exponents, this gives you round(n * (log10(1 + sqrt(5)) - log10(2))) = round(n * log10(1.618033989)). 

That shows you how to quickly compute an approximation of the number of digits of the nth Fibonacci number.


---------------


Now, remember that Fib(n) is NOT T(n). T(n) was the number of operations you need to compute Fib(n) using the algorithm you posted (`FibRecurs`).

What I did in those paragraphs is tell you first how to compute T(n), and then how to express it in terms of the n + 1st Fibonacci number. What I said is that T(n) = 3 * Fib(n + 1) + 2 * (Fib(n + 1) - 1). This is what I used the argument about leaves and internal nodes for. If you don't know what a leaf or a tree or an internal node is, and also don't know how to find closed-form solutions for recurrences, I don't think you're meant to approximate T(n).

In any case. ONCE you see that that relationship between T(n) and Fib(n + 1) is true, THEN you can use the approximation for Fib(n + 1) to compute T(n). Since T(n) is approximately 5 * Fib(n + 1), the number of digits of T(n) is approximately floor(log10(5 * Fib(n + 1))) + 1. Again using logarithm rules, this is floor(log10(5) + log10(Fib(n + 1))) + 1. I showed you above that log10(Fib(n + 1)) is approximately (n + 1) * log10(1.618033989). When n = 100, we get that an approximation for the number of digits of T(n) is floor(log10(5) + (n + 1) * log10(1.618033989)) + 1 = 22.",5e8cn1,t1_daamvlw,functionallycomplete,,Reply,1,0,1
dad5wca,2016-11-23 19:17:02-05:00,functionallycomplete,,"Thanks for the response. Can you diagram/draw out what you mean? I think if I visualized it I could see what you're talking about better.

I've been thinking about this a bit more and thought of 2 ways I could estimate it. Is either approach fine?

Below are the two approaches,

1) T(N) is defined very similar to F(N). The only difference is some constant term (the plus 3) and difference ""base cases"" but other than that it's essentially the same recursive definition as F(N), hence you can estimate how many digits T(N) has by estimate how many digits F(N) has. I did this by calculating F(N) using binet's formula and taking the ceiling of that estimation. So for example... F(100) I used the formula, where a=100	

>double count = Math.ceil(a * Math.log10((1 + Math.sqrt(5)) / (2)) - (Math.log10(5)) / 2):

and obtained count=21. This is between the 20 and 30 answer choice the professor had for that question and hence I would have correctly estimated the range with the above approach.

2) The other approach is to calculate T(N) directly using the recursive formula (yes ignore the terrible run time here) but it'd be something like this:

T(0) = 2

T(1) = 2

T(2) = T(0) + T(1) + 3 = 7

T(3) = T(1) + T(2) + 3 = 12

and so on until T(100)....

How do those approaches look? I think this is the right way to go about it especially since the professor mentioned *nothing* about recurrences or any other methods. ...

The question was simply estimate how many digits T(100) has between a range and the correct answer was 20 to 30... my two approaches above would have led me to that correct estimation.

I fixed the code from my original post as well.

Thank you for your high quality responses. I am learning a lot :).",5e8cn1,t1_daaom4f,flebron,,Reply,1,0,1
5e293a,2016-11-20 23:40:31-05:00,mrtrumpforpresident,Why do people say computer science guys can't get girls?,"I know this is partially because there are almost no girls in this field. However, in the 1st 2 years you take gen ed classes, right? Please enlighten me, I'm a high school student and this is freaking me out a little bit. I might actually graduate college a virgin.",,,,,Submission,0,0,0
da96rhh,2016-11-21 00:29:03-05:00,wafflestealer654,,"I don't feel like this is the right place for such a question. You might want to crosspost to /r/socialskills or /r/AskReddit.

But to answer your question, the reason a lot of people say that CS boys can't get girls are because of a stereotype applied to CS majors. Typically, they are portrayed as being anti-social with no interest in dating people. As with a lot of stereotypes, this isn't true for the majority of students/graduates.

Don't worry about how your major will affect your dating life, because it won't. If your partner doesn't like you only because you are in CS, then they were already a horrible person to begin with.",5e293a,t3_5e293a,mrtrumpforpresident,,Comment,5,0,5
daaq1gh,2016-11-22 02:31:56-05:00,jnzq,,"I wouldn't necessarily say they don't have an interest in dating people, but rather can't. If you've seen the show Silicon Valley, you'll see a lot of the people in our major are pretty socially inept.",5e293a,t1_da96rhh,wafflestealer654,,Reply,2,0,2
da9e423,2016-11-21 06:34:32-05:00,Adnotamentum,,"Its true the ratio is not good in Computing Science lectures like how it isn't good in any STEM field. So, you'll have to meet girls in unrelated societies. Try to sound less insecure though.",5e293a,t3_5e293a,mrtrumpforpresident,,Comment,4,0,4
daa2pkg,2016-11-21 16:32:16-05:00,KronktheKronk,,"Because your typical CS nerd is a social trainwreck.  They're overly analytical about everything, they overvalue unfeeling logic in decision making despite being just as emotionally driven as anyone else, and they would rather be right than charming at any given time in their lives.

CS people understand logic and math and rules and inputs and calculations and outputs, but they tend not to understand people at all.  This leads them to being incredibly awkward whenever they do try to engage with people who aren't CS majors.

The other side of that coin is that being a CS degree'd human being with points in Charisma makes it pretty easy to start and forward a career.",5e293a,t3_5e293a,mrtrumpforpresident,,Comment,3,0,3
da9xcab,2016-11-21 14:46:14-05:00,bigbirdtoejam,,This is a myth that isn't worth your worry.  Just be you and like it: you'll be fine.,5e293a,t3_5e293a,mrtrumpforpresident,,Comment,2,0,2
da9hm5f,2016-11-21 09:02:00-05:00,andybmcc,,Live on campus.  Profit.,5e293a,t3_5e293a,mrtrumpforpresident,,Comment,1,0,1
dac4i8p,2016-11-23 01:23:15-05:00,plantice,,"I am a bit abnormal in most degrees as a CS major but alot of CS majors I know do have wives. 

As many said, the stereotypes about us being antisocial was true for the most part in the early years (think bill gates college years). Now a days I think we have broken that mold a bit, which new ways to reach out and be social. You just need to focus on being you and making sure you have hobbies. Shit, I have seen ppl get dates from playing DnD. Your job doesn't define you if you don't let it.",5e293a,t3_5e293a,mrtrumpforpresident,,Comment,1,0,1
dae59pq,2016-11-24 14:23:39-05:00,Calsem,,"Because of the lopsided girl/guy ratio in CS classes.  College is a very social environment, however, so you will have plenty of opportunities to make friends.  If you want to be extra social, joining a study group / club is a good idea.",5e293a,t3_5e293a,mrtrumpforpresident,,Comment,1,0,1
darjow5,2016-12-04 01:25:28-05:00,windingsandtumtums,,"CS is a major with very specific requirements for degree completion, which usually means that you will likely share multiple classes with the same people, semester after semester, or quarter after quarter.  Nobody wants to date someone freshman year, break up, and spend the next three years in all of the same classes.  You know the saying, don't shit where you eat.  

Also, it's a competitive major, you don't want to feel like you're competing with your boo.  Maybe date an engineering major if you want something a bit removed from your program but still STEM, or a bio major which is like twice removed heh.",5e293a,t3_5e293a,mrtrumpforpresident,,Comment,1,0,1
5e226c,2016-11-20 22:55:59-05:00,redditpirateroberts,What are the most robust open source solutions for object recognition when the objects that need to be recognized are finite?,"The use case for object recognition I am talking about is actually on the more simple end of object recognition as far as I can tell. I need object recognition that works really well, but it only needs to work on specific objects that I would have photos of from different angles and lighting backdrops. What are the best existing open source solutions that you would recommend I check out to do this? 

edit: perhaps a better phrasing of what I am looking for is object identification. I need it to look at a still photo and say what objects are in it that it can recognize.",,,,,Submission,7,0,7
da9g4fo,2016-11-21 08:09:42-05:00,SayYesToBacon,,"Opencv has a lot of really cool features for image manipulation (masking, filtering, etc.) and scipy has some cool AI libraries like random tree classifier. Opencv is for python and c++",5e226c,t3_5e226c,redditpirateroberts,,Comment,1,0,1
5e1rnd,2016-11-20 21:50:07-05:00,Cheeseologist,Do you guys know what exp{..} notation refers to?,"So I'm in a class on AI, and right now we're doing stuff with neural networks. And I'm seeing this notation that I'm unfamiliar with that follows the format of ""exp{...}"". Is anyone familiar with this notation? [Here is an example.](http://i.imgur.com/fIipPY9.png) Note that I'm pretty sure that ""neurone"" is supposed to say ""neuron.""

Right now my best guess is that the exp stands for expression.",,,,,Submission,13,0,13
da91dxy,2016-11-20 22:10:32-05:00,themeaningofhaste,,"It's just the standard exponential function, like e^x but here it's e^(stuff) where the braces are just used as a different separator from parentheses, e is the mathematical constant Euler's number. Sometimes brackets are also used and when there are many layers or parentheses/brackets/braces they'll be nested appropriately and make the equation more readable.",5e1rnd,t3_5e1rnd,Cheeseologist,,Comment,7,0,7
da927c4,2016-11-20 22:28:19-05:00,Cheeseologist,,Blargh okay thanks lol. Should've guessed it was simple. So it's like e^(stuff in curly braces)?,5e1rnd,t1_da91dxy,themeaningofhaste,,Reply,5,0,5
da93hds,2016-11-20 22:58:47-05:00,themeaningofhaste,,Yep!,5e1rnd,t1_da927c4,Cheeseologist,,Reply,2,0,2
da93iqh,2016-11-20 22:59:43-05:00,Cheeseologist,,"True, thanks.",5e1rnd,t1_da93hds,themeaningofhaste,,Reply,2,0,2
da91epj,2016-11-20 22:11:01-05:00,way_runner,,It's e^{...},5e1rnd,t3_5e1rnd,Cheeseologist,,Comment,3,0,3
da928u1,2016-11-20 22:29:09-05:00,Cheeseologist,,Thanka thank.,5e1rnd,t1_da91epj,way_runner,,Reply,2,0,2
5dzzl5,2016-11-20 15:36:34-05:00,achenx75,"If I don't know how to code, is getting a degree in IT even worth it?","So I graduated community college with an Associates in Science for ""Computer Information Systems"" (Although it's not written on my degree). I've since moved to a 4 year college and currently on the path for a Communications major and I'm starting to get worried.

A lot of people say Communications is not a very useful degree and will secure you a low paying sales job. On the other hand, I could try my hand at my school's major of Information Technology and Infomatics and continue doing IT, but here's the problem. I don't know how to code. I tried to code during my time in community college but I ended up having to cheat a lot to pass my computer science classes.

So I ask this...is getting a degree in IT even worth it if I don't know how to code?",,,,,Submission,2,0,2
da8m4yt,2016-11-20 16:09:56-05:00,DM_ME_YOUR_POTATOES,,"Not if you're going to cheat, no it's not worth it. Otherwise it is worth it.",5dzzl5,t3_5dzzl5,achenx75,,Comment,16,0,16
da8mdgr,2016-11-20 16:14:36-05:00,wescotte,,"Programming to automate tasks can be very useful for IT but I would say the majority of IT jobs don't require programming skills.

",5dzzl5,t3_5dzzl5,achenx75,,Comment,8,0,8
da8z2uh,2016-11-20 21:17:36-05:00,rkaz246,,"I think programming is the most useful part of IT because you can get a great understanding of how things work if you can understand it,  but yes I agree with you, it's not required",5dzzl5,t1_da8mdgr,wescotte,,Reply,3,0,3
da8piac,2016-11-20 17:25:11-05:00,radjic,,"You sign up for a course/degree to learn something. They'll teach you everything, just don't cheat.",5dzzl5,t3_5dzzl5,achenx75,,Comment,3,0,3
da8qi02,2016-11-20 17:47:21-05:00,ugknite,,"IT does not necessarily involve programming. Programming can be helpful, but not a necessity?",5dzzl5,t3_5dzzl5,achenx75,,Comment,3,0,3
dabhghk,2016-11-22 15:49:13-05:00,_open,,Don't do what other people say will give you a good earning. Do what you'd love to do and I promise you it will fullfill your life more than a job you just do for the money. ,5dzzl5,t3_5dzzl5,achenx75,,Comment,1,0,1
dac4ru6,2016-11-23 01:33:27-05:00,plantice,,"If you are a detail focused person, have a communications degree and a background in IT (coding is a plus but not always needed). You could go in as a QA tester. They need to be able to communicate problems and give test cases to those that need to fix them. The more IT background you have the better but you dont need to be able to code.",5dzzl5,t3_5dzzl5,achenx75,,Comment,1,0,1
5dyqh1,2016-11-20 11:26:00-05:00,arjunpur,System Monitoring Tool Ideas/Help?,"Hey guys, I'm trying to write a tool that logs my activity on my computer (based on what tabs I have open, what applications I'm using and for how long etc.) and then later runs a job to aggregate this to provide some kind interesting feedback about my behavior. I'd really appreciate some ideas/pointers as to where to begin (especially with regards to APIs into system level processes running in the foreground). It'd be nice to have some sort of a callback mechanism on noticing certain changes in processes that the system I/O is interacting with rather than polling since I don't want to kill my machine's CPU. Thanks in advance!",,,,,Submission,3,0,3
5dwtby,2016-11-20 01:12:10-05:00,AltInnateEgo,"I was recently introduced to the Traveling Salesman Problem. Has a method like this been used before, if so what is it called, if not is there a way to test how well it performs?","http://imgur.com/a/u9rsx

There is only one shape the optimal path could produce, and it would seem that an outer to inner approach of paths of least resistance (like you see when objects are subjected to vacuum packing) would be a decent way to approach the problem. I thought of this by thinking of a rubber band being placed around the points and moving the rubber band inward to the closest points of the outer boundary while working in concentric circles of ""outer most points"". ",,,,,Submission,16,0,16
da7znlq,2016-11-20 03:29:17-05:00,_--__,,"I realise you're not claiming that this will necessarily solve the problem - which is good, because it looks fairly simple to come up with an arrangement where this method doesn't find the best solution.  I don't know much about algorithms that are aimed at the TSP, but searching for **convex hull** methods might yield something akin to what you are proposing.",5dwtby,t3_5dwtby,AltInnateEgo,,Comment,10,0,10
da7zpcf,2016-11-20 03:31:56-05:00,AltInnateEgo,,Thanks!,5dwtby,t1_da7znlq,_--__,,Reply,3,0,3
da7zosu,2016-11-20 03:31:05-05:00,AltInnateEgo,,"The only thing I can add to this is the issue of a node being nearer an outer node rather than an outer boundary line. In this case, the inner node, its nearest outer node, and the two adjoining outer nodes are all connected and the shortest path is calculated.

http://imgur.com/a/cauIA",5dwtby,t3_5dwtby,AltInnateEgo,,Comment,2,0,2
da80mfp,2016-11-20 04:26:15-05:00,Muirbequ,,This looks like you are using connected components and clustering to get a single connected graph. I don't think there are any other interesting properties with this algorithm.,5dwtby,t3_5dwtby,AltInnateEgo,,Comment,2,0,2
5dw5h0,2016-11-19 22:08:34-05:00,Gravey4rd,Why are there different computer languages?,Why are there different computer languages? Is one superior to another? ,,,,,Submission,12,0,12
da7s0x4,2016-11-19 22:37:17-05:00,dragonnyxx,,"1. We've learned a lot. Programming languages designed forty or fifty years ago feel *very* different than languages designed in the last few years. They have more complex syntaxes than they need to (requiring semicolons and extraneous parentheses and various other things), they make arbitrary distinctions between things that shouldn't really be distinct, they lack features now considered essential, etc.
2. They have different strengths and weaknesses. Languages that (say) focus on high performance inevitably sacrifice a degree of user friendliness to get there, and vice versa. Language design is in general a big series of tradeoffs.
3. People have different tastes. Even if you don't care about performance or any other technical considerations, you'll still prefer working in certain languages. Some people like the fast-and-loose style of dynamic languages like JavaScript and Python, others prefer the rigor of a very strict language like Haskell. Most of us like to be somewhere in the middle. And that's only one axis of the decision.
4. Many languages are designed for a very specific purpose that is not well-served by existing languages. When SQL (a database query language) and GLSL (a GPU shading language) were designed, no existing general-purpose language could have substituted.
5. Lots of people aren't completely happy with any existing language, or just want to experiment with creating their own, and it's not actually that hard to make your own language. So there are thousands of them now.",5dw5h0,t3_5dw5h0,Gravey4rd,,Comment,28,0,28
da7v71t,2016-11-20 00:17:42-05:00,Gravey4rd,,Thank you for your lengthy answer.,5dw5h0,t1_da7s0x4,dragonnyxx,,Reply,2,0,2
da7xq34,2016-11-20 01:53:00-05:00,liflo,,"The same reason there are so many different kinds of pliers.  

I know this is a somewhat simplistic analogy but when it comes to programming languages, like pliers, some are older/simpler, some are more specialized, some are more advanced and have features that make them more efficient.

At the end of the day programming languages are tools.  The more people use any tool to do their jobs, the more they want to make a better version of that tool.

Some will make something new that is popular and becomes ubiquitous, others will make some oddball thing that never escapes obscurity.",5dw5h0,t3_5dw5h0,Gravey4rd,,Comment,4,0,4
da8o8ea,2016-11-20 16:55:57-05:00,wescotte,,"The same reason we so many different modes of transportation. Why do we need cars, trucks, semis, motorcycles, speedboats, cruise ships, airplanes, buses, etc.

They all have thier advantages and disadvantages based on what you want to use them for and are constantly evolving.",5dw5h0,t3_5dw5h0,Gravey4rd,,Comment,1,0,1
da7wiyb,2016-11-20 01:03:46-05:00,eid-a,,"because they taste different !!
you could create your own if you so wish and combine what you think is best about a bunch of them . 
",5dw5h0,t3_5dw5h0,Gravey4rd,,Comment,-1,0,-1
da7v5el,2016-11-20 00:16:14-05:00,None,,There is serious evidence pointing toward staying with english C.,5dw5h0,t3_5dw5h0,Gravey4rd,,Comment,-6,0,-6
da7v8ke,2016-11-20 00:19:08-05:00,Gravey4rd,,What is C exactly beside it being used by Unix? I can't seem to find much upon it.,5dw5h0,t1_da7v5el,None,,Reply,3,0,3
da7vgdb,2016-11-20 00:26:40-05:00,None,,"General purpose computing language with mature C language compilers available today, with mature licensing and source code.

You can't trust any one compiler, so might as well have multiple languages.",5dw5h0,t1_da7v8ke,Gravey4rd,,Reply,-2,0,-2
da7wpyv,2016-11-20 01:11:04-05:00,not-just-yeti,,"You mean: ""the language doesn't have well-specified semantics, so we'll take the average behavior of several implementations ""?  Wow, that's some pretty radical apologetism. You may not like Java, but at least  I'm confident that any mature compiler will give the correct results.",5dw5h0,t1_da7vgdb,None,,Reply,8,0,8
da7wrev,2016-11-20 01:12:39-05:00,None,,I write Go like C is going out of business forever.,5dw5h0,t1_da7wpyv,not-just-yeti,,Reply,-1,0,-1
da7vh6d,2016-11-20 00:27:28-05:00,Gravey4rd,,What does that mean?,5dw5h0,t1_da7vgdb,None,,Reply,3,0,3
da7vl93,2016-11-20 00:31:28-05:00,None,,"For any application of computing science, by choosing to write in a language other than the specific processor machine code, then you will at least be trusting that a single compiler implementation is correct.

The compiler enables one code for many processor languages.",5dw5h0,t1_da7vh6d,Gravey4rd,,Reply,0,0,0
da7vpxq,2016-11-20 00:35:37-05:00,Gravey4rd,,So C is a compiler? Sorry for my lack of understanding... ,5dw5h0,t1_da7vl93,None,,Reply,2,0,2
da7vt0r,2016-11-20 00:38:25-05:00,None,,C is a specific way to write a processor procedure (or whatever it takes as input) that reads the C program (your written language set) and changes it into the processor procedure.,5dw5h0,t1_da7vpxq,Gravey4rd,,Reply,0,0,0
da7w37a,2016-11-20 00:48:05-05:00,Gravey4rd,,"Oh I see, thanks! :)",5dw5h0,t1_da7vt0r,None,,Reply,1,0,1
da8jxrb,2016-11-20 15:24:53-05:00,Bosco89,,"The choice of a language has very little to do with the trust in a compiler. The C language itself has many compilers (gcc, icc, clang...) ",5dw5h0,t1_da7vl93,None,,Reply,2,0,2
da8k2rm,2016-11-20 15:27:49-05:00,None,,"Yes - trust is one tradeoff next to exact specific language features (including extensions, omissions, and mistakes), output program optimization bells and whistles, error/warning rejection, licensing of the compiler itself (can you add to the compiler? can somebody you don't like add to the compiler and you have to take their changes?), then there's all of the actual computer science philosophy in code correctness and reliability, how good is your automatic reference counting implementation for Objective-C?",5dw5h0,t1_da8jxrb,Bosco89,,Reply,1,0,1
5duavb,2016-11-19 15:16:23-05:00,itsClarkson,Alternatives to being a programmer after studying CS.,"So after studying CS at uni for 4 years and working for 1 year as a web programmer I've finally realised that I don't like programming. In fact I don't much care for CS but I don't want to ignore the 4 years I've spent studying it, so what alternatives do I have within the field? My issue is obviously personal and complex but I won't bore you with such details, I just want to find a job I won't completely hate while I change the direction of my life. Thank you.",,,,,Submission,15,0,15
da7ctj2,2016-11-19 15:53:14-05:00,high_side,,"Project management, CM, systems/test, IT.",5duavb,t3_5duavb,itsClarkson,,Comment,9,0,9
da7d7hj,2016-11-19 16:03:06-05:00,itsClarkson,,What do you mean by systems/test?,5duavb,t1_da7ctj2,high_side,,Reply,2,0,2
da7dbfd,2016-11-19 16:05:53-05:00,high_side,,"It's a position in a software organization.  Sometimes conjoined, sometimes separate.  Responsibilities include managing requirements, determining test plans, executing test plans, handling test software, understanding systems (software/platforms/apis) and advising on how to best integrate them.",5duavb,t1_da7d7hj,itsClarkson,,Reply,4,0,4
da7w3gb,2016-11-20 00:48:18-05:00,Amburlin,,"I agree with this. I was a software engineer and am now a manager in software test engineering. In my experience in both it's been incredibly beneficial to have someone with working knowledge of CS in any IT role such as project management, business analysis, quality assurance or test engineering, IT management, product ownership, etc. These roles would require some serious self learning and a solid skill in communication and writing, but your understanding of software would allow you to better support development teams through these roles.",5duavb,t1_da7ctj2,high_side,,Reply,1,0,1
da7z4ff,2016-11-20 03:00:35-05:00,high_side,,"Aka the guy who passes your estimates onto middle management, but it's okay cause they ""have a technical background"". :)",5duavb,t1_da7w3gb,Amburlin,,Reply,0,0,0
da7du8c,2016-11-19 16:18:55-05:00,Shadowsoal,,Patent law.,5duavb,t3_5duavb,itsClarkson,,Comment,4,0,4
da7hcwh,2016-11-19 17:48:47-05:00,SayYesToBacon,,"Janitor, ditch digger, prostitute - the possibilities are endless",5duavb,t3_5duavb,itsClarkson,,Comment,14,0,14
da8319e,2016-11-20 07:00:19-05:00,itsClarkson,,The world is my oyster.,5duavb,t1_da7hcwh,SayYesToBacon,,Reply,5,0,5
da7jbvi,2016-11-19 18:40:05-05:00,PM_ME_YOUR_SHELLCODE,,"Depending on how much you dislike programming, perhaps you might want to look into security assessments.

The programming involves is usually minimal, some people don't write code, others like to automate everything they do and build a lot of personal tools. Its pretty much left to you building what you want/need, not just programming for someone else's specifications.

The knowledge from a CS degree is really useful in the field though, as you need to understand how applications work to think up ways to break them. 

The downside is, it will take some self-learning to learn the necessary skills for the field but your background gives you a solid foundation to start from.",5duavb,t3_5duavb,itsClarkson,,Comment,2,0,2
da830ew,2016-11-20 06:58:50-05:00,itsClarkson,,"This I like. I know Cybrary.it offers some sort of introduction course, do you know any others that could be good?",5duavb,t1_da7jbvi,PM_ME_YOUR_SHELLCODE,,Reply,1,0,1
da7uzdn,2016-11-20 00:10:30-05:00,None,,"Can you write jokes? Can you take a joke?

I've been surprised at how easy writing is, given the input of Twitter, past media, current events, and the structures learnable with computers and electronics.",5duavb,t3_5duavb,itsClarkson,,Comment,2,0,2
da82zk8,2016-11-20 06:57:25-05:00,itsClarkson,,"Funnily enough I've always had a knack for humour, but I've never written anything down. I've been thinking about it for a few months, I guess I'll give it a shot.",5duavb,t1_da7uzdn,None,,Reply,2,0,2
da8hq6d,2016-11-20 14:36:25-05:00,None,,Your joke is surprisingly neutral.,5duavb,t1_da82zk8,itsClarkson,,Reply,1,0,1
da7xtxe,2016-11-20 01:57:37-05:00,markgraydk,,"IT Business Analyst. Might require you pick up some extra skills though but CS is far from the worst place to start from. DB/cloud/HPC administration  could be interesting specialized areas of administration to look at. Data science is getting more and more popular so people with more general degrees like a BA in CS could maybe get into it. Business Intelligence might also be an option. IT consulting of various sorts. 

",5duavb,t3_5duavb,itsClarkson,,Comment,2,0,2
da831xc,2016-11-20 07:01:28-05:00,itsClarkson,,"I remember doing a bit of BI at uni, I'll look into it. Thank you!",5duavb,t1_da7xtxe,markgraydk,,Reply,1,0,1
da7grd8,2016-11-19 17:33:26-05:00,SYCarrot,,Academia,5duavb,t3_5duavb,itsClarkson,,Comment,1,0,1
5dtk8y,2016-11-19 12:46:35-05:00,duchain,"Learning Object Constraint Language ( OCL ), would appreciate if someone would look over my work as I can't tell if I am right or not","Question followed by my answers:
http://imgur.com/a/8SUKO

A few specific questions

Can I use an If statement as an invariant?

for i) and ii) I have assumed that a square only exists if it has been given a value, is this a safe assumption to make ?

for iv) I have created my own sets, is this allowed? and how do specify what I am returning?

Thanks!",,,,,Submission,2,0,2
5dt81o,2016-11-19 11:37:59-05:00,JobMar,As a self taught programmer,"Am a self taught programmer mostly, lately I have been getting more and more into asymptotic notation and I would like to know what mathematics modules should I learn, and in which order for programming?",,,,,Submission,11,0,11
da77nbl,2016-11-19 13:45:17-05:00,ACoderGirl,,"Well, the mathematical requirements for programming can be quite diverse because it depends on what kind of programming you do. It's commonly said that you don't *need* any advanced maths knowledge to be a programmer. Which is true, but assumes you don't do certain things. As you're discovering, algorithm design and analysis does require more maths knowledge, as do many other things.

So here's some examples of certain kinds of maths that are used in various fields of programming and CS. None are strictly necessary to be *a* programmer, but certain things are very good to know and others are required if you want to do certain things. It's not an exhaustive list.

* Limits and systems of inequalities are a necessary thing for understanding things like big O. This would be covered under the subject typically called ""precalc"". I would consider understanding of big O and thus algorithm time analysis to be a fundamental thing for being a *good* programmer.
* Calculus (especially derivatives) is used in many AI fields, including machine learning and computer vision.
* Linear algebra is used heavily in computer graphics. It's also used for many, many other problems. Eg, computer vision and image processing would use it (since images are naturally represented as matrices). Quite a lot of arbitrary problems can be represented in a matrix format, in fact. For example, linear regression (a stats thing) can be easier calculated with the usage of matrices. Solving systems of equations can be done with matrices. Balancing chemical formulas can be done with matrices. Honestly, I'd consider linear algebra highly useful for computer science.
* Statistics has many uses. AI is again the leading use. Any kind of decision making often depends on statistical analysis.
* Graph theory is widely used. You've probably seen graph algorithms come up if you've done any form of algorithm class or read an algorithm book, as they're one of the fundamental types of algorithms. A LOT of problems can be represented as graph ones. Eg, how do you plot a path for a video game AI from point A to point B?
* Formal mathematical proofs is a topic that many programmers might not consider, but ever CS student would have to deal with. It's the idea that you need to prove something is correct in a rigid way. For programmers, this is mostly geared towards algorithms. If you come up with some algorithm, how do you know it is correct? Complex problems can be way too hard to actually manually work out the correct answer so that you can be sure that the answer your algorithm gave when you ran it was correct. And how do you know there isn't edge cases you missed? Proof techniques include things like inductive proofs, proof by contradiction, the pigeonhole principle, etc.

You'll note that I didn't mention an order here. That's because largely once you reach the HS level of math (which is a rough prerequisite for most of these things), math becomes more branched out and you can approach things in any order. That is, you can learn graph theory before calculus or calculus before graph theory. There is some dependencies in the more advanced aspects of most fields, though. For example, probability theory (part of statistics) utilizes integrals (part of calculus).

One thing I'd like to point out is that ""discrete mathematics"" is a very nifty combination of a LOT of things that are highly applicable to programming and CS. A lot are loosely related, but to find a class or book that covers discrete mathematics would cover a lot. Particularly graph theory, set theory, formal logic, formal proofs, etc.

Understanding big O and the theory behind it is one thing I think every programmer should know. Linear algebra is the next that I'd suggest because matrices just come up so much. There's even programming languages oriented around matrices for this reason (like MATLAB). I am, of course, very biased because image processing is something that I've worked on a lot and has come up at work before.

Frankly, the rest is entirely dependent on what you want to do. For a large part, calculus is easily put off until you really need it. In fact, it mostly would be useful for the theory. It's totally possible to do some degree of work in the field without knowing the actual math behind things, at least as long as you have an understanding of some of the concepts from it. The most important is almost surely the idea of a tangent line -- the line formed perpendicular to some point on a curve. Tangents would be used, for example, if you had a video game character and wanted it to follow a wall. The direction that you'd want to face at any given time is the tangent of the wall (ie, the direction that the wall faces).",5dt81o,t3_5dt81o,JobMar,,Comment,10,0,10
da7zhq4,2016-11-20 03:20:26-05:00,JobMar,,"This response is awesome and detailed, thanks a bunch for your time and reply.",5dt81o,t1_da77nbl,ACoderGirl,,Reply,1,0,1
da7yhk6,2016-11-20 02:27:59-05:00,None,,A strong algebra coursework as a minor really helped me make this a non-issue as a student.,5dt81o,t3_5dt81o,JobMar,,Comment,2,0,2
da80hzy,2016-11-20 04:18:44-05:00,JobMar,,Thanks for your response,5dt81o,t1_da7yhk6,None,,Reply,1,0,1
5dq64d,2016-11-18 20:35:41-05:00,mr_strong_opinions,"What is the best way to communicate between the front-end and back-end, with python as the back-end?","I'm trying to make a simple application with Spotify Authentication in the front-end where it asks the user for access and it would send the access token to the back-end where python will process it and send me information back to the front-end. I tried looking at the Django tutorial, but this didn't seem very intuitive to me. It looked like a huge cluster-fuck, with templates, views, models.. etc. Would AngularJS(Haven't really used this, but seems highly recommended from what i came across) be a good option for the front-end.? Is there any sample application you could direct me to, where this is exchange of information between the front and back back-end - simple user inputs, ..etc. 

Thank you.",,,,,Submission,4,0,4
da6lcfc,2016-11-18 22:53:23-05:00,skunkwaffle,,"Check out [Bottle](http://bottlepy.org/docs/dev/). It's super lightweight (just one file) and you can get an app off the ground in a couple of hours. It has no db interface so if you're going to get into that you'll need something like [Peewee](http://docs.peewee-orm.com/en/latest/).

I mention these two libraries specifically because, having known almost nothing about either at the time, I was able to build the backend for a game I was working on in about a day. ",5dq64d,t3_5dq64d,mr_strong_opinions,,Comment,6,0,6
da6lhsr,2016-11-18 22:58:16-05:00,mr_strong_opinions,,thank you very much! This is exactly what I was looking for,5dq64d,t1_da6lcfc,skunkwaffle,,Reply,1,0,1
da6o3b4,2016-11-19 00:28:15-05:00,ACoderGirl,,"[Flask](http://flask.pocoo.org/) is always my Python web framework recommendation for starting out. I feel like this is a much better idea than the Bottle that /u/skunkwaffle suggested because the code is just as simple (just look at the example on their website), yet it lets you do more and has a far stronger community.

Maybe you don't need more right now, but between the two, might as well learn the more useful tool, right? Unless you *really* need a minimalistic fingerprint (in which case why would you even use Python?), I can't see why you'd use Bottle over Flask. ",5dq64d,t3_5dq64d,mr_strong_opinions,,Comment,3,0,3
da6okvs,2016-11-19 00:47:08-05:00,mr_strong_opinions,,"Thanks for the suggestion. I was going through Bottle right now and writing some example apps in the tutorial. It seemed very simple when compared to Django, but I do want to give Flask a try as well. Doesn't hurt to learn both when I have time :) 

I was on Stack Overflow trying to see which one was better for my application and came across an answer from the creator of Flask himself :D

http://stackoverflow.com/questions/4941145/python-flask-vs-bottle

From what I understood from the answers, it seems Bottle is good for small applications and Flask is good from small to large applications. I'm not really sure if my application is going to be small or is going to expand into something bigger. ",5dq64d,t1_da6o3b4,ACoderGirl,,Reply,1,0,1
da6johs,2016-11-18 22:00:40-05:00,combuchan,,"A lot of modern web apps are written with something like Django as the backend, but providing a RESTful JSON API over HTTP for your javascript running on the frontend like Angular rather than HTML pages.

A competent app is going to need a lot of things that Django comes with out of the box.   If you choose something lightweight like, eg, flask, you're going to have to provide those things (ORM/db abstraction  layer for one) and that code organization structure.  

Your calling Django a ""clusterfuck"" because it enforces convention over configuration in an MVC paradigm is a good example of the Dunning Kruger effect.  Unless you can competently explain why these sorts of things don't work for your project, stick with them.  A lot of the real world works on it.  ",5dq64d,t3_5dq64d,mr_strong_opinions,,Comment,2,0,2
da6kd1f,2016-11-18 22:21:59-05:00,mr_strong_opinions,,"TIL about the Dunning Kruger effect. Hope it makes you feel good for assuming I'm stupid without knowing anything about me.  

Bear in mind, I did not call it a cluster-fuck. I just said it ""looked"" like one because it seemed too excessive for my needs. My application is simple and I just need to send one input to the back-end. ",5dq64d,t1_da6johs,combuchan,,Reply,0,0,0
5dl38m,2016-11-18 01:50:00-05:00,ComputerScienceStudy,Survey for Computer Science/Computer Engineering students/graduates/wanting to pursue a career in CS/CE. Results will be posted in December.,"Hello /r/AskComputerScience!


I am conducting a study on those interested in Computer Science/Computer Engineering as a career. I am just trying to understand the general opinions of those interested in CS/CE.
The survey shouldn't take too long, please note that it is mostly open ended, but you can make each answer as short or as long as you want, just looking for answers is all.


The results will be posted December 11th on a TBA subreddit (I am trying to find a subreddit where people would be interested in seeing data like this, I want to post it here but I would have to run it past the mods first). If you have any suggestions regarding that, feel free to shoot me a PM or post here.
If you have any questions post them here. 

[Link to the Google Forms Survey, this is a survey]
(https://docs.google.com/forms/d/1nmOxZGTA0KqZdNzzMwFiaKBTHLdh_b2WmN6nMNPUwV8)
This survey is being conducted by me, a nerdy redditor who is just interested in seeing CS/CE data, not being sponsored or conducted by any company, also if this violated this subreddits rules, I am sorry and the mods can feel free to delete it.
Thanks for the help guys!",,,,,Submission,10,0,10
5dk5kg,2016-11-17 21:57:22-05:00,zxy_xyz,What do you see the future with AI like?,"I tend to think about these things as well as humanity and reason for existence. I was wondering what you think about AI. I can't help but imagine that humans will become useless and robots will cary our history and create there own while replicating and finding everything out about the universe. Maybe they'll do so then just die. Makes me question why even bother sometimes, but I do think of fun positives!",,,,,Submission,6,0,6
da57rk7,2016-11-17 22:45:42-05:00,zorkmids,,"It will be a long time until an AI can out-think the smartest humans.  But AIs will provide tremendous assistance to the pursuit of knowledge.  Imagine if every person on earth had free access to a personal tutor with infinite patience that could teach them any topic or skill.  
",5dk5kg,t3_5dk5kg,zxy_xyz,,Comment,4,0,4
da5dm2p,2016-11-18 01:48:44-05:00,watsreddit,,"Actual AI research in computer science is less fanciful than it sounds. Generally speaking, it's about the programmatic techniques we can use to emulate independent behavior, but really that's just an abstraction of what's actually going on. Much of it boils down to techniques such as stochastic algorithms or other forms of nondeterminism. Much of AI research is focused on things like search, constraint satisfaction, and optimization. Of course it has plenty of overlap with many other disciplines, such as the aforementioned constraint satisfaction problems or machine learning (which itself is basically a bunch of statistics with some algorithms on top). Really, AI is not really all that different from other disciplines of computer science. ",5dk5kg,t3_5dk5kg,zxy_xyz,,Comment,1,0,1
da5k5bn,2016-11-18 07:33:34-05:00,zxy_xyz,,"Ah, Interesting. I'm thinking REALLY far into future about this though. I'm interested in getting into it either way",5dk5kg,t1_da5dm2p,watsreddit,,Reply,1,0,1
da78gs0,2016-11-19 14:06:00-05:00,ACoderGirl,,"Predicting the future is very difficult. I quite strongly believe that at some point of time, we'll figure out strong AI. But when is anyone's guess.

The near future would mostly be about AI that assists people, but depends on human guidance or authorization. We simply cannot depend on AI's accuracy or reliability in most fields. Semi-automatic image segmentation is an interesting example. Manually segmenting an image is incredibly slow. Automatic segmentation, however, is quite horrible. Semi-automatic does much better. There a human annotates the image to guide the algorithm. Not to mention that we can do things like review the results, maybe modify the segmentation slightly. All in all, the algorithm helps us, but it can't do things on its own.

That's the case with a lot of AI really. And even with things as revolutionary as the self driving car, they're gonna need human help with tricky stuff for some time.",5dk5kg,t3_5dk5kg,zxy_xyz,,Comment,1,0,1
da59j97,2016-11-17 23:31:45-05:00,Heavy_Weapons_Guy_,,How did you know about me and Al?? We were so careful to keep it a secret!,5dk5kg,t3_5dk5kg,zxy_xyz,,Comment,0,0,0
da5a2m1,2016-11-17 23:46:04-05:00,zxy_xyz,,A little birdie told me. Then i realised the birdie was made in China!,5dk5kg,t1_da59j97,Heavy_Weapons_Guy_,,Reply,0,0,0
5dhhek,2016-11-17 13:41:24-05:00,sparr,Small simple implementation of RAR/PAR-like parity system?,"In the dark ages of low-bandwidth illicit file downloading, it was common to find a large file split into many smaller files using the RAR archive tool, on ftp servers or usenet or elsewhere.

To combat the problem of single files becoming unavailable, solutions like http://www.quickpar.org.uk/AboutPAR2.htm were developed. A lot of math is applied to the split files to produce ""parity files"" which can be used to replace any individual lost file(s).

Is there a simple / free / reusable implementation of this sort of solution? Something that I can pass an arbitrary block of text or data into and get back M chunks of which I only need N to reconstruct my text, without all the file format complexity of RAR or ZIP?",,,,,Submission,4,0,4
da4q5n9,2016-11-17 16:01:41-05:00,teraflop,,"The keyword you're looking for is [""erasure coding""](https://en.wikipedia.org/wiki/Erasure_code). It's not trivial to implement from scratch (decoding requires Gaussian elimination over a finite field) but there are libraries that give you the low-level operations without requiring a specific file format. A few examples:

http://jerasure.org  
https://github.com/tahoe-lafs/zfec  
https://github.com/catid/cm256",5dhhek,t3_5dhhek,sparr,,Comment,4,0,4
da4q7o7,2016-11-17 16:02:50-05:00,sparr,,"Thanks, I'll look into that.",5dhhek,t1_da4q5n9,teraflop,,Reply,1,0,1
5dgz7s,2016-11-17 12:15:17-05:00,esboardnewb,Why does apple mail essentially freeze up all of my computers resources while opening?,"I'm on a video editing (Avid MC) workstation mac pro (2 x 2.4 GHz Quad-Core Intel Xeon w/ 16GB memory) but when I open apple mail I literally can't open other apps or even really use any that are already open for about 30 secs while mail opens and catches up on incoming emails, WTF? 

Processors aren't really engaged, memory doesn't seem taxed (can see their usage in an app called menu meters) anyone have any idea whats going on here? 
",,,,,Submission,0,0,0
da4j70x,2016-11-17 13:41:25-05:00,andybmcc,,"Ask IT, this isn't a computer science question.",5dgz7s,t3_5dgz7s,esboardnewb,,Comment,3,0,3
da4jvs1,2016-11-17 13:55:16-05:00,juuular,,"Apple mail is really poorly coded, is the only real explanation I can come up with. I've never had it work satisfactorily for me.",5dgz7s,t3_5dgz7s,esboardnewb,,Comment,2,0,2
dac4yls,2016-11-23 01:40:35-05:00,plantice,,"while no where near a IT person that knows what is truely going on, I can say that this happens in Windows as well. I believe it has to do with the fact that your OS is trying to fullfill the request and wont do anything until it has (apple tends to be singlar focused for performance reasons). As such, the program loads and with it everything it needs (database, gui, so forth) and if it is indeed poorly coded that would add to the lag. Do you have alot of mail? Is the mail in the cloud? Maybe a poorly indexed database? As you can see there are a ton of reasons.",5dgz7s,t3_5dgz7s,esboardnewb,,Comment,1,0,1
5ddqmp,2016-11-16 22:44:39-05:00,oracular_demon,Does anyone have information/resources on Linear Genetic Programming?,,,,,,Submission,3,0,3
5dbgst,2016-11-16 15:22:06-05:00,hysterian,Does DKIM really verify Clinton's emails as her emails?,"I noticed that most reporting on this were independent bloggers, I also noticed that in the blog I read that all they pointed out was that the emails matched the DKIM, date, addressee, and subject, but does that mean that's all that's publicly verifiable and the bodies of the emails can be edited without proof of the edit?",,,,,Submission,10,0,10
da45yvp,2016-11-17 08:51:42-05:00,galadran,,"The DKIM signatures prove that the email was sent from the origin domain and that neither the header nor the body have been tampered with. Consequently if you have a very large dump of emails over a long period with DKIM signatures, you can be reasonably sure that if the signatures valid (easy to check) the contents of the emails is authentic. 

However, if you only have a single email / small number of emails from a limited timespan, it is possible someone compromised the server and stole the private key, which would then allow them to create a DKIM signature for whatever email they wanted. 

Note that in the Clinton case, not only are there a large portion of emails from a considerable time period, but many of the emails were sent from Google administered accounts which are signed by Google's DKIM key. It is highly unlikely Google's DKIM key has been compromised, and if it was there would be a great deal more mischief that could be done. 

tl:dr - DKIM signed emails from the Clinton dump are almost certainly authentic.",5dbgst,t3_5dbgst,hysterian,,Comment,5,0,5
da3dm83,2016-11-16 17:23:51-05:00,rewardiflost,,"Not really a CS thing.  

DKIM as it is commonly used is a crypto signature to verify the origin domain.   While it can be used to assure the identity of an individual sender, it tends to be avoided because it can confuse come recipients that aren't aware of it.   
Email can be signed to authenticate content, too.  

I have no idea if the Clinton emails used any of these technologies.  Since you only mention DKIM, I'm assuming they were just using the domain authentication. 


Unless the email itself was signed or at least hashed, the content (and the header fields) can all be edited or forged without anyone knowing.    

If one had access, they could verify things like time, date, sender & domain and filesize/mailsize by examining sender and recipient server logs.   
",5dbgst,t3_5dbgst,hysterian,,Comment,3,0,3
da3dw22,2016-11-16 17:29:28-05:00,hysterian,,"Yes it just verified the domain it was sent to + from, the time of the email, and the subject. They used some sort of public search for DKIM, this is the only method I have read that has been used in the process of verifying the emails.

So you're saying DKIM doesn't really prove anything about the legitimacy of an e-mail's content?",5dbgst,t1_da3dm83,rewardiflost,,Reply,3,0,3
da3vpp3,2016-11-17 00:56:16-05:00,rangedDPS,,"In this case DKIM verifies that the emails were sent via the hillaryclinton.com domain and signed by the private key stored by the email server -OR- that someone was able to steal that key and/or send mail directly from that server.

The body of the email was absolutely signed ( as well as the headers ). Per wikipedia:
https://en.wikipedia.org/wiki/DomainKeys_Identified_Mail
""Both header and body contribute to the signature.""

DKIM does in fact verify the body of the email. We also know from the Podesta email leak that the signing is correct between multiple exchanges from hillaryclinton.com to gmail.com and back again. Both have DKIM verification turned on. In this kind of exchange both keys would need to be compromised. 

",5dbgst,t1_da3dw22,hysterian,,Reply,5,0,5
da3e3lg,2016-11-16 17:33:51-05:00,rewardiflost,,">  So you're saying DKIM doesn't really prove anything about the legitimacy of an e-mail's content?

  
Correct.   DKIM is an assurance that you can trust the sender to be whom they claim to be.    It isn't perfect, but it does stop a lot of forged email headers from getting through (if implemented correctly).   

[DKIM is listed among the IETF rules/RTFs.. reference here](http://www.dkim.org/)  ",5dbgst,t1_da3dw22,hysterian,,Reply,1,0,1
da3vs32,2016-11-17 00:58:31-05:00,rangedDPS,,DKIM includes the headers and message body in it's signature generation.,5dbgst,t1_da3e3lg,rewardiflost,,Reply,4,0,4
da3y3jm,2016-11-17 02:33:02-05:00,rewardiflost,,"Thanks!  TIL. 

",5dbgst,t1_da3vs32,rangedDPS,,Reply,2,0,2
da3jr1g,2016-11-16 19:42:30-05:00,bgeron,,"If her server was indeed hacked into, then DKIM proves exactly nothing.",5dbgst,t3_5dbgst,hysterian,,Comment,5,0,5
da461dk,2016-11-17 08:53:46-05:00,galadran,,DKIM signatures are generated by the sending server. Consequently a compromise of Clinton's server would only affect emails sent from that server. Many of the emails in the dump originate from other domains which have different DKIM keys e.g. Google/Gmail. The validity of these DKIM signatures holds whether or not Clinton's server was compromised. ,5dbgst,t1_da3jr1g,bgeron,,Reply,5,0,5
5db06w,2016-11-16 14:02:33-05:00,sweezinator,Can a windows virus damage a linux partition on the same hard drive in any way?,,,,,,Submission,7,0,7
da356y0,2016-11-16 14:33:44-05:00,look_its_a_squirrel,,"That all depends on what the virus is programmed to do. It certainly could be possible. Let me ask you a question. When you're booted into windows is it possible for you to see the physical disk for the linux partition in disk management? If the answer is yes, than a virus programmed to attack hidden volumes could probably damage it. It may not be able to read or write data to the linux volume because it's a different file system but that doesn't mean that a virus couldn't be created with the goal of formatting all hidden or inaccessible disk volumes on a windows system.",5db06w,t3_5db06w,sweezinator,,Comment,5,0,5
da3hvve,2016-11-16 18:57:47-05:00,INCOMPLETE_USERNAM,,"To clarify, a virus *can* be programmed to interact with a non-native filesystem.",5db06w,t1_da356y0,look_its_a_squirrel,,Reply,4,0,4
da3ikr4,2016-11-16 19:14:06-05:00,Bottled_Void,,Yes. Not many will though.,5db06w,t3_5db06w,sweezinator,,Comment,5,0,5
5d778i,2016-11-15 23:02:12-05:00,zxy_xyz,Is there a stigma with open source i dont know about?,"Youngster here in HS who finally figured out passion for CS (even after years of programming and learning about electronics). So I usually write ideas down and script up some of the simple ones. 

But im curious why you dont see people like zuckerburg on github. This one guy i knew online thought open source was dumb but gavr no reasoning and i think it's pretty useful. I dont look into many bigger projects source though but is there something about sharing code that turns people off? Idea stealing or something? Linus torvalds sorta needs one but had only 2 projects on there. Is useful sourcecode usually better for research papers? Just wondering. But I do see things look google or spaceX on github, it's just certain people i wonder why not.",,,,,Submission,8,0,8
da2e7fr,2016-11-16 00:23:30-05:00,ISvengali,,"[Facebook](https://code.facebook.com/projects/) shares a huge amount of stuff open source, even open source hardware.  I believe Zuckerberg has some stuff on there also, but Im not positive.  Twitter, Netflix, and even Microsoft have a ton of OS stuff also.  

Just with all that stuff you could build a cloud enabled, auto scaling^[1] server stack in a very short amount of time.  In fact, a group at a place I work did just that.  

Github isnt the only place to find open source software.  

---
^[1] A little buzzwordy, I know.  ",5d778i,t3_5d778i,zxy_xyz,,Comment,6,0,6
da2yug2,2016-11-16 12:26:44-05:00,Coffee_Revolver,,"It's an abstract idea, not a buzzword",5d778i,t1_da2e7fr,ISvengali,,Reply,1,0,1
da2e7ig,2016-11-16 00:23:34-05:00,spongebue,,"It mostly comes down to money and what a company has to gain or lose by opening its source code.  Linux started as a hobby system, so there was nothing to lose by opening the source code.  When you have companies like Red Hat, they are largely taking existing open source software, packaging it up in a way that they are allowed to in the licensing agreement, and mostly making money on *support* for the software.

On the other hand, software like Microsoft Windows or Office, Photoshop, up to more advanced things like IBM's Watson are developed by in-house developers, and the code is seen as a trade secret that they wouldn't want to get out.  If they opened the source code, they would have very little to gain in comparison to what they have to potentially lose.",5d778i,t3_5d778i,zxy_xyz,,Comment,3,0,3
da2ezrt,2016-11-16 00:50:02-05:00,high_side,,"This may or may not answer your question:

* Some people/companies prefer to keep their IP private, sometimes out of greed, sometimes for good reason (e.g. it is genius, application specific, easily exploitable, etc).
* Open source has burned a lot of companies because they use it but run afoul of the licensing terms and get sued. It can be a dirty word in the corporate environment.
* A lot of freelancers depend on their github as a resume. For better or worse, it's less important for established professionals. It could be conflated with amateurishness.
* Iirc, github blew up a bit after facebook, so there's no reason for Zuckerburg to have a personal account. 
* Open source, for businesses, is either a way to get free software (at the cost of sometimes having to give back) or a way to push your product ""AWS tools are on github!!!"". There's no kumbaya shit.",5d778i,t3_5d778i,zxy_xyz,,Comment,1,0,1
da2klzy,2016-11-16 05:23:05-05:00,DenProg,,"Well his company and most of the other elite software companies support open source projects through the Apache Softtware foundation, and they made React open source. 

I would imagine he doesn't have time and if he does contribute doesn't use his name since that would overshadow any projects he contributed to.There would be people who loved his code just b/c he wrote it and those who would criticize it just b/c he wrote it.
",5d778i,t3_5d778i,zxy_xyz,,Comment,1,0,1
da2svzl,2016-11-16 10:23:25-05:00,netherlandsg,,"I was wondering, is the stigma about Linux users being the kind of people who would be unwilling to pay money for software damaging our chances of obtaining more commercial software/games?

The stigma is still definitely there, I just don't know if it's getting better or worse. I wouldn't mind paying for software, but if companies who are considering releasing Linux versions of commercial products are given the impression that we're such zealots that we'd never bother with software that costs money, then they might not bother.",5d778i,t3_5d778i,zxy_xyz,,Comment,1,0,1
da2t903,2016-11-16 10:31:24-05:00,zxy_xyz,,They should definatly do Linux ports if they can. I dont see why we shouldnt have a choice at least if thats the case,5d778i,t1_da2svzl,netherlandsg,,Reply,1,0,1
da2tejk,2016-11-16 10:34:44-05:00,zxy_xyz,,They should definatly do Linux ports if they can. I dont see why we shouldnt have a choice at least if thats the case,5d778i,t1_da2svzl,netherlandsg,,Reply,1,0,1
da2unkx,2016-11-16 11:01:17-05:00,BonzoESC,,"Commercial desktop and gaming software tends to be expensive to build, and especially for games, making it work right on a single OS and hardware configuration (i.e. Windows with an nVidia GPU running modern drivers) is a huge investment. There's a considerable risk in spending a similar investment on making it work on another platform, such as Windows with an ATI GPU running modern drivers, or Mac with an Intel ""GPU"". If the developer doesn't believe that the expense and opportunity cost will generate more revenue than the alternatives, they probably won't bother.

This cuts both ways too. At my previous job, we worked on database software that we distributed for Debian, RHEL, Ubuntu, Solaris, and SmartOS. We occasionally had potential customers ask about Windows, but the expected revenue from Windows didn't justify the extra costs in porting very UNIX-dependent file manipulation code to Windows, building and licensing a Windows test cluster, hiring Windows-friendly support staff, and writing Windows documentation.

Operating systems aren't religions, they're tools. I don't like Windows, prefer not to use Windows, but if I want to use software that only runs on Windows (recently: MGS5, Doom 4), I'll still use it.",5d778i,t1_da2svzl,netherlandsg,,Reply,1,0,1
5d301n,2016-11-15 10:15:04-05:00,Fawxhox,Memory Content (Java),"On the study guide for my Data Structures midterm there's these two questions. I answered the first which I think is correct, but I'm not sure on the second.

1. Scanner kb;

2.   kb = new Scanner(System.in);

3.   int x;

4.   x = 5;


Describe the memory cell kb and its contents after executing line 1

Kb is allocated to hold a scanner.


Describe the memory cell kb and its contents after executing line 2

Kb points to a new scanner taking its input from the keyboard.


Describe the memory cell x and its contents after executing line 3

X is cleared to hold an integer worth of bits

Describe the memory cell x and its contents after executing line 4

X can still hold an integer, and the integer it holds is 5.


1.   Chicken farm[];

2.  farm = new Chicken[5];

Describe the memory cell farm and its contents after executing line 1

Describe the memory cell farm and its contents after executing line 2

Describe any other interesting memory cells after executing line 2
",,,,,Submission,1,0,1
da1i0ej,2016-11-15 12:33:43-05:00,urielsalis,,"First part is correct(although in java I think it doesnt get assogned memory until its initialized. For the second part, you are creating 5 memory cells for the array when you initialize it of size Chiken and youbassign them to null",5d301n,t3_5d301n,Fawxhox,,Comment,2,0,2
da1nsze,2016-11-15 14:29:24-05:00,visvis,,"> Describe the memory cell kb and its contents after executing line 1
>
> Kb is allocated to hold a scanner.

For C++ this would be correct but objects in Java are always by reference. The system allocates space to store a pointer to a Scanner object and initializes it to NULL. There is no actual Scanner object allocated.

> Describe the memory cell kb and its contents after executing line 2
>
> Kb points to a new scanner taking its input from the keyboard.

System.in need not be the keyboard (can be redirected) but otherwise correct.

> Describe the memory cell x and its contents after executing line 3
>
> X is cleared to hold an integer worth of bits

Yes.

> Describe the memory cell x and its contents after executing line 4
>
> X can still hold an integer, and the integer it holds is 5.

Yes.

> Describe the memory cell farm and its contents after executing line 1

A pointer to an array of pointers to Chicken objects, initialized to NULL.

> Describe the memory cell farm and its contents after executing line 2
>
> Describe any other interesting memory cells after executing line 2

A pointer to an array of pointers to Chicken objects, initialized to point to an array of five pointers that are all NULL.

",5d301n,t3_5d301n,Fawxhox,,Comment,2,0,2
da1t5da,2016-11-15 16:15:44-05:00,drummer_ash,,"All correct, but the word pointer is a bit of a no-no for Java, so I'd say just replace that with ""memory reference"" or just ""reference"" and it's all good. ",5d301n,t1_da1nsze,visvis,,Reply,1,0,1
da38glc,2016-11-16 15:39:26-05:00,Fawxhox,,"I get this mixed up all the time, I'm learning C and Java and though I learned Java first, I seem to think in terms of C more.",5d301n,t1_da1t5da,drummer_ash,,Reply,1,0,1
da30kku,2016-11-16 13:01:02-05:00,ixampl,,"This is a bit tricky.

Considering the bytecode level and the JVM as a simple interpreter of such bytecode: On the call stack, space is (at the beginning of the method call) allocated for every local variable that is actually assigned to in a method body (as well as explicit parameters as well as ""this"" unless it's a static method), however keep in mind that sometimes slots are reused. So it's not as easy as just counting the variables. Also, long and double variables get twice as much space as the other types, such as int or object references.

A local variable declaration on its own (without ever assigning to the variable) is only a compile time construct. If the variable is never used there won't be any call stack space alotted for it. Also, both the compiler (when compiling) as well as the JVM (when class loading) make sure that every local variable has been explicitly assigned to before it is used (definite assignment analysis). So in fact there is no guarantee or need for a default value to be fixed, like 0, since it is irrelevant.

Heap memory is (at least on the managed JVM heap) only allocated for objects when they are instantiated.

Now the tricky part is, that while the above is kind of true when it comes to the bytecode level (though things like stack space reuse etc. may depend on the compiler version) how much space is actually used at run time and how/when will depend on whether and how the method has been further optimized by the JIT compiler. Things become fairly unpredictable once you reach that level.

(However, I am sure that's not the level OP is actually being quizzed about.)

For the chicken array thing, one could say the size on the heap is 6 units (and 1 on the method's call stack) where one unit is the size of the non-long/double types. 1 for the (int) length of the array and 5 references initialized to null.

But here the truth again is that besides the length there are probably some meta fields on the array object that take up some more space.


P.S.
Frankly, without clarifying what level and what kind of memory model is to be applied to the given questions it's pretty hard to give ""the"" answer.

My answer would be that, based on the produced bytecode/class file, at line one (and line zero if you will) the memory cell ""farm"" (some slot in the methods local variable array) is the size of an object reference but with undefined content. (Keep in mind that this can be disputed depending on what abstraction level you consider the memory model. Some may say that farm at line one should not be considered to occupy space.)

At line two the size is the same, but now it contains a reference to an array object.",5d301n,t3_5d301n,Fawxhox,,Comment,2,0,2
da1dvfr,2016-11-15 11:10:06-05:00,121510291029,,"I am not 100% sure and it's been a while since I have taken data structures but I'll give you my guess: many objects in Java do not get allocated memory until they get initialized. So for the line ""Chicken farm[]"" I do not think any memory is allocated. When you initialize it in the next line its size is 5* the size of the object chicken. For the integer x, I believe it is initialized as 0 implicitly so memory should be allocated for that in the line ""int x"". I can't be sure about the scanner ",5d301n,t3_5d301n,Fawxhox,,Comment,1,0,1
5czkdh,2016-11-14 19:40:53-05:00,adbJ114,"Equivalent to Petzold's ""Code"" for the Internet or Operating Systems?","Hey Everyone,

I just finished Charles Petzold's ""Code"" and found it to be one of the most interesting books on computer technology I've ever come across. Are there equivalent books like this about Operating Systems or the Internet?

Specifically what I liked about Code: 

* The way Petzold covers the entire history of the computer, going back to it's spiritual predecessor, the telegraph key

* The fact that he never oversimplifies concepts to make them more easily understandable. He takes the time to build up the readers' understanding, piece by piece

* There is a perfect amount (IMO) of historical context, including short bios on some of the major pioneers that made breakthroughs along the way and companies that played significant roles in advancing computer technologies

* The storytelling aspect of the book. This is key for me. One of my favorite things about Code was that every section flowed into the next, and the human side of computer technology was never ignored 

* a good amount of theory, followed up by how the theory comes into play in the real world, e.g. when he talks about the binary adding machine, and moves into the Altair and other microcomputers

What books would guys recommend? I am computer literate, so I don't need books that talk down to me, and I definitely don't want anything that oversimplifies complex concepts. I'm interested in books that take their time, and go down to the bit and switch (heh, bit and switch) level, when necessary, but still make the journey interesting.

Thanks in advance for any recommendations!",,,,,Submission,8,0,8
da1dmtx,2016-11-15 11:04:59-05:00,HenryJonesJunior,,Andrew Blum's [Tubes](https://www.amazon.com/Tubes-Journey-Internet-Andrew-Blum/dp/0061994952/) tries to cover much of the same ground for the physical layer of the Internet.  I found it an interesting read.,5czkdh,t3_5czkdh,adbJ114,,Comment,1,0,1
da1j1ea,2016-11-15 12:54:13-05:00,adbJ114,,"I will check it out, thanks!",5czkdh,t1_da1dmtx,HenryJonesJunior,,Reply,1,0,1
da46hk4,2016-11-17 09:06:43-05:00,BonzoESC,,"I haven't read ""Code,"" but Paul Ford's ""[What is Code?](http://www.bloomberg.com/graphics/2015-paul-ford-what-is-code/)"" in Bloomberg last year is excellent.",5czkdh,t3_5czkdh,adbJ114,,Comment,1,0,1
da6o1e0,2016-11-19 00:26:19-05:00,adbJ114,,Thanks!,5czkdh,t1_da46hk4,BonzoESC,,Reply,1,0,1
5cz6ne,2016-11-14 18:27:45-05:00,alexschjoll,Live in a rural neighborhood and needing a good ISP.,"I have internet over air I guess is what they call it because no other ISP has cables going out to my neighborhood.
I'm trying to find out how I or we as a neighborhood can have an ISP set lines out to us? I know that century link has phone lines but nothing else out here. When ever I call century link or any other ISP they always say the same thing. ""We don't know when we will have service in your area.""
What are my options?",,,,,Submission,0,0,0
da0l0bo,2016-11-14 19:28:18-05:00,INCOMPLETE_USERNAM,,Https://en.wikipedia.org/wiki/Computer_science,5cz6ne,t3_5cz6ne,alexschjoll,,Comment,1,0,1
5cyzue,2016-11-14 17:52:03-05:00,WarHawk098,Queueing Theory/Little's Law Question,"There's a problem in my textbook that states, ""Using Queueing Theory and Little's Law, Explain how a single-core CPU 2x faster would be preferable to keeping the speed the same and adding another core"".

Any thoughts? I'm not sure how a single-core CPU that's 2x faster would be better.  ",,,,,Submission,1,0,1
da0lkue,2016-11-14 19:41:36-05:00,arbitrarycivilian,,Compare the two systems when there is only a single job being processed ,5cyzue,t3_5cyzue,WarHawk098,,Comment,1,0,1
5cxf4t,2016-11-14 13:17:12-05:00,1pecman,Essay in an application of computer science,"Struggling to find a topic, - have to write an essay explaining a certain application of computer science to a problem and finding it hard to find something to write about.
If anyone could suggest anything that would be great",,,,,Submission,1,0,1
5cwrae,2016-11-14 11:20:04-05:00,einosensei1,Can i save the pages of my digital book somehow?,"Can i save the pages of my digital book somehow? I have school books that are on the internet on site i must login into. I know i could just screenshot every single page, but is there any other way to do it? Can i somehow save all the pages from the websites source code or something?",,,,,Submission,0,0,0
da0einh,2016-11-14 17:03:59-05:00,INCOMPLETE_USERNAM,,I hope you aren't a computer science student. What browser do you use?,5cwrae,t3_5cwrae,einosensei1,,Comment,2,0,2
5cwmmr,2016-11-14 10:56:20-05:00,jored1990,Advice for Literature Review,"I'm in my first year, and about to write my first literature review. I was just wondering if anybody could suggest directions I might go in to address the following question:

'Mitigating personal information exposure on the web'",,,,,Submission,2,0,2
da1qsip,2016-11-15 15:28:31-05:00,visvis,,"This subject is too broad without specifying a field you're doing this in? The solutions look completely different based on that (just some ideas):

* HCI: how do you design a user interface that properly warns users about exposing personal data?
* AI: how do you recognize personal information automatically?
* Software Engineering: how do you design a system that properly isolates personal data and ensures it is only exposed to the right users?
* Computer systems: how do you prevent security leaks that expose personal data?
* Theoretical computer science: using encryption to ensure confidentiality/integrity of personal data transit",5cwmmr,t3_5cwmmr,jored1990,,Comment,2,0,2
da2zilr,2016-11-16 12:40:02-05:00,jored1990,,"Thanks for your post, i'm in the introductory portion of the subject and I agree the question is very general. Those are interesting topics you posted, I appreciate it. ",5cwmmr,t1_da1qsip,visvis,,Reply,1,0,1
5cvkk5,2016-11-14 06:54:24-05:00,souldust,"I just woke up from a dream where I taught/created a CompSci question, but I've never taken a CompSci course in my life. Is the following a legitimate CompSci question?","Disclaimer/Context: I have never taken a Computer Science course, nor a discrete math course, but I have done programming and I have tried several of the challenges offered by ProjectEuler.net - I also understand that ""does P=NP?"" pretty much means ""Is there a better algorithm for this computation?""

I was dreaming that I was teaching a Computer Science class, and I would write 100/97 on the board and tell the students to write out the most efficient code for dividing these two numbers.  Then 100/96, 100/95, then weird ones like 77/55.  Then I told the students to come up with at least 3 division algorithms, and then plot which algorithm was the best/most efficient one for 1000/x (x < 1000).  And then asked is it possible to ""predetermine"" which algorithm to use given x.

Then I woke up, astonished by my sleeping brain.

**Question 1**: Is it true that some algorithms are better than others for certain numbers?

**Question 2**: Is this a legitimate/interesting Computer Science question?

**Question 3**: IS IT possible to predetermine which algorithm to use given certain numbers?

I didn't know where to post this question.  I was going to post it to /r/CasualMath, but they're not THAT casual.  There is no /r/CasualCompSci.  Do you know of a better place for me to be asking this?

(Why was I dreaming of computer science?  In my dream I was playing inside of a hyper realistic 3D engine running on an Nintendo 8bit system. (impossible I know)  This girl wanted to compete with the guy who made it because of class prestige and I was like ""naw thats boring, its just like finding faster ways to divide"" and she was all huh? and I was all...)

Thanks Reddit",,,,,Submission,4,0,4
d9zw4hs,2016-11-14 10:46:37-05:00,andybmcc,,">Question 1: Is it true that some algorithms are better than others for certain numbers?

Yes!  An easy example is performing integer division on a power of two.  e.g. X / 2^n .  This just translates to a simple bit-shift operation.

>Question 2: Is this a legitimate/interesting Computer Science question?

There are a lot of different division algorithms, this would be a valid question.  These can be hardware or software solutions in practice.  There are always trade-offs (accuracy, precision, speed, space, etc).

>Question 3: IS IT possible to predetermine which algorithm to use given certain numbers?

A lot of modern compilers will do this.  For example, if there is a division by a constant floating point number, the compiler may change it to be a multiplication by a constant that is the reciprocal.  Integer division by a constant power of two is usually changed to a bit shift operation.  More generic means of calculation are used when the values are only known at runtime.
",5cvkk5,t3_5cvkk5,souldust,,Comment,5,0,5
5cu1ed,2016-11-13 23:01:48-05:00,triadlink,"[Serious] I'm looking at doing a program in Poland for a BSc in EIT. Based on the classes in this course, will this prepare someone for a Software Enginneer/Web Developer role?",,,,,,Submission,2,0,2
5cqfjo,2016-11-13 10:39:16-05:00,GurdonFremon,I came across a strange pattern in computation time in an algorithm I wrote. I'm curious to know what's going on here.,,,,,,Submission,26,0,26
d9yig0u,2016-11-13 10:56:21-05:00,None,,[deleted],5cqfjo,t3_5cqfjo,GurdonFremon,,Comment,13,0,13
d9zmt7o,2016-11-14 04:45:18-05:00,GurdonFremon,,"Thank you, but I'm confused as to how these results relate to processor architecture and the like. 

If I were to conduct this algorithm manually with a pencil and a piece of paper -- drawing points, connecting them with a ruler and measuring them repeatedly -- do you think I would get differing results?

I will investigate that further though, and maybe run it on my Windows 95 to see how that processor handles it.",5cqfjo,t1_d9yig0u,None,,Reply,1,0,1
d9zwkhb,2016-11-14 10:56:41-05:00,andybmcc,,"> I'm confused as to how these results relate to processor architecture and the like.

That's because it doesn't.  I don't know why that has so many upvotes.",5cqfjo,t1_d9zmt7o,GurdonFremon,,Reply,1,0,1
d9yvhsn,2016-11-13 15:53:20-05:00,Bosco89,,"Just to be sure, did you check if anything changes with different seeds or different pseudorandom generator functions?

Edit: found it! Your target length is always rounded down to an integer:

    target = 1000 / (nodeCount - 1):",5cqfjo,t3_5cqfjo,GurdonFremon,,Comment,10,0,10
d9zmz40,2016-11-14 04:55:20-05:00,GurdonFremon,,"Thank you for spotting that bug! I fixed it, and am still getting the exact same results though...

As for the pseudorandomness, I did run the program twice more after writing the blog post - still producing the same shape (with your fix, of course).

I even forced the seed to a bunch of different numbers, all giving me the same shape series:

* [seed = 1](http://imgur.com/C9fHzZ6)

* [seed = 420](http://imgur.com/ku4ZLPT)

* [seed = 22222222](http://imgur.com/NcBIGyO)

* [seed = 22222221](http://imgur.com/jaex83x)
",5cqfjo,t1_d9yvhsn,Bosco89,,Reply,1,0,1
d9zqn67,2016-11-14 08:08:21-05:00,Bosco89,,"Are you sure of your fix? The peaks in computation steps seem to roughly match 1000/x for integer x (500, 333, 250, 200...)",5cqfjo,t1_d9zmz40,GurdonFremon,,Reply,1,0,1
da0uq37,2016-11-14 23:18:47-05:00,GurdonFremon,,I'm certain my target lengths are not rounded down [(I got the program to print them — decimals present)](http://i.imgur.com/u0AxDf1.jpg) but you are very correct about the integer x pattern! I let it run for 11hrs to go from 500 nodes to 1000 and got [this beauty.](http://i.imgur.com/8JrKBTU.jpg) Thank you!,5cqfjo,t1_d9zqn67,Bosco89,,Reply,2,0,2
dam26gj,2016-11-30 09:47:30-05:00,_HyDrAg_,,What ide is that?,5cqfjo,t1_da0uq37,GurdonFremon,,Reply,1,0,1
dam8dqj,2016-11-30 11:57:56-05:00,GurdonFremon,,"Screenshot 1 is Sublime Text with Material Theme and IBM PxPlus font, second one is just GNU Octave in the background ",5cqfjo,t1_dam26gj,_HyDrAg_,,Reply,1,0,1
damacoc,2016-11-30 12:36:00-05:00,_HyDrAg_,,I didn't realize sublime had themes even though I use it. Thanks!,5cqfjo,t1_dam8dqj,GurdonFremon,,Reply,2,0,2
d9yhw7a,2016-11-13 10:40:03-05:00,GurdonFremon,,Apologies if this is the wrong sub. I swear this isn't a homework assignment,5cqfjo,t3_5cqfjo,GurdonFremon,,Comment,3,0,3
d9yi3z9,2016-11-13 10:46:36-05:00,featheredsnake,,That's intriguing! I'm interested to hear what you find out,5cqfjo,t3_5cqfjo,GurdonFremon,,Comment,3,0,3
5coxu1,2016-11-13 02:20:42-05:00,pakaraki,Technology selection: read-only filter & view large table of data,"Can anyone suggest a simple elegant solution to this task?

We have a flat table of data (6 columns, 700,000 rows) of text, about 60MB of data in a csv file (or 100MB as a dbf file). I'm after a user friendly interface that enables a non-technical person to filter on 1 or more columns and then view or print the result. The data should be read only to the user. It is an index of a club's library of documents and records.

This is to run on a standalone PC running Windows 10. It doesn't have MS Office installed. I was thinking about a big html table which could be filtered with jquery or similar, but it looks like there is just too much data for this to work well. It would be good to have a simple solution, and avoid having to develop any bespoke software.

The software I've found on the web for dbf viewers have complicated user interfaces and can edit the data. (We don't want users to inadvertently change the data.) 

Thanks for your help.",,,,,Submission,1,0,1
5cnohc,2016-11-12 20:32:49-05:00,Tofik759,Algorithm for finding how many areas are inside of exactly 3 circles with given center and radius.,"Hi,

as I wrote in title, I have a task where i have any number of circles. All i know is their center and radius. Now I need to find the number of areas which are overlapped by exactly 3 circles. I tried to solve it knowing that circles overlap when distance between their centres is shorter than sum of radiuses, but it got me nowhere. 

Could You help me a little?",,,,,Submission,6,0,6
d9xyz1d,2016-11-12 21:56:46-05:00,magikker,,"Start with a working brute force solution then optimize from there. 

Two circles overlap if they share if they have two points in common. Those two points and the radii give you the over lapping segments. If a third circle shares a point with each segment there is a triple overlap. 


",5cnohc,t3_5cnohc,Tofik759,,Comment,2,0,2
d9yg41e,2016-11-13 09:41:45-05:00,singham,,"First try the brute force method with 3 nested forloops.

http://cs.stackexchange.com/questions/1466/circle-intersection-with-sweep-line-algorithm

Find list of pairs of circles which intersect. Make sure the list is sorted by centers (sort on min(x_1, x_2) if equal on min(y_1, y_2)).

Now you need to find three pairs from the list such that they mutually overlap. This will give you areas which are overlapped by 3 circles.
But it might be possible that fourth circle could overlap this area (you mentioned that you need areas *exactly* overlapped by 3 circles). 

Search for other circles which might overlap with at least two of the above circles from the list of pairs. See how this new circle cuts the overlapped area. Based on that, decide whether to count the area or not.
 

",5cnohc,t3_5cnohc,Tofik759,,Comment,1,0,1
5cle0x,2016-11-12 12:27:22-05:00,453523523,"For the activity selection problem, why can't we just select the activities which interfere with the fewest other activities possible? Why is selecting my finish time optimal but this not?","For all setups I try, I can't see any way for selecting activities by the number of other activities they interfere with to not produce an optimal solution. However I have been told that it isn't a suitable greedy solution. Why is this?",,,,,Submission,2,0,2
d9xed4z,2016-11-12 13:13:11-05:00,tbrownaw,,Does the wikipedia page help?,5cle0x,t3_5cle0x,453523523,,Comment,1,0,1
d9xf3ut,2016-11-12 13:31:27-05:00,453523523,,The wikipedia page only talks about the finish time first solution.,5cle0x,t1_d9xed4z,tbrownaw,,Reply,1,0,1
d9xhkva,2016-11-12 14:33:57-05:00,tbrownaw,,"Can you construct a set of tasks where the two solutions give different answers?

Or since we're in AskComputerScience instead of Math, can you randomly generate a set of tasks where they give different answers?

...what does ""optimal"" mean here? It is *only* finding the maximum activity set, or does running time matter? I'm pretty sure collision testing is slower than sorting.

How, *exactly* does your proposed algorithm work? How does it know when to stop?",5cle0x,t1_d9xf3ut,453523523,,Reply,1,0,1
5cfyno,2016-11-11 13:16:48-05:00,Ernie_Si2,primary research on communication,"In my English class I have to do primary research on communication in computer science, so I made a survey asking questions down below, please fill it out to help me. While I did say computer science, anything that is similar is okay too, Computer programming, Computer engineering. 

https://goo.gl/forms/YLuwrc3Lvwtw9Ixq2",,,,,Submission,1,0,1
5cfraf,2016-11-11 12:40:56-05:00,Grey_0,Advice for a first year CS student,"Hi, this is my first year as CS student and i try to improve myself but, i don't know what should i do ? OSSU courses are provided by the highest colleges in the world and they can be far better than my university courses - they aren't bad but courses from Havard an MIT as i think are better in terms of knowledge and CV - or to read more books, to try to improve my coding skills. to participate in projects, to enter contests and so on. 
Thank you for your help and sorry if my English isn't easy to understood as it isn't my first language",,,,,Submission,3,0,3
d9wrmlu,2016-11-11 22:22:03-05:00,ProfessorAlgorithm,,"Start working on projects (your own, or open source) and participate in hackathons.  You can learn something from the MOOCs, but then you have to actually apply it before it means anything to an employer/grad school.",5cfraf,t3_5cfraf,Grey_0,,Comment,2,0,2
d9wwpy6,2016-11-12 01:08:23-05:00,Grey_0,,"I haven't heard before about hackathons, thanks for the tip ",5cfraf,t1_d9wrmlu,ProfessorAlgorithm,,Reply,1,0,1
5cfn03,2016-11-11 12:20:22-05:00,Megablast13,"Does running the ""tree"" command in the command prompt affect the life of an SSD?","Hey guys, just a quick question. I was messing around on the command prompt when I decided to run the tree command from the home directory. I'm on a Surface Pro 4, so I only have an SSD. I just want to know if running the tree command shortens the life of the SSD, and if it has a huge impact or if the effects are negligible. Thanks :)",,,,,Submission,2,0,2
d9w3ohd,2016-11-11 12:54:18-05:00,thatwasntababyruth,,"I can't see why it would. SSD's have a limited number of writes, not reads. If you write too much, the drive effectively becomes read-only. Tree is a read command. I'm also not aware of anyone who has actually managed to reach the write-limit of an SSD with normal use.

Now defragmentation, that's something you should not do on an SSD, because it's an extremely write-heavy operation.",5cfn03,t3_5cfn03,Megablast13,,Comment,6,0,6
d9w8ow1,2016-11-11 14:40:30-05:00,Megablast13,,Thanks! I was under the impression that both reads and writes were limited but I guess I was wrong. Thanks for the info,5cfn03,t1_d9w3ohd,thatwasntababyruth,,Reply,2,0,2
d9xa68v,2016-11-12 11:27:13-05:00,Nu11u5,,"Also, tree is just going to walk the directory structure, a lot of which will be cached in RAM by the OS.",5cfn03,t3_5cfn03,Megablast13,,Comment,3,0,3
d9ym77j,2016-11-13 12:33:47-05:00,Majiir,,"Other commenters are correct about reads vs. writes.

It's not worth worrying about SSD longevity. Experiments have shown they can hold up against extreme loads for a *long* time. Unless you're provisioning hardware for an IO-heavy server cluster, you don't need to think about it.",5cfn03,t3_5cfn03,Megablast13,,Comment,2,0,2
5ce3aw,2016-11-11 06:47:18-05:00,pirate_sally,Help with building an Inference Engine for an expert system.,"Im very new to Java but I had to construct an Expert System project in Java for this semester. Being very new into programming made me have so many obstacles along the way and I cant help to feel so ""silly"" sometimes. I really need help constructing the Inference Engine for this system which I have all the other classes but I have to implement the reasoning behind them. I would very much appreciate advices and such for helping me pass thru this!",,,,,Submission,1,0,1
5cdvrn,2016-11-11 05:42:08-05:00,Vetii,How do people study how programming languages are used in practice?,"I recently got interested into parsing of programming languages and working on the syntax of languages led me to a number of questions. 

Designing a language is always a trade-off between expressiveness and readability. Some language designers decided to give a lot of flexibility (Scala, F#, Ruby, which all support several paradigms), while some others made the choice of making the language quite restricted so it is easier to read new code (Go, Java to some extent). However, for some very flexible languages, some syntactic sugar might not be used often in practice. 
Are there efforts to simplify the syntax of languages based on its usage? Do language designers have a systematic approach to study how the syntax of the language is used in practice? Or do they take decisions based on their own experience of the code they see in the wild? 

[EDIT:] Grammar",,,,,Submission,22,0,22
d9vzwzj,2016-11-11 11:34:57-05:00,MCPtz,,"[Evidence Orientated Programming](https://www.youtube.com/watch?v=uEFrE6cgVNY)

Quorum:  
https://www.quorumlanguage.com/evidence.php",5cdvrn,t3_5cdvrn,Vetii,,Comment,2,0,2
d9w54ep,2016-11-11 13:24:52-05:00,Vetii,,"The article was really interesting! It is for instance quite surprising to see that novice programmers do not consider for-loops to be intuitive. 

On the other hand, I imagine that most programming languages are built for programmers, and the designers try, for instance, to squeeze as much semantics in as few keystrokes as possible (I am thinking about Haskell's or Perl's operator soup). 

I will look for more studies of this type, it is refreshing to see programming language design that is not grounded in the designer's opinions about programming.",5cdvrn,t1_d9vzwzj,MCPtz,,Reply,2,0,2
d9w674o,2016-11-11 13:47:17-05:00,MCPtz,,"Excellent, I figured this is what you wanted. 

I remembered the keyword ""evidence based programming language"" and google let me find it again. It was extremely refreshing when I watched it and I shared it around.",5cdvrn,t1_d9w54ep,Vetii,,Reply,1,0,1
d9wihso,2016-11-11 18:22:50-05:00,Vetii,,"Well, originally, what I had in mind was closer to ways to ""learn"" a parser for a specific programming language directly from source code. But I never realised one could see the problem of language design from a psychological standpoint, so this was very interesting too. It's quite obvious when thinking about it.",5cdvrn,t1_d9w674o,MCPtz,,Reply,2,0,2
d9way8z,2016-11-11 15:28:43-05:00,east_lisp_junk,,"> programming language design that is not grounded in the designer's opinions about programming

That's overselling it quite a bit. Most aspects of the design were (and had to be) chosen without reference to a particular study.",5cdvrn,t1_d9w54ep,Vetii,,Reply,1,0,1
d9vu6eo,2016-11-11 09:20:29-05:00,myfavcolorispink,,"I can write a little about Python, as that is what I know best, specifically as a brief case study on the keyword `yield`. 

The meaning and use of `yield` has evolved through recent Python versions. `yield` started out as a keyword roughly similar to `return` but for use in generators. Generators were used for iterating through things, `yield` being the keyword to pass out the current thing. At this time in Python `yield` had basically only one meaning, but things were soon to change. Since generators saved internal state (the progress they've made through whatever they are iterating over/through) it came to be known that generators could be used for more than just iterating. The saved state could be used to partially run something, and then let it lay fallow until it is needed again. As I understand it, this was more a process of discovering the inherent coroutine functionality, rather than designing that in initially. But generators at this time didn't do a very good job of being coroutines. So the language designers then added features — the `.send()` method to pass information into a coroutine style generator comes to mind — to better compliment the newly found features. What this did though was add new functionality to `yield`, not only was `yield` equivalent to a return for generators but also now a receiver from outside. When someone calls the `send()` method, it passes that information into where the `yield` is. This opened up two way communication into and out of a coroutine style generator, all hinging around the keyword `yield`. 

`yield` started out with one clearly defined use case, but now it is much more complex. I believe it is the process of interplay between discovering the implications of existing features and developing new features based on need that drives a language's development/evolution. 


(Disclaimer: my entire history of generators and `yield` could be wrong, as it's based on a combination of 8+ poorly remembered Python talks on the subject. But I think my thesis still stands.) ",5cdvrn,t3_5cdvrn,Vetii,,Comment,2,0,2
d9vwup9,2016-11-11 10:27:41-05:00,east_lisp_junk,,"> Do language designers have a systematic approach to study how the syntax of the language is used in practice?

It can go both ways. If you have a specific enough question, you could possibly perform some systematic investigation of how a language is used in practice. The first one that comes to my mind is [also my second-favorite paper title in PL](https://the.gregor.institute/papers/ecoop2011-richards-eval.pdf). I've spent quite a bit of time looking at how people use (and don't use) J and APL, but my question was much broader than identifying the main use cases for a single language feature (there's also a lot less publicly available J/APL code). That said, I don't see many PL researchers taking an interest in individual bits of syntactic sugar. Actual language features (like eval) are more interesting.",5cdvrn,t3_5cdvrn,Vetii,,Comment,1,0,1
d9x1fr9,2016-11-12 05:15:44-05:00,Vetii,,"> I've spent quite a bit of time looking at how people use (and don't use) J and APL, but my question was much broader than identifying the main use cases for a single language feature (there's also a lot less publicly available J/APL code). 

I suppose J and APL would probably be considered very un-intuitive by most programmers, so there might be a few features that people do not know at all. How did you proceed? Did you find interesting things?",5cdvrn,t1_d9vwup9,east_lisp_junk,,Reply,1,0,1
d9xwwfw,2016-11-12 21:04:46-05:00,east_lisp_junk,,"There's a fair amount of collected example code, like Perlis's ""APL Idiom List,"" which is meant to demonstrate a style that takes advantage of automatically lifting operators to higher rank. The J mailing list also gets a lot of talk comparing the various ways to solve some programming problem. ",5cdvrn,t1_d9x1fr9,Vetii,,Reply,1,0,1
d9wlgd8,2016-11-11 19:38:13-05:00,umib0zu,,"A word of warning: If this sticks, you may become a Haskell Programmer.

[Practical Foundations of Programming Languages](https://www.cs.cmu.edu/~rwh/pfpl/2nded.pdf) was recommended to me as a start to understanding PL theory. Also, [Denotational Semantics](http://people.cs.ksu.edu/~schmidt/text/densem.html) seems to be used a lot by Haskell Programmers.",5cdvrn,t3_5cdvrn,Vetii,,Comment,0,0,0
d9x1ivb,2016-11-12 05:21:21-05:00,Vetii,,"I am afraid it's already too late, as I spend a significant amount of my time trying to write Haskell code and learning about the language. However, my interest for programming languages as a whole came after my interest for Haskell (I just liked the elegant syntax and the powerful type system). ",5cdvrn,t1_d9wlgd8,umib0zu,,Reply,1,0,1
5cbk90,2016-11-10 19:46:16-05:00,finessseee,Help with this basic string based java assignment?,"Prompt for a string and read in..
            Then randomly select one of the characters in the string entered...
            If the character randomly selected is equal to A (cap) print Yes...",,,,,Submission,0,0,0
d9v9869,2016-11-10 20:58:30-05:00,SayYesToBacon,,"Break the problem down into pieces, google each piece, and figure it out. For example, google ""java read input"" etc.  If you have a specific question i am happy to answer it. ",5cbk90,t3_5cbk90,finessseee,,Comment,6,0,6
d9v9f6q,2016-11-10 21:02:53-05:00,mpiz,,Expanding on this: Learn how to read in a string from the user. Then learn how to break that string into characters. Then learn how to randomly choose a character. Then learn how to compare that character to 'A'. Then learn how to print a response if that comparison is true. ,5cbk90,t1_d9v9869,SayYesToBacon,,Reply,3,0,3
d9vferz,2016-11-10 23:27:50-05:00,cravenspoon,,"Already half done OP, this is almost psuedocode. ",5cbk90,t1_d9v9f6q,mpiz,,Reply,1,0,1
d9v8bh4,2016-11-10 20:37:36-05:00,esmith327,,"Do you have an actual question, or do you just want us to do your homework for you?

I know it sounds harsh, but if you're not ready to do the work of figuring it out, CS/IT is not a good choice of careers. ",5cbk90,t3_5cbk90,finessseee,,Comment,7,0,7
d9vh00n,2016-11-11 00:12:11-05:00,RonMexico69,,The first part just Google. The second one part you'll want to use the Random class. Instantiate an instance and use it to generate a number which you will modulus by the length of the string to get a random index.  The last part is simple.,5cbk90,t3_5cbk90,finessseee,,Comment,1,0,1
5c3zr8,2016-11-09 16:52:54-05:00,whoreo_,Help with LMC (little man computer),"I need to determine whether a number is a square number or not, heres my code so far.


http://pastebin.com/bvuWXHAZ",,,,,Submission,3,0,3
d9tl5mi,2016-11-09 18:47:37-05:00,PM_ME_ALL_DOWNVOTES,,"Ahhhh brings me back to GCSE computing. I won't make the code for you because I can't remember the code but apply this logic. 

Insert number a 
X = 1

Do 

       Let a = y

       Divide y by x

       If answer = x then it's a square number goto endsq

       If answer not = x then not square number 

       X = X + 1

       If x = 32 then goto endnosq

Loops

Endnosq -- output isnt square

Endsq -- output is square


This works for lmc assuming that the highest number it goes up to is 999 as I remember. It would work for this but may not be the logic you wanted
",5c3zr8,t3_5c3zr8,whoreo_,,Comment,1,0,1
d9tn7ml,2016-11-09 19:35:27-05:00,whoreo_,,Yeah I've done it the reverse way - by multiplying numbers from 1-33 and comparing them with the number. But i don't understand why my code doesn't work :(.,5c3zr8,t1_d9tl5mi,PM_ME_ALL_DOWNVOTES,,Reply,1,0,1
5c3fta,2016-11-09 15:18:44-05:00,helpplz6325,Logic Gates,"Hi, so I need help with an assignment I'm currently working on for my class. The question involves a logic gate (in the picture link), and the question is:

Which boolean function is computed by the gate?  

Using a truth table for the outputs, A exclusive or B, A implies B, I believe that the boolean function computed by the gate is true. However, I am not sure if I'm correct, and if I am I would like a more detailed explanation as to why I'm correct.

Thank you for your help!

http://imgur.com/a/1EorT",,,,,Submission,11,0,11
d9td93y,2016-11-09 16:00:30-05:00,andybmcc,,"It's an XOR function.

(A && !B) || (!A && B) is equivalent to A ^ B

Google XOR CMOS network and you'll find some good info.  A truth table should suffice to see that it's an XOR.",5c3fta,t3_5c3fta,helpplz6325,,Comment,1,0,1
d9tdrs8,2016-11-09 16:10:52-05:00,helpplz6325,,"Ok thanks, I was thinking of programming where boolean is either true or false. Could you explain your answer? I'm having a bit of trouble understanding the process of how to get there.

There is also a follow-up question asking to design a logic gate which will compute the other boolean function. Would that be or without the exclusive in this case?",5c3fta,t1_d9td93y,andybmcc,,Reply,2,0,2
d9te0s6,2016-11-09 16:15:46-05:00,andybmcc,,"Ok, so some explanation here, the A and B are your inputs.  Think of the high/low as connecting to the output when you satisfy conditions of the inputs.  So a high A and low B will ""connect"" the high to the output.  Think high = 1, low = 0.

So this picture represents a boolean function.  You can't know the output unless you know the inputs, A and B.",5c3fta,t1_d9tdrs8,helpplz6325,,Reply,1,0,1
d9te8rw,2016-11-09 16:20:18-05:00,andybmcc,,"> There is also a follow-up question asking to design a logic gate which will compute the other boolean function

""Other"" boolean function is completely ambiguous, there are a lot of boolean functions.  There has to be more information somewhere, or the question doesn't make sense.",5c3fta,t1_d9tdrs8,helpplz6325,,Reply,1,0,1
d9uk3eb,2016-11-10 12:19:49-05:00,Wh1te_Rabbit,,"I think the assigment asks for !XOR
which means its XNOR

(A && B) || (!A && !B)

A ≡ B",5c3fta,t1_d9te8rw,andybmcc,,Reply,1,0,1
d9us0to,2016-11-10 14:51:53-05:00,Pardomatas,,"Boolean is either true or false, you're correct about that. I think you have the wrong idea. Look up the truth tables for XOR.",5c3fta,t1_d9tdrs8,helpplz6325,,Reply,1,0,1
d9tsj6l,2016-11-09 21:42:46-05:00,Madsy9,,"There are [16 possible boolean logic operators](https://en.wikipedia.org/wiki/Truth_table#Truth_table_for_all_binary_logical_operators), assuming two inputs and one output, and they can all be represented with either venn diagrams or truth tables.

But sometimes a function isn't represented in its shortest possible form. For example, you might have to convert the logic with [De Morgan's laws](https://en.wikipedia.org/wiki/De_Morgan%27s_laws) in order to recognize it. Really complex boolean logic can also be simplified with tools such as [Karnaugh maps](https://en.wikipedia.org/wiki/Karnaugh_map), but you won't need that for a long time.

All you need to know is to learn those 16 different logical operators and roughly recognize them, and learn to apply De Morgan's laws. When everything else fails, generate the truth table of the logical expression (or circuit) by evaluating it for every possible input (a 0/false or an 1/true). You can then compare the resulting truth table against the list of operator truth tables.",5c3fta,t3_5c3fta,helpplz6325,,Comment,1,0,1
5bxfqe,2016-11-08 20:28:35-05:00,vuvcsuy,Computer Science majors- how much math is involved?,"So I'm at the point where I'm selecting my major for college next year, and I'm having a problem choosing between Computer Science or Network and Computer Security. Initially, I had my mind dead set on Computer science, but upon further investigation, I was told a lot of math is involved (which isn't a bad thing, it's just not my strong suit). So now I switched over to Network and Computer Security because that's the closest thing to an IT major at my school of choice.Can anyone describe their experience with the major or field? Anything you have to say helps me, and I thank you for your time.",,,,,Submission,17,0,17
d9s4sxp,2016-11-08 22:13:47-05:00,PastyPilgrim,,"It's worth noting that computer science *is* a branch of mathematics. Even when you're not using numbers and conventional math, you're using discrete math, graph theory, logic, and other applied mathematics.

You may find that the kind of math you do in computer science is more suited to the way you think and learn, but if you're opposed to mathematics as a whole, then you may want to look at a different major. It's not just about how many required math courses you'll have to do to graduate because every computer science class is also a math class.",5bxfqe,t3_5bxfqe,vuvcsuy,,Comment,25,0,25
d9saydx,2016-11-09 00:12:12-05:00,0x23f81e3c,,"Any undergrad cs program worth its salt will cover the technical side of computer science, **and** the theoretical side of it.

The former will likely take the form of programming and software engineering: and the latter, of discrete mathematics, graph theory, algorithms, and autonoma and computability theory.

That's a decent amount of math—it's not necessarily continuous math (e.g., calculus) which you might be used to, but it's mathy and proof-based nonetheless, and often, a lot more interesting.",5bxfqe,t3_5bxfqe,vuvcsuy,,Comment,4,0,4
d9rzuzk,2016-11-08 20:33:24-05:00,tyggerjai,,This gets asked a lot - I recommend searching the sub and /r/compsci for related topics. For example: https://www.reddit.com/r/compsci/comments/5blb27/what_level_of_proficiency_do_i_need_to_have_in/,5bxfqe,t3_5bxfqe,vuvcsuy,,Comment,3,0,3
d9rzxmf,2016-11-08 20:35:00-05:00,vuvcsuy,,"Thanks for the link, ill check it out ",5bxfqe,t1_d9rzuzk,tyggerjai,,Reply,1,0,1
d9svap3,2016-11-09 09:53:49-05:00,ollee,,"Everyone says that there is so much math blah blah so hard yada yada, and they're not wrong that there is a substantial amount of math present, however, it isn't really that hard. College level mathematics is different than Secondary mathematics. It's all the same foundation, but it is more than just ""this is how we do it"", instead it's much more ""this is how it's done and this is why"". I find that math started to make actual sense as I went through my college courses, and it really wasn't all THAT difficult. I recommend giving it a go, as your other base courses for CS will also likely be requirements of NCS, and if you find that the math is too much, no pressure, you can always switch to the NCS degree. There are usually structures in place within the math department to help you succeed. I suggest taking full advantage of a math assistance center or office hours and the like. Don't be afraid to ask for help. If you are not grasping a concept as described in class, chances are good your instructor can relate it in a different way that will help you understand it. Consistent work, not just hard work, is the key to success in mathematics.",5bxfqe,t3_5bxfqe,vuvcsuy,,Comment,2,0,2
d9t0og7,2016-11-09 11:48:20-05:00,andybmcc,,"I hate to break it to you, but networking and security will require a significant portion of math, too.  Graph theory, number theory and modern algebra, combinatorics, etc.",5bxfqe,t3_5bxfqe,vuvcsuy,,Comment,2,0,2
d9t14ph,2016-11-09 11:57:03-05:00,DenProg,,"If you have to ask, then too much :) .

Kidding aside, calculus, statistics, and discrete structures ( logic & proofs ). ",5bxfqe,t3_5bxfqe,vuvcsuy,,Comment,2,0,2
d9s8dpr,2016-11-08 23:21:04-05:00,-tonybest,,1/3,5bxfqe,t3_5bxfqe,vuvcsuy,,Comment,1,0,1
d9ul1n6,2016-11-10 12:37:46-05:00,lordwax3,,"For my university, its calculus 1, calculus 2, basic statistics and Discrete math. It's very doable. ",5bxfqe,t3_5bxfqe,vuvcsuy,,Comment,1,0,1
d9s1way,2016-11-08 21:15:54-05:00,Merad,,"Most legit CS curriculums will require Calculus 1 and 2, as well as Linear Algebra. It's also not unusual to require a statistics course and/or calc based physics, both of which tend to be math heavy. None of those classes are really *hard*, however. After school you aren't likely to use a ton of math unless you go into certain sub fields (3D graphics/simulation, AI, machine learning, data science, etc). ",5bxfqe,t3_5bxfqe,vuvcsuy,,Comment,0,0,0
d9sejbz,2016-11-09 01:26:53-05:00,DukeBerith,,"For most of my job I've rarely had to do anything besides basic addition and multiplication. Recently I was tasked with making an interactive user area where they can point and click and create a funky design with their images, which involves a lot of algebra, trigonometry, and linear algebra. But guess what? I didn't do a single calculation by hand because my libraries took care of it :) 

Webdev rarely has to touch non basic maths. ",5bxfqe,t3_5bxfqe,vuvcsuy,,Comment,-4,0,-4
5bxfqd,2016-11-08 20:28:35-05:00,j-m-,Class Diagram Help!!,"I'm building a Class Diagram where an <<entity>> extracts its data from a CSV file. Do I include the CSV file in the Class Diagram? and if so, How?
Thanks in advance.",,,,,Submission,1,0,1
d9t9eh0,2016-11-09 14:43:57-05:00,spatialdestiny,,"It shouldn't, in my opinion.  The <<entity>> has its responsibility to grab the CSV and store it's data (or do whatever it's supposed to with it's data).  The medium in which the data is stored doesn't matter to the class and the class diagram doesn't care about data.  


The question would be, if the csv changed to some data in a database, would your class diagram change?  Probably not.  The method inside <<entity>> would change to go to the database instead of to a csv.",5bxfqd,t3_5bxfqd,j-m-,,Comment,2,0,2
d9tc5hn,2016-11-09 15:38:29-05:00,j-m-,,Thanks. I had a similar idea but I wasn't 100% confident in my answer.,5bxfqd,t1_d9t9eh0,spatialdestiny,,Reply,1,0,1
5bppmo,2016-11-07 18:31:53-05:00,Everlastinglol,Search time in a heap,"I read that the search time in a heap would be O(1) when finding max/min, however that seems to only apply when finding max in a max heap or finding min in a min heap. It doesn't apply when finding min. in a max heap.

Am I understanding this correct?

Also, what would be the time complexity of finding min. in a max heap?

Thank you",,,,,Submission,6,0,6
d9qdyhr,2016-11-07 19:51:03-05:00,dandrino,,"A heap is only optimized to find one value (e.g. the min of a min-heap or the max of a max-heap). Finding other values basically devolves into a linear search.

A binary search tree based data structure would be more suited to finding the min as well as max (or other ranks if you are using an order statistic tree).",5bppmo,t3_5bppmo,Everlastinglol,,Comment,2,0,2
d9qclks,2016-11-07 19:19:23-05:00,SThor,,"The min in a max heap, or max in a min heap are values that can be saved in the structure. It is only one value, so the complexity memory wise is not changed. And it only needs adding a simple comparison to procedures which may change the min/max, so the time complexity doesn't change either.

I'm not 100% sure of this, so anyone more knowledgeable feel free to correct me if I'm wrong :)",5bppmo,t3_5bppmo,Everlastinglol,,Comment,0,0,0
d9qdgbj,2016-11-07 19:39:16-05:00,logicx24,,"So this is true if your heap is static, but that's rarely ever the case (if it is, then you don't need a heap). For a dynamic heap, finding a min takes O(1) time, yeah, but it takes O(log n) to find a new min after removing the last min. 

There's no efficient way to find a max in a min heap, or vice versa, beyond just going through all the elements. ",5bppmo,t1_d9qclks,SThor,,Reply,2,0,2
5bpmc8,2016-11-07 18:15:40-05:00,Ran4,"How do I calculate the x and y position of a ""spiral number""?","(X-post from /r/askmath thread [here](https://www.reddit.com/r/askmath/comments/5bpjy9/finding_x_and_y_position_of_ith_spiral_number/))

Let's say that we have a ""spiral"" of numbers like this (here given in base 16 with a-f = 10-15):

    6789
    501a
    432b
    fedc

Given that the 0 is at position (0, 0), is there a way to calculate the position of the i'th ""spiral number""?

E.g. we want to find a function f(i) : Z -> Z^2 such that (assuming the x direction is to the right and y goes down):

    f(0) = (0, 0)
    f(1) = (1, 0)
    f(2) = (1, 1)
    f(3) = (0, 1)
    f(4) = (-1, 1)
    f(5) = (-1, 0)
    f(6) = (-1, -1)
    f(7) = (0, -1)
    f(8) = (1, -1)
    f(9) = (2, -1)
    f(10) = (2, 0)
    f(11) = (2, 1)
    f(12) = (2, 1)
    f(i) = ...

Does anyone know what this function is called? It's a very common pattern in computer programming competitions, and I'd love to know more about it. Can you calculate in a way that isn't O(i)?

I once made a (really, really bad) ""aimbot"" in Quake which used this spiral pattern to be quick enough (as a proof of concept, I replaced the textures of all the enemies colors with all-white, then every frame I took an image of the screen, and starting in the middle I looked for a white pixel. Going through 640x480 pixels 60 times per second took too long, so I instead started in the middle and circled outward).",,,,,Submission,12,0,12
d9qh0tn,2016-11-07 21:00:32-05:00,teraflop,,"Yeah, it's possible to convert in either direction in O(1) time. Do you want to know the details, or are you just looking for hints?

(Not being sarcastic -- I don't want to spoil it if you want to figure out the details yourself!)",5bpmc8,t3_5bpmc8,Ran4,,Comment,3,0,3
d9r3qxe,2016-11-08 09:54:14-05:00,Ran4,,"I mostly want to implement this and understand the reasoning behind it, I don't really have a strong background in math (never really learned much CS, only two years of college-level theoretical mathematics, little of which relates to these types of problems).

Do you happen to know what this type of spiral/this function might be called?",5bpmc8,t1_d9qh0tn,teraflop,,Reply,1,0,1
d9r4dx5,2016-11-08 10:08:18-05:00,teraflop,,"Unfortunately I don't know a name for this function. But it doesn't actually require any math beyond high school algebra.

Here's a hint: let's break the pattern down into concentric squares. Define the ""ring"" of radius N to be the square whose corners are at (±N, ±N), with a side length of 2N+1. Let's say that such a ring covers the integers from K+1 to K+8N, inclusive. (For instance, the ring of radius 2 covers the integers from 9 to 24.)

So in order to explicitly calculate the values at the corners of ring N, all you need to do is find a simple relationship between N and K.",5bpmc8,t1_d9r3qxe,Ran4,,Reply,1,0,1
5bl3ww,2016-11-07 03:26:46-05:00,Quillot,"For those who have coding blogs, how did you start out?","Whenever I read blog posts and tutorials, it often seems that the writers are really experienced about what they're writing. I myself am thinking of starting a programming blog to somewhat document what i'm learning.

The question is, even if the blog is to serve as a showcase of what i'm learning, how can I fix it in such a way that it would be helpful to others if there are other, better blogs and posts that are easily google-able? ",,,,,Submission,13,0,13
d9pcl73,2016-11-07 04:04:07-05:00,donthelpme,,"I wanted to learn JavaScript, so I just wrote about little projects I did. Havent updated the website in a while, but you can check it out here: http://codeblaze.it",5bl3ww,t3_5bl3ww,Quillot,,Comment,2,0,2
d9pu8sg,2016-11-07 13:06:00-05:00,okiyama,,">About codeblaze

>Codingpen is a blog about (mostly) JavaScript coding

Small typo",5bl3ww,t1_d9pcl73,donthelpme,,Reply,2,0,2
d9piwkr,2016-11-07 08:59:33-05:00,bro_hugger,,What are you using for syntax highlighting on your blog? It looks awesome. ,5bl3ww,t1_d9pcl73,donthelpme,,Reply,1,0,1
d9pjlo5,2016-11-07 09:18:13-05:00,donthelpme,,"Got it from http://prismjs.com/
I think I used the 'Coy'-theme!
",5bl3ww,t1_d9piwkr,bro_hugger,,Reply,1,0,1
d9qg5em,2016-11-07 20:41:04-05:00,nycthbris,,"I took a class on Coursera about machine learning and decided to re-implement the algorithms in Python rather than Octave/MATLAB (which the class was taught in). I would write a post about each one along with an example of how to use it as I went along. It was really helpful to solidify the content I learned. Also telling friends about it would help keep me accountable and motivated.

Honestly, the best way to learn about a concept is to write about it. If you can effectively communicate it then most likely you have a good handle on it. I've found [this post](https://emptysqua.re/blog/the-write-an-excellent-programming-blog-page/) by A. Jessie Jiryu Davis have some good guidelines on what to write about. Even if you're not an expert you still know more than someone who has less experience than you. Take a stab at it and just keep going. If you put the effort in, you can only improve.",5bl3ww,t3_5bl3ww,Quillot,,Comment,1,0,1
da53s9h,2016-11-17 21:08:29-05:00,redditornerd,,"I started out by just posting whatever code I practised in my coursework, then I began publishing my notes in an organised manner. Sometimes I also solve interview questions in my blog. 

As for your question, you should try writing the solution in your own words. Often when I seek solutions to my problems on the internet, it is not the first Google search link that helps me. I open multiple links and read all of them, and sometimes only 2-3 of those links actually help me. If you want to influence a large audience, make easy to follow tutorials, use keyword research to find out what people are actually searching for related to your topic, and if you get time, learn some SEO for your blog.  ",5bl3ww,t3_5bl3ww,Quillot,,Comment,1,0,1
d9q6eyp,2016-11-07 17:03:45-05:00,AndyIbanez,,"I started writing as a way to document what I learn. Eventually I started sharing all kinds of stuff I know.

andyibanez.com",5bl3ww,t3_5bl3ww,Quillot,,Comment,0,0,0
d9q6f45,2016-11-07 17:03:49-05:00,AndyIbanez,,"I started writing as a way to document what I learn. Eventually I started sharing all kinds of stuff I know.

andyibanez.com",5bl3ww,t3_5bl3ww,Quillot,,Comment,0,0,0
d9q6f9p,2016-11-07 17:03:53-05:00,AndyIbanez,,"I started writing as a way to document what I learn. Eventually I started sharing all kinds of stuff I know.

andyibanez.com",5bl3ww,t3_5bl3ww,Quillot,,Comment,0,0,0
5bk2eg,2016-11-06 22:41:44-05:00,NorthsideB,How can I setup an Android phone to view an IP camera setup if I don't know the IP address of the dvr?,"I'm trying to connect my boss's Samsung Galaxy S7 to the work network so he can view the IP camera's remotely. Years ago when the company set it up, they showed him how to hook it up to his iPad, but the iPad doesn't work with it anymore. I have an app called IP cam pro but am unfamiliar with it. If anyone is familiar with setting up multi-camera setups on a new android phone please help me! I'd be in your debt.",,,,,Submission,4,0,4
d9p62qd,2016-11-06 23:39:29-05:00,robot_one,,"Why don't you just Google ""what is my IP?"" from the work network? ",5bk2eg,t3_5bk2eg,NorthsideB,,Comment,2,0,2
d9pbdaa,2016-11-07 02:57:15-05:00,NorthsideB,,I tried that and I need the specific external IP address for the dvr. I also tried using the app Fing to see all of the hardware on the network but couldn't find it. ,5bk2eg,t1_d9p62qd,robot_one,,Reply,1,0,1
d9pc842,2016-11-07 03:42:54-05:00,revvupthosefryers,,Can you log into the router and find its external ip from that? ,5bk2eg,t1_d9pbdaa,NorthsideB,,Reply,1,0,1
d9pelpl,2016-11-07 06:05:24-05:00,NorthsideB,,Not for the dvr tho. ,5bk2eg,t1_d9pc842,revvupthosefryers,,Reply,1,0,1
d9pi3zb,2016-11-07 08:35:53-05:00,robot_one,,Port might not be open. It's unlikely that dvr has its own dedicated internet connection. Have you looked up a guide to setup that dvr? ,5bk2eg,t1_d9pbdaa,NorthsideB,,Reply,1,0,1
d9piimn,2016-11-07 08:48:22-05:00,NorthsideB,,I don't even know what company makes it. It has no identifying marks. ,5bk2eg,t1_d9pi3zb,robot_one,,Reply,1,0,1
5bfo1i,2016-11-06 08:18:06-05:00,Bill_Murray2014,"Need some help with a question from my CS assignment, calculating with 8 bit two's Compliment binary numbers.","Hi /r/AskComputerScience,

The question asks me to carry out calculations using ""8-bit 2's compliment binary numbers, the answer should be an 8 bit number of the world 'overflow', Ignored carry bits should be shown thus (1).

The first problem is 121+67.

Please can someone help. At the moment I am converting the numbers to binary, adding the sign (0 for positive, 1 for negative) , so they are 8 bits. Then I just do the sum, is that it?

I.e. 121+67 is equal to 01111001 + 01000011 = 10111100.

That's it, is that right, is that what the question is asking for.

All help would be greatly appreciated. Thanks.",,,,,Submission,1,0,1
d9o581k,2016-11-06 09:25:58-05:00,Bottled_Void,,"If I gauge the question right, it's to show that the two positive numbers overflow into the sign bit resulting in a negative number... which is?

Also, you'll find it easier to always write out all 8 binary digits.",5bfo1i,t3_5bfo1i,Bill_Murray2014,,Comment,1,0,1
d9o5ce4,2016-11-06 09:29:40-05:00,None,,[deleted],5bfo1i,t1_d9o581k,Bottled_Void,,Reply,1,0,1
d9oizmf,2016-11-06 14:54:36-05:00,spongebue,,"Right, the first digit is 1, which is the -128 place, and the remaining digits will not be enough to make it positive",5bfo1i,t1_d9o5ce4,None,,Reply,1,0,1
d9o5kxa,2016-11-06 09:36:50-05:00,spongebue,,"The thing with 2s compliment is that you cannot just put a 1 in front to make it negative. On an unsigned 8 bit number, the first bit is your 128 place. That is, it represents 128 in base 10. The highest you can go is 255 (1111 1111). On a signed 2s complement, the first number represents negative 128, and your range is now -128 (1000 0000) to +127 (0111 1111). If you go outside that, it's an overflow.

To expand on the negative numbers, you start by going to ""rock bottom"" of -128, then use your remaining bits to add numbers in. So +127 is 0111 1111. -128 is 1000 0000.  -128 + 127 = 1000 0000 + 0111 1111 = 1111 1111 = -1.

Another example, -2 = -128 + 126 = 1000 0000 + 1111 1110 = 1111 1110.

By the way, spaces are only added to make it easy to read each nibble. See if you can figure out numbers like -55, and use one of those online tools out there to check your work. Once you have that down pat, the arithmatic is basically the same.

Edit: there's some article floating on the internet about some guy using the Socratic method to teach binary to a bunch of elementary school students. It doesn't go into 2s complement, but it does really do a great job teaching binary, if you read through the whole thing.",5bfo1i,t3_5bfo1i,Bill_Murray2014,,Comment,1,0,1
5bd9s5,2016-11-05 20:57:34-04:00,SuperCool468,Neural Network Identifying Seven-Segment Numerals,"Hello, I am studying machine learning and I am working on my first neural network as a project for one of my classes. I am programming the network in java. The point of the network is to identify seven-segmented numeral (like on a regular digital clock). The network does not actually have to be linked to any real sensors, it just needs to work in theory based on inputs as 0's and 1's in text form, not binary, which correspond to a hypothetical sensor matrix laid across the top of the number.

My question is, what sort of output am I looking to get? Will the binary output be just correspond to the same sort of matrix as input or is the binary output supposed to represent the input number in binary such as returning 111 for 7?

if it does just return another matrix, what is the point of the network?
",,,,,Submission,1,0,1
d9pq53e,2016-11-07 11:44:53-05:00,andybmcc,,"You'd probably have something like 7 inputs (one for each segment), and 10 outputs (one for each possible digit).  The inner layers are kind of black magic, you'll have to figure that out.  Once trained, you'll look for the highest output and determine that what you're looking at is the corresponding digit.  Training should converge very quickly for this problem, assuming you have enough inner-layer nodes/connections to capture the behaviour.  Assuming you're just learning backprop?

It looks like the point of this exercise is to implement a simple example of a network.  You can obviously directly arrive at the correct answer with a few simple conditionals.",5bd9s5,t3_5bd9s5,SuperCool468,,Comment,2,0,2
d9px6da,2016-11-07 14:03:15-05:00,SuperCool468,,"First off thank you for the answer, I don't have too many resources that are at my level for this. Yes the point of the net is basically to show that I can do it, not to make anything particularly efficient or useful. Long story short, I am in a programming class at my school and I'm hoping to get on doing undergraduate research in machine learning with my professor in the class and I need to impress him because it is extremely competitive. Unfortunately that's just how it works around here.

My goal is to make a very simple neural net and this seemed to be a good concept. I do plan to use backpropogation, but I'm am still learning about it. I also haven't figured out what activation function would be simplest, but still function, i.e. step, linear, sigma.
Any advice on that would literally save me hours of confusion. 

I'm reading Jeff Heaton's math for ANN, and Packt's machine learning in java, and neural nets in java, but they have been slow reads thus far.",5bd9s5,t1_d9pq53e,andybmcc,,Reply,1,0,1
d9pxp9g,2016-11-07 14:13:32-05:00,andybmcc,,"Because of the nature of the problem, you probably want a saturating transfer function, like a step or a tight sigma.  Sigma functions are kind of the go-to for general use.  A linear function doesn't really make sense for this problem.",5bd9s5,t1_d9px6da,SuperCool468,,Reply,2,0,2
d9pyo9w,2016-11-07 14:32:57-05:00,SuperCool468,,thanks!,5bd9s5,t1_d9pxp9g,andybmcc,,Reply,1,0,1
5bcrgm,2016-11-05 19:09:48-04:00,alreadyheard,what is a 'satisfying assignment' in a boolean formula?,"So lets say we have something like (p and q) or (p and not(p)). One, would this be satisfiable  because of the first clause? And two, when one says satisfying assignment are they referring to the clause(s) that satisfies the whole formula?",,,,,Submission,2,0,2
d9nmqe1,2016-11-05 21:49:46-04:00,_--__,,"An **assignment** is a function that maps a truth value (i.e. True or False) to all of the variables.  So for instance, the function that maps p to True and q to False is an assignment.  An assignment is **satisfying** if, after applying the assignment, the formula simplifies to True.  So in your example the assignment I gave is *not* a satisfying assignment, but the assignment that maps p to True and q to True is a satisfying assignment.  We say a formula is **satisfiable** if there is some satisfying assignment (in this case the formula is satisfiable): **unsatisfiable** if there are no satisfying assignments: and **valid** if all assignments are satisfying (e.g. p or not(p)).",5bcrgm,t3_5bcrgm,alreadyheard,,Comment,3,0,3
d9wusnp,2016-11-11 23:58:20-05:00,alreadyheard,,late reply but thank you! this was helpful.,5bcrgm,t1_d9nmqe1,_--__,,Reply,1,0,1
5banf3,2016-11-05 12:15:08-04:00,ryan051601,How do you code a planet,"I've always wondered how a spherical planet with working gravity is coded. In real life gravity hold you to the planet and you never feel like you are upside down. How would you replicate something like that in a game. How would you code a curved surface to feel like a flat one. I'm not looking for a tutorial, just an explanation!

Edit: Thanks for the responses!",,,,,Submission,15,0,15
d9n38dr,2016-11-05 13:54:52-04:00,ukkoylijumala,,"Couldn't you just define a point in the center of a sphere and simply calculate the 3D vector from entities on the spheres surface to that center point to get the direction of the gravitational force?

This would be my first idea, I have never tried this, though.",5banf3,t3_5banf3,ryan051601,,Comment,9,0,9
d9nc5iy,2016-11-05 17:30:29-04:00,rfinger1337,,"yes,  and then rotate the model so it's always ""standing"" upright on the surface.",5banf3,t1_d9n38dr,ukkoylijumala,,Reply,3,0,3
d9n6zey,2016-11-05 15:23:42-04:00,Yomarao,,One doesn't simply code a planet,5banf3,t3_5banf3,ryan051601,,Comment,7,0,7
d9nc6hr,2016-11-05 17:31:11-04:00,rfinger1337,,"you do if you... <puts on sunglasses>  plan it...


YEAAHHHHHHH!!",5banf3,t1_d9n6zey,Yomarao,,Reply,12,0,12
d9mzgle,2016-11-05 12:25:59-04:00,946789987649,,"As in you're on a large surface that acts as though you're on a planet, or something akin to Mario Galaxy where you can see the entire planet and run around it?",5banf3,t3_5banf3,ryan051601,,Comment,2,0,2
d9mzng3,2016-11-05 12:30:31-04:00,ryan051601,,"Like being on a large surface that acts like a planet but when you ""fly"" off of it you can see the whole planet. ",5banf3,t1_d9mzgle,946789987649,,Reply,1,0,1
d9n1mtz,2016-11-05 13:16:58-04:00,_Azota_,,"Taking a look at [this](http://proland.imag.fr/) library would be a nice place to start I guess, sorry I don't have an ELI5 type answer for you.",5banf3,t1_d9mzng3,ryan051601,,Reply,2,0,2
d9nev7v,2016-11-05 18:38:58-04:00,xxkid123,,"Vpython is a very simple way to code simple, moving shapes. It's designed for scientific examples and really the only thing you need is a rough working knowledge of mechanics. A physics subreddit would probably work better.",5banf3,t3_5banf3,ryan051601,,Comment,1,0,1
d9nrmpe,2016-11-05 23:55:23-04:00,manlycoffee,,"A very simple math equation (ignoring the accelerating effects of the force of gravity) would essentially be this: `newPosition = position - velocity * norm(position)`. And then, have an if-statement to check and see if the distance of the position is less than or equal to the radius of the planet. If it is, just set the position to the radius of the planet. Otherwise, let it keep ""falling"" (via a the above equation).

In order to keep the object upright: simply, depending on the library that you're working with, there is a vector known as ""an up vector"". Just set the ""up vector"" to the norm of the object's position.",5banf3,t3_5banf3,ryan051601,,Comment,1,0,1
5b8mqt,2016-11-05 02:04:23-04:00,Cedricium,Combining Computer Science and Engineering for fun,"Hello everyone,

&nbsp;

I am currently studying computer science and am learning C++. I've always had a love for robotics and engineering in general, but I have never actually practiced making things. 

&nbsp;

Now that I have a somewhat decent knowledge of programming, I would like to create my own projects for fun that include both programming and mechanical engineering per se. I was thinking of making some sort of robot or small autonomous vehicle - however, I'm not exactly sure where to begin.

&nbsp;

What are good resources that can get me started with this sort of undertaking? Any and all advice is much appreciated! Thanks.",,,,,Submission,1,0,1
d9momaf,2016-11-05 04:54:05-04:00,None,,"Motor tradeoffs, motor control system tradeoffs, battery, wheels?",5b8mqt,t3_5b8mqt,Cedricium,,Comment,2,0,2
d9mp4e2,2016-11-05 05:27:37-04:00,amtw7083,,Maybe look into Arduino?,5b8mqt,t3_5b8mqt,Cedricium,,Comment,2,0,2
d9pqh8a,2016-11-07 11:51:42-05:00,andybmcc,,"Grab yourself a microcontroller development kit.  It's probably easiest to start with one that comes with expansion boards for motor controllers, sensors, comms, etc.  Arduinos are an OK start for a hobbyist.",5b8mqt,t3_5b8mqt,Cedricium,,Comment,2,0,2
d9ps882,2016-11-07 12:26:35-05:00,Cedricium,,"Like u/amtw7083 said, I was looking into Arduino. These MDK you speak of sound like a good idea too, do you have any recommendations for a specific one? ",5b8mqt,t1_d9pqh8a,andybmcc,,Reply,1,0,1
d9pvara,2016-11-07 13:26:37-05:00,andybmcc,,"I like the STM32 Nucleo stuff.  ST is used a good bit in industry.  It has really good support on mbed.org, and there are a pile of extension boards.",5b8mqt,t1_d9ps882,Cedricium,,Reply,1,0,1
d9pvks5,2016-11-07 13:32:08-05:00,Cedricium,,"I'll check it out, thanks for the suggestion!",5b8mqt,t1_d9pvara,andybmcc,,Reply,1,0,1
5b7ype,2016-11-04 23:03:02-04:00,Flexible_Perplexity,"Not sure if you guys help with assignments, would be greatly appreciated. Instructor did not go over this. (C++ programming)","So here is my question and some test runs

http://imgur.com/a/ZVvdl

I'm prett sure I understand how function calling works... only a little bit. I've done some googling and haven't found anything similar to what it's asking.

I just don't know where to put what variables where, and what to ""initiate"" them as (bool, float, double, int, string, char). I tried doing a bunch of ""if"" statements but my compiler would not even allow me because something was wrong with the functions. I've been working on this for hours and hours and I'm back to square one.

I'm not a very c++ savvy person and would like some help (or answers) with what to do.",,,,,Submission,1,0,1
d9miogf,2016-11-05 00:17:06-04:00,urielsalis,,"Your best bet would be talking with your professor or other classmates so they teach you how to THINK the problem. Mostly from what I read is drawing shapes, if you check your notes or google you should be able to find what type you need for each. The variables go in the scope you need them. Most of the thinking is dobe by the professor when he gave you the function names and arguments, you only havr to fill them.


Good luck! (And check with your instructor as using code from other people or just asking question might count as plagarizing, thats why I tried to keep it vague)",5b7ype,t3_5b7ype,Flexible_Perplexity,,Comment,1,0,1
5b7cmq,2016-11-04 20:42:34-04:00,forexperimentsonly,Which OS is the best for computer science undergraduates? (Laptop),"First of all, Thank you all in advance! I am a prospective computer science undergraduate looking to buy a new laptop for college use (and potentially, gaming). My question is:

**Among Windows, Linux, and OS X, which one is the best choice for computer science undergraduate?**

I have done a bit of research myself, and the consensus *seems to be* that:
> * Windows alone is generally inferior to Linux and OS X in terms of learning computer science. However, it's the only choice for high-specs gaming on laptop, while it also has some useful windows-exclusive software. It is also the most generally used system (thus more common support/application).

> * OS X, although more expensive (Macbook), deserves the price in terms of popularity, aesthetics, customer support and ""it just works"" - a very intuitive, Unix-like system and with Unix-certified hardware, which is stable and helpful for CS starters. Plus, Macbook is the only legal way to run OS X, and it enables its users to more easily develop iOS apps. It also has better battery life (more portability for college students).

> * Linux, being ""free as in freedom,"" has a ""much deeper learning curve"" and can be the most fundamental OS to learn CS on, although buying a laptop that exclusively runs Linux is pretty hard, and it's less convenient than OS X because it needs a lot more of configurations/constant maintenance (better for geeks but not for starters?). 

> * Dual-booting/getting virtual machine for Linux on Windows PC (not discussing running Linux on OS X) gives students advantages from both OS, but is not as powerful as ""genuine Linux machines"" (and can cause a variety of hardware and software issues due to compatibility), and it's still not as efficient/user-friendly as OS X.

> * Some other voices online do say that it doesn't matter which system the student gets at first, and that it's all a ""preference problem"" - they say that students will touch and learn about all these operating systems eventually.

For myself, I do prefer Windows for gaming purposes. To more effectively learn computer science at the same time:

* Is dual-booting Windows-Linux the best choice for me?

* Or is OS X/Linux so much better than Windows for students that gaming is not even a factor worthy of consideration?

* What misconceptions do I have on the three operating systems?

* What other reasons do I have for choosing any one of them over the other two?

**Thank you all again!**

Edits: reformatting research results and the questions.",,,,,Submission,13,0,13
d9mbckc,2016-11-04 20:49:37-04:00,tyggerjai,,"Run whatever is most convenient as a desktop. Run other things in a VM. 

Mac hardware is the simplest way to get OSX, but not worth it unless you really want a Mac. If I were you I would get a windows laptop and run a Linux VM. It really doesn't matter what you use. ",5b7cmq,t3_5b7cmq,forexperimentsonly,,Comment,20,0,20
d9mbolt,2016-11-04 20:58:20-04:00,forexperimentsonly,,"Thank you! I have some other questions:

By ""most convenient,"" if I consider gaming, then Windows should be the ""most convenient?""

Also, is VM a better choice than dual-booting (Safety-wise/convenience-wise)?

(By the way, I myself agree that the price range of Macbooks is hardly justified.)",5b7cmq,t1_d9mbckc,tyggerjai,,Reply,3,0,3
d9mbznl,2016-11-04 21:06:34-04:00,tyggerjai,,"Your host should be whatever you use most often, or whatever you can't VM. If your games require GPU access and you play a fair bit, then that will be your host. I don't see a lot of advantage to dual boot these days, but if you used Linux as your daily OS, and needed GPU access for games, then you might dual boot. I usually just use VMs. They're far more convenient unless you absolutely need hardware access. ",5b7cmq,t1_d9mbolt,forexperimentsonly,,Reply,5,0,5
d9mc8f7,2016-11-04 21:13:08-04:00,forexperimentsonly,,"I see, so VM doesn't grant Linux as much hardware access as dual-booting? Does this matter a lot to a general CS undergraduate? If not, I'll probably go with VM Linux. ",5b7cmq,t1_d9mbznl,tyggerjai,,Reply,1,0,1
d9mdi5s,2016-11-04 21:47:08-04:00,tyggerjai,,"Unless you are doing driver development, it makes very little difference to a Linux VM. It doesn't matter much either way -I run Linux with a windows VM for various CAD things, and a VM is fine even though they are graphics heavy. But my laptop dual boots because games like direct access. ",5b7cmq,t1_d9mc8f7,forexperimentsonly,,Reply,2,0,2
d9mdqej,2016-11-04 21:53:05-04:00,forexperimentsonly,,"That's really nice! Since major games run on Windows and I'll be using Windows as the host, I should not need to have a dual boot then. Thank you very much!",5b7cmq,t1_d9mdi5s,tyggerjai,,Reply,1,0,1
d9me0xt,2016-11-04 22:00:58-04:00,tyggerjai,,"Yep. I have various Linux VMs running on my Linux desktop machine, for most development stuff they work fine. ",5b7cmq,t1_d9mdqej,forexperimentsonly,,Reply,2,0,2
d9mlimo,2016-11-05 01:59:56-04:00,bo1024,,"You could consider building a desktop to run Windows and game, and getting a lower-power laptop to run Linux. The combined cost might be similar to getting a high-powered gaming laptop.

In any case, I definitely recommend getting as much Linux experience as you can, as soon as you can. Running Linux in a fullscreen virtual machine is not bad at all, but try to do everything you can in that VM.

(edit) dual-booting Windows and Linux on a laptop is also a great way to go, I did that for a couple years. The advantage over just booting windows and using a linux VM is that Linux is a bit more responsive and gets access to more system resources in dual-boot. The disadvantage is that hardware compatibility can sometimes be a challenge.",5b7cmq,t3_5b7cmq,forexperimentsonly,,Comment,6,0,6
d9mo7kh,2016-11-05 04:27:37-04:00,falafel_eater,,"There's a big misconception that a computer for CS has to be a high-spec machine. It's pretty far from the truth. 

Most of what you'll be doing with your computer for school will be in languages like Java, Python and possibly Javascript. You will can also expect to do some programming in C and C++.  
With the first three languages, your OS doesn't matter. Portability issues like ""this Python library is only supported on Linux"" will be so rare that you can ignore them.  
With C and C++, it's not quite the same. At least in my own school we always needed to compile C/C++ on a Linux machine. However, it was usually very short code that ran and compiled super quickly, so any performance degradation caused by using a VM was just negligible. Alternately, if you're required to submit code specific to a given environment, it's pretty likely that your school will allow you to access a computer with such an environment.  

It sounds like you want to buy a gaming laptop. If that's the case, buy one. It will be plenty good for at least 85% of everything you need, and if you need more than that, boot up a Linux VM or check what support your school provides.  
You can also try to ask people already 2-3 years into your school's CS program.  

Finally, I do recommend trying to develop familiarity with Unix systems. It's a very good skill to have.",5b7cmq,t3_5b7cmq,forexperimentsonly,,Comment,6,0,6
d9mpy91,2016-11-05 06:24:15-04:00,gyroda,,"Can confirm: I have a gaming desktop that dual boots and a cheap as fuck ARM based chromebook from 2012. The latter could do the majority of my assignments, especially as I could just SSH in to the university's server to work. 

I'll also back asking people already there. They'll know better than anyone. There's probably a Facebook group you can find and post in to ask.",5b7cmq,t1_d9mo7kh,falafel_eater,,Reply,1,0,1
d9memtb,2016-11-04 22:17:56-04:00,colohan,,"Note that if you want to play with Linux, in addition to installing it in a VM on your own machine you can also for very little money ($6/month) get a hosted Linux VM from one of the many cloud vendors (Microsoft Azure, Amazon EC2, Google GCE).",5b7cmq,t3_5b7cmq,forexperimentsonly,,Comment,5,0,5
d9mf36v,2016-11-04 22:30:42-04:00,forexperimentsonly,,"Hmmm this is some new advice! What are some advantages of having a cloud Linux VM? Is it to ""remote"" and get faster computing power and support over yonder?",5b7cmq,t1_d9memtb,colohan,,Reply,1,0,1
d9mfuj8,2016-11-04 22:52:04-04:00,colohan,,"Usually folks set one of these up because they want to have a computer with a persistent network connection.  This is mostly important if you want to run a web server or other online application.

Also, if you want to do some computation you can just rent a VM as large as you need it.  Need to run a big program for a while?  Just rent a VM (or group of VMs) with the number of CPUs and RAM you need.

If you rent a VM, you don't have to worry about installing an OS, setting it up, debugging driver issues etc.  You just click a button, and done.

The downside -- the computer you are using is somewhere else.  You won't have a super fast connection to it for displaying graphics or playing games.  For running a server or doing a batch computation this is irrelevant.  For interactive stuff, this may matter more.",5b7cmq,t1_d9mf36v,forexperimentsonly,,Reply,3,0,3
d9mg481,2016-11-04 22:59:36-04:00,forexperimentsonly,,I see. Thanks for the very detailed explanation!,5b7cmq,t1_d9mfuj8,colohan,,Reply,1,0,1
d9n3x30,2016-11-05 14:11:04-04:00,iCanHelpU2,,"+1 on that. Also many CS programs offer these VMs to you as is. We get access to a general linux environment for all our assignments etc. but our school club also offers a high performance-computing environment for ~0.50 cents a month. 

I personally use Digital Ocean VMs to run my gameserver/projects, the school VMs to test my assignments, and then the club resources to try some stupid stuff. Just last week me and a few friends were trying to compute a dataset that ate up 80 gigs of RAM, so it was nice having a cheap environment to do that. A similar spec machine costs like 3 dollars an hour on Amazon. Then finally I use my personal machines for development. All in all, I spend roughly $20 a month for access to 5 virtual linux environments.",5b7cmq,t1_d9mg481,forexperimentsonly,,Reply,1,0,1
d9mfamy,2016-11-04 22:36:31-04:00,acfman17,,Macbook pricing and features are a joke so there are really two options. You can use a VM to do what you need on Linux but imo just installing Linux is the best option (Mint imo but Fedora/Ubuntu are both also fine for people new to linux). Learning how to navigate a Unix system will be something you need to do anyway at some point in your degree and it is just so much easier to develop on Linux. Dual booting is also fine but unless you are buying a gaming laptop almost all of the games the laptop could run will work on Linux natively or with Wine (check out the PlayOnLinux frontend) and it's not worth the hassle of rebooting whenever doing development.,5b7cmq,t3_5b7cmq,forexperimentsonly,,Comment,3,0,3
d9mg2lu,2016-11-04 22:58:19-04:00,forexperimentsonly,,I see! I am indeed trying to get a gaming laptop. It seems I had a misconception on how games work - I thought Windows games don't usually run on Linux and Wine is a necessary add-on - but that's not the case?,5b7cmq,t1_d9mfamy,acfman17,,Reply,1,0,1
d9mgflr,2016-11-04 23:08:40-04:00,acfman17,,You can't play Windows games natively but many developers support Linux natively and Steam can run on Linux and lets you install any games that have a Linux version how you normally would. Wine is a compatibility layer that lets Windows software run on Linux and PlayOnLinux is a great frontend that makes it very easy to install almost every game that works properly with Wine that have no DRM or something like Steam. Wine often doesn't work for extremely graphically intensive games or games with a lot of DRM so if you are trying to run AAA games then Windows with a VM is probably the way to go.,5b7cmq,t1_d9mg2lu,forexperimentsonly,,Reply,1,0,1
d9mgio3,2016-11-04 23:11:09-04:00,forexperimentsonly,,Got it! Thanks for clearing up my question!,5b7cmq,t1_d9mgflr,acfman17,,Reply,1,0,1
d9mhazi,2016-11-04 23:34:06-04:00,None,,"I think you'll be the most productive as a first and second year student in Windows, then maybe transition to another operating system as a specialization experience if you want.

Its not that it will be easy in Windows, just that you can do it in Windows and figuring it out will set you just as far ahead as doing it in a UNIX-like operating system.

If I were to be in university again I'd be using the school's computer labs more often, which for me were CentOS Linux. The point is to be learning computer science, not messing with operating systems.

Also game on! Buy games :) These are computer programs that show humans rules about rules, which is totally valuable, especially in context of learning how computers do and can function.",5b7cmq,t3_5b7cmq,forexperimentsonly,,Comment,2,0,2
d9mo8g1,2016-11-05 04:29:04-04:00,nilleo,,"I'm primarily a Linux user myself, but I would say use Windows as your primary OS since that's what you're most comfortable with and because it seems gaming support is a factor for you.
 
Windows 10 is a good choice these days because it has the option to run the Windows subsystem for Linux now since the anniversary update. While it's not perfect, it's decent enough to get your feet wet with a Linux console and take advantage of Linux dev-tools.",5b7cmq,t3_5b7cmq,forexperimentsonly,,Comment,2,0,2
d9mpvwg,2016-11-05 06:19:46-04:00,gyroda,,"What does your course primarily use? For me it was Linux. All assignments were tested on Linux and so you needed to be able to use it. The university offered a server you could SSH into and a lab full of Linux PCs. 

I dualboot windows and Linux. It's not hard to do. You can use whatever you need at the time. Just select the OS on startup. You should be comfortable developing in both Windows and Linux to an extent, even if your course only relies on one. Dualbooting with one windows and multiple Linux installations has never been an issue for me. 

Only get a mac if you want a mac. Other than iOS development there's not much you can do on one that you can't do in Linux or Windows, they're meant to have a nice UI but I've not personally used them much. They're nice machines, but you can get nice non-macs.

If you haven't yet, search for this question in this sub via Google. There's a lot of great responses in other relevant threads. 

Tl:dr instead of using a VM you could just install both Linux and Windows. ",5b7cmq,t3_5b7cmq,forexperimentsonly,,Comment,2,0,2
d9mu7xg,2016-11-05 09:58:48-04:00,swilliamseu,,You could buy a proper high tech gaming laptop for personal use as well as the majority of the course and get a smaller cheaper Linux machine for learning the basics of bash and terminal commands.,5b7cmq,t3_5b7cmq,forexperimentsonly,,Comment,2,0,2
d9qufv8,2016-11-08 03:46:06-05:00,pranitkothari,,"Windows. It's user friendly. Much easier to setup and install applications like IDE. Unless you are working in kernel stuff, not need to have Linux. ",5b7cmq,t3_5b7cmq,forexperimentsonly,,Comment,2,0,2
d9msot9,2016-11-05 08:58:51-04:00,Nazfera2,,"Simple.

If you want a job, use windows.

If you want to push yourself to the limit and earn a high pay job, learn linux.

If you want to move to California and be a part of the rest of the hipsters who want to work for apple, buy a mac.


Notice i said 3 key words, use, learn and buy.
This says a lot in this industry.",5b7cmq,t3_5b7cmq,forexperimentsonly,,Comment,2,0,2
d9n53kn,2016-11-05 14:39:04-04:00,JustAnAccount1234567,,"> Windows alone is generally inferior to Linux and OS X in terms of learning computer science.

What?",5b7cmq,t3_5b7cmq,forexperimentsonly,,Comment,1,0,1
5b3xl6,2016-11-04 10:43:23-04:00,9euhe895uhj89eghe89,Can anyone explain why this website malfunctions this way?,"I went to https://paulschou.com/tools/xlate/ to translate decimal into hex. I entered ""31 52 96 37 76 21 29 53 33 99 44 53 49 73 41 68 09 59 18 47 99 89 94 78 43 91 32 77 67 94 07 03 18 04 31 91 90 90 21 18 97 31 98 04 97 17 81 99 19 98 59 31 49 19 31 41 65 14 91 61 97 99 84 30 89 11 93 31 39 37 48 49 18 91 98"" and hit decode, when a whole bunch of Chinese text popped up. What is going on?",,,,,Submission,3,0,3
d9lpqj9,2016-11-04 13:03:12-04:00,Bottled_Void,,"It's just printing out the ASCII values of those numbers.

e.g. 65 = A, 32 = <space>

Depending on your browser and installed character sets will determine what you see.


Do you mean chinese text? Or are you getting a little box with 4 numbers in it? (which corresponds to the hex value)


There is nothing wrong with the website",5b3xl6,t3_5b3xl6,9euhe895uhj89eghe89,,Comment,3,0,3
d9lwngs,2016-11-04 15:17:08-04:00,9euhe895uhj89eghe89,,"https://gyazo.com/632b48504147e894537112d61892ab8d

This is what I get. Didn't test it on other browsers before posting the main post, but when trying it on Internet Explorer instead of Chrome, everything works fine.",5b3xl6,t1_d9lpqj9,Bottled_Void,,Reply,2,0,2
5b3uw1,2016-11-04 10:29:54-04:00,Hustlean,Can someone explain what an API is in the most simplest terms possible ?,,,,,,Submission,19,0,19
d9lkb03,2016-11-04 11:17:11-04:00,PastyPilgrim,,"It's a set of procedures for two applications to share information.

Take twitter. If I wanted to make a twitter application, it would be really difficult to make some program that could understand all of the information that twitter sends when you visit it in the browser. And if twitter ever changes up their layout/design (which they do all the time), then my program would immediately break.

To solve this problem, twitter has an API. In the simplest sense, this API just means that my program can say ""hey twitter, I need Obama's tweets"". Then twitter sends an easily parsable list of Obama's tweets. Or it can say ""hey twitter, tweet out 'Cubs world champs #dope''' and it will do it without your program needing to navigate the buttons and menus needed to send out a tweet.",5b3uw1,t3_5b3uw1,Hustlean,,Comment,17,0,17
d9mm9t4,2016-11-05 02:35:18-04:00,ataturk1993,,Im not a computer science guy but the idea of apis has always been very fascinating for me.,5b3uw1,t1_d9lkb03,PastyPilgrim,,Reply,1,0,1
d9ltevp,2016-11-04 14:14:37-04:00,veritasserum,,"It's the set of rules and behaviors you have to follow to interact with a system.  Typically, this is an abstraction that hides the actual detail of how things get done.

For example, your car has an API: A steering wheel, brake and gas pedal.  You don't see the tyres, brake pads, or engine, nor do you have to care how they work.  You use the familiar ""interface"" which hides the implementation details.

Programs and entire systems use the same idea.  I don't have to know *how* an RDBMS works, I just talk to it via an SQL interface, for instance.   ",5b3uw1,t3_5b3uw1,Hustlean,,Comment,9,0,9
d9ml6jv,2016-11-05 01:45:33-04:00,With0utRemorse,,"An electrical wall outlet is an 'API' for electricity.

An API generally gives you data formatted a specific way or performs a function, based on the inputs provided. You have to conform to the expected inputs to get the valid and correct data/functionality. ",5b3uw1,t3_5b3uw1,Hustlean,,Comment,3,0,3
d9m1v40,2016-11-04 17:01:31-04:00,deftware,,It's an Interface for Programming Applications using existing code. Instead of having to understand all the inner-workings of the code to utilize it you are presented with a sort of 'protocol' that makes it as cohesive as the designer could manage.,5b3uw1,t3_5b3uw1,Hustlean,,Comment,2,0,2
d9mjupt,2016-11-05 00:56:34-04:00,None,,"Functions are a ""carpentry"" tool in computer programming.

An Application Programming Interface (API) is a set of functions related to an application.

An application is something you do with a set of computer things.",5b3uw1,t3_5b3uw1,Hustlean,,Comment,2,0,2
d9lj8gg,2016-11-04 10:54:48-04:00,None,,[deleted],5b3uw1,t3_5b3uw1,Hustlean,,Comment,-8,0,-8
d9lk84h,2016-11-04 11:15:35-04:00,Hustlean,,Thank You !,5b3uw1,t1_d9lj8gg,None,,Reply,2,0,2
d9llthx,2016-11-04 11:47:13-04:00,lneutral,,"I don't want to be a jerk, but the above answer is kind of misleading. /u/PastyPilgrim's answer is more accurate. APIs are not about giving someone a key to use a service, although that could be one mechanism employed by providers.

APIs are literally an [Application Programming Interface](https://en.wikipedia.org/wiki/Application_programming_interface?wprov=sfla1), which mean they define ways your programs can interact with some other technology. Java has an API. Android has an API. In neither case do you request a token or create a session in order to use the technology. 

We sometimes use the term contract - an API provides a bunch of them. If you call toString in Java, you receive a String representing the object you passed in, for example. The API is a collection of contracts, while the actual implementation of the code that satisfies those contracts is provided by someone else. There have even been court cases over whether the contracts themselves - not the code - are subject to copyright infringement when a second company provides its own implementation.",5b3uw1,t1_d9lk84h,Hustlean,,Reply,8,0,8
d9m4zpp,2016-11-04 18:11:46-04:00,yes_thats_right,,"I agree with you, that answer wasn't a description of the general case for an API.

Authentication for example does not exist in the majority of APIs",5b3uw1,t1_d9llthx,lneutral,,Reply,3,0,3
d9m5qc5,2016-11-04 18:29:23-04:00,lneutral,,Relevant username,5b3uw1,t1_d9m4zpp,yes_thats_right,,Reply,2,0,2
5b3rk7,2016-11-04 10:12:45-04:00,TalismanSaber,Book Recommendations,"I am registered to take a low-level programming course (in C/unix) and a data structures course (Java) next semester. I'm pretty much a novice to programming (only two intro courses under my belt) so I know I'm in for a world of hurt in the spring, so just looking for some books that I can use to get ahead. ",,,,,Submission,1,0,1
5b2vek,2016-11-04 06:49:33-04:00,throwfaraway2310,Is a computer security or distributed systems course more important?,"For a general cs background, which course should I take if I can only choose one?",,,,,Submission,3,0,3
d9lba6l,2016-11-04 06:57:35-04:00,crookedkr,,Flip a coin. They were two of my favorite courses and both are meaningful in a general cs education. I guess you could argue that security is applicable to all code and not everything is distributed...,5b2vek,t3_5b2vek,throwfaraway2310,,Comment,5,0,5
d9ld5um,2016-11-04 08:15:19-04:00,moeseth,,I'd choose distributed system anytime. It has more demands and it's really useful in this cloud era. But it comes down to your interest.,5b2vek,t3_5b2vek,throwfaraway2310,,Comment,4,0,4
d9lrv4l,2016-11-04 13:44:03-04:00,TheTarquin,,"I'm biased, since I'm a security engineer, but please, for the love of all that is right and good in the world, take the security course.

It won't teach you everything. It won't prevent all of your mistakes.  That's fine.  If it prevents the most common mistakes and helps you think about abuse cases and to think like an attacker, you'll be miles ahead of most of the other college grads out there.

And hey, you might end up loving the course and want to pursue security as a career!  Which would be an excellent bonus.  We need TONS more people helping to fix the rancid dumpster fire that is modern security.",5b2vek,t3_5b2vek,throwfaraway2310,,Comment,1,0,1
5b2qv3,2016-11-04 06:09:56-04:00,noobeeee,Can a Machine Learning Engineer be any good without distributed computing skill?,"Hello,

I have got an applicant who seems promising. He is familiar with machine learning models and etc.

However, he has not done any work related to distributed computing.

Most of his work was done on his single computer.

Is it a false flag that he is not a good candidate since I believe you cannot be a good machine learning engineer without distributed computing knowledge.",,,,,Submission,2,0,2
d9ldem2,2016-11-04 08:23:42-04:00,watsreddit,,"It's a skill like any other. If he's solid on the tech and theory he does know and he seems like he would be a good culture fit and work hard, I don't see much of an issue personally. I guess it just comes down to how much time you are willing to give to your candidates to get up to speed.",5b2qv3,t3_5b2qv3,noobeeee,,Comment,3,0,3
d9lk949,2016-11-04 11:16:09-04:00,noobeeee,,we are pretty short on time.,5b2qv3,t1_d9ldem2,watsreddit,,Reply,1,0,1
d9mbwhe,2016-11-04 21:04:10-04:00,None,,How are we supposed to know? Interview him. See how it goes.,5b2qv3,t3_5b2qv3,noobeeee,,Comment,1,0,1
5b22ut,2016-11-04 02:23:35-04:00,StellaAthena,Online Master's Advice,"I'm a recent college graduate (BS Math, BA Philosophy from the University of Chicago) who has a job doing mathematics for scientific modeling and CS research. I'm hoping to expand my skills by getting a masters in CS. I took a bunch of CS theory courses as an undergrad but my non-theory skills are on the weaker side. I'm really interested in modeling and AI, but would probably be happy in most spécialisations.

I don't have any interest in quitting my job, so I'm looking for part-time masters programs, preferably online. Right now I'm looking at programs at Johns Hopkin's and Georgia Tech. The Georgia Tech program w/ a [specialization in Interactive Intelligence](https://www.omscs.gatech.edu/specialization-interactive-intelligence) seems likely to be my #1 choice.

What programs would y'all recommend I look at? Are there common pitfalls or similar to avoid when looking at these types of programs (besides for-profit degrees obviously). Any advice would be appreciated.",,,,,Submission,2,0,2
d9le73k,2016-11-04 08:49:34-04:00,andybmcc,,"Some places offer a thesis and non-thesis option.  You probably want to decide which route to go.  If you're doing a thesis, make sure to look at potential advisors.",5b22ut,t3_5b22ut,StellaAthena,,Comment,3,0,3
d9lwoxl,2016-11-04 15:17:56-04:00,StellaAthena,,"Thanks! I haven't seen any thesis options, but I'll be sure to keep an eye out.",5b22ut,t1_d9le73k,andybmcc,,Reply,1,0,1
5ax6pf,2016-11-03 11:00:50-04:00,mad_scientist42,How do I get product information from an amazon link?,"I need to get the price from different sellers. I understand Product Advertising API does this however I do not want to generate affiliate links or even use them. 

I already have a link, I just need to get the product information from it programmatically. ",,,,,Submission,3,0,3
d9kb9k6,2016-11-03 14:22:55-04:00,lgastako,,The Product Advertising API is the source for that information as well.  See https://docs.aws.amazon.com/AWSECommerceService/latest/DG/ReturningPrices.html for example.,5ax6pf,t3_5ax6pf,mad_scientist42,,Comment,3,0,3
d9sr8t3,2016-11-09 08:01:44-05:00,mad_scientist42,,thanks,5ax6pf,t1_d9kb9k6,lgastako,,Reply,1,0,1
d9kccpy,2016-11-03 14:43:18-04:00,Treyzania,,"/u/lgastako is correct, but /r/AskProgramming would be a better sub for this kind of question.  This subreddit is intended more for questions relating to the theory of CS, rather than actual, specific problems.",5ax6pf,t3_5ax6pf,mad_scientist42,,Comment,2,0,2
d9sr8pk,2016-11-09 08:01:38-05:00,mad_scientist42,,noted,5ax6pf,t1_d9kccpy,Treyzania,,Reply,1,0,1
5aqwcu,2016-11-02 12:30:08-04:00,stupidCSstudent,"Second question on linked page, why is it only 2 threads created from fork() call? Why not multiple?",,,,,,Submission,8,0,8
d9ind2d,2016-11-02 13:45:40-04:00,humodx,,"1. process A forks, creating process B (the child)
2. process B enters the if block, process A doesn't
3. B forks, creating C
4. B and C create a thread, 2 threads created
5. A, B and C fork, creating 3 more processes",5aqwcu,t3_5aqwcu,stupidCSstudent,,Comment,1,0,1
d9iu0xu,2016-11-02 15:54:24-04:00,stupidCSstudent,,"So process B goes in the if block, forks and creates C, why doesn:t C stay in the if block?",5aqwcu,t1_d9ind2d,humodx,,Reply,1,0,1
d9liqkh,2016-11-04 10:44:00-04:00,humodx,,"What do you mean, stay in the if block?


Process A: forks B -> if condition fails, skip -> forks D

Process B: forked by A -> if condition succeeds -> forks C -> create thread -> forks E

Process C: forked by B -> create thread -> forks F",5aqwcu,t1_d9iu0xu,stupidCSstudent,,Reply,1,0,1
d9llgc2,2016-11-04 11:40:03-04:00,stupidCSstudent,,">Process C: forked by B -> create thread -> forks F

This part is the one that confuses me, why does process C create a thread if that thread is created in a an IF?  does every process inside the if create a thread? I thought threads were only created by the process that enters the IF.  So B goes into the if, forks with a C and B creates a thread, the new C forked process inside the if with B and so because its a second process it creates another thread? Then outside the if, all processes fork again at the last statement. Right?",5aqwcu,t1_d9liqkh,humodx,,Reply,1,0,1
d9outjm,2016-11-06 19:07:33-05:00,humodx,,"The forked process starts executing from the point it was forked from, so C already starts off inside the if",5aqwcu,t1_d9llgc2,stupidCSstudent,,Reply,1,0,1
5an0ci,2016-11-01 20:56:26-04:00,EntangledAndy,Struggling a lot with writing C programs that involve bitwise operations on various data types. How can I get better at these?,"I'm having a lot of trouble with puzzle problems for a class lab. For example, one of the problems I have to write a program for is as follows:

""Return bit-level equivalent of absolute value of f for floating point argument f. Both the argument and result are passed as unsigned int's, but they are to be interpreted as the bit-level representations of single-precision floating point values. When argument is NaN, return argument.. Legal ops: Any integer/unsigned operations incl. ||, &&. also if, while""

I can't seem to find a good place to start to find a solution to problems like these. Any suggestions to documents I should read or practice problems I should be doing to have a more firm grasp of these types of problems?
",,,,,Submission,1,0,1
d9ij8r7,2016-11-02 12:27:55-04:00,wolverineoflove,,"Review unsigned int binary encoding (get good at this)
Review floating point binary encoding. Single precision is a gimme to keep it easy.

Iterate through your int bits by shifting or masking them a bit at a time. I'd start on the least significant bit but that's just me.
Index of your bitwise traversal should instruct what you build up for output.

Good luck!",5an0ci,t3_5an0ci,EntangledAndy,,Comment,1,0,1
5akryp,2016-11-01 14:29:19-04:00,Fit_Wolf,Single scion?,"Hi

I am currently working on a project. I am new to encryption and a question was recently asked to me about the diagram I created depicting the high level encryption process. Name changed for obvious reasons. 

""Jim wanted to know if the diagram you sent is single scion?""

I am searching for information but I can't seem to find anything besides car parts. Can anyone point me towards some literature on this? ",,,,,Submission,10,0,10
d9h6jge,2016-11-01 14:40:03-04:00,obscureyetrevealing,,"Are you sure they didn't mean ""single sign-on""? ",5akryp,t3_5akryp,Fit_Wolf,,Comment,11,0,11
d9h6qv2,2016-11-01 14:43:58-04:00,Fit_Wolf,,"oh my god

In the email that was sent, they literally spelled ""Single scion."" This is freaking hilarious. Thank you. ",5akryp,t1_d9h6jge,obscureyetrevealing,,Reply,4,0,4
d9hju2p,2016-11-01 19:06:26-04:00,lneutral,,"Any chance they dictated their email contents via speech-to-text?

I occasionally get a weird clunker from a relative who does that.",5akryp,t1_d9h6qv2,Fit_Wolf,,Reply,1,0,1
5akcch,2016-11-01 13:16:45-04:00,Zeekawla99ii,"Any recommendations? Need open-source code for Gibbs sampling---possibly in C/C++/Python, etc.","I actually don't think there are many great options here for the community. Many scientific papers use Gibbs sampling, but there are surprisingly few open-source options available to do this.
Often when I bring this subject up, people will recommend https://pymc-devs.github.io/pymc/ or suggest just do Metropolis-Hastings.

Is there any C/C++ code lying around? Thanks for the help",,,,,Submission,2,0,2
d9h5q4t,2016-11-01 14:24:24-04:00,sandwichsaregood,,"Most people I know (which is quite a few, I specialize in MCMC methods) code it themselves, it's relatively easy. [This](https://darrenjw.wordpress.com/2011/07/16/gibbs-sampler-in-various-languages-revisited/) article has several implementations, including one in C (with GSL) and you can find lots more with a quick Google. The algorithm is pretty straightforward and there aren't really any huge gotchas like you get with something like factorizing matrices: you can pretty much just implement the pseudocode for the Gibbs sampler and expect it to work.",5akcch,t3_5akcch,Zeekawla99ii,,Comment,5,0,5
5aj6f8,2016-11-01 09:54:51-04:00,inviztj,"Not sure if in the right sub, but in software development, does end-user testing or file conversion come first?","Basically, I was given [this PERT chart](http://puu.sh/s2QvJ/932a6348e4.png) and we were supposed to identify the letters with the correct activities.

This is my current answer:
>A: Write code

>B: Install new hardware

>C: Test code

>D: Convert files

>E: Write user documentation

>F: Test system

>G: End-user testing

>H: Train users

I'm stuck between letting G or D be end-user testing. I know end-user testing is the last of the tests, but if D is end-user testing, then why are we able to run both system testing (F) together with end-user testing?

And if you do have the time, can you help me to check my other answers as well. Thank you!

EDIT: Duration is in weeks

EDIT 2: If a new activity, write technical documentation, is to be added, where is an appropriate starting and finishing point?",,,,,Submission,1,0,1
d9gyc62,2016-11-01 12:02:28-04:00,xiongchiamiov,,"This is going to depend on your situation, your product, and your company.

The right answer is the one in your textbook.",5aj6f8,t3_5aj6f8,inviztj,,Comment,6,0,6
d9hq9ml,2016-11-01 21:19:12-04:00,inviztj,,"Unfortunately this was an exam question and my teacher didn't provide answers. Let me try to elaborate more on the question and my doubts:

The context of the question is that a supermarket chain has hired a software company to create a software that would implement new uses of their customer purchase data. So the software developers came up with the PERT chart I've linked to, with the activities as stated. The question is then to replace the letters in the PERT chart with the correct activities.

I'm guessing that activity G is end-user testing, because I know training users have to be the last activity so end-user testing would most likely be the 2nd last. But if activity G is end-user testing, then the activity file conversion, which I assume to be converting the old customer purchase data to a new format which the new software supports, would come before end-user testing. But it doesn't make sense to me to convert your files before the system is tested by the client and accepted. Thus my confusion and question.",5aj6f8,t1_d9gyc62,xiongchiamiov,,Reply,1,0,1
d9hy76g,2016-11-02 00:19:31-04:00,xiongchiamiov,,"This is a made-up situation, and thus the answer of ""what did the software company provide?"" is also made-up.

In most real-life situations, no one creates a complicated chart like that.  It's more like ""so and so complained about this thing, so I thought maybe we'd fix it.  I went and talked to Joe about it, and he suggested I use method X.  So I did that, and the pull request included the implementation code and the tests for it, and then we shipped it after code review.""

Most software shops now try to at least parts of [the Agile Manifesto](http://agilemanifesto.org/), which usually means that you're doing all these things over and over again.  So would we do end-user testing before or after converting all of the production data?  The answer is both.",5aj6f8,t1_d9hq9ml,inviztj,,Reply,1,0,1
d9hyx42,2016-11-02 00:40:50-04:00,inviztj,,"I see, thank you!",5aj6f8,t1_d9hy76g,xiongchiamiov,,Reply,1,0,1
d9gxz4u,2016-11-01 11:55:34-04:00,njaard,,"I've been in industry for more than 10 years, and I have no idea what you're talking about.",5aj6f8,t3_5aj6f8,inviztj,,Comment,3,0,3
d9hq60a,2016-11-01 21:17:09-04:00,inviztj,,"Sorry for the ambiguity. The context is that a supermarket chain has hired a software company to create a software that would implement new uses of their customer purchase data. So the software developers came up with the PERT chart I've linked to, with the activities as stated. The question is then to replace the letters in the PERT chart with the correct activities.

I'm guessing that activity G is end-user testing, because I know training users have to be the last activity so end-user testing would most likely be the 2nd last. But if activity G is end-user testing, then the activity file conversion, which I assume to be converting the old customer purchase data to a new format which the new software supports, would come before end-user testing. But it doesn't make sense to me to convert your files before the system is tested by the client and accepted. Thus my confusion and question.

I hope this clarifies my question.",5aj6f8,t1_d9gxz4u,njaard,,Reply,1,0,1
5agix5,2016-10-31 21:48:27-04:00,Everlastinglol,Hash table run time,"I read that as you increase the size of the array of buckets, the time complexity will generally decrease as there are less collisions. So why not use a very large array size?
I am guessing it is because even though the run time decreases the loading time( does memory size increase loading time?) increases due to memory size needed.


1. So how is the array size determined to balance between run time and load time.

2. and also, does array size increase initial loading time?

3. What memory does the the array size use and what memory does the run use? Do trhey use the same?
Correct me if I am wrong.
Thank you",,,,,Submission,8,0,8
d9gdxw0,2016-10-31 23:32:49-04:00,daV1980,,"The reason to not make it larger is because of the memory consumption, not because of load time. Most hash tables in practice use buckets of lists to avoid a couple of problems:

- no need to preallocate a massive table, because you can just add more links in each bucket. 
- no hard max size. 
- no need to resize the table (so no surprise 1000x as expensive inserts)

This does have some downsides. First, the worst case performance is linear (if everything hashes to the same bucket). Regular hash tables can too though, so that's not terrible. 

Performance falls to death of a thousand cuts. It just gets gradually worse, to which can be hard to notice. 

Initializing a hash table is an O(buckets) operation, but that's not really a significant concern because as a practical matter the operation on each bucket is diiiiiiirt cheap.",5agix5,t3_5agix5,Everlastinglol,,Comment,4,0,4
5af6vi,2016-10-31 17:39:27-04:00,quinntuplets,Loading a webpage,"I dont know if anyone here will be able to help me with this question but basically tomorrow I am registering for classes at my university. The system works in a way where at 7:00 AM the registration link activates and everyone rushes to type their course numbers in before classes fill up. My question is, when i hit the link to open the registration page, will the pge load if I do it 5 or so seconds before 7:00am? My laptop hs taken 5-15 seconds to load the page in the past after me having clicked the link exactly at 7. ",,,,,Submission,0,0,0
d9g2q24,2016-10-31 19:02:40-04:00,adbJ114,,"The request that you send to their server is a small packet, so it will likely be delivered quickly. When your uni receives the request for the page it will send the page back to you in its current state. So even if it takes a little while for it to load on your laptop, what will eventually load is the fifteen second old version of the page.",5af6vi,t3_5af6vi,quinntuplets,,Comment,1,0,1
d9ga9wy,2016-10-31 22:01:00-04:00,quinntuplets,,So a few seconds before should redirect me the quickest?,5af6vi,t3_5af6vi,quinntuplets,,Comment,1,0,1
d9gc325,2016-10-31 22:45:11-04:00,adbJ114,,Your only truly safe bet is to make the request at 7:00. My point is it will likely be much faster (a matter of milliseconds) to send the request than to receive and render it in your browser. The latter is likely where most of the 10-15 seconds take place.,5af6vi,t1_d9ga9wy,quinntuplets,,Reply,1,0,1
5aeo20,2016-10-31 16:14:04-04:00,pas43,Machine learning math problems,"Are there any books that have plenty of examples of the mathematics of ML? I'm getting behind in class and would love to learn more as it is a field I am very interested in. 

I have learnt things like Bayes and decision trees, I understand the concept but not the math. I came to uni as an mature student and had been out of education for a while I love coding but as I've just come into my final year started to realise how important math is. I failed all math at high school and college 8 years ago just wondered if there is a nice easy way to explain the math I want to learn. 

Thanks",,,,,Submission,2,0,2
d9fx7as,2016-10-31 17:04:06-04:00,Mukhasim,,"What you're asking for is a pretty tall order. Understanding ML deeply requires knowledge of linear algebra, calculus and probability. That's a few years' worth of study for most people, and if you failed at math in school before then I wouldn't advise you to go into it expecting to do it faster than most people.

In short, I doubt you'll be able to catch up on math fast enough to help you very much with any course that you're in the middle of right now. This is going to take a considerable commitment on your part.

What you might try is joining Andrew Ng's machine learning course (on Coursera), and completing the first 2-3 weeks of work minimum. It has a pretty useful intro to the basics of linear algebra, although even as basic as it is it might still be too rough for you if you haven't seen matrices before. He also has [a linear algebra review on his Stanford website](http://cs229.stanford.edu/section/cs229-linalg.pdf), but if the Coursera intro is too fast for you then I doubt you'll get much out of this.

Ng's course gives a great overview of important ML techniques without getting into too much math, so the whole thing might help you out in general (especially with neural networks) if you're struggling with your course.

If you're comfortable with programming in Python, I suggest that you try Philip Klein's linear algebra course, called [Coding the Matrix](http://codingthematrix.com/). It will get you comfortable with the basics of linear algebra, which is probably going to be the most important gap for you to fill in your knowledge. You don't need a textbook for the course, but if you want one, I suggest [Jim Hefferon's *Linear Alegebra*](http://joshua.smcvt.edu/linearalgebra/), which is available for free. (The *Coding the Matrix* textbook is unnecessary IMO, as much of its contents is available as online notes with the course. I did buy it myself but I didn't use it much.) Note that Klein's course doesn't cover a lot of material (it's not long enough), but it will get you off the ground nicely.

For calculus, there are lots of options to learn it but I don't think there are any shortcuts unless you took those courses before and at least understood the basics. I suggest Ash & Ash's *The Calculus Tutoring Book*, which teaches you the concepts of calculus with a minimum amount of formalism. I find it to be the best quick-and-dirty calc book, but it's not *that* quick, so you'll still have to invest considerable time to learn it. You might also try Khan Academy, people praise it often. If you took calc before and sort of understood it (you at least understood limits), then you might also profit from the [MIT OpenCourseWare lectures](https://ocw.mit.edu/courses/mathematics/18-01-single-variable-calculus-fall-2006/video-lectures/).

For probability and stats, a gentle introduction is Freedman, Pisani and Purves (try to get it from the library because it's expensive and I doubt you'll want to read it more than once). Then Ross's *A First Course in Probability* is good for more probability. His proofs are a bit flaky but the book has lots of good problems (including the example problems in the chapters). I don't know a great second stats book.

For ML itself, a good non-math-oriented book is Kuhn and Johnson's *Applied Predictive Modeling*.",5aeo20,t3_5aeo20,pas43,,Comment,2,0,2
5ae6t4,2016-10-31 14:58:12-04:00,dumstick,What are some classic CS problems?,"There seems to be an understood pool of problems from easy like fizzbuzz and the beer song. To NP complete / Hard ones unsolved like TSP, MSP and knapsack. Can you list more of the hard interesting ones?",,,,,Submission,12,0,12
d9fvtdt,2016-10-31 16:37:26-04:00,soulwatcher,,Tower of Hanoi is another famous problem. ,5ae6t4,t3_5ae6t4,dumstick,,Comment,12,0,12
d9fx1tf,2016-10-31 17:01:05-04:00,dumstick,,Interesting ,5ae6t4,t1_d9fvtdt,soulwatcher,,Reply,1,0,1
d9g5zyr,2016-10-31 20:21:38-04:00,xonelast,,Stable-marriage problem,5ae6t4,t3_5ae6t4,dumstick,,Comment,10,0,10
d9g9xxv,2016-10-31 21:53:13-04:00,kkradical,,"boolean satisfiability is the basis of a lot of things. dominating set, independent set and clique are a few graph ones.",5ae6t4,t3_5ae6t4,dumstick,,Comment,2,0,2
d9gb8tv,2016-10-31 22:24:28-04:00,umib0zu,,"Just tossing some out here:

Some classics are [clique problems, 2-SAT, Prime Decidability](https://www.youtube.com/watch?v=msp2y_Y5MLE). If I'm not mistaken, graph isomorphism is still unsolved, and is fascinating because it has a lot of applications.

Some applied CS polynomial ones could include are Viterbi algorithms. Also, there's lots of stuff in Operations Research. Max Flow, Transportation, TSP, Knapsack, Scheduling, etc. all fit into Linear Programming which is what I think fits into the Polynomial class.

Science has interesting ones that are iterative problems, which I'm not sure fit into a complexity class model. They are finding zeros, eigenvector, function minimization, PCA/SVD, and of course Fourier or Laplace Transforms. Pagerank and any function minimization problem related to Ax=b in gradient descent/stochastic gradient descent which is like, all of machine learning/stats comes to mind. ",5ae6t4,t3_5ae6t4,dumstick,,Comment,2,0,2
d9ggbln,2016-11-01 00:46:49-04:00,craiig,,https://en.wikipedia.org/wiki/CAP_theorem,5ae6t4,t3_5ae6t4,dumstick,,Comment,1,0,1
d9gh8z2,2016-11-01 01:21:41-04:00,Slatts02,,Minimum change received from a vending machine ,5ae6t4,t3_5ae6t4,dumstick,,Comment,1,0,1
d9h2if0,2016-11-01 13:22:50-04:00,lordvadr,,[Eight queens on a chessboard](https://en.wikipedia.org/wiki/Eight_queens_puzzle) has always been a favorite of mine.,5ae6t4,t3_5ae6t4,dumstick,,Comment,1,0,1
d9h37g2,2016-11-01 13:36:08-04:00,dumstick,,Sweet!,5ae6t4,t1_d9h2if0,lordvadr,,Reply,2,0,2
5abhne,2016-10-31 06:24:08-04:00,uriles,CS Research Institutes and Companies,"Hi all,

I am looking for a good research institute for my Master's Thesis, starting January 2018. However, The problem I have found lately is that there is no source of information that brings together all these institutes, or at least I have not found it. I would like to get to know the ones you know (specially in Europe, or alternatively I would rather USA, Canada or Australia) or any source of information where I can get to know them. 

My Master focuses on Internet Technology and Architecture (Internet of things, User-Centric Networking, p2p, etc.), and my ideal job would imply a lot of Maths and algorithms, while a lot of programming as well. 
However, I would appreciate any research institute that you know, and any website or other info you have.

Thanks in advanced!

P.S.: Here are the ones I know:
IMDEA Institutes (Madrid, Spain): http://www.imdea.org/
CERN (Geneva, Switzerland): https://home.cern/
CNRS (France): http://www.cnrs.fr/en/institutes/insmi-mathematical-sciences.html
Fruenhofer (Germany): https://www.fraunhofer.de/en.html",,,,,Submission,4,0,4
d9fgicm,2016-10-31 11:48:45-04:00,StellaAthena,,"You might want to consider military or intelligence word jobs in the country you're a citizen of. Both actual government organizations as well as consulting and contracting groups hire people to do mathematical modeling, algorithm design, statistical analysis, and similar jobs. The applications can be anything from cryptography, to cyber security, to scientific modeling for a research group (e.g. space agencies), communications technology, to military tech development (in the US there's a group called DARPA that does a lot of this).

I do something in this sphere and can tell you a bit about it if you pm me",5abhne,t3_5abhne,uriles,,Comment,2,0,2
5a511s,2016-10-30 03:23:51-04:00,jnzq,Four Principles of Object-Oriented Programming,"Why is it that there are four different principles in object-oriented programming? I've always wondered this because aren't abstraction and encapsulation too similar to be classified as separate concepts, and the same goes for inheritance and polymorphism? I know they're different, but I feel like they can be counted as subsets of each other.",,,,,Submission,8,0,8
d9dqvhu,2016-10-30 03:53:01-04:00,tyggerjai,,"There aren't ""4 principles"". That's one interpretation as taught, and you'll find very little consensus. Those are the 4 taught at my institute, and they're taught that way because abstraction and encapsulation are key principles of OO *or* procedural, they're just good design, and they lead well into polymorphism, which really is key to OO. Inheritance is taught because it's a common and powerful tool for polymorphism, and because it demonstrates polymorphism simply, but modern OO favours composition over inheritance, so as far as ""key principles"" go, it's not necessarily a great one - it's an implementation. 

Between them, they capture the essence of OO design and a reasonable implementation, but other institutes might teach SOLID as ""the 5 principles"", for example.

Edit: tl:dr, you have been taught some useful sound bites, you are right to question them, and you will grow beyond them. ",5a511s,t3_5a511s,jnzq,,Comment,9,0,9
d9dt9vg,2016-10-30 06:31:09-04:00,bartturner,,"""but modern OO favours composition over inheritance ""

Thanks so much for including this.    I continue to be frustrated that ""students"" are NOT being taught composition and continue to learn Inheritance.    

My oldest two sons are both Computer Science majors.   There is an incredible opportunity to teach proper software engineering early and avoid bad habits but, IMO, many schools are messing it up.



",5a511s,t1_d9dqvhu,tyggerjai,,Reply,10,0,10
d9dtf1d,2016-10-30 06:40:47-04:00,tyggerjai,,"Yep. I teach inheritance as one of the 4 principles, but we're in the process of re-visiting that, and I mention as much as possible that it's a terrible idea. I emphasise interfaces as strongly as possible.

To be fair, OO is just hard to teach, and inheritance is ""easy"" polymorphism. But yes, we need to stop teaching the hammer of inheritance, because then everything looks like your thumb.",5a511s,t1_d9dt9vg,bartturner,,Reply,4,0,4
d9erx42,2016-10-30 21:48:26-04:00,som_gye,,"I've graduated from a programming tech school, and will graduate somewhat soon from a state university with a computer science degree, and I've never even heard composition mentioned. What am I missing out on?",5a511s,t1_d9dt9vg,bartturner,,Reply,4,0,4
d9ete2i,2016-10-30 22:21:01-04:00,tyggerjai,,"Has anyone mentioned interfaces?
Basically, inheritance is a powerful tool for things that are sufficiently similar that there are only small differences in how they implement a particular method. 

Usually, though, that's not the case, and attempting to force things into one category so you can use inheritance generally creates more problems than it solves. Usually, you are more interested in what a thing can do than in what it is, and ""yes I can do this thing"" is rarely the same as ""yes I am this thing"". So ""composition"", and ad-hoc polymorphism, give you a technique for leveraging polymorphism with techniques that say ""Even though a Camel is an Animal and a Motorbike is a Vehicle, they both implement ""Ridable"", so if you need a ""thing you can ride"", either of them will do *regardless of type*"".",5a511s,t1_d9erx42,som_gye,,Reply,6,0,6
d9f6ua6,2016-10-31 07:34:12-04:00,som_gye,,Thanks for the write up. I'll have to look into that and save some resources on it.,5a511s,t1_d9ete2i,tyggerjai,,Reply,1,0,1
d9dwquc,2016-10-30 09:37:57-04:00,paithanq,,Dale Skrien's book has a great section about composition vs. inheritance. ,5a511s,t1_d9dt9vg,bartturner,,Reply,1,0,1
d9e1yxz,2016-10-30 12:15:43-04:00,None,,"I think ""this is a thing"", it can ""be a thing"" or ""have a thing"" is all any regular programming student needs right? Those specific terms are easy to pick up after inventing the concept yourself.

For ""students"" these terms are just unnecessarily confusing, and the ""haha gotcha you can't remember that term"" is silly.",5a511s,t1_d9dt9vg,bartturner,,Reply,1,0,1
d9ejkht,2016-10-30 18:42:24-04:00,tyggerjai,,"The issue isn't the names. The issue is that modern OO considers one of them a good thing and one of them a necessary evil, whereas university courses still teach as if the dubious one is the holy grail and the good one is an afterthought. ",5a511s,t1_d9e1yxz,None,,Reply,3,0,3
d9emnmv,2016-10-30 19:52:31-04:00,jnzq,,"Well, I understand they're just guidelines on how to program, but why are they being taught as separate concepts when a lot of these can be grouped into one? (from my perspective at least)",5a511s,t1_d9dqvhu,tyggerjai,,Reply,1,0,1
d9en56l,2016-10-30 20:03:53-04:00,tyggerjai,,"I teach abstraction and encapsulation as a chunk, because encapsulation is really just the implementation of abstraction. I also teach polymorphism as a concept or principle, and then inheritance and interfaces/ad-hoc as implementations. So yes, they absolutely overlap, and if you search stack overflow you'll see that confusion between abstraction and encapsulation is incredibly common for that reason. 

I have issues with the ""4 principles"" for that reason, but OO is difficult to teach for various reasons, so those 4 will do as well as any others. I definitely teach them as overlapping, but there are certainly nuances in there - at some level they're  not all the same. Inheritance is a mechanism for providing polymorphism, it's not the same thing as polymorphism. ",5a511s,t1_d9emnmv,jnzq,,Reply,2,0,2
d9e1u7c,2016-10-30 12:12:21-04:00,None,,"> and they're taught that way because abstraction and encapsulation are key principles of OO or procedural, they're just good design

As a professional programmer I cannot agree with this statement in all or most cases.",5a511s,t1_d9dqvhu,tyggerjai,,Reply,-4,0,-4
d9eja2l,2016-10-30 18:35:44-04:00,tyggerjai,,"You don't think encapsulation and abstraction are good design? Abstraction *layers* can lead to the ""there is no problem that cannot be solved by another layer of indirection"" problem, but surely the process of designing a program so that the discrete sections of it do clearly defined and separate things is just good practice. 

If a program needs to talk to other things, then you split the talky bits out from the data bits and the UI bits and you have clearly defined API/interfaces between them so your Comms layer doesn't have bits of presentation layer. Which bit is controversial?",5a511s,t1_d9e1u7c,None,,Reply,4,0,4
5a1kvm,2016-10-29 13:38:59-04:00,serve11,Are there any co-non-context free languages?,I know that context free languages are not closed under complement and I suspect that's true of non-context free languages as well. However does there exist any single non-context free or even context free language that is closed under complement?,,,,,Submission,2,0,2
d9czt1s,2016-10-29 14:17:48-04:00,east_lisp_junk,,"> However does there exist any single non-context free or even context free language that is closed under complement?

Are you asking about a set (of strings) being closed under some complement operation on strings? Or about a set that is a superset of its own complement (true for the whole universe and nothing else)?",5a1kvm,t3_5a1kvm,serve11,,Comment,1,0,1
d9d1625,2016-10-29 14:51:37-04:00,serve11,,I'm asking whether there exists a non-context free language whose complement is also non-context free.,5a1kvm,t1_d9czt1s,east_lisp_junk,,Reply,1,0,1
d9d5vbq,2016-10-29 16:54:19-04:00,east_lisp_junk,,"Just off-hand, I would expect a^(n)b^(n)c^(n) to satisfy that.",5a1kvm,t1_d9d1625,serve11,,Reply,1,0,1
d9d8s7h,2016-10-29 18:13:58-04:00,serve11,,"Yes, that language is non-context free, but it's complement is context free.",5a1kvm,t1_d9d5vbq,east_lisp_junk,,Reply,2,0,2
d9dgdjr,2016-10-29 21:38:53-04:00,tavianator,,Uh how so?  What CFG generates the complement of a^n b^n c^n ?,5a1kvm,t1_d9d8s7h,serve11,,Reply,1,0,1
d9djlfr,2016-10-29 23:05:56-04:00,serve11,,http://cs.stackexchange.com/questions/7190/construct-a-pda-for-the-complement-of-anbncn,5a1kvm,t1_d9dgdjr,tavianator,,Reply,2,0,2
d9dklpo,2016-10-29 23:34:50-04:00,tavianator,,"Indeed, seems obvious in hindsight!  Here is an example though: the language of Turing machines that terminate is not context free.  Its complement is not even recursively enumerable, therefore also not context free.",5a1kvm,t1_d9djlfr,serve11,,Reply,2,0,2
d9fm992,2016-10-31 13:33:32-04:00,serve11,,"Yeah, I think that is what I'm looking for, thank you.",5a1kvm,t1_d9dklpo,tavianator,,Reply,1,0,1
d9fmkip,2016-10-31 13:39:19-04:00,tavianator,,"Another example (that uses only decidable languages) is something like {a^n b^n c^n } U ~{d^n e^n f^n }, where ~L denotes the complement of L.",5a1kvm,t1_d9fm992,serve11,,Reply,1,0,1
59y1lj,2016-10-28 20:48:25-04:00,Everlastinglol,Why is my destructor giving me double free or corruption(fasttop) error?,"    This is my code. Any help greatly appreciated

    ~HashTab()
        {
            for (int i = 0; i < TABLE_SIZE; i++)
            {
                if (htable[i] != NULL)
                {
                    Node *prev = NULL;
                    Node *current = htable[i];
                    while (current != NULL)
                    {
                        prev = current;
                        current = current->next;
                        delete prev;
                    }
                }
                delete[] htable;
            }
        }",,,,,Submission,0,0,0
d9c9keu,2016-10-28 21:40:14-04:00,th3t,,move delete[] htable:  after the for loop?,59y1lj,t3_59y1lj,Everlastinglol,,Comment,9,0,9
59uezn,2016-10-28 09:32:19-04:00,tergeonkarells,[FR] - Configuration avis,"Bonjour!

Pouvez vous me donner votre avis sur cette configuration, s'il y a des incompatibilités etc.

Boitier: Zalman H1
Alim:   Cooler Master V850 Modulaire - 850W
Disque dur: Seagate BarraCuda 2 To
SSD: Corsair Force Series LS - 120 Go
Carte mère: Asus Z170 Pro Gaming
Processeur: Intel Core i5 6600K
Carte Graphique: Zotac GeForce GTX 1070 AMP! Extreme - 8 Go
Refroidissement Processeur: Watercooling Corsair Hydro Series - H110i
RAM:  Corsair Vengeance LED White DDR4 2 x 8 Go 3000 MHz CAS 15

J'ai fait la config via Materiel.net

Je n'ai que des connaissances basiques, je m'en remet à vous pour déceler défauts et aberrations.


Merci! Bonne journée / week-end",,,,,Submission,1,0,1
d9bz5vi,2016-10-28 17:05:21-04:00,MikeBenza,,"[pcpartpicker.com](http://pcpartpicker.com/) est le mieux pour choisir et configurer un ordinateur.  Vous pouvez découvrir là s'il y a des incompatibilités.  Mais c'est en anglais.

Ce subreddit est pour informatique théorique.  Vous cherchez /r/buildapc.  

(Le propriétaire de pcpartpicker.com, /u/pcpartpicker, est un ami.)",59uezn,t3_59uezn,tergeonkarells,,Comment,2,0,2
d9benzv,2016-10-28 10:00:36-04:00,nbp615,,Just letting you know most people probably don't speak French here.,59uezn,t3_59uezn,tergeonkarells,,Comment,1,0,1
d9bfbey,2016-10-28 10:16:21-04:00,capcom1116,,"Cet endroit est pas pour savoir comment construire des ordinateurs. Cet endroit est pour des questions sur le domaine universitaire de la science informatique. Cependant, cette configuration semble ok. Cette réponse a été créée en utilisant Google Translate.",59uezn,t3_59uezn,tergeonkarells,,Comment,1,0,1
d9bh5cy,2016-10-28 10:57:00-04:00,tergeonkarells,,"Thanks.
Sorry for the inconvenience, seemed to me as the most appropriate subredit.
",59uezn,t1_d9bfbey,capcom1116,,Reply,2,0,2
59sca0,2016-10-27 23:16:11-04:00,PerfectisSh1t,Need idea for a research subject.,"I have a research to do where i can choose my subject, I am very interested in quantic computing, but i am very new to it and am kind of intimidated by the subject itself, but i think this is a good opportunity for me to dip my toes in. I need a subject that is of actuality ( where i can find recent news talking about it or something of the sort.) and it needs to not be too abstract where ill be able to wrap my head around the concept discussed in the thesis and research i read. I was thinking something along the line of ""the effect of quantic computing on modern cryptography"" But after i quick research i found alot of thesis and research on the subject were dating all the way back to the early 90s (which makes me think the problematic isn't very actual.) maybe the problem has already been solved? I am very out of the loop and i am not aware of what is being done or the problem of the era in quantic computing.

I would like to keep quantic computing as my research subject but if anyone could point me in a direction where i can concentrate on my homework and not worry too much about the pertinence of my work.",,,,,Submission,3,0,3
d9b6ui9,2016-10-28 04:36:19-04:00,epiphanius_zeno,,"If you're an expert in the mathematics of infinite-dimensional Hilbert spaces and their operators, go for it! As far as I am aware, the quantum computers from D-Wave use quantum annealing, analogous to simulated annealing, to perform computations. It might help if you are familiar with other hill-climbing algorithms as well. There are other ways to do quantum computation, at least theoretically, but I don't know if anyone has overcome the engineering problems yet. 

One thing to keep in mind is that quantum computers aren't magical. Anything a quantum computer can do a ""regular"" computer can do. The only difference may be efficiency and/or time needed to perform the computation. 

What attracts you to quantum computing? If you know that then it may help to narrow down your study or perhaps pivot into a new area. ",59sca0,t3_59sca0,PerfectisSh1t,,Comment,4,0,4
d9bt8uz,2016-10-28 14:59:36-04:00,PerfectisSh1t,,"I am not an expert by any stretch of the mind.

This research is an entry level class at my university to teach you how to do a university level research. maybe quantum computing is a little too technical.

the things that attract me to quantum computing are the possible effect it could have on our lifes, i think its fascinating.

After looking a little into it, i was thinking maybe something along the line of : Decoherence and the possible solution to the hurdle of quantum computer.

would that be a decent angle?  ",59sca0,t1_d9b6ui9,epiphanius_zeno,,Reply,1,0,1
d9aztya,2016-10-27 23:51:56-04:00,cse218,,"Random thought to throw out - quantum computers are great at parallel processing, maybe you could talk about their impact on algorithms?",59sca0,t3_59sca0,PerfectisSh1t,,Comment,1,0,1
59qz30,2016-10-27 18:31:12-04:00,Irkutsk2745,"What could be some sexy(or less sexy, doesn't really matter) computer science networking research?","I am definitely an 'admin' type of guy when it comes to my skill sets.
However, I do really want to continue to participate in science even after I finish college(perhaps a doctorate).

So, since I am highly interested into computer networking and security I wonder what kind of research I could do without going deep down into programming too much. Scripting and some easier programming is fine though.
I am thinking about trying out implementations of various stuff and meassuring and anlysing the results, compile research as well.",,,,,Submission,11,0,11
d9aoc6b,2016-10-27 19:08:33-04:00,I_Do_Not_Abbreviate,,"Study viruses and other malicious programs (rootkits, spyware, et cetera). Set-up your own virtual honeypots then directly expose them to the network edge so you can look for new code in the wild: plus you get to feel like a hacker since you have to learn how cybercriminals think in order to stop them. 

Whitehats are paid well because they want to keep them on the side of the company: studying information security can only add to any administrator skillsets.",59qz30,t3_59qz30,Irkutsk2745,,Comment,5,0,5
d9ararg,2016-10-27 20:20:53-04:00,0x68656c6c6f,,"I'd suggest taking a look at MegaMIMO: http://www.csail.mit.edu/solving_network_congestion

The original paper is here: http://people.csail.mit.edu/rahul/papers/megamimo-sigcomm2012.pdf

And the recent one: http://dl.acm.org/citation.cfm?id=2934905",59qz30,t3_59qz30,Irkutsk2745,,Comment,1,0,1
59qxge,2016-10-27 18:22:16-04:00,jamjam89,How to be more consistent with personal performance?,"Some weeks I'm on FIRE, and code like a mad man. But a couple of days out of the week I can barely figure out how if statements work. I've come up with some solutions at the work place... so I only program when I'm feeling ""it"" otherwise I do project planning. 

The problem comes when I have to do tests or interviews, where you have to be ""on"" the day of. I'm trying to be a little more consistent in my my performance, anyone have any personal solutions they'd like to share?",,,,,Submission,14,0,14
d9av3za,2016-10-27 21:53:10-04:00,combuchan,,"I would consider this a bug in our version of Human.  It's very widespread. 

You can work around it by taking care of yourself (exercise right, eat right, increase sleep quality, moderate or abstain from alcohol, etc) to stabilize your energy levels.  Pace yourself as to not get burned out.   Find your work interesting enough to get it done or find a new place to work.  Recreate, especially outdoors or go on vacation.  

I have a lot of off days but the last time I had too many I realized I was burned out and found a new job that was much better.",59qxge,t3_59qxge,jamjam89,,Comment,12,0,12
d9b0d27,2016-10-28 00:07:14-04:00,ToothpasteSandwich,,"It's more important to look at bigger picture of your productivity. Did you get all lot done during the last month? If you yes then no need to worry about a few unproductive days.

Also to try and be productive when things aren't going well, look for a few quick easy things you can do to make progress. Fix an simple bug, write some tests, or refactor some code. Just check something in.",59qxge,t3_59qxge,jamjam89,,Comment,3,0,3
d9bb987,2016-10-28 08:23:31-04:00,SayYesToBacon,,Start doing meth,59qxge,t3_59qxge,jamjam89,,Comment,-1,0,-1
59pznh,2016-10-27 15:32:51-04:00,MicrowavedPizza,Link between Fermat and encryption,"Hi!

I have to write a text for school about *Fermat's Little Theorem* and I was wondering what the link is between it and modern day encryption. I've been reading up on the subject but it's really hard to understand it when you don't have any real knowledge about encryption. So I was wondering if you could help me. 

Thanks in advance.",,,,,Submission,5,0,5
d9ajuvq,2016-10-27 17:23:35-04:00,granddave,,"Take a look at RSA cryptography, I believe that it uses fermat's little theorem. ",59pznh,t3_59pznh,MicrowavedPizza,,Comment,1,0,1
59pifd,2016-10-27 14:09:58-04:00,stupidCSstudent,Operating systems question. how many process created in code?,"Including the initial parent process, how many processes are created by the program shown in Figure 3.32?
    #include <stdio.h>
    #include <unistd.h>
    int main()
    {
    int i;
    for (i = 0; i < 4; i++)
    fork();
    return 0;
    }
Figure 3.32 How many processes are created?

this is a HW question, got it wrong and am curious if anyone can explain why it is 15 processes?  

From this question http://stackoverflow.com/questions/20028026/os-how-many-process-are-created-by-the-program I can see but I'm not grabbing the explanation given there. forks create copies of themselves. this loops 4 times, first fork creates 1 process and another copy i assumed, then loop again and so on until 4. ",,,,,Submission,2,0,2
d9afl29,2016-10-27 15:53:08-04:00,pi_stuff,,"[Edited--accidentally duplicated process 5]

Keep in mind that when the process hits fork(), the new process gets a copy of whatever i is at that moment.

Call the initial process number 1.
It creates processes 2, 3, 4, 5.

At the instant process 2 was created, i was 0. So process 2 will run 3 more iterations of the loop, when i is 1, then 2, then 3, creating processes 6, 7, 8.

When process 3 was created, i was 1, so it will make two more processes 9, 10.

Similarly, process 4 creates process 11, and process 5 won't create anything because it'll be at the end of the loop already.

Here's is what the process tree will look like, generation by generation:

    First generation
    process 1
      process 2(i=0)
      process 3(i=1)
      process 4(i=2)
      process 5(i=3)
    
    Second generation
    process 1
      process 2(i=0)
        process 6(i=1)
        process 7(i=2)
        process 8(i=3)
      process 3(i=1)
        process 9(i=2)
        process 10(i=3)
      process 4(i=2)
        process 11(i=3)
      process 5(i=3)
    
    Third generation
    process 1
      process 2(i=0)
        process 6(i=1)
          process 12(i=2)
          process 13(i=3)
        process 7(i=2)
          process 14(i=3)
        process 8(i=3)
      process 3(i=1)
        process 19(i=2)
          process 15(i=3)
        process 10(i=3)
      process 4(i=2)
        process 11(i=3)
      process 5(i=3)
    
    Fourth generation
    process 1
      process 2(i=0)
        process 6(i=1)
          process 12(i=2)
            process 16(i=3)
          process 13(i=3)
        process 7(i=2)
          process 14(i=3)
        process 8(i=3)
      process 3(i=1)
        process 19(i=2)
          process 15(i=3)
        process 10(i=3)
      process 4(i=2)
        process 11(i=3)
      process 5(i=3)
",59pifd,t3_59pifd,stupidCSstudent,,Comment,2,0,2
d9ak707,2016-10-27 17:30:59-04:00,stupidCSstudent,,"Thanks for answering, still some confusion if you don:t mind a follow up, why do those p2, p3, p4, p5 create other processes?  It loops once and a fork is called, a process is created which creates a new process.  but why does on the third generation does p2 have p5 p6 p7?  ",59pifd,t1_d9afl29,pi_stuff,,Reply,2,0,2
d9az6z8,2016-10-27 23:34:02-04:00,pi_stuff,,"When p2 is created, it's a duplicate of p1 at the moment fork() is called the first time. So you've now got two processes (p1 and p2) both running with i==0. They both continue running, they increment i to i==1, and they both run fork() again. p1 will start p3 and p2 will start p5.  Does that make sense?

When p1 created p2, it still had 3 more processes to start, so when p2 starts, it too will have 3 more processes to start.

Keep in mind that when fork() is called, it doesn't start the new process from the beginning of the program. The new process starts the program from the moment its parent process called fork(), with all variables and memory set up identically.
",59pifd,t1_d9ak707,stupidCSstudent,,Reply,1,0,1
d9berwa,2016-10-28 10:03:17-04:00,stupidCSstudent,,"ok so thats why p2 gets 3 iterations but not the others?  how does p5 end up under p2 at the last generation of teh process tree?

So just to be clear i is getting passed to the fork() call?",59pifd,t1_d9az6z8,pi_stuff,,Reply,1,0,1
d9bghyl,2016-10-28 10:42:57-04:00,pi_stuff,,"> thats why p2 gets 3 iterations but not the others?

Yep. It was the only process created when i==0.

> how does p5 end up under p2 at the last generation of teh process tree?

p2 created three processes: p5, p6, and p7.  These correspond to the three loop iterations that p2 did. It created p5 when i was 1, p6 when i was 2, and p7 when i was 3.

In the process tree I wrote, I included the value of i at the moment the process was created. You can see that p2 was the only one created with i==0, so it was the only one to start 3 children.

> i is getting passed to the fork() call?

Not exactly. fork() doesn't take any arguments: you don't pass anything to it. It clones the entire process. So if i is 2 when you call fork(), your clone will have i set to 2 as well.
",59pifd,t1_d9berwa,stupidCSstudent,,Reply,1,0,1
d9bj4ll,2016-10-28 11:38:47-04:00,stupidCSstudent,,"Oh sorry I meant, why are there two p5s. one at the bottom on its own and one under p2.  So the bottom p5 doesn't execute because its the end of the loop?",59pifd,t1_d9bghyl,pi_stuff,,Reply,1,0,1
d9botje,2016-10-28 13:33:38-04:00,pi_stuff,,"Oops!  That was a mistake. I corrected it. p1 starts p2,p3,p4,p5, and p2 starts p6,p7,p8, and so on.

Before the correction, there were processes p1 through p15, so 14 were created, which was wrong. Now it's p1 through p16, so 15 were created which is right.
",59pifd,t1_d9bj4ll,stupidCSstudent,,Reply,1,0,1
d9bvqze,2016-10-28 15:50:58-04:00,stupidCSstudent,,ok thanks! ,59pifd,t1_d9botje,pi_stuff,,Reply,1,0,1
59p0lr,2016-10-27 12:44:50-04:00,expressmailbox,Sourcing Video?,"Hey AskCompSci, 

I was wondering if any of you guys could help me understand what's going on here. I'm working on a project that involves using videos downloaded from the internet. To avoid using any feasibly sketchy programs, I had been ""inspecting"" the page source and finding the source .mp4's URL, and then just directly downloading that .mp4. 

I was curious to see if that strategy would work for YouTube videos as well (unsurprisingly, it does not). Instead I began checking the network timeline while the video was buffering to try to ascertain the source of the video. Using the [""History of Japan""] (https://www.youtube.com/watch?v=Mh5LY4Mz15o) video, I found that I could whittle the links down to these two. 

[This] (https://r1---sn-5uaezney.googlevideo.com/videoplayback?upn=85H2R87lK5w&mn=sn-5uaezney&clen=71674886&source=youtube&mv=m&mt=1477583877&ms=au&dur=540.120&ei=5iQSWILwOoumugLcmrKYAg&ip=74.219.17.220&initcwndbps=822500&sparams=clen%2Cdur%2Cei%2Cgir%2Cid%2Cinitcwndbps%2Cip%2Cipbits%2Citag%2Ckeepalive%2Clmt%2Cmime%2Cmm%2Cmn%2Cms%2Cmv%2Cnh%2Cpl%2Crequiressl%2Csource%2Cupn%2Cexpire&mm=31&id=o-AE02sS8X6CHbtA_TlJxWw2no2uRk0qkP1NidZxjD9Ujw&expire=1477605703&nh=IgpwcjAyLmF0bDAxKgw3Mi4xNC4yMjEuODE&gir=yes&pl=19&keepalive=yes&requiressl=yes&ipbits=0&key=yt6&itag=247&mime=video%2Fwebm&signature=9505E1E660502273083D6EEF098C831995E881B7.1690A5066F9FF0FA7E204D98720EE723456DC22F&lmt=1454445392978404&cpn=iIeo1J-kXWPboCLp&alr=yes&ratebypass=yes&c=WEB&cver=1.20161026) link goes to the source video. 

[This] (https://r1---sn-5uaezney.googlevideo.com/videoplayback?upn=85H2R87lK5w&mn=sn-5uaezney&clen=9323181&source=youtube&mv=m&mt=1477583877&ms=au&dur=540.161&ei=5iQSWILwOoumugLcmrKYAg&ip=74.219.17.220&initcwndbps=822500&sparams=clen%2Cdur%2Cei%2Cgir%2Cid%2Cinitcwndbps%2Cip%2Cipbits%2Citag%2Ckeepalive%2Clmt%2Cmime%2Cmm%2Cmn%2Cms%2Cmv%2Cnh%2Cpl%2Crequiressl%2Csource%2Cupn%2Cexpire&mm=31&id=o-AE02sS8X6CHbtA_TlJxWw2no2uRk0qkP1NidZxjD9Ujw&expire=1477605703&nh=IgpwcjAyLmF0bDAxKgw3Mi4xNC4yMjEuODE&gir=yes&pl=19&keepalive=yes&requiressl=yes&ipbits=0&key=yt6&itag=251&mime=audio%2Fwebm&signature=42339F7DC8EEEE968339ADC30B5F090245AE95FD.E10A8BDA4E8E3D2D617B672D6CBAA4683ABC5290&lmt=1454443640680897&cpn=iIeo1J-kXWPboCLp&alr=yes&ratebypass=yes&c=WEB&cver=1.20161026) one goes to the audio. 

So what's the rub? Is youtube simply running sourced audio and video simultaneously? Is there no way to accomplish what I'm hoping to? Thanks for any help. 

Edit: Apologies if this is the wrong place to be posting",,,,,Submission,1,0,1
d9a9eei,2016-10-27 13:49:43-04:00,urielsalis,,"Use youtube-dl to download from youtube or other sites. Open source and not sketchy.

What youtube does is have the same audio for all formsts, then getting the video with the specific resolution somewhere else. This is to save space as the audio is the same for all formats and so the audio doesnt stop while changing quality",59p0lr,t3_59p0lr,expressmailbox,,Comment,3,0,3
d9aadb7,2016-10-27 14:08:59-04:00,expressmailbox,,"Okay very interesting, thanks for responding. So what do you make of [this] (https://r1---sn-5uaezney.googlevideo.com/videoplayback?clen=139985627&mt=1477588818&itag=137&gir=yes&pl=19&ei=RTgSWO2mLIj-uAKU-LHADA&expire=1477610661&upn=Mm11cCjaJGM&signature=C94F00F10A3450ED345479A5648B53F49B7CE261.395751C07CD468B9B2F3D532A865372C932E1DAD&mime=video%2Fmp4&initcwndbps=827500&requiressl=yes&keepalive=yes&nh=IgpwcjAyLmF0bDAxKgw3Mi4xNC4yMjEuODE&sparams=clen%2Cdur%2Cei%2Cgir%2Cid%2Cinitcwndbps%2Cip%2Cipbits%2Citag%2Ckeepalive%2Clmt%2Cmime%2Cmm%2Cmn%2Cms%2Cmv%2Cnh%2Cpl%2Crequiressl%2Csource%2Cupn%2Cexpire&ipbits=0&lmt=1472040709968080&key=yt6&ip=74.219.17.220&mv=m&source=youtube&ms=au&mn=sn-5uaezney&mm=31&dur=540.160&id=o-AKmS-FFOsGfrUmQVZeYnVPsrhE-0O43VBoj4ZLe7XYau)? I'm confused about how I managed to find the combined audio and video source, which is un-downloadable, but individually the audio and video sources are downloadable",59p0lr,t1_d9a9eei,urielsalis,,Reply,1,0,1
d9ab2ju,2016-10-27 14:22:51-04:00,urielsalis,,You download the audio and video separately and combine them using ffmpeg or similar. Just like youtube-dl does or youtube itself. You can check the code for youtube-dl too see how it does it,59p0lr,t1_d9aadb7,expressmailbox,,Reply,1,0,1
d9abldy,2016-10-27 14:32:54-04:00,expressmailbox,,"But do you know why that third link won't download? (Sorry, I realize my questions are less practical than theoretical)",59p0lr,t1_d9ab2ju,urielsalis,,Reply,1,0,1
d9abnce,2016-10-27 14:33:59-04:00,urielsalis,,"The link is not static, it changes depending on the region, server you connect to and if you are logged in they might even give you a different one each time

Note the ""&expire=1477610661"" in the link",59p0lr,t1_d9abldy,expressmailbox,,Reply,1,0,1
d9abzul,2016-10-27 14:40:46-04:00,expressmailbox,,"Word, thanks for all the help ",59p0lr,t1_d9abnce,urielsalis,,Reply,1,0,1
59nz3i,2016-10-27 09:32:27-04:00,39452,Engineering discipline project: Computer science,"So to start, im doing a report on the engineering discipline: computer science, I dont have any background in this field however I am looking to learn.

I am looking for information for certain questions to be asked by the report.
To start im looking for info to get me started on (the FAQ is lacking tbh)
To start looking to learn the basics on 

What they are, where they are going, whats going on in those fields?
These aspects:

* Computation
* Information system/Technology?
* Artificial Intelligence
* Robotics
* Algorithms
* Programming

I'd also Like to know more about the emerging technologies in these fields:

* Robotics
* Artificial intelligence
* algorithms/Programming
* Information storage/technology

This question in my report is asking for certain  ethical cases:
(Is this Computerscience at fault?)

* Faulty programming -> Crashes/ cases of death?

Im gonna post what I find online in the comments and If you all will add to it or tell me what is wrong, it would be appreciated.",,,,,Submission,1,0,1
d99xc5l,2016-10-27 09:32:55-04:00,39452,,"What they are, where they are going, whats going on in those fields?
These aspects:

* Computation
> Computational science (or scientific computing) is the field of study concerned with constructing mathematical models and quantitative analysis techniques and using computers to analyze and solve scientific problems. In practical use, it is typically the application of computer simulation and other forms of computation to problems in various scientific disciplines.
* Information system/Technology?

* Artificial Intelligence
>  intelligence exhibited by machines. In computer science, an ideal ""intelligent"" machine is a flexible rational agent that perceives its environment and takes actions that maximize its chance of success at some goal.
* Robotics
> Robotics is the branch of mechanical engineering, electrical engineering and computer science that deals with the design, construction, operation, and application of robots,[1] as well as computer systems for their control, sensory feedback, and information processing.
* Algorithms
> In computer systems, an algorithm is basically an instance of logic written in software by software developers to be effective for the intended ""target"" computer(s) to produce output from given (perhaps null) input. An optimal algorithm, even running in old hardware, would produce faster results than a non-optimal (higher time complexity) algorithm for the same purpose, running in more efficient hardware: that is why algorithms, like computer hardware, are considered technology.
* Programming
>  is a process that leads from an original formulation of a computing problem to executable computer programs. Programming involves activities such as analysis, developing understanding, generating algorithms, verification of requirements of algorithms including their correctness and resources consumption, and implementation (commonly referred to as coding[1][2]) of algorithms in a target programming language. Source code is written in one or more programming languages.

I'd also Like to know more about the emerging technologies in these fields:

* Robotics
> * Swarm robotics: Swarm robotics is a new approach to the coordination of multirobot systems which consist of large numbers of mostly simple physical robots. It is supposed that a desired collective behavior emerges from the interactions between the robots and interactions of robots with the environment. This approach emerged on the field of artificial swarm intelligence, as well as the biological studies of insects, ants and other fields in nature, where swarm behavior occurs.
> to create autonomous construction
>* Molecular nanotechnology:  a technology based on the ability to build structures to complex, atomic specifications.....would employ original definitive processes to obtain definitive results. The desire in molecular nanotechnology would be to balance molecular reactions in positionally-controlled locations and orientations to obtain desired chemical reactions, and then to build systems by further assembling the products of these reactions.
>* Modular self-reconfiguring robotic systems: autonomous kinematic machines with variable morphology. Beyond conventional actuation, sensing and control typically found in fixed-morphology robots, self-reconfiguring robots are also able to deliberately change their own shape by rearranging the connectivity of their parts, in order to adapt to new circumstances, perform new tasks, or recover from damage.
>* 

* Artificial intelligence
> * Artificial intelligence: is intelligence exhibited by machines. In computer science, an ideal ""intelligent"" machine is a flexible rational agent that perceives its environment and takes actions that maximize its chance of success at some goal.
> * 
* algorithms/Programming
* Information storage/technology
>* denotes the proposed next major phase of mobile telecommunications standards beyond the current 4G/IMT-Advanced standards. Rather than faster peak Internet connection speeds, 5G planning aims at higher capacity than current 4G, allowing higher number of mobile broadband users per area unit, and allowing consumption of higher or unlimited data quantities in gigabyte per month and user. According to :  Next Generation Mobile Networks Alliance ....5G is 

This question in my report is asking for certain  ethical cases:
(Is this Computerscience at fault?)

* Faulty programming -> Crashes/ cases of death?",59nz3i,t3_59nz3i,39452,,Comment,1,0,1
d99zqyw,2016-10-27 10:32:15-04:00,dorkus,,For the last question about ethics...you should totally look at self driving cars and the [trolly problem](https://en.wikipedia.org/wiki/Trolley_problem). Very relevant to what's going on in the auto industry today.,59nz3i,t3_59nz3i,39452,,Comment,1,0,1
59n8ap,2016-10-27 06:16:23-04:00,WarGLaDOS,I'm doing the schema right?,"I've goth this language:

L={ σ  ∈ {0,1} | lσl >= 3 where first and second-last symbols of σ must be equal }

For example: 001, 0100, 110010 ∈ L but 010, 01110, 10100 ∉ L

The schema that i've made is here: http://imgur.com/a/bhNHw
Is this solution right?

EDIT: updated schema here: http://imgur.com/a/EfB6f",,,,,Submission,2,0,2
d99ta35,2016-10-27 07:13:26-04:00,None,,Think about the input 0010. I believe your machine accepts this input even though that second to last input is 1 and the first input is 0,59n8ap,t3_59n8ap,WarGLaDOS,,Comment,3,0,3
d9a4obl,2016-10-27 12:15:10-04:00,WarGLaDOS,,"you're right!

I think this correction could work.
If i'm in Q2 and the next imput is 1, i'll go to the final state Q7: when i'm on Q7 i have two case:

- if the next imput is 0, i return to Q2

- if the next imput is 1, i return to Q1

I noticed that on Q1 i forgot the case of input 1: in this case, i'll rest on Q1.

The same thing is specular for the bottom side

EDIT: thanks to  /u/_--__ , i've noticed that when i'm in Q3 and i receive 1 as imput the string is accetable but does't go in a finite state: so, instead to return to Q1 i'll go to Q7.

(now i make a new pick)",59n8ap,t1_d99ta35,None,,Reply,1,0,1
d9a43yg,2016-10-27 12:03:31-04:00,_--__,,"In addition to what /u/Ten3nbaum says, 0001 is not accepted by your machine but should be in L.  Incidentally, why do you say 011100 is not in L?",59n8ap,t3_59n8ap,WarGLaDOS,,Comment,2,0,2
d9a6a4x,2016-10-27 12:47:30-04:00,WarGLaDOS,,"Thanks :)

I did't considered that case, so i've edit the answer to /u/Ten3nbaum with your consideration: and 011100 is in L, i've put an extra zero for misclick (now it's correct)",59n8ap,t1_d9a43yg,_--__,,Reply,1,0,1
59n3lj,2016-10-27 05:32:44-04:00,JustAnAccount1234567,Does this statement make sense?,"So ∀ ∈ List, assign P or N

So for all elements in list, assign P or N to the element.",,,,,Submission,11,0,11
d99s0q2,2016-10-27 06:05:31-04:00,Irony238,,No it does not make sense. Your quantifier does not bind any variable. The way you put it it would mean that the universal quantifier ∀ is an element of List. I doubt that this is what you want to convey.,59n3lj,t3_59n3lj,JustAnAccount1234567,,Comment,10,0,10
d99se7i,2016-10-27 06:27:01-04:00,JustAnAccount1234567,,"∀ W ∈ List

So this would mean for all W elements in List. ",59n3lj,t1_d99s0q2,Irony238,,Reply,3,0,3
d99shty,2016-10-27 06:32:47-04:00,Irony238,,"I am not quite sure what you mean with ""for all W elements"".
What it means is that the following statement holds for any element from the List and that you choose to refer to this element in the future as W.

For example
> ∀ W ∈ {1,2,3}: W < 4

means that any element from the set {1,2,3} is smaller or equal to 4.",59n3lj,t1_d99se7i,JustAnAccount1234567,,Reply,5,0,5
d99sqmr,2016-10-27 06:45:59-04:00,JustAnAccount1234567,,"Okay I'm being special here, sorry. So I have a list of words which I want to classify, and assign N or P. So in English

for each *word* in the list, if the word is positive, assign P, else N

What I  mean by positive in this context is if the word is 'good' in such context. So the word ""beautiful"" is a 'positive' word.",59n3lj,t1_d99shty,Irony238,,Reply,3,0,3
d99ugic,2016-10-27 08:03:47-04:00,ChadtheWad,,"Generally universal quantifiers are for logical statements, not algorithmic procedures. So you could say ""for all w in list, w is in {P,N}"", (although this will also be confusing this List is often assumed to be a set) but it sounds to me like you want to define something. It would probably be easier  to define a function on words, f, and then have the function map each word to P or N. There's a [cases environment](http://tex.stackexchange.com/questions/262079/typesetting-a-function-defined-by-case-analysis) in Latex (and probably something similar in other word processors) that presents a case-by-case definition of a function pretty well.

Of course, if this is part of a larger algorithm that you are describing, it might be best to include this inside some pseudocode. You could do something like a for loop to assign each word P or N or assume the function f will do that for you.",59n3lj,t1_d99sqmr,JustAnAccount1234567,,Reply,5,0,5
d9aljdn,2016-10-27 18:01:27-04:00,JustAnAccount1234567,,"Thanks Chad, I used cases and I think it clearly shows my message. 

http://i.imgur.com/JjZE2vV.png

$$f(w) =
\begin{cases}
assign\:P,  & \text{if $w$ is positive} \\
assign\:N, & \text{if $w$ is negative}
\end{cases}$$

Edit: I'm pretty sure I can get rid of the word assign so

$$f(w) =
\begin{cases}
P,  & \text{if $w$ is positive} \\ 
N, & \text{if $w$ is negative}
\end{cases}$$",59n3lj,t1_d99ugic,ChadtheWad,,Reply,2,0,2
d9biflp,2016-10-28 11:24:28-04:00,ChadtheWad,,"No problem. :) And yeah, without the word assign is much better, I think.",59n3lj,t1_d9aljdn,JustAnAccount1234567,,Reply,1,0,1
d9a3k7z,2016-10-27 11:52:20-04:00,_--__,,"P and N are *predicates* - that is they are a ""property"" that elements of the list can have. What I think you want to say is:

∀ w ∈ List: P(w) or N(w)

This says ""for all elements w in the list, either P(w) or N(w) holds (or both)"".  To eliminate the case where P(w) and N(w) hold you would say: 

∀ w ∈ List: (P(w) or N(w)) and not (P(w) and N(w)) 
",59n3lj,t1_d99sqmr,JustAnAccount1234567,,Reply,1,0,1
59lx0t,2016-10-26 23:26:51-04:00,nbcu,any advice on what kind of math do I need to learn to better understand computer science?,I tried MIT's free online course about CompSci (6.00.1x) but having a hard time understanding some of it's math. So what do you math subject do you recommend for this kind of course? i'm studying on my own and I like self-paced courses or reading ebooks on my personal time.,,,,,Submission,3,0,3
d99nax9,2016-10-27 01:53:31-04:00,falafel_eater,,"This is a very broad question. However, most programmes for CS will include a course in Discrete Maths, intro to Graph Theory, and Set Theory.  

Linear Algebra and Calc are also included but will probably not be as absolutely necessary for the basics.",59lx0t,t3_59lx0t,nbcu,,Comment,2,0,2
d9avce4,2016-10-27 21:58:30-04:00,nbcu,,any books you can recommend for discrete math?,59lx0t,t1_d99nax9,falafel_eater,,Reply,1,0,1
d9b7ziw,2016-10-28 05:44:45-04:00,falafel_eater,,"Unfortunately, the textbook used in my university for discrete math was not written in English, and I have not read any other textbooks on the subject.  
I'd recommend looking at a syllabus for a basic Discrete Math course in a computer science BSc program at some university like MIT, Stanford or Caltech, see what book they use and just pick that up.  
Otherwise, hopefully someone on this sub that knows a good discrete math book written in English might be able to suggest something. Maybe searching the sub for previous posts on the subject might reveal something.",59lx0t,t1_d9avce4,nbcu,,Reply,2,0,2
d99qchi,2016-10-27 04:25:30-04:00,ChaoticxSerenity,,"Calc 1, Linear Algebra 1, Stats 1 and Discrete Math was the minimum at my school.  ",59lx0t,t3_59lx0t,nbcu,,Comment,1,0,1
d99s17l,2016-10-27 06:06:18-04:00,Irony238,,Logic,59lx0t,t3_59lx0t,nbcu,,Comment,1,0,1
d9ag6a6,2016-10-27 16:05:07-04:00,xerxesbeat,,a 101 on Lambda Calculus would be a fun read,59lx0t,t3_59lx0t,nbcu,,Comment,1,0,1
59lmqb,2016-10-26 22:22:06-04:00,SirRocketJumper,I once met a CS engineer who claimed to have helped create the computers used in the Genesis sequence in Star Trek II: The Wrath of Khan. What can you tell me about the computers used?,,,,,,Submission,0,0,0
d99ixu8,2016-10-26 23:32:46-04:00,theGentlemanInWhite,,Wrong place for this question ,59lmqb,t3_59lmqb,SirRocketJumper,,Comment,2,0,2
d99lqp1,2016-10-27 00:56:04-04:00,IWentToTheWoods,,There is some pretty interesting computer science that went into the early computer graphics systems. I don't think it's out of line to ask for more information on a part of CS history.,59lmqb,t1_d99ixu8,theGentlemanInWhite,,Reply,1,0,1
d99los4,2016-10-27 00:54:14-04:00,IWentToTheWoods,,"There a first-hand account of the process here: http://alvyray.com/Papers/CG/StarTrekII_GenesisDemo.pdf

For further reading you want to search ""E&S Picture System II""",59lmqb,t3_59lmqb,SirRocketJumper,,Comment,2,0,2
59lalr,2016-10-26 21:08:20-04:00,progsucks,Programming Languages: Haskell Help,"Q.1 Write a function ""reverse"" that takes an input list l and returns another list that is the reversal of l. What is the most general polymorphic type of ""reverse""?
I have tried to type in the following:
~~concat l1 l2 = if l1 ==[] then l2 else (head l1): (concat(tail l1)l2)~~ I realize this was not the smartest attempt.
and it doesn't work for the tryhaskell.org or for the browser Haskell. I'm new to this language so I'm having difficulties understanding the errors, etc...",,,,,Submission,0,0,0
d99scsn,2016-10-27 06:24:49-04:00,Irony238,,"1. You should really name your function reverse if you are asked to do it. Especially because concat is a predefined function is Haskell. I think you should be able to redefine concat anyway, but I would not recommend doing it because concat can be quite useful.

2. I think try Haskell.org does not let you define functions in the way you did because it only allows you to evaluate expressions and a function definition is not an expression. Try

        let
            reverse ...
        in
            reverse


3. Your function does take 2 arguments instead of one. While it might be a good idea to use an accumulator to write your function the final function should just take one argument (the list) and return a (reversed) list.

4. Your function does not reverse either of the lists. Do you see why?",59lalr,t3_59lalr,progsucks,,Comment,2,0,2
d99sf54,2016-10-27 06:28:28-04:00,Irony238,,Does anyone know why the 4 spaces for code formatting do not seem to work?,59lalr,t1_d99scsn,Irony238,,Reply,1,0,1
d99wg8c,2016-10-27 09:08:40-04:00,icendoan,,"You need two newline characters to show up:
this appears on the same line, even though a `\n` was entered,

    whereas this had two newlines and then four spaces, so it is code.",59lalr,t1_d99sf54,Irony238,,Reply,1,0,1
d9aoijx,2016-10-27 19:12:55-04:00,Irony238,,This does not seem to work for me. But 8 spaces seem to work for some reason.,59lalr,t1_d99wg8c,icendoan,,Reply,1,0,1
d9b2fzd,2016-10-28 01:13:05-04:00,progsucks,,"This is what I did: 
let reverse l = if l == [] then [] else (reverse(tail l))++(head l) in reverse [""a"",""b"",""c""]

and it works for reversing the list! :)",59lalr,t1_d99scsn,Irony238,,Reply,1,0,1
d9b4mgf,2016-10-28 02:40:00-04:00,Irony238,,Looks good to me.,59lalr,t1_d9b2fzd,progsucks,,Reply,1,0,1
d99wgw2,2016-10-27 09:09:13-04:00,icendoan,,You might want to ask this over on /r/haskell or (preferably) /r/haskellquestions ,59lalr,t3_59lalr,progsucks,,Comment,1,0,1
d9b2gvk,2016-10-28 01:13:53-04:00,progsucks,,thanks! I didn't realize Haskell had its own category,59lalr,t1_d99wgw2,icendoan,,Reply,1,0,1
59imjr,2016-10-26 12:52:22-04:00,NeedsCash,Quick SQL question.,"Not sure if this would be the correct sub. 

I've hit a bit of a wall in something I've been doing. I have two versions of the same table, (A and B). However, a column in Table B has its length reduced by 5. Directly inserting (INSERT INTO TABLE A SELECT * FROM TABLE B) the contents of Table A into B caused the data to be shifted to the left by 5 characters. Is there any way to handle this in SQL?

I'm fairly certain that using each column in the insert would work, but it has over 200 columns and would take a few minutes to do it this way. And I got curious if it is indeed possible in SQL.

Thanks in advance!",,,,,Submission,2,0,2
d98stz1,2016-10-26 13:56:50-04:00,BonzoESC,,"> I'm fairly certain that using each column in the insert would work, but it has over 200 columns and would take a few minutes to do it this way. And I got curious if it is indeed possible in SQL.

Write a script to do this for you, and don't make tables with 200 columns next time.",59imjr,t3_59imjr,NeedsCash,,Comment,2,0,2
d98tqgd,2016-10-26 14:14:49-04:00,NeedsCash,,"The scripts already been done. But I'm thinking if this could be done a lot more efficient ^(or nicer) than that since I have to send the instructions on how to do it to non-programming people. 

It's technically a physical file on an AS400 system. I'm just using SQL to transfer stuff since it's faster. And I didn't make this one, it's an old file from the 90s and looking at it's log, the stuff was added over the years.",59imjr,t1_d98stz1,BonzoESC,,Reply,1,0,1
d98xynp,2016-10-26 15:39:31-04:00,pleasantstusk,,Depending on DBMS you could do the insert you're doing and then use a merge query to fix the one column???,59imjr,t3_59imjr,NeedsCash,,Comment,2,0,2
d993swj,2016-10-26 17:35:53-04:00,DukeBerith,,"If it's literally just one column, you have 2 very simple bandaid fix options:

*  Update the table's columns so they match the bigger one from both tables, then run the query.


or 


*  do what you're doing, with a single follow-up statement to perform an update on that column.


PseudoSQL since I don't know your server/syntax, i'm sure you can figure it out.

    UPDATE TABLE A 
    SET COL = SUBSTR(B.COL,0,N) 
    WHERE A.ID = B.ID",59imjr,t3_59imjr,NeedsCash,,Comment,1,0,1
d98rjxn,2016-10-26 13:31:51-04:00,quzR,,"try this instead 

SELECT *
INTO B
FROM A:",59imjr,t3_59imjr,NeedsCash,,Comment,0,0,0
d98sip4,2016-10-26 13:50:45-04:00,NeedsCash,,Got this: *SQL statement not allowed.* ,59imjr,t1_d98rjxn,quzR,,Reply,1,0,1
d98vjdo,2016-10-26 14:50:59-04:00,quzR,,do it without the :,59imjr,t1_d98sip4,NeedsCash,,Reply,1,0,1
d98wwq9,2016-10-26 15:18:33-04:00,NeedsCash,,Of course I tried that. Same results.,59imjr,t1_d98vjdo,quzR,,Reply,0,0,0
59ilqx,2016-10-26 12:48:55-04:00,pinch_it,How do I select the value of the “DELETED” flag in Open Address Hashing for a probing strategy?,"I'm trying to implement a double hashing algorithm probing scheme for an open addressing hash table using C++. I'm making use of a simple hash algorithm and following the formula `h(key, tries) = (h(k) + tries.h(k)) mod table_size` where `h(k)` is some simple hash algorithm.

So while deleting `(key, value)` pairs, I need to implement a `DELETED` flag that goes into deleted table slots. This would allow me to continue searching as the `DELETED' flag indicate that I should continue my search even if a slot is empty as such. (I'm still a little hazy myself about the details, so please bear with anything that may sound obscure or incomplete)

For now, I've just decided use a macro like `#define DELETED -999` and it's working perfectly well. But what if a user wants to use the value `-999` as a key?? According my algorithm, I would ignore and continue my search. I'm not sure what value my `DELETED` flag should hold.

My question is, how are such flags implemented? What kind of value do they hold so as to not conflict with a key value that user might want to enter?

",,,,,Submission,4,0,4
d99ll5p,2016-10-27 00:50:54-04:00,GotBetterThingsToDo,,"Your deleted flag should ideally be separate from your data element, and looked up directly, or if for some weird reason that just isn't possible, some value which cannot be present in the field otherwise. I'm not your guy for questions about open addressing hashes, but I did some work with COBOL 30 years ago and know exactly what happened on September 9th, 1999 to a metric shitton of database-driven apps. ",59ilqx,t3_59ilqx,pinch_it,,Comment,1,0,1
d99m0ah,2016-10-27 01:05:08-04:00,pinch_it,,"Thanks for the reply!

I've trying to do some research to find out what these flags look like but with no luck :(",59ilqx,t1_d99ll5p,GotBetterThingsToDo,,Reply,1,0,1
d99u3gp,2016-10-27 07:49:12-04:00,pi_stuff,,"It sounds like both the key and the value in your table are integers, right?  The easiest solution would be to include another integer in each table entry that represents whether the entry was empty, full, or deleted.

    struct TableEntry {
      int status:
      int key:
      int value:
    }:

    TableEntry myTable[]:

You could save memory by using a couple bits in the key or the value, and then documenting the fact that these bits are unavailable. For example, you could specify in the documentation for your table that the range for the key is less than a full integer. Assuming 32-bit integers, they would normally hold values from -2147483648 to 2147483647 (-2^31 .. 2^31 - 1). If you accept keys only in the range -536870912 to 536870911 (-2^29 .. 2^29 - 1), then you can use the upper two bits to mark each entry as empty, full, or deleted.

    enum EntryStatus {EMPTY, FULL, DELETED}:  /* values 0, 1, or 2, taking 2 bits to store */

    int getStatus(int entryNo) {
      return (myTable[entryNo].key >> 30) & 3:
    }

    void setStatus(int entryNo, int status) {
      myTable[entryNo].key = (myTable[entryNo].key & ~(3 << 30)) | (status << 30):
    }

If you're not familiar with the ""&"", ""~"", ""<<"", and "">>"" operators, [take a look at this](https://en.wikipedia.org/wiki/Bitwise_operations_in_C).

Another memory-efficient option would be to have a parallel array storing those bits. Using 32-bit integers, each entry in this array would hold 16 pairs of bits, so it would need 1/16 of the number of entries in your main table.

    uint32_t statusArray[]:

    int getStatus(int entryNo) {
      int bitShift = (entryNo % 16) * 2:
      return (statusArray[entryNo / 16] >> bitShift) & 3:
    }

    void setStatus(int entryNo, int status) {
      int bitShift = (entryNo % 16) * 2:
      uint32_t tmp = statusArray[entryNo / 16] & ~(3 << bitShift):
      statusArray[entryNo / 16] |= status << bitShift:
    }

",59ilqx,t3_59ilqx,pinch_it,,Comment,1,0,1
59d4sz,2016-10-25 15:50:22-04:00,imaque,"What's the accepted colloquialism for Terrabytes, if any?","K is kilobytes, megs is megabytes, and gigs is gigabytes. Is there a similar term that's become the accepted one for terrabytes? Terras? Terrs?",,,,,Submission,12,0,12
d97n1sy,2016-10-25 17:54:43-04:00,falafel_eater,,Tera.,59d4sz,t3_59d4sz,imaque,,Comment,15,0,15
d97jr9m,2016-10-25 16:43:36-04:00,1ndigoo,,"I kinda like the sound of ""T"". 
Ex: ""Did you see they're making 4T SSDs already!? That's wild.""

FYI, it's spelled Terabyte. ",59d4sz,t3_59d4sz,imaque,,Comment,8,0,8
d97nck7,2016-10-25 18:01:23-04:00,themeaningofhaste,,"I do know people who use ""T"", as well as ""G"" on occasion.",59d4sz,t1_d97jr9m,1ndigoo,,Reply,3,0,3
d97ui2p,2016-10-25 20:48:32-04:00,zetas2k,,"I prefer ""teebee's"" as in ""Check out my 18 teebee nas"" ",59d4sz,t3_59d4sz,imaque,,Comment,9,0,9
d98m5lb,2016-10-26 11:46:49-04:00,TheCommador,,"I'm partial to ""Tyrone Bytes"".",59d4sz,t3_59d4sz,imaque,,Comment,6,0,6
d984ij3,2016-10-26 00:57:42-04:00,MaShinKotoKai,,I usually say teras.  But just terabyte is fine too.,59d4sz,t3_59d4sz,imaque,,Comment,3,0,3
d98lbdr,2016-10-26 11:30:05-04:00,KronktheKronk,,"I've heard people refer to them as ""TB's"" and I've heard people say the whole word.

Those are the only 2.",59d4sz,t3_59d4sz,imaque,,Comment,3,0,3
d983v1j,2016-10-26 00:37:23-04:00,bobthecowboy,,"At my last job (an ""actually big"" big data company) we called them tubs. ",59d4sz,t3_59d4sz,imaque,,Comment,2,0,2
d981r88,2016-10-25 23:38:07-04:00,chucksef,,"We could go with tubs, tabs, tibs, or so. Sort of Aussie like.",59d4sz,t3_59d4sz,imaque,,Comment,1,0,1
d9835ju,2016-10-26 00:16:44-04:00,brennanfee,,"Most use ter, pronounced like tear.  For instance, ""I have a ter of movies.""  So, plural would be Ters.",59d4sz,t3_59d4sz,imaque,,Comment,-4,0,-4
d98bmh4,2016-10-26 06:55:59-04:00,dragonnyxx,,"""Most""? I work in the computer industry and have literally never heard anyone say this.",59d4sz,t1_d9835ju,brennanfee,,Reply,11,0,11
d98xpbk,2016-10-26 15:34:31-04:00,brennanfee,,Talk to your sys admins.  They'll use it.,59d4sz,t1_d98bmh4,dragonnyxx,,Reply,1,0,1
d98fd3d,2016-10-26 09:14:56-04:00,ReginaldDouchely,,That's what the storage guys where I work use.,59d4sz,t1_d9835ju,brennanfee,,Reply,2,0,2
59cv46,2016-10-25 15:02:10-04:00,BitOfAnAssembler,[Multiplexers] Question about Mux,I'm learning how to build multiplexers and I'm a bit confused on how to build larger ones from smaller ones (ie building a 16:1 Mux using only 2:1 Muxes) I can see how to build a 16:1 using two 8:1 and one 2:1 muxes but not seeing how to do it using only 2:1. ,,,,,Submission,1,0,1
d988arq,2016-10-26 03:45:03-04:00,thegreatunclean,,"It's the exact same process as breaking a 16:1 down.

An 8:1 is the same as two 4:1 going into a 2:1.

A 4:1 is the same as two 2:1 going into a 2:1.

So an 8:1 is the same thing as four 2:1s feeding into a pair of 2:1s that all feed a final 2:1.

****

The select lines can be a bit messy but just think of each select line selecting the top or bottom set at each branch.  ",59cv46,t3_59cv46,BitOfAnAssembler,,Comment,2,0,2
d9a7dhy,2016-10-27 13:09:35-04:00,BitOfAnAssembler,,Thank you so much! That definitely helps,59cv46,t1_d988arq,thegreatunclean,,Reply,1,0,1
595fyz,2016-10-24 12:26:49-04:00,finessseee,Is a CSIS-Arts degree worth anything compared to your basic CS-Science degree?,"My University offers CSIS and I'm planning on going the CS route, but I'm terrible at math and idk if I want to be working on an IDE screen all my life (no offense to programmers, it's just not all for me).. I do know that I have a passion for technology, computers and electronics. I definitely want to work in a career field like that someday.

 The CSIS- Arts degree is a lot less math and I can use those course openings to take some other Computeresk classes like Information Systems, computer organization, and other like courses. Maybe even some IT stuff if I get the chance.. 

So would it be bad to get an Arts degree in CS or should I just hold my breath and stick with the CSIS Bachelors in Science degree? (I just don't want the heavy math courses to lower my GPA).

Any advice?

Thanks! ",,,,,Submission,1,0,1
d96hv3f,2016-10-24 22:08:27-04:00,combuchan,,"Your worries sound like a would-be astronomer that hates telescopes.   If you don't want to sit around in an IDE all day, then CS might not be the right career path for you.  

We hired a guy at my last job who had just got a similar digital media or something or other degree as kind of a generic IT helpdesk/lower level admin.  Those types of role have a lot of caveats (easily outsourced, the reasons they call it ""helldesk,"" low pay, can be dead-endish, etc) but it honestly is a waste of a 4-year program.  You could probably get your foot in the door today in a generic help desk/tier 1 role and take certifications in your own time like Cisco/RedHat/AWS to make yourself more valuable to potential employers rather than spend 4 years for a degree that could be easily replaced by a few years' direct work experience.",595fyz,t3_595fyz,finessseee,,Comment,3,0,3
d95v4nl,2016-10-24 13:51:30-04:00,revvupthosefryers,,GPA is pretty much meaningless and get tutoring for math. You can do more if you choose in the future with a more advanced degree. ,595fyz,t3_595fyz,finessseee,,Comment,2,0,2
d96a7g5,2016-10-24 19:06:58-04:00,jeffreydontlook,,"G.P.A. is meaningless unless it's below a 3.0, then you're obviously a leper. At least that's how I felt when I entered the job market after graduating.",595fyz,t1_d95v4nl,revvupthosefryers,,Reply,1,0,1
d96j5x7,2016-10-24 22:39:57-04:00,magikker,,"I recruit for my company and spend time with other recruiters at events.  I know that other companies refuse to consider B.A. degrees and will only take B.S.

That said, many don't care about the degree a long as you can code. 

That said, what do you plan on doing with a CS degree if not coding?",595fyz,t3_595fyz,finessseee,,Comment,1,0,1
d99wc75,2016-10-27 09:05:34-04:00,andybmcc,,"> I know that other companies refuse to consider B.A. degrees and will only take B.S.

This seems fairly common.  Where I went, the B.A. was essentially the introductory CS courses plus general electives.  I highly suggest going the B.S. route if you actually want to work in the field.  B.A. is probably fine for getting in the door with generic IT/Help Desk.",595fyz,t1_d96j5x7,magikker,,Reply,1,0,1
594j2v,2016-10-24 09:35:49-04:00,Mr_Monster,Current computing (binary) is digital. Could quantum computing be considered analog?,"Our current computing model is based on logic gates or bits which are either ON or OFF. 

Quantum computing allows for near infinite possibilities between ON and OFF, and to me that suggests a kind of probability wave function...and a wave suggests analog. 

Right?",,,,,Submission,1,0,1
d95ksy3,2016-10-24 10:09:53-04:00,andybmcc,,"Even though it is a superposition of states, the reading of a qubit is still binary.",594j2v,t3_594j2v,Mr_Monster,,Comment,3,0,3
d95lp5j,2016-10-24 10:32:07-04:00,farstriderr,,">Quantum computing allows for near infinite possibilities between ON and OFF

Not really. There is only ever ""on"" or ""off"" at the time of measurement. The possibilities are only calculated, they don't ""exist"" in reality. It's not both ""on and off"" at the same time. 

The reason for the confusion lies in sources of misinformation like this: https://en.wikipedia.org/wiki/Quantum_computing

>Quantum computers are different from binary digital electronic computers based on transistors. Whereas common digital computing requires that the data are encoded into binary digits (bits), each of which is always in one of two definite states (0 or 1), quantum computation is analog and uses quantum bits, which can be in an infinite number of superpositions of states.

https://en.wikipedia.org/wiki/Qubit

> In a classical system, a bit would have to be in one state or the other. However, quantum mechanics allows the qubit to be in a superposition of both states at the same time, a property which is fundamental to quantum computing.

These are not facts, but physical interpretations of quantum mechanics. With classical information processing you have to program in terms of definite on/off states, because the only physical writable materials available to us were things that could be either definitely on or off. There is no on/off state physically unless you run the algorithms on a computer or other physical device. Then the 'nonphysical' code can be 'written' to physical objects that can exist in a corresponding on/off state, say by manipulating the magnetic properties of a hard drive (+/- charge) or punching holes in a card (hole/no hole). 

Quantum computing algorithms work similarly, except instead of writing the code to take advantage of a physical on/off state, you write it to take advantage of a ""superposition"" of states in a qubit. A qubit being the 'physical object' that the code takes advantage of. Say polarization of a photon, which can be in a ""superposition"" of possible states. It is not definitely ""on or off"" before measurement. So what you really are asking is what does ""superposition"" mean? Does it mean that all the states of a photon exist somewhere physically, independent of measurement? You can say that, but no experiment has ever proven it. All we ever have measured and ever can measure is one outcome, regardless of what is actually happening before that. Because you can treat the object (photon) **as if** it were in multiple states at once or even **think about it** that way, does not mean that it then actually is existing in these states somewhere ""out there"".",594j2v,t3_594j2v,Mr_Monster,,Comment,0,0,0
d95nn37,2016-10-24 11:17:00-04:00,TiarnaNaTuaithe,,"Actually quantum mechanics isn't just a mathematical construct, it is meant to correspond to reality. So it's correct to talk about a qubit being ""on and off"" simultaneously.",594j2v,t1_d95lp5j,farstriderr,,Reply,1,0,1
d95t4km,2016-10-24 13:12:04-04:00,farstriderr,,">quantum mechanics isn't just a mathematical construct

Of course it is.

>it is meant to correspond to reality

No, it's interpreted to correspond to reality. It's meant to predict some effects of or within reality. The two are not equivalent.

>So it's correct to talk about a qubit being ""on and off"" simultaneously.

Talking about it is different from ""it"" actually being that way.",594j2v,t1_d95nn37,TiarnaNaTuaithe,,Reply,-1,0,-1
d95zf62,2016-10-24 15:18:18-04:00,TiarnaNaTuaithe,,"You are correct in saying that it's just an interpretation, but its an interpretation that is accepted by the vast majority of the physics community. You're free to disagree if you want.",594j2v,t1_d95t4km,farstriderr,,Reply,1,0,1
d961865,2016-10-24 15:54:44-04:00,farstriderr,,It doesn't matter what the vast majority of the physics community accepts. Only the truth and what is testable experimentally matters.,594j2v,t1_d95zf62,TiarnaNaTuaithe,,Reply,-1,0,-1
593ak5,2016-10-24 03:23:11-04:00,DestinyCrusader,How to give directions in reverse?,"Hi guys, please let me know if I'm in the wrong place. So I'm a very new programmer, learning Java this year. For our latest class homework, our program has to read a file that has directions that look like this:

* Start at A
* Right on College Ave 
* Left on Blue Hwy 
* R on Red Blvd 
* R on Silver Pkwy 
* B is on the Right

We have to make the program print these directions in reverse; from point B to point A. There's more to the program but this is the part that's got me stumped. Apparently we can't simply change the L's to R's and R's to L's. 

The correct output would be:

* Start at B
* L on Silver Pkwy 
* L on Red Blvd 
* L on Blue Hwy
* R on College Ave 
* A is on the L

How is this output correct? Basically, what am I telling my program to do when it comes to changing the directions? Is there a pattern I'm not seeing?

All help would be appreciated. Thanks guys!",,,,,Submission,1,0,1
d962n70,2016-10-24 16:23:26-04:00,revvupthosefryers,,"Can you just use a stack and then swap the L and Rs. 
Maybe when told not to simply switch them it means, don't ONLY switch the directions. ",593ak5,t3_593ak5,DestinyCrusader,,Comment,2,0,2
d96pzl9,2016-10-25 02:19:51-04:00,DestinyCrusader,,"Nah, swapping the Ls and Rs is simply incorrect, it doesn't give you the right answer. ",593ak5,t1_d962n70,revvupthosefryers,,Reply,1,0,1
d95bwod,2016-10-24 03:42:23-04:00,UKCSTeacher,,"Output new list with index 1 to length-2 reversed and replace A with B and L with R and vice versa 

Edit: just read the no L to Rs. What have you been studying? Any graph theory? ",593ak5,t3_593ak5,DestinyCrusader,,Comment,1,0,1
d95c9kh,2016-10-24 04:02:18-04:00,DestinyCrusader,,"Nope, nothing of the sort. This is a basic file I/O program. We just have to read the file, output the directions in reverse, and then do some small calculations that I didn't include in the question. I feel like it's something conceptual that I'm not understanding, lol.",593ak5,t1_d95bwod,UKCSTeacher,,Reply,1,0,1
d95cjww,2016-10-24 04:18:45-04:00,UKCSTeacher,,Are you sure they didn't mean 'don't simple swap L and R' to be 'you have to do more than just swap L and R'. ,593ak5,t1_d95c9kh,DestinyCrusader,,Reply,1,0,1
d95drlw,2016-10-24 05:32:13-04:00,_--__,,Swapping L & R doesn't work: you can turn left onto the same street (e.g. Blue Hwy) in both directions.,593ak5,t1_d95cjww,UKCSTeacher,,Reply,1,0,1
d95djlq,2016-10-24 05:18:39-04:00,PM-ME-YO-TITTAYS,,"Maybe they're saying don't just do a string replace and switch all the Ls to Rs (and vice versa). You could parse the directions, and hold them in memory as a list of street names and a direction enum. Then, when you reverse the directions, print out the opposite enum to what's set.",593ak5,t3_593ak5,DestinyCrusader,,Comment,1,0,1
d95drw5,2016-10-24 05:32:42-04:00,_--__,,Swapping L & R doesn't work: you can turn left onto the same street (e.g. Blue Hwy) in both directions.,593ak5,t1_d95djlq,PM-ME-YO-TITTAYS,,Reply,1,0,1
d95dwdp,2016-10-24 05:40:21-04:00,PM-ME-YO-TITTAYS,,"Good point. I guess you'd have to keep the directions and the street names in 2 separate lists or something. You reverse them both, but move the directions along one step because the starting point doesn't have a direction.",593ak5,t1_d95drw5,_--__,,Reply,1,0,1
d95dqo3,2016-10-24 05:30:35-04:00,_--__,,"I suggest drawing out the map of the forward directions and then tracing the route in reverse.  You will see that the second set of directions do indeed take you back - and since ""Blue Hwy"" and ""College Ave"" both have the same ""direction"" (i.e. both are ""L on Blue Hwy"" and ""R on College Ave"") simply swapping L and R **will not work**.

Drawing it out should also help you see the pattern.",593ak5,t3_593ak5,DestinyCrusader,,Comment,1,0,1
d95taq0,2016-10-24 13:15:31-04:00,DestinyCrusader,,"If I hadn't already spent several hours trying to draw it out trying to get it, I wouldn't have come here, lol. Maybe I'm just drawing it wrong, but something more than ""just draw it out and you'll see the pattern"" would be helpful. I already know flipping L and R won't work.",593ak5,t1_d95dqo3,_--__,,Reply,1,0,1
d962jag,2016-10-24 16:21:13-04:00,_--__,,"Build up a few cases then.  E.g. what is the opposite of:

* L, L, L, on L
* L, L, L, on R
* L, L, R, on L
* L, L, R, on R
* L, R, L, on L
* L, R, R, on L
* etc",593ak5,t1_d95taq0,DestinyCrusader,,Reply,1,0,1
592ih5,2016-10-23 23:32:38-04:00,savagecub,"What, if anything, could replace binary based computing?","Additionally, are there any signs that it does need to be eventually replaced?",,,,,Submission,16,0,16
d958uwk,2016-10-24 01:21:46-04:00,lneutral,,"Binary is used for many good reasons.

One is it's very easy to represent two values with two voltage levels, and the more values you want to represent, the more difficult it is to keep ""noise"" from making them look like each other.

Another is that even if you built a ternary computer (and they have existed) you're not really gaining expressivity. We can build programs for binary computers that do the same things that you could do in another numeric base. We already do.

Finally, newer computing paradigms - quantum computing, for example - are also easier to represent if they're treated as extensions of binary. Quantum computing represents computing as sets of states that can be ""superimposed"" - multiple states simultaneously. This uses a model of computing where the ""quantum bit,"" or ""qubit"" is the analog component that corresponds to the bit in digital, traditional computing. 

Numerically, though, this is not the same as choosing a different base to work in (like substituting three-state computing or base-10 for binary) - it's actually modelling computing in terms of probabilities and all the other strange behaviors quantum mechanics give rise to. *Importantly, a binary, traditional computer can do everything a quantum computer can,* just not necessarily as efficiently.",592ih5,t3_592ih5,savagecub,,Comment,18,0,18
d95dbyo,2016-10-24 05:05:37-04:00,Dijit,,"In the 60's there was 'tenary' computers developed by the Soviets.[0]

However you have to understand how the world is outside of academia, companies will never switch to something that requires investment with no return on capital. So, Nobody is going to spend the trillions in R&D that Intel has on binary computing, and nobody is going to spend millions rearchitecting their applications for new systems.

We couldn't even make Itanium a thing when 32bit CPUs were reaching the end of use. Entrenchment is a serious debuff to innovative ideas even when there is good reasons. (Just look at microsoft).

And there's no evidence that it's superior except that it's easier for humans to reason about.

[0] https://en.wikipedia.org/wiki/Ternary_computer",592ih5,t3_592ih5,savagecub,,Comment,8,0,8
d9603w7,2016-10-24 15:32:03-04:00,crookedkr,,"> We couldn't even make Itanium a thing

Itanium had poor performance didn't it? It wasn't that x86 is entrenched (though it certainly is) so much as Itanium was just a bad implementation that wasn't fast enough to justify the move to a completely new instruction set.",592ih5,t1_d95dbyo,Dijit,,Reply,3,0,3
d96ifwt,2016-10-24 22:22:31-04:00,UtterlyDisposable,,Itanium is kind of a weird case because it was mostly directed towards high performance computing applications where POWER and Sparc were fairly popular. x86 and ARM have pretty much driven everything else far into the margins these days.,592ih5,t1_d9603w7,crookedkr,,Reply,1,0,1
d95ljwm,2016-10-24 10:28:37-04:00,lorddimwit,,"Binary is best because as you add more distinct values you have either more chance for confusion or you need to add more information to distinguish cases.

For example, if 0 is defined as voltage 0 and 1 is defined as voltage 1, what is voltage 0.25? It's 0. But if you have three values, 0, 0.5, and 1, what is 0.25? Is it 0 or 0.5?

You can get around this by adding side channels or other encoding mechanisms to disambiguate cases, but it's more expensive for no real gain.

We've built non-binary computers before (many early computers were decimal), but it's more expense for little gain.",592ih5,t3_592ih5,savagecub,,Comment,2,0,2
d95rffu,2016-10-24 12:36:57-04:00,tbrownaw,,"From what little I know of information theory, I think binary is the optimal representation for classically-valued discrete information. For some definition of ""optimal"". 

If you're dealing with continuous values, then analog computation *may* work better depending on how much you care about speed vs precision. In practice, this apparently isn't that common.

If you're dealing with certain very specific problems, quantum computation is enormously better. Or rather, it will be once someone actually builds working hardware.

There are also things like FPGAs and vector machines (graphics cards) that while still based on ones and zeroes, don't work in quite the same way as a traditional CPU.

None of these will replace traditional CPUs. Some of them will become standard ""extra"" hardware for common specialized tasks, the way graphics cards already have.",592ih5,t3_592ih5,savagecub,,Comment,2,0,2
d95a413,2016-10-24 02:13:24-04:00,adamnemecek,,I think that analog computing will make a comeback. Instead of bitd you will be working with signals.,592ih5,t3_592ih5,savagecub,,Comment,2,0,2
d9566mq,2016-10-23 23:58:54-04:00,d0m58,,Quantum computing,592ih5,t3_592ih5,savagecub,,Comment,1,0,1
d95dkyv,2016-10-24 05:20:51-04:00,ghazal_listener,,Neural computation is a promising field. ,592ih5,t3_592ih5,savagecub,,Comment,0,0,0
58za8n,2016-10-23 12:20:52-04:00,newzilla7,How do 8-bit CPUs access memory?,"How does a CPU limited to dealing with 1 byte of information at a time access kilobytes of memory? For example, what actually happens in the background when an 8-bit CPU loads a value from RAM at index, say, 326 (in decimal)?",,,,,Submission,12,0,12
d94daey,2016-10-23 12:30:55-04:00,lorddimwit,,"Most 8-bit CPUs use 16-bit addresses. The 6502, for example, uses little-endian addresses. You store the two parts of the address using separate instructions, and when an instruction takes an address it reads (for indirect) or contains (for literal) two bytes.

On that CPU there is an additional addressing mode called ""zero page"" as well. Instructions that use that mode provide a single byte address with an implicit zero in the high byte, saving a byte when accessing data in the zeroth page. 

A more modern example is MIPS. In older versions of the architecture, instruction words were always exactly 32 bits wide. To load a literal 32-bit address (which obviously couldn't fit in an instruction), you loaded the high part of the address into a register, shifted it, then loaded the low part. Most assemblers did that for you automatically using what amounted to macro instructions.",58za8n,t3_58za8n,newzilla7,,Comment,18,0,18
d94dfoq,2016-10-23 12:34:17-04:00,newzilla7,,"Thanks for the explanation! Could the multiple-addresses approach work with larger values, e.g. loading four 8-bit addresses to create a 32-bit address?",58za8n,t1_d94daey,lorddimwit,,Reply,1,0,1
d94dpy0,2016-10-23 12:40:48-04:00,lorddimwit,,"In theory, sure, but I don't know of any 8-bit architecture that has an address size larger than 16-bits. Many systems built around such processors use bank switching, where different chunks of memory are swapped in and out of the processor's address space by twiddling some register or something, but that's different from having a truly larger address space. 

Again, for a more modern example, later 32-bit x86 chips had an extension called PAE (Physical Address Extension) that was essentially bank switching to get past the 4GB barrier.

Some busses and addressing schemes use much larger addresses too. IPv6 uses 128-bit addresses, so they're manipulated in chunks as well. The IBM AS/400's virtual architecture defines all pointers as 128-bit and translates them into whatever format the real processor expects.",58za8n,t1_d94dfoq,newzilla7,,Reply,3,0,3
d9522ia,2016-10-23 22:11:42-04:00,None,,"I don't know of any 8-bit microprocessor with an address size larger than 16 bits, but there are 8-bit computers which use bank switching to access more RAM. The Commodore 128 is one example.",58za8n,t1_d94dpy0,lorddimwit,,Reply,2,0,2
d958koq,2016-10-24 01:11:03-04:00,lneutral,,"This is also the case on several gaming consoles.

The Game Boy, for example, included a bank controller inside each cartridge - and more than one type was in use, so that the specific logic for how addresses mapped to multiple chips differed from cartridge to cartridge. Later cartridges were developed, for instance, to support the faster clock speeds used by Game Boy Color games.

Worse, systems like the Game Boy color also used their memory map for accessing devices like the sound controller, display, and serial communication systems. This means that if they *hadn't* used some sort of bank-switching, there would have been very few addresses to go around for RAM, ROM, external battery-backed save RAM, and so forth.",58za8n,t1_d9522ia,None,,Reply,2,0,2
d94dtm6,2016-10-23 12:43:18-04:00,newzilla7,,"Great, thanks!",58za8n,t1_d94dpy0,lorddimwit,,Reply,1,0,1
d96gu4a,2016-10-24 21:44:31-04:00,arbitrarycivilian,,"Wait, what? I thought 8-bit CPUs *by definition"" used 16-bit addresses. ",58za8n,t1_d94daey,lorddimwit,,Reply,1,0,1
d96yo0y,2016-10-25 09:14:02-04:00,lorddimwit,,"I don't know of any that have larger addresses, but the Intel 8008 used 14-bit addresses, for example. ",58za8n,t1_d96gu4a,arbitrarycivilian,,Reply,1,0,1
58x101,2016-10-23 00:24:20-04:00,Nadramon,What makes a Client-Server Model Complex?,"For the highest mark in my coursework it states I need to create a complex Client-server model, while for the lower marks it says to create a simple Client-server model.

What makes a Client-server model simple or complex? Can someone show me the different levels of each?",,,,,Submission,6,0,6
d93x1sh,2016-10-23 00:50:31-04:00,lordvadr,,I'll give you one hint...you should be able to find others by searching. A server/client model is often complicated by what's called the Two Generals Problem.,58x101,t3_58x101,Nadramon,,Comment,5,0,5
58wyuu,2016-10-23 00:06:52-04:00,Kicker774,Can we create an anti-virus virus?,"So I'm reading into the Mirai botnet that was created for the DNS DoS attack ...

Basically a virus/malware created to spread through thousands of internet enabled devices.

Since these are things like standalone webcams, mediaplayers, even internet enable refrigerators we can't simply load Norton on these devices or run Anti-Malware bytes as we would our PC.

But if someone can create a virus to replicate through all these devices to delete or change system settings, can we not create an anti-virus virus that goes out and replicates on it's own just like a traditional virus only searching out whatever virus/malware signatures these botnets are infected with?

Would it still be considered unethical as it would still be getting into someones device without permission? Or would it be more equivalent to walking by a random persons house, seeing it's on fire and pulling the occupant to safety. You didn't have permission to enter but your being a good samaritan and doing the right thing.
",,,,,Submission,11,0,11
d940nda,2016-10-23 03:32:47-04:00,Madsy9,,"There have been plenty of such ""helpful"" worms in the past, for example [W32.Welchia](https://www.symantec.com/security_response/writeup.jsp?docid=2003-081815-2308-99)

Obviously, spreading such worms is illegal for the same reasons that computer viruses, trojans and other malware is. You're making changes to equipment without people's consent and the changes can (will) have negative effects.

In a way one could argue that laws covering this issue in most countries nowadays is really outdated, and it's causing problems for malware researchers who modify malware in order to gather data on botnets and similar. After getting access to the C&C of bot networks, researchers often have the capability of shutting down the whole network and put a stop to criminal activity. They could also patch the infected machines. But because of the lack of consent and the current legislation, doing so would pose a risk and could in theory lead to prosecution no matter how good the intentions.

Then again, you can never be quite sure that your code will always do the right thing and you can only test your program on so many different machine configurations. Even ""friendly"" code can have unintended consequences and blow up in your face. Combined with lots of infrastructure and important services being online, and it would be a disaster to legalise unauthorized access/modification of machines you don't own.

Extreme scenarios, but which could happen:

You make a worm like Welchia which tries to remove another worm and patch systems, and it manages to sneak into hospital equipment which leads to loss of life.

Or you spread a worm to mitigate DDOS attacks on some common port by adding a firewall rule to common routers and switch equipment. Only, your worm successfully closes the port on the network equipment on a stock exchange, leading to millions of dollars lost as no trading is done. I'm sure you can think of other examples yourself.

So, yeah I think it's unethical. But the even larger problem is unintended side-effects. No one ought to know more about a system than the owner of that system. If people own IoT devices and are ignorant regarding security, that's on them.",58wyuu,t3_58wyuu,Kicker774,,Comment,14,0,14
d96d9kr,2016-10-24 20:20:39-04:00,None,,"What about if it's done in self-defence by the target of a DDoS attack. In the offline world people have certain rights regarding self-defence. Surely, an action which disables a botnet without any ill intent toward infected machines ought to be permissible as self defence?",58wyuu,t1_d940nda,Madsy9,,Reply,1,0,1
d99c6xx,2016-10-26 20:49:57-04:00,BonzoESC,,"> Surely, an action which disables a botnet without any ill intent toward infected machines ought to be permissible as self defence?

This is abhorrent and unworkable. From the point of view of some random small business with an IP camera, they got hacked by a large corporation for no obvious reason, and would then be allowed to take a self-defensive measure. I don't really want to have to deal with an Internet that's all hacks, counter-hacks, and proxies for such. A much better long-term solution is for vendor to quit selling broken shit, for ISPs to have better controls for broken shit, and provide better, easier-to-use, and more ubiquitous tools for cases when people need to use broken shit. ",58wyuu,t1_d96d9kr,None,,Reply,1,0,1
d93xvdd,2016-10-23 01:21:06-04:00,gstuartj,,There have been good citizen worms as early as 20+ years ago that would remove other worms/malware. It's still illegal since the system owner doesn't consent.,58wyuu,t3_58wyuu,Kicker774,,Comment,2,0,2
d945w2b,2016-10-23 08:47:09-04:00,None,,"Unrelated but I've always imagined a concept which would counter infect the machine that attempted to intrude and it would and erase all its data.


Inspired by people trying to SSH into my vps. In this example such a virus would work by infecting a machine that attempted too many SSH logins into my machine and would erase all the data on the attackers machine.",58wyuu,t3_58wyuu,Kicker774,,Comment,1,0,1
58t17e,2016-10-22 09:03:32-04:00,Tebbathy,How to prepare for CS degree?,"I have little idea what the course is going to involve. Coming from an arts background. I know there will be a lot of work in comparison and in a more structured way.
How can i prepare now to make my life a lot less stressful while im at uni?",,,,,Submission,7,0,7
d931vvc,2016-10-22 10:41:30-04:00,ChaoticxSerenity,,"Brush up on your math. You'll probably need to take Calc 1, Linear Algebra 1 and Discrete maths, at the minimum. ",58t17e,t3_58t17e,Tebbathy,,Comment,8,0,8
d931y17,2016-10-22 10:43:25-04:00,Tebbathy,,Word. any good online courses for maths,58t17e,t1_d931vvc,ChaoticxSerenity,,Reply,2,0,2
d93d658,2016-10-22 15:45:47-04:00,x68zeppelin80x,,[Khan Academy](https://www.youtube.com/user/khanacademy) is probably the best. I also recommend that you check out [patrickJMT](https://youtube.com/user/patrickJMT) over at YouTube.,58t17e,t1_d931y17,Tebbathy,,Reply,5,0,5
d93dwsw,2016-10-22 16:05:33-04:00,Tebbathy,,Cool cheers,58t17e,t1_d93d658,x68zeppelin80x,,Reply,1,0,1
d931z9a,2016-10-22 10:44:32-04:00,ChaoticxSerenity,,"I've heard Khan Academy is pretty good. Also, check out /r/learnmath ",58t17e,t1_d931y17,Tebbathy,,Reply,5,0,5
d92zk6s,2016-10-22 09:19:20-04:00,assface,,Learn to program now if you don't know how to already. Any language. It doesn't matter.,58t17e,t3_58t17e,Tebbathy,,Comment,5,0,5
d92zs8q,2016-10-22 09:27:53-04:00,Tebbathy,,Yeah I'm learning Java just started code academy,58t17e,t1_d92zk6s,assface,,Reply,6,0,6
d9lj06l,2016-11-04 10:49:47-04:00,chummer7,,"Just so you know, the Java course on codecademy is really awful. I think the web design/development courses are great (with the exception of Rails) and the Ruby and command line ones are good too, but the Java and Rails ones hold your hand way too much and really only teach you syntax. It won't hurt to take it, but you may want to start programming in a good IDE all by yourself, maybe with a YouTube series. 

If you understand variables, data types, branches (if/else), loops, and methods (calling, parameters and arguments) you will be plenty prepared for an introductary college programming course. ",58t17e,t1_d92zs8q,Tebbathy,,Reply,1,0,1
d9mr3dg,2016-11-05 07:37:30-04:00,Tebbathy,,cool thanks man.,58t17e,t1_d9lj06l,chummer7,,Reply,2,0,2
d93fuol,2016-10-22 16:55:44-04:00,GunslingerJones,,"If you can take stat 2 over calc 2, do it. Unless you really like calc, stat is generally the 'easier' course. Brush up on discrete mathematics as other have said, it is usually mandatory and can be quite confusing at first.

Don't spend too much time studying anything other than the introductory lessons on the programming languages you intend to learn in your school program. You'll pick up the more difficult stuff later as you learn and research for your projects, but it's good to know the basics to give yourself a little bit of a head start. If you swamp yourself with too much information you'll just end up being confused before you've even learned anything.

Remember that the syntax of most programming languages are very similar, there are some (LISP, Assembly, etc) that will seem to be from a whole different world, but for the most part the major languages share many commonalities.

Technical writing and, honestly, just skill in writing in general is very helpful. I know my professors loved good documentation and comments, and it will come in handy when you are eventually out in the working world. Being able to write good, concise, understandable documentation is a wonderful skill.

Don't burn yourself out. Programming a large project can suck you in and leave you confused and frustrated more often than not. Don't get tunnel vision, there is always a solution to the problems that will presented to you, and there is almost always  a multitude of different ways to solve them. Don't get caught chasing one idea for too long when there could be a much more efficient and simpler solution that you just haven't realized yet.

Learn to love it. Go home and work on your own personal projects with the new skills you learn over time. Don't burn yourself out, but it's good to take pride and enjoy your work. Write a website or an app. Make a tool for yourself that aids in your daily tasks. 

Best of luck!",58t17e,t3_58t17e,Tebbathy,,Comment,3,0,3
d943b2p,2016-10-23 06:18:32-04:00,Tebbathy,,Cheers for the reply man!,58t17e,t1_d93fuol,GunslingerJones,,Reply,1,0,1
58qcvo,2016-10-21 19:30:54-04:00,SirCutRy,Why are many popular domain IPs only stored on a very limited amount of DNS servers?,"Regarding the recent DDOS on Dyn's services, I would like to know how this happens. Why aren't the IPs corresponding to the domain names of popular sites such as Twitter and Reddit on almost every DNS server?",,,,,Submission,8,0,8
d92gsfl,2016-10-21 19:50:55-04:00,minimim,,"The DNS servers you're thinking of are a private service. The owner of the domain has to pay to be there.
",58qcvo,t3_58qcvo,SirCutRy,,Comment,4,0,4
d92gsic,2016-10-21 19:50:59-04:00,rewardiflost,,"Dyn is a managed DNS provider.   
They undertake the work of ensuring that DNS entries are legitimate, rather that each administrator having to do that for every ISP, college,or independent provider.     
There are other DNS providers that we can reroute to,  but this drastically slows traffic down.    Even within some of those providers, a lot of material is pasted together from dozens of different servers.  They need accurate (and current) DNS to provide their services.  

Most colleges and places I've worked do have a fail over DNS setup, but that still requires some knowledge and administrative work: it still shows down service: it still results in many users experiencing timeouts and outage behaviors.  


If this were easy or simple, we'd still be using *hosts* files on our workstations. ",58qcvo,t3_58qcvo,SirCutRy,,Comment,3,0,3
d92h3j0,2016-10-21 19:59:53-04:00,SirCutRy,,That makes sense. Thank you.,58qcvo,t1_d92gsic,rewardiflost,,Reply,3,0,3
d92j6b9,2016-10-21 21:00:44-04:00,Hargbarglin,,"I work for a small town ISP and in theory it's not too complicated to run a DNS server. Like, I'm pretty sure you could set something up on a $4 virtual private server with a public static IP. This isn't my area, but what was my area was writing a basic interface and backend so our technical support staff could manage DNS records without having to have access to the server and doing some basic authentication/validation (though not truly extensive, they are quite capable still of screwing something up). In theory any other DNS service that wants to talk to something on our network may have to look at our DNS server. The thing is, we could change those records ten times a day if we had to (exeptionally unlikely). Nobody else might even know about those records until they have to ask for it. I don't know much more beyond that, as I'm not network operations, more of an internal systems developer, but just knowing that basically we are the authority for those records and we can change them at any time implies some things to me.",58qcvo,t3_58qcvo,SirCutRy,,Comment,2,0,2
58kv7e,2016-10-20 22:11:22-04:00,DJGreenHill,How would one go to get a heat map of distances from a point on a map?,"Say, for example, I have a map of a parking lot, and I would like to get a heat map of where to park in order to be the nearest possible to the place I want to go. Many times, parking slots are taken, so having a map of where is the most advantageous would be very handy.

My inputs would be the map, the available spots, the ""walls"" (places where I cannot walk through, like a bareer not seen on the map or the actual parking slots that should not be taken into account since I can only imply that there are cars parked in them therefore cannot walk into them) and finally the point where I want to be near to.

I might have a slight idea of how to get this done via some kind of A* algorithm and some clever mapping, but I thought I'd ask before reinventing the wheel so to say.

Thanks!",,,,,Submission,7,0,7
d91f9ir,2016-10-21 01:35:17-04:00,you-get-an-upvote,,"If you have a graph-representation (i.e. nodes representing locations, edges representing paths from nodes to neighboring nodes, and the weights of an edge representing the distance the edge represents) then sure, you *could* run A* from your destination G to every single node.  Then you'd have the 'true' distance from G to every node, and could color the nodes that represent parking spots accordingly.

For realistically-sized graphs, this is actually a fine solution!  You're spot on about reinventing the wheel though.  Dikstra's algorithm is a simpler form of A* that works just as well *if* you're going to be running it on the entire graph anyway.  In fact, Dikstra's can be run on the entire graph at once, whereas A* cannot (... at least, not without making the heuristic basically meaningless, in which case it is basically just a complicated version of Dikstra's).  In other words, you can run it once, starting with your goal node G, and it will give you the distance of *every* node in your graph from the goal node G.

From the [wiki article](https://en.wikipedia.org/wiki/Dijkstra%27s_algorithm):

>The algorithm exists in many variants: Dijkstra's original variant found the shortest path between two nodes,[2] but a more common variant fixes a single node as the ""source"" node and finds shortest paths from the source to all other nodes in the graph, producing a shortest-path tree.

If you'd like me to explain the algorithm, I will do my best.  Good luck.

P.S. since all you want is the nearest distance, this actually also helps simplify the algorithm.  Rather than storing the shortest path to every node, you only have to store the distance.",58kv7e,t3_58kv7e,DJGreenHill,,Comment,3,0,3
58ivco,2016-10-20 15:35:32-04:00,TissueReligion,Am I likely to learn anything by just reading an Operating Systems book?,"I'm trying to get a more first principles understanding of computers, but with a full-time job its hard for me to really dedicate to working through a 1-2 semester course on OSs.

I see a lot of people online that recommend reading an OS book to get a deeper understanding of how computers work, but without serious implementation experience, I'm skeptical that things really stick...?

Curious to hear any experiences.",,,,,Submission,6,0,6
d90s9q3,2016-10-20 16:05:38-04:00,arbitrarycivilian,,"I recently read through the entirety of *Operating System Concepts, 8th ed*, including all the on-line appendices. I learned *a ton*. Do I have every page memorized? Of course not. But a lot has certainly stuck with me so far, and I have a deeper understanding of computers now. I made sure to read thoroughly (no skimming), and I also took the time to reflect on what I had read after each chapter. It also helps that this was not my *introduction* to computer systems. I had taken a systems course in college, using *Computer Systems: A Programmer's Perspective*, so many of the concepts were not completely new to me: just the level of depth.",58ivco,t3_58ivco,TissueReligion,,Comment,14,0,14
d90uk0e,2016-10-20 16:52:45-04:00,tyggerjai,,"I wouldn't recommend it. I'd suggest starting with _Code_ by Charles Petzold, or nand2tetris (website and book), both of which are fairly accessible and comprehensive introductions. ",58ivco,t3_58ivco,TissueReligion,,Comment,5,0,5
d9lj23p,2016-11-04 10:50:57-04:00,chummer7,,"Wow, I own this book and completely forgot, I read the first few chapters a while back. Does Code really go into detail about operating systems too? ",58ivco,t1_d90uk0e,tyggerjai,,Reply,1,0,1
d94bohx,2016-10-23 11:51:24-04:00,etahnp,,"If you want to go at your own pace and make things really stick with serious implementation experience, you could try going trough https://www.ops-class.org/. I didn't do it, but it looks similar to (actually better than) my OSs course.",58ivco,t3_58ivco,TissueReligion,,Comment,1,0,1
d94rvbu,2016-10-23 17:53:18-04:00,TissueReligion,,"Ooh, this looks nice. It also seems not to assume much in the pre-reqs (just de&a), and seems to walk you through the basics of systems programming. That's good for me, because I basically know nothing about anything lower than python.",58ivco,t1_d94bohx,etahnp,,Reply,1,0,1
58fgg8,2016-10-20 02:31:55-04:00,FormerLie,Formal algorithm that accepts two finite automatons and outputs finite automaton,"Formally, I have  

Language L1 ⊆ {a,b,c}*  

Language L2 ⊆ {0,1}*  

and a function bin: {a,b,c}* -> {0,1}* defined as follows  

bin(epsilon) = epsilon  

bin(a) = 00  

bin(b) = 01  

bin(a) = 11  

I am supposed to design and formally write down an algorithm that accepts two finite automatons (that may be non-deterministic) M1 = (Q1, {a,b,c}, delta1, q01, F1) and M2 = (Q2, {0,1}, delta2, q02, F2) and outputs one finite automaton Mf where L(Mf) = { w | w € L(M1) /\ bin(w) € L(M2)}.  

Where:  

w is string that is contained in language L, generated (or accepted?) by automaton Mf.  

/\ stands for conjunction  

What is the approach I apply for this task? I searched the internet for possible algorithms and I always end up empty handed, mainly because there is that bin() function in the resulting L(Mf) description.
Could anyone please hint me up on where to start with task like this?",,,,,Submission,1,0,1
d90eipf,2016-10-20 11:30:12-04:00,HeraclitusZ,,"Regular languages are closed under inverse homomorphic image. 

That is, if you have a function h from Sigma-star to Gamma-star where h(epsilon)=epsilon and h(ax)=h(a)h(x) (making it a homomorphism on the free monoid that is strings), then for any regular language L subset Gamma-star, h^-1 (L) is also regular in Sigma-star.

""bin"" appears to be a homomorphism. And then you just have intersection.

Edit: Slight issue with domains. No change to main point.",58fgg8,t3_58fgg8,FormerLie,,Comment,2,0,2
58fe3d,2016-10-20 02:11:42-04:00,exiiia,do I need a computer science degree for a job in software development?,"I recently swapped my major because looking down the line, I am not into learning about operating systems or anything deep like that (I also hate math and physics), but will that hinder my ability to get a job in something related to CS? My university calls my major 'media information', but I guess it's basically IT. If I build myself up with projects in my free time and courses online from stuff like Coursera, would it still be possible to get a job as a software dev? Also, would you recommend minoring in CS? ",,,,,Submission,8,0,8
d907zpv,2016-10-20 09:00:21-04:00,_--__,,"While I'd say a CS degree isn't strictly necessary, swapping out of a CS degree is going to be a big red flag that could certainly harm your chances.",58fe3d,t3_58fe3d,exiiia,,Comment,12,0,12
d915fer,2016-10-20 21:11:58-04:00,KronktheKronk,,Nobodies gonna know you swapped out of a CS degree unless you tell them.,58fe3d,t1_d907zpv,_--__,,Reply,1,0,1
d9017l5,2016-10-20 03:26:02-04:00,NeedsCash,,"A degree would greatly help as it's one of the things most employers look for. Without one, you'd have are hard time even getting your foot on the door for an interview. 

That said, if you have an amazing portfolio, some employers are willing to overlook the lack of degree. ",58fe3d,t3_58fe3d,exiiia,,Comment,7,0,7
d905k9g,2016-10-20 07:30:07-04:00,neoluxfashion,,I don't think this is true. I think it's pretty important to have a STEM degree but not necessarily CS as they're primarily interested in your abilities,58fe3d,t1_d9017l5,NeedsCash,,Reply,3,0,3
d8zzwcx,2016-10-20 02:24:33-04:00,UKCSTeacher,,/r/cscareerquestions/,58fe3d,t3_58fe3d,exiiia,,Comment,7,0,7
d908g8i,2016-10-20 09:13:50-04:00,code_guerilla,,"It's doable.  I have a degree in math, just loaded my electives with cs courses.  Built a portfolio to show I could code.  Got an internship while in my senior year, and turned that into a full time job.  
  
It would be easier to just get the CS degree.  I think switching out of the program might be indicative that you want to do something else.  However there are lots of jobs in software development that are not actual devs.",58fe3d,t3_58fe3d,exiiia,,Comment,2,0,2
d90f5bq,2016-10-20 11:42:42-04:00,qoou,,Yes you can get a job as a software developer without majoring in CS. But majoring in CS will help. ,58fe3d,t3_58fe3d,exiiia,,Comment,1,0,1
d90s30o,2016-10-20 16:01:49-04:00,arbitrarycivilian,,"You'll be able to get a job in IT, but not Software Engineering / Development. But it sounds like that's what you want anyhow",58fe3d,t3_58fe3d,exiiia,,Comment,1,0,1
d90xsya,2016-10-20 18:06:23-04:00,anamorphism,,"i dropped out of college and have been a software developer/engineer/whatever-you-want-to-call-it for almost ten years now.

i never look at education history when interviewing candidates. all i care about is if you demonstrate that you can problem solve and program effectively.

granted, i only see candidates after they get through the initial resume screening and such. so, not having a degree may hurt you there, but a good portfolio is probably just as good, if not better.",58fe3d,t3_58fe3d,exiiia,,Comment,1,0,1
d910exz,2016-10-20 19:09:46-04:00,brennanfee,,"No.  Being able to do the job, and to be able to demonstrate you can do the job, is far more important than having pieces of paper saying you can do the job.  (The demonstrate part is key to *getting* the job.)

I've worked with guys that had Masters degrees and even a Ph.D's once that couldn't code their way out of a paper bag.

That being said, I have also worked with people with BS's, Masters, or Ph.D's who were brilliant.  Foundational knowledge can definitely help, but it is by no means a correlation with success.",58fe3d,t3_58fe3d,exiiia,,Comment,1,0,1
d911jfk,2016-10-20 19:38:09-04:00,g0pats,,"Will it be impossible to get a dev job? No. Will it hinder your odds? Absolutely.

If writing software is something you want to do after graduation, I absolutely recommend that you stick it out with CS. You will be putting yourself in the best position to succeed.

I have been in your shoes, I picked an IT degree over CS, and I got burned on many interviews on concept I would've learned if I chose to pursue a CS degree",58fe3d,t3_58fe3d,exiiia,,Comment,1,0,1
d9422ms,2016-10-23 04:58:41-04:00,dhobsd,,"No, you don't need one. I've no degrees, no certifications, and taken no courses. I'm pretty OK at what I do, and I've been writing software professionally since late 2000. A degree would have saved me a lot of time and hassle in some ways. Not getting a degree saved me a lot of time and hassle in others. I recommend getting one if you intend to do anything more complex than e.g. websites and databases. (And even if you end up in webdev, understanding CS theory at least in broad strokes will help you not make silly / common performance mistakes.)

That said, CS is not software engineering, and plenty of people come out of school with CS degrees who can't write fizzbuzz. Most CS courses do not include any training on debugging whatsoever. Depending on your school / program, you may not get much professionally applicable stuff out of your degree. So regardless of your choice, I'd recommend participating in and contributing to some sort of long-standing open source project in an area that interests you.",58fe3d,t3_58fe3d,exiiia,,Comment,1,0,1
d9ghp8y,2016-11-01 01:40:56-04:00,paulflorez,,"You are going to have to network to make up for the lack of a com sci or engineering degree.  You'll want to have lots of hiring managers in your network.  You may have to take a QA, IT or BI job, find some processes to automate using software and then do an internal transfer.  You will often get filtered out by recruiters if you apply via a standard web portal.

You don't like learning about OS and hate math and physics?  I wasn't a huge fan of math but I at least found portions of it interesting.  Why do you want to do software development?",58fe3d,t3_58fe3d,exiiia,,Comment,1,0,1
58f85k,2016-10-20 01:24:32-04:00,instagramlol,Help with pseudo code,My prof gave us a practice midterm that has [this](https://i.gyazo.com/936224b58a04bc5cea633fb78149a5e3.png) question on it. I've tried to figure out how to do it but I'm stuck on how to actually get the integers in the list to be be squared separately. Help would be greatly appreciated ,,,,,Submission,1,0,1
d8zyiel,2016-10-20 01:30:07-04:00,Alphabambi,,"Im not sure what format your professor wants, but I think something like this would work:

sumOfSquares(L:list of numbers)
    let sum = 0
    For n in L,
        sum += (n*n)
    return sum",58f85k,t3_58f85k,instagramlol,,Comment,7,0,7
d90al64,2016-10-20 10:08:17-04:00,instagramlol,,"Thanks, this helped a lot",58f85k,t1_d8zyiel,Alphabambi,,Reply,1,0,1
58crfo,2016-10-19 16:56:41-04:00,CrystalMiami,Help With Nesting Boxes Problem?,"I'm trying to find a dynamic solution to the nesting boxes problem. Basically I want a more efficient way of finding the boxes then n! time.

You're basically given a set of ""boxes"" which all have different dimensions. The goal is to find the maximum set of boxes that can be nested inside of each other.

So more formally,

Given set B = {b_1, b_2, . . . b_n} where each b_i contains a box with a width, height and depth.

Find the set of boxes that will allow you to nest as many as possible together.",,,,,Submission,3,0,3
d8zbtdw,2016-10-19 17:11:57-04:00,minesasecret,,"Hint:
I believe to get this into a nice format for dynamic programming you want to sort the boxes by volume in reverse order.

SPOILER:
Then, given a box and biggest sets you can form with the boxes smaller than it, you can just filter the boxes smaller by the ones which fit and add the current box to the biggest set.

The reason I say to sort by volume is because a box can only fit into another box if all sides of the one box are greater than all sides of the other box, assuming the sides of the boxes are sorted.

Hope that works/helps",58crfo,t3_58crfo,CrystalMiami,,Comment,2,0,2
d8zz085,2016-10-20 01:48:30-04:00,RobbieXtreme,,"the algorithm to improve sort time would be a affected by the choice of sorting algorithm.

I recommend a recursive quick sort as it will get sort in O(n^2) time worst case, and O(n * log n) best and average case, though there are plenty of choices to beat the old Bogo sort of O(n!) time. In fact, if the list is big enough, you might never sort the list.

The quick sort, in English, takes the first box in the unsorted list (a pivot point), then moves all smaller boxes to one side of the pivot, and then the larger to the other side. Then recursively calls itself on the two smaller lists. It will do this each time until it finds a list with only one box, at which that box has found its exact sorted spot. As the call stack resolves, each box will find it's spot until it backs out all the way to a fully-sorted list.

https://en.m.wikipedia.org/wiki/Quicksort",58crfo,t3_58crfo,CrystalMiami,,Comment,0,0,0
58c48f,2016-10-19 15:06:14-04:00,xiZaRk,How does CS relate to Calculus (Specifically Calc 2),"I'm a Sophomore attending University for a BS in Engineering for CS. I'm taking Engineering Calc 2 and we have a final project that involves taking a real-life problem in our major that involves Calc 2. I have asked my Calc professor and my professor in my programming class (keep in mind it's a programming concepts class, meaning I have limited knowledge about programming right now, did all my Gen Eds in my first year) and they have both said that Calculus *definitely* applies to CS, which I don't doubt, but neither have been able to provide an example.

Would anyone be able give an example? Bonus points for a problem.

[Here](https://drive.google.com/file/d/0B40IwsrOjCHbYVBrTEFjTWR2V00/view?usp=sharing) is the guideline pdf they gave us if anyone cares. It's pretty unhelpful considering my Calc prof has done nothing mentioned on the page.

edit: words",,,,,Submission,1,0,1
d8z8461,2016-10-19 16:01:28-04:00,BigDk,,"Summing sequences and series, limits as n -> infinity, etc. are present in proofs for discrete math and algorithm analysis.",58c48f,t3_58c48f,xiZaRk,,Comment,2,0,2
d8zi9xo,2016-10-19 19:31:09-04:00,xiZaRk,,"This sounds like the most applicable to what I could do with my current level of knowledge. Still not sure how I will apply it to CS without it just basically becoming a Calc problem.

Thanks for the help",58c48f,t1_d8z8461,BigDk,,Reply,1,0,1
d8zby7n,2016-10-19 17:14:36-04:00,weasel1453,,"So, calc isn't really used for actual programming. It's largely used for theories of computing and computing analysis. 

At some point, if you haven't already, you'll start learning about time complexity, which is basically calculating how long a program takes to run (Big O is a term you can search for more on this) that is based on calc. 

Later on, you may need the calculus background to understand more complex computing problems such as optimization problems, while you may not necessarily need to use the calc in a program, without an understanding of what's happening, you can quickly get lost trying to understand the math behind why some solutions work. 

General robotics and ai are also entirely based on calculus principles for determining optimal solutions to problems such as how a robot should move it's arm joints to put its hand in a certain position. ",58c48f,t3_58c48f,xiZaRk,,Comment,2,0,2
d8zi545,2016-10-19 19:27:58-04:00,xiZaRk,,"Considering I haven't even fully learned one language yet (Java) this seems a little out of my league. Although considering this project is 40% of my grade I might have to bring it into my league. :P

Thanks for the help still",58c48f,t1_d8zby7n,weasel1453,,Reply,1,0,1
d8zae0f,2016-10-19 16:44:28-04:00,donghit,,"In undergrad I rarely ran into applied calculus in my CS courses.  In grad school it ramped up considerably, and many classes use it.  e.g. ML, convex optimization, CV, etc.  

Specific example would be using a Taylor Expansion for estimating optical flow. 
Not sure if this answers your question.",58c48f,t3_58c48f,xiZaRk,,Comment,1,0,1
d8zi78z,2016-10-19 19:29:22-04:00,xiZaRk,,"This is basically what I've found with the ~10 minutes of researching I've done, at the level I'm currently at there's basically no way to apply Calculus.",58c48f,t1_d8zae0f,donghit,,Reply,1,0,1
d901cs2,2016-10-20 03:33:31-04:00,NeedsCash,,"Mostly used with optimization and improve program run-times. Unless you are into research, PL design, optimizations, you most likely won't use it (thankfully).

I think the only time I got to use it was back in college when we built a drone and optimizing it's pathfinding routine.

4 years as a Software Engineer and I've yet to apply the stuff I learned in Calculus classes.",58c48f,t3_58c48f,xiZaRk,,Comment,1,0,1
58c2eh,2016-10-19 14:57:28-04:00,veryqwerty,"Assistance figuring out memory related numbers and stats. (Keywords: TLB, bus, cache, page number bits, cache lines)","This is the assignment:

>Put numbers instead of [A] – [W].

>Consider virtual memory implemented using a TLB. A virtual >address is 32 bits and we have 2 GB physical memory (RAM) >with a fixed page size of 4 kB. Our memory is byte-addressed, >so for 2 GB of physical memory we need an address bus [A] bits >wide. The [B] least-significant bits of an address are used for >page offset, [C] bits are used for virtual page number and [D] >bits are used for physical page number.

>Assume a fully-associative TLB with 8 lines. For parallel tag >comparison we need [E] comparators, each [F] bits wide. The >total memory requirement for TLB ""tags"" is [G] bits.

>Memory access is accelerated using a cache of 32 kB, each cache >line holds 8 words of data (word = 4 bytes). Our cache holds [H] >lines.

>Assume the cache to be direct mapped. The address bits [I]..[J] >are used as cache ”tag”, address bits [K]..[L] point to a cache >line, address bits [M]..[N] point to a byte in the cache line. For >parallel tag comparison we need [O] comparator(s), each [P] bits >wide. In the cache we will need [Q] bits of memory for the tag >fields.

>Now assume the cache to be set associative, where each set is 4 >lines. The address bits [R]..[S] are used as cache ”tag”, address >bits [T]..[U] point to a cache set, address bits [V]..[W] point to >a byte in the cache line.

I am not looking for straight number answers, I am looking for explanations or tips on how to figure this out. This assignment is a big stepup in difficulty compared to the previous ones.

Any help is greatly appreciated :)",,,,,Submission,1,0,1
58akvn,2016-10-19 10:34:52-04:00,Pr1sma,What topics in Java should I review over for a college Data Structures/Algorithms class?,I want to prepare for this class but I do not know how,,,,,Submission,3,0,3
d8zznz1,2016-10-20 02:14:32-04:00,yes_thats_right,,"I suggest having a look at various java collection classes for data structures.

For algorithms, your class will probably start by going through various sorting algorithms to show how they can be useful in different circumstances and when to pick which for their best, worst and expected case run times. I suggest having a quick look at these. The [wikipedia page](https://en.m.wikipedia.org/wiki/Sorting_algorithm) looks pretty good.

This will then lead in to the crux of algorithm courses which is algorithm complexity/run times. You should look up Big-O notation and how it is calculated.

[this cheat sheet is a good sample of what to know](http://bigocheatsheet.com). You should thoroughly understand the underlying logic behind things on this cheat sheet.",58akvn,t3_58akvn,Pr1sma,,Comment,3,0,3
d8yw4a1,2016-10-19 12:13:17-04:00,dwizzle13,,"I'm probably not the most qualified to answer, but I'd say look at generics and if you're feeling rusty, the basics such as looping and functions. In my class, all we did was create data structures and search algorithms within java from scratch. 

I also remember my course dealt with some difficult mathematical formulas for the latter half. We tried incorporating our knowledge of trees into creating something with jogl, and thus there are matrices etc. 

That said, I think an online data structures course, such as one on coursera or mit open courseware may be a great start. There are links within the conpsci reddit I think. Unfortunately, I'm on mobile and can't link. ",58akvn,t3_58akvn,Pr1sma,,Comment,2,0,2
d8yy5ts,2016-10-19 12:53:22-04:00,Pr1sma,,Thanks!  Do you know anywhere online that I can get free practice besides codeacademy?,58akvn,t1_d8yw4a1,dwizzle13,,Reply,1,0,1
d8zyfp8,2016-10-20 01:27:28-04:00,UrGrandmasCat,,"http://www.tutorialspoint.com/data_structures_algorithms/
Code examples are written in C, but it's still a good learning resource / reference for data structures and algorithms. ",58akvn,t1_d8yy5ts,Pr1sma,,Reply,1,0,1
d9397wk,2016-10-22 14:01:34-04:00,JakesInSpace,,"Learn about the following Java classes before taking this class: Stack, List, Deque, LinkedList, OrderedList, ArrayList.
Make sure you understand how the .peek(), .pop() and .push() methods work in each of the classes. I remember taking that class in college, and those classes and methods were the main topic in the first few weeks. Then the class went on to learning sorting of data, then basic algorithmic analysis using Big-O.",58akvn,t3_58akvn,Pr1sma,,Comment,2,0,2
58aeks,2016-10-19 10:00:32-04:00,mario_from_statefarm,Is there a way to make a program to rapidly compress and decompress files?,"I was thinking about how to save space and I'm wondering this. Since compressed files are smaller, theoretically if a file is zipped up when it isn't being used and then decompressed when its being used, would it save space? And is there a way to do this quickly? ",,,,,Submission,3,0,3
d8yq0qf,2016-10-19 10:03:37-04:00,anossov,,"It's already being done everywhere it matters — video, audio, images, executables, game resources, etc. A majority of all filetypes are compressed and have to be decompressed when used.",58aeks,t3_58aeks,mario_from_statefarm,,Comment,8,0,8
d8yq4ih,2016-10-19 10:06:14-04:00,brendanrivers,,"To expand on this, this idea is referred to as a ""COmpression DECompression"" algorithm, or CODEC for short.  [more on wiki](https://en.wikipedia.org/wiki/Codec)

",58aeks,t1_d8yq0qf,anossov,,Reply,3,0,3
d8yygdj,2016-10-19 12:59:02-04:00,stuckatwork817,,"Yes, enable disk compression in Windows.  Requires a checkbox.

No guarantee on how big the savings will be or how much it will affect your read or write speeds without knowing what your files contain.

There will always be a tradeoff between compression ration and 'effort' required to compress and decompress.

In this context, 'effort' is a unitless measure of work similar to CPU cycles per whatever.

Broadcast codecs are often asymmetrical with substantially more 'effort' required to compress, examples include the Motion Picture Expert Group ( MPEG ) codecs which allow decoding on low cost hardware in realtime. ",58aeks,t3_58aeks,mario_from_statefarm,,Comment,4,0,4
d8yvj5n,2016-10-19 12:01:30-04:00,splenetic,,"There used to be a number of utilities that would do exactly that - compress files automatically and then decompress them on the fly when they were being accessed. There's a list of them [in this wikipedia article](https://en.wikipedia.org/wiki/Disk_compression). 

Speed of access wasn't as bad as you might think. Although it took CPU time to decompress the data that was offset by the fact that as the compressed file being read off of the disk was smaller than it would have been if it wasn't compressed, it took less time to read. 

They were useful back in the days when disk space was limited, slow and expensive. Given that, today, you can buy a 2TB disk for less than $100 it's much less of an issue on most PCs. Also many file formats these days are effectively pre-compressed. Image files, audio and video are already compressed with algorithms that are much more efficient than the general-purpose algorithms a disk compression system generally uses. Even Microsoft Word pre-compresses its document files. Also a compressed disk was fine when it was working but if it suffered some corruption the fact that it was compressed made it much harder to recover.

That being said many big storage systems (eg, NetApp) have compression facilities available. High-performance network storage isn't cheap and so it makes sense to maximise what you've got, particularly for data backup systems.

",58aeks,t3_58aeks,mario_from_statefarm,,Comment,2,0,2
d8yywbx,2016-10-19 13:07:54-04:00,heywire84,,"Several file systems offer this feature and similar features.  NTFS which Windows uses has the capability to compress files with a variant of the LZ77 compression algorithm.

ZFS and its open source cousins have the capability to compress individual files using a variety of compression algorithms.  The file system can also use the entire storage system for [deduplication](https://en.wikipedia.org/wiki/Data_deduplication).  Deduplication is a process where any files which have duplicate data have that data only written once and the data is replaced with a reference.  For example, if you have MS Word documents, they may share a file header which is identical for all the different files.  That header information would only be stored once.

Any compression scheme will trade CPU cycles and memory usage for storage efficiency.  Quickly can also be a subjective measurement.  If you turned compression on for your filesystem, you may never notice, depending on your use case.  A home user who accesses a file infrequently and has a decently powerful computer is a different case from a server handling thousands of requests simultaneously where the CPU cycles spent compressing and decompressing adds up very quickly!",58aeks,t3_58aeks,mario_from_statefarm,,Comment,2,0,2
d8zfhv6,2016-10-19 18:28:13-04:00,xonelast,,"I completed a school assignment just like this in C++. Read into the greedy compression algorithm called **Huffman Coding**. Works well with compression and decompression of all file formats: txt, video, images, etc.

Here's the gist:
Huffman compression allows text encryption using binary patterns. Cryptography achieves security by encoding or transforming messages to make them non-readable. The input file is tested for each character frequency before encoding. These frequencies numbers are used to build a binary tree whereas each character of the text is represented by the tree's leaf-node. Each left-going edge is represented as 0 and each right-going edge is represented as 1. In the end, we get a binary representation of the character after tracing the nodes of the tree. Using a few bits per character, the original file size is reduced a significant amount in the new encrypted output file. Huffman decompression is the same reverse logic as compression. You traverse the binary tree backwards.

This project is a great learning experience and I highly suggest taking a look into the links below.

[Explanation of Huffman Coding](http://web.stanford.edu/class/archive/cs/cs106b/cs106b.1126/handouts/220%20Huffman%20Encoding.pdf)

[Step-by-step Stanford Assignment on Huffman Encoding / Decoding](https://web.stanford.edu/class/archive/cs/cs106b/cs106b.1166/assignments/hw6-huffman.pdf)

",58aeks,t3_58aeks,mario_from_statefarm,,Comment,1,0,1
58a12o,2016-10-19 08:38:10-04:00,Picabooi,"What can you say about ""Python Programming In Context"" book?","So yeah should I get that book? It's currently the only thing available for me now (only book about python from online stores that offers Cash-on-delivery and I tried searching at book stores and I can't findany).

Someone might suggest online tutorial since alot of them are free, there are even videos and audio books but I prefer having something physical. Like I tried doing those online stuffs and makes me bored somehow. I however use online guitar tutorials and it was not as boring since I have something physically there (my guitar) so I came up with a theory that it'll be less boring.",,,,,Submission,1,0,1
588ou9,2016-10-19 01:24:00-04:00,Senjukotentaiho,UTF beginner,How does utf-8 and utf-16 works? Can you explain it to me? I actually tried to research through online but I can't understand it,,,,,Submission,6,0,6
d8yiht0,2016-10-19 05:08:53-04:00,chipmandal,,"The explanation will depend on how much you already know. 

UTF-8 and UTF-16 are byte encodings of Unicode (https://en.wikipedia.org/wiki/Unicode). Unicode describes a character and assigns a value to it. Say For example U+1F600 is 😀. 

Unlike ASCII, U+1F600 is not usually physically encoded as 1F600 in bytes.  Naively, this would take 3 bytes. to represent upto the max: U+10FFFF. Actually UTF-32 uses 4 bytes to encode the actual ""code point"" i.e 0001F600 in our example, but that encoding is rarely used.

Most of the characters we type fit into 1or 2 bytes. It would be wasteful to use 3/4 bytes for everything. This is where UTF-8 and UTF-16 comes in. They are ""variable byte"" encodings. 

This means, one ""code point"" or character will be represented by anywhere between 1-5 bytes in UTF-8. Common characters like ""A"" will be represented by 1 byte. Less common characters get longer byte sequences. The wikipedia page (https://en.wikipedia.org/wiki/UTF-8#Description) has a good description of the mechanics of it. 

Comment if you have specific questions and I will try to explain further.



 
",588ou9,t3_588ou9,Senjukotentaiho,,Comment,3,0,3
daa1air,2016-11-21 16:04:17-05:00,None,,"Yes. When I convert the ASCII value from text to binary, [it instead shows me 5 bytes](https://i.imgur.com/DKMJx4C.png), not 3 as you mentioned. Did I misunderstand something or...? ",588ou9,t1_d8yiht0,chipmandal,,Reply,1,0,1
d8yijrk,2016-10-19 05:12:06-04:00,tuankiet65,,"To explain UTF-8 and UTF-16 you need to understand what Unicode is.

So what Unicode is? Well basically it's a way to represent a character using a number. So for example the character 'A' is assigned the number 65. You can also call that number a __code point__, so the character 'A' is codepoint 65. The reason for this is because computers work with numbers, binary numbers to be exact, and obviously there must be a way to work with characters using numbers. Unicode is a standard, so if a program can properly handle Unicode characters it will display the same text as other programs that can also handle Unicode.

Now we have a problem. A computer can only handle either 8, 16, 32 or 64 bit numbers. The biggest codepoint is 0x10FFFF, far too big for a 16 bit number. So unfortunately you have to use 32 bit number, which means now each character takes 4 byte.

Enter UTF-8. It's a way to convert (or encode) from Unicode code point to a list of 8 bit numbers, and vice versa. UTF-8 is a variable length encoder, so the length of that list depends on how big the code point is. For example, code points in the range of 0-127 will only take 8 bit (or one byte). Larger code points takes more space obviously.

UTF-16 is just the same, but using 16 bit numbers instead of 8 bit.

There's also a thing called UTF-32, which is just storing the entire code point in a 32 bit number.

The reason UTF-8 is popular and not UTF-16 or UTF-32 is that in the past, mostly everyone used ASCII, which is the same as Unicode but only supports 256 characters and uses one byte to store the number. Because the first ~~256~~ 128 characters in Unicode is the same as ASCII, and UTF-8 encodes them to one byte, documents written in ASCII will properly display in UTF-8.

(Edit: change to using *code point*, and fix the Unicode code point range which is encoded in UTF-8 as 1 byte)",588ou9,t3_588ou9,Senjukotentaiho,,Comment,3,0,3
d8ymm1f,2016-10-19 08:27:38-04:00,Madsy9,,"> Because the first 256 characters in Unicode is the same as ASCII, and UTF-8 encodes them to one byte, documents written in ASCII will properly display in UTF-8.

The first 128 characters, not 256. ASCII is 7 bit wide. The remaining bit in UTF-8 is used to signal whether a control byte follows. The *code points* U+0080 through U+00FF is the Latin1 supplement block, but that is based on ISO 8859-1, not ASCII.

Otherwise a good explanation :)",588ou9,t1_d8yijrk,tuankiet65,,Reply,3,0,3
d8yn839,2016-10-19 08:47:46-04:00,tuankiet65,,"Thanks, when I wrote that I was on my phone so I don't really have time to research / verify things.",588ou9,t1_d8ymm1f,Madsy9,,Reply,1,0,1
583l4t,2016-10-18 08:50:11-04:00,awesumguy,Simple algorithm question,"This is basically the first homework assignment and we need to trace the values for I J and R at each line, but the answer doesn't make since to me. Start with 32 and 20.

1)Get two positive integers as input; call the larger value I and the smaller value J 

2)Divide I by J, and call the remainder R 

3)If R is not 0, then reset I to the value of J, reset J to the value of R, and go back to Step 2 

4)Print out the answer, which is the value of J

5)Stop

My answer ends up being 4 because that is what the final J value is .  After the first division the i change the variables to 20/12 even though my instinct tells me it is supposed to be the other way. 
Am I plugging it in wrong or is the code algorithm flawed?

Edit- I am stupid the algorithm is designed to get the greatest common divisor 4 is correct",,,,,Submission,1,0,1
d8x5bsy,2016-10-18 09:17:35-04:00,FluffyOgreJet,,"Your instinct is wrong. You always want i > j so after the first division you should have i=20, j=12.",583l4t,t3_583l4t,awesumguy,,Comment,2,0,2
d8x5lf4,2016-10-18 09:24:33-04:00,awesumguy,,"When I run it like that when R=0 J=4 is that supposed to be the answer. I feel like it isn't correct. This is my work is it correct?

I=32 J=20 R=0

I/J 32/20= 1 remainder 12 R=12

I=20 J=12 R=0

20/12= 1 remainder 8 R=8

I=12 J=8 R=0

12/8=1 remainder 4 R=4

I=8 J=4 R=0

8/4=2 remainder 0 R=0",583l4t,t1_d8x5bsy,FluffyOgreJet,,Reply,1,0,1
d8x7m35,2016-10-18 10:13:14-04:00,FluffyOgreJet,,"Yes, 4 is the correct answer. This algorithm is a form of the [Euclidean algorithm](https://en.m.wikipedia.org/wiki/Euclidean_algorithm) where the answer should be the GCD of 32 and 20, which is 4.",583l4t,t1_d8x5lf4,awesumguy,,Reply,2,0,2
58139c,2016-10-17 21:10:52-04:00,The_Code_Runner,University of Miami vs Embry Riddle for a Computer Engineering/Software Degree?,"Hello everyone!

I am an upcoming College freshman and have been blessed with the opportunity to pick between several great colleges. Ive narrowed it down to these two, from a hiring perspective, which one is more prestigious?

Thanks!",,,,,Submission,1,0,1
d8xcsel,2016-10-18 12:00:41-04:00,KronktheKronk,,"From a hiring perspective, your experience and portfolio coming out of college is going to matter way more than the name of the college on your degree.",58139c,t3_58139c,The_Code_Runner,,Comment,1,0,1
580xow,2016-10-17 20:38:34-04:00,DanTheManWithDaPlan,1st Semester CS major and Discrete math is kicking my ass,"Im halfway through my first semester, and right now Discrete math is kicking my ass. I have no clue what is going on and am really far behind. The Professor teachers very fast, and the textbook is complete jargon to me.


Anyone here have any resources, textbooks, online lectures, tips, etc that might help me out?


Also the textbook we are using is [Discrete Math for Computer Science Students](http://www.cse.iitd.ernet.in/~bagchi/courses/discrete-book/fullbook.pdf).",,,,,Submission,19,0,19
d8wkok3,2016-10-17 21:03:36-04:00,macnor,,"Not a resource but have you tried making a study group? Discrete was kicking my ass until I made a study group. I also made some good friends as a result of that study group too.

Another suggestion is have you tried the tutoring your school probably provides? Have you checked out any of the computer science related clubs? I ask these questions since these students are more likely to have taken the class with your professor before and therefore can give you better insight into how to do well in *that* class. It also is an easy way to start networking from the very beginning of going to college.

Haven't use it myself but the MIT videos are good for other subjects so you might want to try it: https://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-042j-mathematics-for-computer-science-spring-2015/

Good luck.",580xow,t3_580xow,DanTheManWithDaPlan,,Comment,8,0,8
d8wtn07,2016-10-18 00:38:50-04:00,arichi,,"* Study groups

* Office hours -- prof has them, I'm sure your TAs do too.

* Talk to students ahead of you in the program

* Above all, _work problems_.",580xow,t3_580xow,DanTheManWithDaPlan,,Comment,5,0,5
d8wu5us,2016-10-18 00:56:10-04:00,covertc,,"Great suggestions in here.  Discrete math kicked my ass too. I managed to get through it by doing the following:  
  
* Start the textbook over and go through it. For every page or section, challenge yourself with ""Do I understand this?""  
* Once you get to the first place where it loses you, that's where you either seek help or find alternative explanations, say another textbook, site, youtube.  
* If you're still stuck after that, seek help from your classmates, TA and prof  
* Repeat for every section  
  
This worked for me but was a real grind.  I wish you the best of luck. ",580xow,t3_580xow,DanTheManWithDaPlan,,Comment,4,0,4
d8wva49,2016-10-18 01:36:58-04:00,techhead57,,"No real advice off the top of my head other than what's been said. But I have words of encouragement.

When I took it 9 years ago I really liked it but just had trouble to really understand it. I felt like I was treading water just trying to keep afloat. I had almost decided not to keep up with math beyond what I had to. I passed but not with a grade I was happy with.

I wound up reading a book my aunt got me for Christmas called ""The Cryptonomicon."" It had a lot of mathematical thinking that I had started to learn about during the discrete math course. I really enjoyed the book and it kind of inspired me to continue with math. 

Here I am almost a decade after, with an MS in computer science which was in Theory working on algorithms for graphs. To prove their correctness and approximations I used many concepts I learned in that class. I've TA'D that class twice. I'm in my 6th year of my PHD, though I've changed focus to machine learning I've always considered myself a theoretican at heart. I also have a great job lined up in industry starting soon and I should be hopefully finishing my dissertation to defend in about a year if all goes well.

My point being, struggling in an important class is tough. My first year in college was rough, I wasn't used to being challenged like that. I know I'm a walking cliche but it's true. Use this as a learning experience on how to approach problems you struggle with and you'll come out of it better than those who had no trouble. 

Good luck!",580xow,t3_580xow,DanTheManWithDaPlan,,Comment,3,0,3
d8wlvso,2016-10-17 21:29:48-04:00,epictylerone808,,"I recommend the different YouTube videos, such as NesoAcademy and others. ",580xow,t3_580xow,DanTheManWithDaPlan,,Comment,2,0,2
d8wsjih,2016-10-18 00:05:05-04:00,ostracize,,"Many of those topics can be found here:

https://betterexplained.com",580xow,t3_580xow,DanTheManWithDaPlan,,Comment,2,0,2
d8xppaa,2016-10-18 16:02:28-04:00,yooman,,What do you not understand?  I love answering questions on this stuff.,580xow,t3_580xow,DanTheManWithDaPlan,,Comment,2,0,2
d8xjugu,2016-10-18 14:15:38-04:00,dxk3355,,Wow that's a shitty book.  Mine was nothing like that one since it was taught by the math department and not the CS department.  Maybe try looking at one the math department would use.,580xow,t3_580xow,DanTheManWithDaPlan,,Comment,1,0,1
d8xyga5,2016-10-18 18:58:41-04:00,albatrek,,"Khan Academy has some decent videos on this stuff as well. 

I'll also chime in that discrete math was the first hard class I took, and I really struggled with it, but ended up loving it. Office hours, TAs, professors, older students, and classmates are all good resources. Chances are, other people are confused by the same things, and the professor wants to help you understand. They likely don't realize that they're losing people, and would appreciate a head's up that they could do to slow down a bit. 

I'm also willing to try to answer specific questions, if you have some you want to share. Feel free to PM me.",580xow,t3_580xow,DanTheManWithDaPlan,,Comment,1,0,1
d8xyly7,2016-10-18 19:02:11-04:00,SmoothB1983,,"This is the best textbook for learning discrete math: https://www.amazon.com/dp/B00B6FAZ2Q/ref=dp-kindle-redirect?_encoding=UTF8&btkr=1

If linear algebra didn't make you comfortable with proofs then also this: https://www.amazon.com/How-Prove-Structured-Approach-2nd/dp/0521675995

Between those two you will be good. Unfortunately halfway through the semester is tough. If you are below a C I would drop it and retake it with better prep next time.

On the plus side, if this puts you behind consider taking on a Math or Statistics minor with the slack time. That can put you in the running for better grad school or job opportunities later on.",580xow,t3_580xow,DanTheManWithDaPlan,,Comment,1,0,1
5807f2,2016-10-17 18:11:57-04:00,redditronald,Alternative to Dr. Dobbes,The Dr. Dobbes online and print publications have been ended.  Does anyone know of a good alternative?,,,,,Submission,5,0,5
57y31e,2016-10-17 12:00:18-04:00,cromissimo,What is this combinatorial optimization problem?,"Consider a set S composed of a high number of discrete elements.  The goal is to determine which subset of n elements of S has the lowest cost.

Does anyone know what's the name of this particular type of problem?  Also, which algorithms are able to solve this problem?

Thanks,",,,,,Submission,2,0,2
d8wb02x,2016-10-17 17:25:39-04:00,None,,[deleted],57y31e,t3_57y31e,cromissimo,,Comment,3,0,3
d8wz6da,2016-10-18 04:47:48-04:00,cromissimo,,"> Sounds like a general combinatorial optimisation problem if there's no further information available.

That is my assumption as well, but does it correspond to a particular type of combinatorial optimization problem?

> For larger problem instances you might have to use approximate algorithms or a metaheuristic. You won't get guarantees of optimality without opening up to some analysis of the cost function.

That is my assumption as well, but I was hoping that the problem I described would fit a specific category of problems for which specific algorithm (heuristic or not) was devised to take advantage of the problem's particular structure (considering the [no free lunch theorem](https://en.wikipedia.org/wiki/No_free_lunch_theorem)).",57y31e,t1_d8wb02x,None,,Reply,0,0,0
d8w55m0,2016-10-17 15:30:33-04:00,bo1024,,What is the cost of a subset of n elements?,57y31e,t3_57y31e,cromissimo,,Comment,1,0,1
d8w7fxa,2016-10-17 16:15:05-04:00,cromissimo,,"> What is the cost of a subset of n elements?

That would be determined by a [cost function](https://en.wikipedia.org/wiki/Loss_function) that would be specific for the problem at hand.",57y31e,t1_d8w55m0,bo1024,,Reply,1,0,1
d8wbe9f,2016-10-17 17:33:58-04:00,bo1024,,"So in general, this is NP-hard and you can't do substantially better than querying the cost function on all subsets and picking the best one. If there are n elements, there are up to 2^n subsets so this is not very good.

If you assume the function is, for instance, supermodular, then there is a good greedy algorithm. Usually instead of phrasing it as minimization, we phrase the problem as maximization (e.g. take the negative of the loss function) and study [submodular maximization](https://en.wikipedia.org/wiki/Submodular_set_function).",57y31e,t1_d8w7fxa,cromissimo,,Reply,2,0,2
d8vxbow,2016-10-17 12:55:18-04:00,andybmcc,,"That just sounds like a sorting problem to me.  You sort the set based on cost, then take the n lowest elements.

Quicksort is on average O(nlogn).",57y31e,t3_57y31e,cromissimo,,Comment,1,0,1
d8vxriu,2016-10-17 13:04:06-04:00,phantom23,,It is faster to find the n lowest numbers through a selection algorithm https://en.wikipedia.org/wiki/Selection_algorithm,57y31e,t1_d8vxbow,andybmcc,,Reply,1,0,1
d8w02ff,2016-10-17 13:49:32-04:00,andybmcc,,"Yeah, selection algorithms are mostly subsets of sorting algorithms.",57y31e,t1_d8vxriu,phantom23,,Reply,1,0,1
d8w2wkj,2016-10-17 14:46:07-04:00,cromissimo,,"> That just sounds like a sorting problem to me. 

Doesn't this approach require that all combinations are evaluated beforehand?  This would lead the problem to become computationally intractable due to the number of possible combinations alone.  I mean, the number of all possible subsets of 6 elements taken out of set of 100 elements has over 1 billion elements.

I was assuming that this would be a discrete optimization problem, more likely a knapsack problem of sorts.",57y31e,t1_d8vxbow,andybmcc,,Reply,0,0,0
d8w3arx,2016-10-17 14:53:59-04:00,AnArtistsRendition,,"I think you may have left out a detail in the problem description. For example, if each element can be represented as a number, and the cost of a subset is the sum of these numbers, then the problem becomes a selection problem like /u/phantom23 suggests. However, if the cost function for a subset is arbitrary (namely, it can be non-associative), then it's some other type of problem (as you would need to evaluate all possible subsets).",57y31e,t1_d8w2wkj,cromissimo,,Reply,1,0,1
d8w3wfo,2016-10-17 15:05:55-04:00,cromissimo,,"> I think you may have left out a detail in the problem description.

I mentioned in the text post that the goal was to determine which subset of n elements of S has the lowest cost.  How that cost is determined was never specified.",57y31e,t1_d8w3arx,AnArtistsRendition,,Reply,1,0,1
d8w44ez,2016-10-17 15:10:20-04:00,east_lisp_junk,,So are you just asking how to enumerate all n-subsets of S?,57y31e,t1_d8w3wfo,cromissimo,,Reply,1,0,1
d8w838p,2016-10-17 16:27:36-04:00,cromissimo,,"> So are you just asking how to enumerate all n-subsets of S?

No, as that would lead to [combinatorial explosion](https://en.wikipedia.org/wiki/Combinatorial_explosion).

If anyone knows what kind of combinatorial optimization problem would fit the problem I've described, it would be possible to pick suitable algorithms to find solutions, exact or approximate.",57y31e,t1_d8w44ez,east_lisp_junk,,Reply,1,0,1
d8wbl0o,2016-10-17 17:37:56-04:00,east_lisp_junk,,"Unless you know something significant about the cost function, there is no way to prune off any of the n-subsets.",57y31e,t1_d8w838p,cromissimo,,Reply,0,0,0
d8wz77s,2016-10-18 04:49:09-04:00,cromissimo,,"> there is no way to prune off any of the n-subsets.

This is blatantly not true.

https://en.wikipedia.org/wiki/Branch_and_bound",57y31e,t1_d8wbl0o,east_lisp_junk,,Reply,1,0,1
d8x24tb,2016-10-18 07:29:14-04:00,east_lisp_junk,,Branch and bound does not work if you know nothing about the cost function.,57y31e,t1_d8wz77s,cromissimo,,Reply,1,0,1
d8xau52,2016-10-18 11:22:16-04:00,cromissimo,,"> Branch and bound does not work if you know nothing about the cost function.

That depends on what you mean by ""know nothing"".  Some branch-and-bound algorithms are direct optimization algorithms developed to handle black box optimization functions. This means that nothing about the cost function is known beforehand.  If you have nothing to contribute, why reply in the first place?",57y31e,t1_d8x24tb,east_lisp_junk,,Reply,0,0,0
57x430,2016-10-17 08:44:28-04:00,caffeine_depend,Anyone know why I can't read an integer into the array?,"    import java.util.Scanner;
    class LegalDates {
	    public static void main(String[] args) {
		    Scanner in = new Scanner(System.in);
		    int dates[][] = {
			    {0, 0},
			    {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31},
			    {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28},
			    {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31},
			    {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30},
			    {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31},
			    {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30},
			    {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31},
			    {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31},
			    {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30},
			    {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31},
			    {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30},
			    {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31}
		    };
		    String months[] = {
			    {"""", ""January"", ""February"", ""March"", ""April"", ""May"", ""June"", ""July"", ""August"", ""September"", ""October"", ""November"", ""December""}
		    };
		    System.out.println(""What's the month of the date you'd like checked?"");
		    dates[0][0] = in.nextInt();
		    System.out.println(""The month you selected is "" + months[dates[0][0]] + ""! What date of that month would you like checked?"");
	    }
    }",,,,,Submission,0,0,0
d8vnjeu,2016-10-17 09:16:10-04:00,_--__,,"You have declared months as a String[], but it is defined as a String[][] (i.e. you have too many {}'s)",57x430,t3_57x430,caffeine_depend,,Comment,7,0,7
d8wfxwq,2016-10-17 19:16:44-04:00,zkxs,,"If you ever find yourself typing the same thing on 12 subsequent lines, there's probably a better way of doing it.

If you think about it, the only real information in your array is the maximal day of any month.  

`int daysInMonth[] = {31, 28, 31, 30, 31, 30, 31, 31, 30, 31, 30, 31}:` contains the same data, but much more compactly.",57x430,t3_57x430,caffeine_depend,,Comment,2,0,2
57w1jm,2016-10-17 02:51:32-04:00,AroundtheTownz,How do I change the background color of 2 different columns in a web page?,"I've been trying for hours and I can't seem to do it. Essentially I'm writing code for a web page in HTML and i've done everything I need too except I can't seem to make the 2 sides any color at all. I split them up using <div> tag. I have an idea of what the background tag is (bg?), but I don't know where to place it in the code?",,,,,Submission,0,0,0
d8vixiw,2016-10-17 06:11:00-04:00,SGVsbG8gV29ybGQ,,"    <div style=""background-color:#FFFFFF"">

Replace ""#FFFFFF"" with the color of your choice. Or, if you want to style the columns in an external css-file, you could do it like this:

HTML:
    
    <div class=""bg1""> column1 </div>
    <div class=""bg2""> column2 </div>

CSS:

    .bg1 {
      background-color:#FFFFFF:
    }
    
    .bg2 {
      background-color:#000000:
    }


For more info: http://www.w3schools.com/cssref/pr_background-color.asp
        
",57w1jm,t3_57w1jm,AroundtheTownz,,Comment,1,0,1
d90kvwk,2016-10-20 13:36:28-04:00,AroundtheTownz,,"Trying it, but it's not working?",57w1jm,t1_d8vixiw,SGVsbG8gV29ybGQ,,Reply,1,0,1
57scv4,2016-10-16 13:02:59-04:00,shinobi791,Can a barcode or QR code hold multiple UPC numbers?,If so is it possible to get all the items onto a point-of-sale (POS) terminal from a single scan?,,,,,Submission,1,0,1
d8uhwe2,2016-10-16 13:18:32-04:00,NotTacoBell,,"Possibly a QR code could contain multiple but separate item numbers, but not barcodes. 

The POS must have a import function via CSV. Get all the item numbers into a CSV document and upload. ",57scv4,t3_57scv4,shinobi791,,Comment,3,0,3
d8uosyj,2016-10-16 15:41:16-04:00,dxk3355,,The basic laser scanner read linearly so in theory if you had two bar codes in a row you could read them.  Image type scanners like the iPhone apps could do it too if they were all in focus.  I don't think there's regular software that does this though. ,57scv4,t3_57scv4,shinobi791,,Comment,3,0,3
d8ukhg5,2016-10-16 14:12:01-04:00,BonzoESC,,This is a question for your POS vendor. ,57scv4,t3_57scv4,shinobi791,,Comment,2,0,2
d8vcp78,2016-10-17 01:04:44-04:00,proskillz,,"Edit: I reread your question, and this answer is not relevant, but I'm going to leave it since not everyone know about this. 

Barcodes are essentially a font, so they could do whatever you'd like. Here's an example: http://www.dafont.com/barcode-font.font",57scv4,t3_57scv4,shinobi791,,Comment,1,0,1
57q9z2,2016-10-16 02:19:09-04:00,newl_survivor,Which upper tier Comp Sci schools offer a standalone Masters degree?,I know schools such as Stanford and University of Michigan offer standalone master degrees (not earned in route to a phd). Some schools do not offer a standalone Masters program. What standalone Masters programs exist at top tier schools? Does anyone know of a list?,,,,,Submission,4,0,4
d8u8hsi,2016-10-16 08:35:33-04:00,Irony238,,"Any German University teaching Computer Science, University of Cambridge, University of Oxford, University of Edinburgh.",57q9z2,t3_57q9z2,newl_survivor,,Comment,3,0,3
d8ug9sy,2016-10-16 12:39:23-04:00,albatrek,,"Carnegie Mellon does: 
http://www.csd.cs.cmu.edu/academics/masters/overview#mscsoverview",57q9z2,t3_57q9z2,newl_survivor,,Comment,3,0,3
d8v5fyh,2016-10-16 21:53:55-04:00,assface,,Every school but MIT offers a terminal MS program.,57q9z2,t3_57q9z2,newl_survivor,,Comment,3,0,3
d8u84t6,2016-10-16 08:17:55-04:00,dmooney1,,Georgia Tech does both on campus and through their OMSCS online program. ,57q9z2,t3_57q9z2,newl_survivor,,Comment,3,0,3
d8uiik1,2016-10-16 13:31:56-04:00,Pandahx,,Is UCSD considered upper tier?,57q9z2,t3_57q9z2,newl_survivor,,Comment,1,0,1
d97e6ro,2016-10-25 14:50:26-04:00,jesbu1,,Yeah probably,57q9z2,t1_d8uiik1,Pandahx,,Reply,2,0,2
57pe2x,2016-10-15 22:03:08-04:00,Winged__Hussar,I recently read CODE: The Hidden Language of Computer Hardware and Software and have some questions.,"In the computer that Charles Petzold describes (the one built from telegraph relays) how does it know what the instruction codes mean (the load, store, add, subtract, etc. codes), and then act on them?

What would the computer (the relay one) look like when drawn out? We never see a completed version of it in the book.",,,,,Submission,12,0,12
d8u0pp4,2016-10-16 01:34:24-04:00,BonzoESC,,"Instruction codes are used to turn on different switches, similar to a rail yard. If you have one train track coming in, and four going out, you need log2(4), or two switch controls to control them. The first switch puts a train on either tracks 1-2 or 3-4. the second switch puts a train bound for 1-2 on to 1 or 2, or a train bound for 3-4 on to 3 or 4.

In the same way, in an arithmetic unit in a computer processor, there may be eight operations: addition and subtraction: bitwise AND, OR, and XOR: shift left and shift right: and maybe NOT. log2(8) is three, so we need three control signals. Just like rail switches, the combination of each three signals activates a specific operation. These three signals are really just three-bit instruction codes. 

A modern CPU isn't so simple, but it's all just switches routing pieces of data around.",57pe2x,t3_57pe2x,Winged__Hussar,,Comment,5,0,5
d8votze,2016-10-17 09:50:51-04:00,Winged__Hussar,,"In the book, codes are held in the ram that, when activated by the clock, tell the rest of the computer to add, subtract, add with carry, subtract with borrow, etc.

How those codes do that is never explained. Would there be some wire system set up to active the switches that is never mentioned? Or is there something I am missing in how a basic, Von Huagman (I think I got his name wrong) computer.",57pe2x,t1_d8u0pp4,BonzoESC,,Reply,1,0,1
d8vpfqb,2016-10-17 10:06:17-04:00,BonzoESC,,"Let's just have two operations, addition and subtraction, with one control signal. The two input numbers are available, and they're split so they go into both the adder and the subtractor. After the time it takes for the adder and subtractor to perform the calculation, the results become available. The instruction code, which might be 0 for adding and 1 for subtracting, goes to a [multiplexer](https://en.wikipedia.org/wiki/Multiplexer), which uses the control signal to pick the desired result.

The switches are just transistors.",57pe2x,t1_d8votze,Winged__Hussar,,Reply,1,0,1
d8wqspx,2016-10-17 23:19:25-04:00,Winged__Hussar,,"So you are saying that both operations are done, and then depending on what to code was an answer is selected? Wouldn't this require an entire separate circuit system for each action? Is there a better way, preferably one that can uses the same system for each, since that is what is done in CODE.",57pe2x,t1_d8vpfqb,BonzoESC,,Reply,1,0,1
d8ws9nh,2016-10-17 23:57:11-04:00,BonzoESC,,"I'm not actually sure! I suspect that addition and subtraction can share the same circuit since they're really just the same. Multiplication and division would probably be best implemented with different circuitry, and most of the Boolean and shifting operations are probably too simple to combine. 

The questions that answer this one are: Is the extra physical space and power usage worth spending on simpler logic? Can the unused bits for a given instruction be efficiently depowered? ",57pe2x,t1_d8wqspx,Winged__Hussar,,Reply,1,0,1
d8xabqr,2016-10-18 11:11:50-04:00,Winged__Hussar,,"Multiplication and division can both be done by repeated addition and subtraction, respectively. The real question is just to get the proper switches flipped to get each task to be done. I suppose something like a multiplex could be used to do that.",57pe2x,t1_d8ws9nh,BonzoESC,,Reply,1,0,1
d8txm15,2016-10-15 23:51:07-04:00,IAmNotMyName,,I've never read the book. Would you recommend it?,57pe2x,t3_57pe2x,Winged__Hussar,,Comment,1,0,1
d8tyei7,2016-10-16 00:14:41-04:00,Winged__Hussar,,"It tells you everything about computers at a not terribly specific level. It does require quite a bit of though, and I still have those two questions left. It reads very similar to Guns, Germs, and Steel.

9/10, have some questions left

10/10, with rice",57pe2x,t1_d8txm15,IAmNotMyName,,Reply,7,0,7
d8umj1u,2016-10-16 14:54:59-04:00,nilleo,,My gf got me this for Christmas one year. I'd definitely recommend it. It's an easy and entertaining read.,57pe2x,t1_d8txm15,IAmNotMyName,,Reply,2,0,2
57nxsk,2016-10-15 16:28:53-04:00,sexuallytransformed,Was just in a disagreement with my internet provider over what would be considered consistent download speeds.Whats your opinion?,"I live in southern Alberta Canada, in a city called Lethbridge. Canada is known for its terrible internet providers and their monopoly over the industry. 

I have up to 25 mbps plan with 300gb worth of data. I always have issues with consistent connection when it comes to downloading and playing games. I called the provider and gave them control over my computer and had them do the internet speed test. It clocked in at 17mbps which is great and at functional levels. I explained that the speed is fine its just inconsistent and had him watch as i downloaded Arma 3 for shit and giggles. We started out strong at 2 mbps but it eventually fell to 300kbs per second and then steadily climbed and fell back and forth. Only ever making it back to 1.2 mbs then dropping quickly to 300kbs. 

Is this acceptable? The tech support seem to think I was in the wrong for expecting a better performance and that i just didn't understand how bandwidth worked.  I have used better connections in the past that allowed for online gaming and downloading at more consistent levels.

Is there anything i can do to help fix the problem?

Im just frustrated with these Canadian cocksuckers ripping us off for shit service... thanks for letting me rant. 



",,,,,Submission,0,0,0
d8tgrvs,2016-10-15 16:36:38-04:00,UKCSTeacher,,"This sub is for theoretical questions related to Computer Science, not plucking pseudo-random numbers out of the air to satisfy your desire to argue with your ISP",57nxsk,t3_57nxsk,sexuallytransformed,,Comment,4,0,4
d8tgssw,2016-10-15 16:37:15-04:00,sexuallytransformed,,"My bad, what would be more appropriate sub to ask my question?",57nxsk,t1_d8tgrvs,UKCSTeacher,,Reply,1,0,1
d8tgy46,2016-10-15 16:40:56-04:00,UKCSTeacher,,"A local sub, if the issue is related to your locality. But in reality I'm not sure you will obtain any useful information from such a question, as it's essentially asking 'how long is a piece of string'. Either you will feel bad the answers you get are inadequate or when you turn around to the ISP and say ""People on the Internet said a good connection should be 100Mbps"" they will laugh. ",57nxsk,t1_d8tgssw,sexuallytransformed,,Reply,1,0,1
d8tharo,2016-10-15 16:49:38-04:00,sexuallytransformed,,"I'm asking about a consistent connection, not the overall speed. I don't think it should fluctuate so dramatically. ",57nxsk,t1_d8tgy46,UKCSTeacher,,Reply,1,0,1
d8thn2v,2016-10-15 16:58:11-04:00,UKCSTeacher,,"I've not had Internet installed since I moved house 3 months ago, so any kind of Internet including inconsistent Internet sounds amazing. In an ideal world it shouldn't fluctuate. This is certainly an ISP issue, but strangers on the internet can't really help you with that ",57nxsk,t1_d8tharo,sexuallytransformed,,Reply,1,0,1
d8thwqc,2016-10-15 17:04:53-04:00,sexuallytransformed,,That is fai. I just get mad when they sit there and tell me it's fine. Thanks for your help. ,57nxsk,t1_d8thn2v,UKCSTeacher,,Reply,1,0,1
d8tqu7f,2016-10-15 20:51:51-04:00,brokenyard,,"[This guy](https://www.reddit.com/r/technology/comments/43fi39/i_set_up_my_raspberry_pi_to_automatically_tweet/) considered it bad enough whenever his speeds were 1/3 or less of what he was paying for. In your case, it's pretty slow, but remember their advertised speed and the speed test are in megabits, and your download speed is in megabytes. Basically, multiply megabytes by 8 to get the number in megabits, and you'll see that the 2 Mb/s that you initially got and the 17 mbps are about identical. ",57nxsk,t1_d8thwqc,sexuallytransformed,,Reply,1,0,1
d8tzev4,2016-10-16 00:47:25-04:00,sexuallytransformed,,I never knew that. Thanks buddy,57nxsk,t1_d8tqu7f,brokenyard,,Reply,1,0,1
57mm2s,2016-10-15 11:47:57-04:00,inlovebadly123,Need help with basic project/task [Willing to pay],"As the title mentions, I have a project which needs completing and I'm running out of time therefore i require the time of more hands to help me.

Willing to pay. Very basic programming. Takes no longer than 30 mins probably. Willing to pay. Pm for more info",,,,,Submission,0,0,0
d8t64we,2016-10-15 12:05:38-04:00,UKCSTeacher,,Takes no longer than 30 minutes but that's 30 minutes you don't have? ,57mm2s,t3_57mm2s,inlovebadly123,,Comment,3,0,3
57mi0r,2016-10-15 11:21:59-04:00,HarvsG,"What is the difference between a password, a passphrase and a pre-shared key.",if any,,,,,Submission,9,0,9
d8t5jkm,2016-10-15 11:50:27-04:00,SEMW,,"Password and passphrase are basically the same thing -- 'passphrase' just removes the (usually/hopefully wrong) implication that it has to be a single word (ie without spaces).

But passwords cannot be used directly as 'keys' in the sense of the actual input of an encryption function. The actual key has to be a certain byte length, and have various other properties. So if you want to encrypt some data and share it with someone else, there are two things you could do:

-  you could share a key: use a key that was generated directly, with a cryptographically-strong pseudorandom number generator, that was previously shared with the other person -- a 'pre-shared key'. (I'm ignoring public-key encryption for the moment)

- you could share a password: use with a password that was previously shared with the other person, and generate a key deterministically from that, using something called a 'password-based key derivation function'. (Basically a hash with certain desirable properties). So when they do the same thing with the same password, they'll get the same key out.

The term 'pre-shared key' can also be used more loosely, to mean any kind of pre-shared secret, which might either be a key, or a password or something from which a key is derived. (As opposed to a public-key encryption type system which doesn't require the secret to be shared beforehand). In this sense, a password would be a type of pre-shared key.",57mi0r,t3_57mi0r,HarvsG,,Comment,3,0,3
d8txybj,2016-10-16 00:00:54-04:00,albatrek,,"These terms are also all used frequently in talking about wifi and wireless security. To make things a little more confusing, the pass{word,phrase} for a wifi network is often called the pre-shared key, since connecting devices will generate a temporary key (which is the specific # of bytes, good crypto properties kind of key).

Basically, it's usually worth looking at the context - don't make too many assumptions based off of the name.",57mi0r,t1_d8t5jkm,SEMW,,Reply,1,0,1
d8tg95f,2016-10-15 16:23:51-04:00,epictylerone808,,"Additionally, if you want to get really good encryption, there's a type called PHP where the user needs their own public and private key, and the recipient's public and private key. ",57mi0r,t3_57mi0r,HarvsG,,Comment,-6,0,-6
d8tm52m,2016-10-15 18:50:48-04:00,yes_thats_right,,"You mean PGP.

PHP is a scripting language.",57mi0r,t1_d8tg95f,epictylerone808,,Reply,6,0,6
d8ttbrs,2016-10-15 21:55:38-04:00,epictylerone808,,I swear it was auto correct. I even had the joke really good encryption. PHP stands for pretty good encryption. ,57mi0r,t1_d8tm52m,yes_thats_right,,Reply,-1,0,-1
d8ttrn8,2016-10-15 22:07:19-04:00,SayYesToBacon,,Pretty Good Privacy ,57mi0r,t1_d8ttbrs,epictylerone808,,Reply,1,0,1
d8txtwh,2016-10-15 23:57:17-04:00,albatrek,,It's pretty god-awful privacy if you have the recipient's private key...,57mi0r,t1_d8tg95f,epictylerone808,,Reply,2,0,2
57l9k3,2016-10-15 04:48:55-04:00,cheesePopsicle616,Need help understanding Sub-domains and CNames,"I'm taking a course on Web Development, but I'm having a hard time understanding a couple concepts, and I have midterm coming up , so I was wondering if anybody here might be able to help me.

The teacher had us set up a web site, using am instructional web server, then create a sub-domain for that site. To get the sub-domain to work, we also had to create a CName record that had the same name as the sub-domain, but had a target URL of the main web site.

I understand that CNames are aliases for A records, and A records return IP addresses, but after that I'm lost. Why do you we need to create a CName for the sub-domain? And why does the CName target the main site?

Thank you in advance for your help",,,,,Submission,6,0,6
d8sw2z8,2016-10-15 05:01:30-04:00,UKCSTeacher,,"You can set up wildcard record to remove the need for cname records for sub domains but they are not recommended from a security point of view. 

The cname doesn't have to target the main site either. Frequently, it doesn't. The most common cname is www. which will be an alias for the domain and point at the main Web hosting server, but mail or database servers will be kept separate",57l9k3,t3_57l9k3,cheesePopsicle616,,Comment,1,0,1
d8tv3fs,2016-10-15 22:42:51-04:00,cheesePopsicle616,,"Oh, so www is a CName. It's starting to make more sense now. Thanks!",57l9k3,t1_d8sw2z8,UKCSTeacher,,Reply,1,0,1
d8syryg,2016-10-15 07:51:47-04:00,brtt3000,,"They are semantically different: one is *""this domain is an alias for this other domain""*, and the other *""this domain is handled by this machine""*. 

In general you can use A-Record for the subdomain and it is slightly more optimal as you remove the extra alias lookup, but sometimes it is convenient or required for management reasons to daisy-chain domain names.

Maybe in your course they didn't want to rely on IP changing hassle and just have all of the students point to the managed domain (so the teachers can move the applications to another host or whatever without having to get everyone to update their records).",57l9k3,t3_57l9k3,cheesePopsicle616,,Comment,1,0,1
d8tv47e,2016-10-15 22:43:26-04:00,cheesePopsicle616,,"Yeah, that makes sense. I think I understand better now. Thank you!",57l9k3,t1_d8syryg,brtt3000,,Reply,1,0,1
57i3xs,2016-10-14 15:07:22-04:00,friendlyantagonist,Question Regarding some Big Commerce coding problem,"I know, I know. Big Commerce and custom code is like ketchup on watermelon. Just ugh. 

I created a media kit for my client and they requested it be put on their website, hosted on BC's platform. BC doesn't have a lot of options as far as uploading a pdf and displaying it nicely so I opted to create a new ""text"" page and inserted some code to create a slideshow of jpg versions of the pdf pages. The code worked well enough, but the arrows that allow you to flip through the media book are VERY small and at the bottom of the page so really not even noticeable unless you are looking for them. 

I adapted code that I found on W3 for this project, but even though I directly copied and pasted the code from W3 to BC (with my edits of srl and other small things like the numbers) but after it was saved and I looked at the code again BC had reordered it, I assume to make it compatible with the platform's base code. 

I'm not really proficient enough with code to understand how to make this code work with BC's base code. ",,,,,Submission,0,0,0
d8s6024,2016-10-14 15:28:23-04:00,friendlyantagonist,,"    <div class=""slideshow-container"">
    <div class=""mySlides fade"">
    <div class=""numbertext"">1 / 5</div>
    <img style=""width: 100%:"" src=""https://cdn3.bigcommerce.com/s-e03gcwmkzi/product_images/uploaded_images/mediakit-1.jpg?t=1476471366"" alt="""" /></div>
    <div class=""mySlides fade"">
    <div class=""numbertext"">2 / 5</div>
    <img style=""width: 100%:"" src=""https://cdn3.bigcommerce.com/s-e03gcwmkzi/product_images/uploaded_images/mediakit-2.jpg?t=1476471376"" alt="""" /></div>
    <div class=""mySlides fade"">
    <div class=""numbertext"">3 / 5</div>
    <img style=""width: 100%:"" src=""https://cdn3.bigcommerce.com/s-e03gcwmkzi/product_images/uploaded_images/mediakit-3.jpg?t=1476471383"" alt="""" /></div>
    <div class=""mySlides fade"">
    <div class=""numbertext"">4 / 5</div>
    <img style=""width: 100%:"" src=""https://cdn3.bigcommerce.com/s-e03gcwmkzi/product_images/uploaded_images/mediakit-4.jpg?t=1476471395"" alt="""" /></div>
    <div class=""mySlides fade"">
    <div class=""numbertext"">5 / 5</div>
    <img style=""width: 100%:"" src=""https://cdn3.bigcommerce.com/s-e03gcwmkzi/product_images/uploaded_images/mediakit-5.jpg?t=1476471403"" alt="""" /></div>
    <a class=""prev"" onclick=""plusSlides(-1)"">❮</a> <a class=""next"" onclick=""plusSlides(1)"">❯</a></div>
    <p>&nbsp:</p>
    <div style=""text-align: center:"">&nbsp:</div>
    <script type=""text/javascript"">// <![CDATA[
    var slideIndex = 1:
    showSlides(slideIndex):
    
    function plusSlides(n) {
      showSlides(slideIndex += n):
    }
    
    function currentSlide(n) {
      showSlides(slideIndex = n):
    }
    
    function showSlides(n) {
      var i:
      var slides = document.getElementsByClassName(""mySlides""):
      var dots = document.getElementsByClassName(""dot""):
      if (n > slides.length) {slideIndex = 1}    
      if (n < 1) {slideIndex = slides.length}
      for (i = 0: i < slides.length: i++) {
          slides[i].style.display = ""none"":  
      }
      for (i = 0: i < dots.length: i++) {
          dots[i].className = dots[i].className.replace("" active"", """"):
      }
      slides[slideIndex-1].style.display = ""block"":  
      dots[slideIndex-1].className += "" active"":
    }
    // ]]></script>",57i3xs,t3_57i3xs,friendlyantagonist,,Comment,1,0,1
57hrn6,2016-10-14 14:04:20-04:00,seabee494,Good book on understanding how internet works and the infrastructure?,"Question in the title. Just wanting to learn more of a technical understanding on how the internet works, how the networking works from the ISP to the ""last mile"", what a backbone is, how fiber works on a deeper level (I know it's transmitting light, but I'd like to understand how the waveforms factor into it), etc. 

",,,,,Submission,9,0,9
d8spjkg,2016-10-14 23:56:03-04:00,BonzoESC,,"Wikipedia has a lot of really good stuff! For example, the [article on Optical fiber](https://en.wikipedia.org/wiki/Optical_fiber#Principle_of_operation) has a lot of detail.

I'd recommend looking up the OSI model, playing around in Wireshark, and remembering that it's all packets and ways to encode them.

Loading a web page? http://igoro.com/archive/what-really-happens-when-you-navigate-to-a-url/ is the basics, but there's a fractal complexity to a lot of it. It's cool to learn, but for better or worse, most of the writing that I can think of just abstracts away a lot of the really gnarly stuff in favor of what most readers need to get their job done. Someone making an iOS or Android app doesn't need to know the minutiae of [GSM](https://media.ccc.de/v/25c3-3007-en-running_your_own_gsm_network) or [UMTS/HSPA](https://media.ccc.de/v/32c3-7412-running_your_own_3g_3_5g_network) mobile networks, and most of the time you only need to know the basics of TCP and UDP (I'd tell a UDP joke right here, but I don't know if you'd get it.)

I literally just started reading ""Networks of New York"" by Ingrid Burrington earlier today, but I'm only a few pages in so far. Ask me tomorrow :P",57hrn6,t3_57hrn6,seabee494,,Comment,2,0,2
d8swoca,2016-10-15 05:40:21-04:00,publysher,,"""Computer networks"" by Tanenbaum. It goes all the way from ""this is a copper cable"" to ""and that is how the internet works""
",57hrn6,t3_57hrn6,seabee494,,Comment,2,0,2
d8slvji,2016-10-14 22:06:32-04:00,dron57,,I think this is the best overall book on computer networking - [Kurose and Ross](https://www.amazon.com/Computer-Networking-Top-Down-Approach-6th/dp/0132856204). If you want even more depth about electronic and optical transmission check the bibliography of this book. ,57hrn6,t3_57hrn6,seabee494,,Comment,1,0,1
d985633,2016-10-26 01:20:49-04:00,devshady,,"Computer Networking A top down approach by Kurose Ross is good one.
Also you can try Computer Networks and Internets by Douglas Comer and Narayanan.",57hrn6,t3_57hrn6,seabee494,,Comment,1,0,1
57ej7t,2016-10-13 23:56:25-04:00,krazypandag,PLEASE HELP with MIPS code,"Consider the following MIPS code and a 5 stage processor:
  Loop: 
 lw r1. 4(r7)           
 lw r2, 8(r7)
 add r1, r1, r3       
 sw r1, 4(r7)    
 bne r1, r2, loop

Rewrite the code so that the prediction in the previous item yields a more efficient pipeline.",,,,,Submission,0,0,0
d8s1g8r,2016-10-14 13:54:51-04:00,james41235,,"I'm assuming this is some sort of homework.  Essentially you need to move the instructions to take advantage of the pipelined nature of CPU instructions, as well as cedrtain branch productions.  I can't read assembly without a reference manual, but I'm assuming you need to move the branch instruction somewhere else, and maybe reorder the inside of the loop, so that all 5 stages of the CPU pipeline are busy more often than not.",57ej7t,t3_57ej7t,krazypandag,,Comment,1,0,1
57e73b,2016-10-13 22:32:21-04:00,reticentpeach,Single mom interested in Computer Science career path,"I'm a newly single mom and a struggling freelance graphic artist, without a degree (some college) and looking at new career options. I've always enjoyed computers, math and problem solving so I'm interested in looking at a career in computer science. I have no experience with programming but I think I'd enjoy it based off of my interests. I'm needing any and all info on what my options are as far as an education goes. Online degrees? Reputable online schools? Tech school? A bachelors in CS? A career in computer science as a female? I'm interested in any tips or advice on whether or not this is a realistic course of action considering my circumstances and interests, and if it is, where is the best place to start?",,,,,Submission,13,0,13
d8rcm1f,2016-10-14 00:33:34-04:00,ranci,,"Had a similar path here.  Was a struggling graphic designer for a long time (wildly oversaturated job market).  Got a MSCS and became a developer.  Feel free to message me, happy to help.

How realistic this is depends in many ways upon you.  Are you dedicated enough to work on this and learn it fruitlessly for some time, and through many frustrations?  How much time do you have, given that you have a child or children?  The answers to these really matter in determining if and how to move forward.",57e73b,t3_57e73b,reticentpeach,,Comment,7,0,7
d8r8naf,2016-10-13 22:54:09-04:00,gbay,,One way to start is to pick up a beginners book and start coding. See if you like it. Since you're a designer a good place to Start would be web development. Search googles for some good books. PM me and I'll send you some links (am tired and lazy and on phone now),57e73b,t3_57e73b,reticentpeach,,Comment,5,0,5
d8rbrmm,2016-10-14 00:09:29-04:00,albatrek,,"There's a lot you can learn on your own. To start off, you'll need to think about what direction you want to go: programming or CS. While they're very related, they aren't the same. Programming is, well, programming. It's coding, learning languages, designing, and building projects. CS is math, logic, theory, and writing code. Programming is solving problems using computers. CS is solving problems for computers. Both are fun, both are valuable skills, and you need at least a little of both to do either. Learning to program on your own is very doable (codeacademy.com is a good place to start). Learning CS on your own is harder, though still somewhat doable.

(A note on your ""career in computer science as a female"": I'm a woman in CS as well. It's sometimes hard. I've had many situations where I am the only woman in a group of 30+ guys. I love CS enough that it's always worth it. The balance is shifting.)

Based on the little I know of your situation, I'd start learning on sites like codeacademy. See how you like it, then decide whether you want to pursue something more formal. If you do, you have options in CS, software engineering, and math. Putting projects up online (github.com) is a good way to start building a portfolio that you can use for job or school applications.

Good luck! Enjoy the journey!",57e73b,t3_57e73b,reticentpeach,,Comment,2,0,2
d8vabgn,2016-10-16 23:52:49-04:00,deong,,"That's actually a decent description of CS and programming, but nearly everyone doing CS is in practice preparing to be a programmer. For most people aspiring to enter the field, I don't think the difference is that relevant. It's more, ""programming is a skill, and CS is the four year degree that most closely certifies your knowledge of the skill"".

The more academically advanced you go, the less true that becomes, but for most undergrads, it's pretty accurate.",57e73b,t1_d8rbrmm,albatrek,,Reply,1,0,1
d8rrfsv,2016-10-14 10:27:50-04:00,umib0zu,,"> Online degrees?

My friend has had a lot of success with [FreecodeCamp](https://www.freecodecamp.com/). The hard part about online degrees is they're hard, but you won't know its hard until after you've sunk money into them. This is a major problem in the CS education space right now, and in tech in general, but I recommend starting with a free camp first and getting through that before you take more risk and further your study taking on courses that you may decide isn't for you.

> Reputable online schools? Tech school? A bachelors in CS?

[Coursera is a great place to start.](https://www.coursera.org/browse/computer-science?languages=en) For something formal, maybe an [online degree from a school would help](https://www.google.com/#q=online+computer+science+degree). I don't know much about them, but I'd look into them if you're truly interested.

> A career in computer science as a female?

It's not as bad as it was 20 years ago, but I'm not a female and can't speak to this. There are still issues, but think computer science is a career path that is definitely trending better in regards to awareness and treatment of women when compared to other fields.


> I'm interested in any tips or advice on whether or not this is a realistic course of action considering my circumstances and interests, and if it is, where is the best place to start?

Tips:

1. [It's hard, and people might as well be up front about it.](http://blogs.wsj.com/economics/2013/07/08/math-science-popular-until-students-realize-theyre-hard/) This field takes a lot of self-study, and social interaction to help you learn concepts. I spend about 3 hours a day reading, 6 programming for work, maybe 2 programming on side projects, and lots of chatting in [IRC](https://kiwiirc.com/client/irc.freenode.org/programming) or my companies hipchat.

2. It's beautiful and deep, once you get past the breadth coursework that will get you paid. I'm more on the mathy side now, but I've hit a point where I'm reading about how physics, math, and computer programs are all the same thing, but this is only about 1/15 of the area of computer science. Neural Networks? Mobile Development? Data Science? Quantum Computation? Complexity theory? Algorithm design? Video game production? Pick your poison. You'll easily find something to do for the rest of your life and you'll be happy about it, but you'll need to put in the work.

3. You will be told to do things you don't like, but it's for a reason and you'll need it later. Especially for CS undergrads, the first thing they say is ""Why do I need calculus? I just want to make games?"". You'll find yourself saying ""Why do I need to (do|make) X, I just want to (do|make) Y?"" a lot, especially if you go the university route, but there's usually some deep reason for it. Get used to exploring and trying new things.",57e73b,t3_57e73b,reticentpeach,,Comment,2,0,2
d8rv9d7,2016-10-14 11:48:39-04:00,trucekill,,"If you want to use some of the skills you've already got, I recommended learning HTML, JS, and CSS/LESS. If you focus on making high quality user interfaces and if you master front end development, then you can combine it with your graphic design skills and you should be in high demand.

",57e73b,t3_57e73b,reticentpeach,,Comment,2,0,2
d8r9df2,2016-10-13 23:10:09-04:00,adriano515,,"Might i suggest hour of code? (Hourofcode.org i think is the webpage) its for beginners (mostly used by kids i believe) but i think i might have learned a lot from it when i started. 

Anyone care to give an opinion on this? Curious to know hat this sub thinks of it",57e73b,t3_57e73b,reticentpeach,,Comment,1,0,1
d8rohl4,2016-10-14 09:14:22-04:00,CoopNine,,"A Bachelors in CS is the best option in my opinion.  There are no reputable online degrees that I am aware of.  There are also some intensive training programs that are quality, and help with placement after completion, but it could be hard for someone in your situation to commit to.  The ones I'm aware of are basically 3-6 month programs which are essentially a full time job.  

You can self-learn, but it can be difficult to focus yourself... It will also be difficult to find work, but if you have a good portfolio as a graphic artist, you might be able to piece together some web-work to add in to get someone to take a chance on you.",57e73b,t3_57e73b,reticentpeach,,Comment,1,0,1
d8rqwvi,2016-10-14 10:15:52-04:00,Bonananana,,"I can't see why being a single mom would prevent this path, but I can see why being a single mom would make ANY path pretty tricky.  I have two...soon to be three kids and even with my wife here its a lotta work.  I think if you feel passion for the career you can make it work and that following through on a career change will be a great example for your kid.  Seeing a parent engage in work they're excited about could be pretty inspiring.

I'm not a woman working in development, but I've worked with women in development, hired them, managed them and actually married one too.  My wife started as a developer and is now a director over a section of her company.  Generalizing horribly, I've found the women I've worked with to be more capable on average and certainly easier to work with on average.  I do know that they've had to cope with sexism, childishness, immaturity and outright discrimination.  My wife was the only woman in many of her classes in undergrad and grad school.  She is often the only woman on a team or on a project.  Hah, once in a meeting she was taking notes and someone asked if she was going to share the notes out.  She said ""No, they're for me.  Are you expecting me to take notes because I'm a woman?""  They laughed and made fun of the guy who brought it up for being a lazy idiot.  I've often suggested she throw it back and begin meetings of director or vp level guys with the phrase ""Which one of you pretty ladies wants to take notes for us?"" But, she has too much dignity for that approach.  Also, computers don't care if you're a woman, they'll still work the same.

What are you really looking to do?  You want to do CS research?  Data science?  OS work?  Embedded systems?  Mobile?  Web apps for business?  Games?  CS is a huge area.  Virtually everything needs software today.  If you want to keep your options open a CS undergrad degree will help enable anything you want to do.

If you're just looking to get into programming I'd guess that rolling the graphic design skills into webapp design and implementation or mobile app design and implementation would be a shorter path.  I know on many of my teams I'd have killed for someone who could make a webapp look pretty AND do the css/html work to get layouts implemented.  Being able to get the application pieces done too would have been a dream come true.

",57e73b,t3_57e73b,reticentpeach,,Comment,1,0,1
d8rv4sc,2016-10-14 11:46:05-04:00,xiongchiamiov,,"Yes, it's definitely a feasible option.

Two places on reddit that may be useful: r/cscareerquestions and r/learnprogramming .",57e73b,t3_57e73b,reticentpeach,,Comment,1,0,1
57dg7p,2016-10-13 19:42:42-04:00,inlovebadly123,"15 minutes of your time needed in one sitting, willing to pay","Hello, 

I have a few basic foundation CS/math related questions which need answering before next week monday. I have about 10 questions that I need help on. 

If you need a little $ for your time I am willing to pay

Pm me for further info",,,,,Submission,4,0,4
d8r1xz2,2016-10-13 20:27:38-04:00,SayYesToBacon,,What are your questions? Maybe I or someone will answer on the house,57dg7p,t3_57dg7p,inlovebadly123,,Comment,3,0,3
d8ruc8w,2016-10-14 11:29:51-04:00,wafflestealer654,,"I won't provide full answers for you to copy down, but I can certainly help you (no pay :-)) with the questions. What's one of the questions?",57dg7p,t3_57dg7p,inlovebadly123,,Comment,2,0,2
d8r6vqx,2016-10-13 22:16:05-04:00,NotTacoBell,,Mid terms?,57dg7p,t3_57dg7p,inlovebadly123,,Comment,1,0,1
57d67a,2016-10-13 18:44:05-04:00,neptunefox,How To Study For a Design & Analysis of Algorithms Graduate Exam?,"I'm a CS Grad student at NYU-SoE coming from a non-cs background. To my luck (or misfortune), I'm taking an Algorithms course under a really crappy professor. She literally comes and copies contents from CLRS on white board without going through what it is or means. I tried reading CLRS on my own but failed miserably (that book is terribly written for beginners and introduction to algorithms is a mis-title for it). 

I have an exam coming up for which I know nothing, and need your help in as to how can I prepare well for the exam and what resources should I use? In general, how should I approach studying algorithms: What to memorize/learn, what to conceptualize and what to practice? I need to do well in this exam as it is a core course. Please help!",,,,,Submission,3,0,3
d8r45pv,2016-10-13 21:16:29-04:00,Not_Ayn_Rand,,"Damn, sorry your prof sucks. I had Siegel at the main campus and he was da best. Idk what's being covered but [visualgo](https://visualgo.net/) might be a good place to start. Stanford puts a free algorithms class online so you might want to look at it too if you have a bit more time.",57d67a,t3_57d67a,neptunefox,,Comment,1,0,1
d8r9y8k,2016-10-13 23:22:55-04:00,neptunefox,,"In 4 classes, Prof Kahrobei has called covered complete Graph Algorithms section from CLRS and the first 7 chapters. Its crazy! And she was literally copying all of it from the CLRS instructor's manual. Now the problem is with her grading that's crazy: 40% Mid terms, 50% finals, closed book/no notes exams.Most students and my classmates dont even bother sitting in class since theres no point. 

I'll definitely look at Visualgo. 

Other than, from an exam's point of view, how should I approach prepping for algos?",57d67a,t1_d8r45pv,Not_Ayn_Rand,,Reply,1,0,1
d8rt2eo,2016-10-14 11:03:19-04:00,Not_Ayn_Rand,,"I found repetition, visualization, and office hour visits helped most. You might actually consider going to Siegel's office hours as he is a great teacher (imo). He uses a book that he wrote, but the material should be the same.",57d67a,t1_d8r9y8k,neptunefox,,Reply,1,0,1
d8rt9pe,2016-10-14 11:07:35-04:00,neptunefox,,"Thanks! I'll try that. Any advice regarding where to practice from and how to practice algorithms?
",57d67a,t1_d8rt2eo,Not_Ayn_Rand,,Reply,1,0,1
d8t84ny,2016-10-15 12:56:29-04:00,Not_Ayn_Rand,,"I just did problems in the book. I have never looked at CLRS as Siegel's book was sufficient, but if it has problem sets, I imagine those would be even better. I also used very verbose flash cards, but I think I'm the only person I know who used flash cards for algorithms.",57d67a,t1_d8rt9pe,neptunefox,,Reply,1,0,1
57b3ec,2016-10-13 12:16:28-04:00,asdffdsdf,What does it mean that Internet is an information system?,"A well-known definition of Internet provided in 1995 by the FNC is:

""The Federal Networking Council (FNC) agrees that the following language reflects our definition of the term ""Internet"".

""Internet"" refers to the global information system that --

(i) is logically linked together by a globally unique address space based on the Internet Protocol (IP) or its subsequent extensions/follow-ons;

(ii) is able to support communications using the Transmission Control Protocol/Internet Protocol (TCP/IP) suite or its subsequent extensions/follow-ons, and/or other IP-compatible protocols; and

(iii) provides, uses or makes accessible, either publicly or privately, high level services layered on the communications and related infrastructure described herein.""

It says than Internet is an information system.  According to the definitions of ""information system"" I could find, the term refers to hardware, software, data, process and people collecting, storing, processing and distributing information. I could understand how hardware, software and data are what makes the Internet. I have a hard time understanding how the process of managing data is part of the Internet. I have never seen people say that the users of the Internet are part of it. Why?

 I suppose that the last two parts of the information system aren't part of the ""information system"" in FNC definition. If so, what does the term refer to?",,,,,Submission,6,0,6
d8r0fp7,2016-10-13 19:53:18-04:00,anamorphism,,"the FNC definition of 'Internet' is intentionally broad. they were primarily describing the hardware infrastructure with no idea of what was to come.

the definition of 'information system' is also extremely broad. basically it can refer to any hardware system. just like when someone says s/he works in 'information technology' or IT, s/he can be anything from a software engineer to a server installer.

when most people talk about the 'Internet' today, it's less about the hardware, protocols, etc. and more about the web and the sites/services available there.

you're focusing too much on details when the words used are there to define a much more generic concept.",57b3ec,t3_57b3ec,asdffdsdf,,Comment,1,0,1
d8rfky1,2016-10-14 02:17:09-04:00,asdffdsdf,,"What do you mean by ""hardware system""?",57b3ec,t1_d8r0fp7,anamorphism,,Reply,1,0,1
d8qp3lp,2016-10-13 15:42:37-04:00,KronktheKronk,,"The internet's wheelhouse is definitely the ""distribution"" part, but in recent history the internet has come to facilitate the cloud offerings that are so popular today.

How do you have remote storage/processing in the cloud without the internet?  you don't.",57b3ec,t3_57b3ec,asdffdsdf,,Comment,0,0,0
578tzn,2016-10-13 01:58:16-04:00,The_Grey_Wolf,Good books on syntax analysis using formal grammars?,"Hello guys,

I already understand the concept of a grammar and have already built a calculator that uses a grammar to parse expressions. However, now I'm writing a paper that discusses syntax analysis using formal grammars. Any books that come to mind?

P.S. Those who have read ""The Art of Computer Programming"" by Knuth, does he mention this topic in any of his books?",,,,,Submission,2,0,2
5784kj,2016-10-12 22:50:07-04:00,Underbar_Kuk,Advice on what foreign language to learn as a future software developer?,"Little bit of background:

I'm conversationally fluent in German, Swedish, and English (naturally) and with some months of training, would be relatively fluent in Italian (studied as a child, forgotten quite a bit). I currently live in the United States and am about to start school for my degree (a full-ride to a tech school in my area). The thing is, I'd like another more ""practical language"" under my belt before I enter the workforce (meaning at least two years of study). By practical, I mean a language that is spoken in a country with a strong software development sector or perhaps a language with >100 million speakers. 

For context, I'm not super sure as to what my career path will be. I'd like to keep it open in my first couple years as a student and as a junior developer, so I'm not opposed to really any recommendation as I'd like to keep all options on the table. 
",,,,,Submission,1,0,1
d8pxzk2,2016-10-13 02:21:18-04:00,Teemperor,,"[deleted]  
 ^^^^^^^^^^^^^^^^0.5739 
 > [What is this?](https://pastebin.com/64GuVi2F/68099)",5784kj,t3_5784kj,Underbar_Kuk,,Comment,1,0,1
d8q41h3,2016-10-13 07:41:29-04:00,artillery129,,I think you're basically good with just english. 99% of software developers speak english,5784kj,t3_5784kj,Underbar_Kuk,,Comment,1,0,1
d8pxy8j,2016-10-13 02:19:44-04:00,exo762,,"Mandarin, Korean.",5784kj,t3_5784kj,Underbar_Kuk,,Comment,1,0,1
576pep,2016-10-12 17:41:29-04:00,G_vintez,Short CS course in the UK?,"So, im travelling to the UK(london specifically) and want a 2 month (max) duration course in my area, im at second year at a very good university in my country doing computer science and would be really nice to learn something new in my trip.
Any suggestions? I'm open to anything(websites, physical schools,etc)",,,,,Submission,1,0,1
d8pxj21,2016-10-13 02:02:01-04:00,UKCSTeacher,,/r/AskUK/ is a more suitable place to ask. I have personally not heard of such a course before ,576pep,t3_576pep,G_vintez,,Comment,1,0,1
d8q8lcp,2016-10-13 10:01:33-04:00,G_vintez,,"Ty, I will look into it",576pep,t1_d8pxj21,UKCSTeacher,,Reply,1,0,1
5759dp,2016-10-12 13:18:47-04:00,bootrec,Performance testing a Computer Science practice?,"I need to better understand... if someone has a bachelor of computer science, are they expected to know how to do a performance testing? Apparently it is a subset of performance engineering, but I do not know if this is something that anyone graduating with a B.Sc.- Applied Mathematics In Computer Science would be expected to know how to do. If anyone has a valuable insight it would be appreciated, thanks.",,,,,Submission,3,0,3
d8p94sg,2016-10-12 15:54:36-04:00,KronktheKronk,,"I don't expect that people with bachelor's degrees in computer science with no experience to be able to do anything.

When you start a job, you'll get trained to do a massive portion of the work you're going to be asked to do.  You aught to be able to talk about the idea of performance testing, but I don't think you should be expected to understand the practice.",5759dp,t3_5759dp,bootrec,,Comment,2,0,2
d8pi40u,2016-10-12 19:10:43-04:00,Muirbequ,,"It's more on the computer systems/software engineering side, so unless you are heavy in theory, it's not unreasonable to expect someone to know how their program runs on hardware. Not saying you have to be good at performance engineering, but you should know the process in a general way or be able to derive something from intuition. You can't write good code unless you can tell what is better code.",5759dp,t3_5759dp,bootrec,,Comment,1,0,1
d8pkhb0,2016-10-12 20:07:27-04:00,BonzoESC,,"Performance testing is really important in industry. Computer time costs money, user time costs money, and that frequently tilts the economics towards spending engineer time to make things faster.

In many environments (not just games) performance is extremely important. Twitter spent an unthinkable amount of developer time on moving from a single Rails app talking to MySQL to a microservices architecture because it was cheaper than their anticipated computer expenses. Facebook has compiler engineers working on making their massive codebase faster for the same reason. 

It comes down to the scientific method.

1. Have a goal: can I make bulk-fetching data take less wall-clock time by sending fetch requests in parallel?
2. Build a hypothesis: some number of workers between 1 and 256 will fetch 1000 records the fastest.
3. Build an experiment: given 1000 records, measure how fast fetching all of them takes with 1, 2, 4, 8, 16, 32, 64, 128, or 256 workers. 
4. Interpret results: putting all the numbers in a spreadsheet, I saw that having 64 workers took the least amount of wall-clock time.
5. Conclusion: multi-get works really well, and this is a good feature.

Software's kinda magic because frequently, the experiment you build is really really close to production-ready. GitHub uses a system called ""science"" to run experiments in production with new code, both for correctness and speed. A developer there has a great presentation about it: http://wynnnetherland.com/talks/refactoring-with-science/ and it's open-source (useful if you're working with Rails).",5759dp,t3_5759dp,bootrec,,Comment,1,0,1
573flb,2016-10-12 06:39:26-04:00,BerkeleyCSMajor,What do you guys think of this resume layout?,"I found a tech resume template that I really liked and recreated it.
 
[This is the original template](http://www.jamesmaa.com/wp-content/uploads/2015/09/James-Maa-Tech-Resume.pdf)

(I believe the guy is a software engineer at reddit from his linkedin)

[And this is my rendition](https://drive.google.com/file/d/0B4JZHNajqr-rcjMtNFVaNE1hMDQ/view?usp=sharing)

I personally think it looks much cleaner than most standard resume layouts but I'd like to get some other opinions on it. 

Would it be too risky to apply to internships?",,,,,Submission,0,0,0
d8oo67s,2016-10-12 08:26:46-04:00,UKCSTeacher,,/r/cscareerquestions/ is a more suitable place ,573flb,t3_573flb,BerkeleyCSMajor,,Comment,4,0,4
d8omg1y,2016-10-12 07:16:08-04:00,i_takes,,"You know, I don't frequent this sub much. But I believe this sub is more for answering theoretical computer science questions, not for resume advice. ",573flb,t3_573flb,BerkeleyCSMajor,,Comment,6,0,6
d8ownal,2016-10-12 11:50:47-04:00,VallemK,,Looks pretty good!,573flb,t3_573flb,BerkeleyCSMajor,,Comment,0,0,0
56ygw5,2016-10-11 11:11:43-04:00,braincased,How do you study for CS/programming based tests?,I've switched in to computer science and I'm feeling completely overwhelmed. I've just finished the intro to programming course at my school and I'm in the end of first year/second year courses now and I have no idea how to actually study. I barely finish the weekly assignments (though everyone else seems to have a much easier time) and when I try and look it over I don't know what else I could possibly do to make it better. Our exam is coming up and I don't know how to actually review and practice what we've done. Any help?,,,,,Submission,9,0,9
d8nef4e,2016-10-11 11:19:40-04:00,greeniguana6,,"Depends on the class. For my programming classes, I usually study by making my own program and implementing the test topic into it (while obviously studying from the book, notes, and quizzes as well).

For example, if you're learning object classes in Java you can make a fun program that utilizes object classes. As a bonus, you can also upload it to Github and get a head start on your portfolio.

Edit: For stuff like discrete structures, good luck. Just try to do practice problems and hope your test isn't a mindfuck",56ygw5,t3_56ygw5,braincased,,Comment,3,0,3
d8nkswf,2016-10-11 13:29:04-04:00,MeditatingLemur,,"Very much depends on your course but the tests are going to be directly related to your course. Either showing you understand class structure, encapsulation, recursion or even just understanding what variables should be used to store various values/strings. Look over your power-points or whatever you have from class and make sure you can implement it in your chosen language ",56ygw5,t3_56ygw5,braincased,,Comment,1,0,1
d8nkrvr,2016-10-11 13:28:30-04:00,dxk3355,,Some groups like frats and clubs keep old tests.  If you can't find those at your college find a practice test from another college with the same material.,56ygw5,t3_56ygw5,braincased,,Comment,1,0,1
56whu6,2016-10-11 01:00:26-04:00,emotaylorswift,"Does placing requirements on a password (such as much contain uppercase, number and special character) make it easier to hack because a bot would know what it's looking for?",,,,,,Submission,0,0,0
d8mz26y,2016-10-11 01:25:28-04:00,enigma_x,,"It would, but no to any significant extent. This means you need not over engineer your password guessing bot to understand these constraints. 

Have you every played hangman or twenty questions? A password guessing bot would work that way. Explaining the constraints would obviously help it, but depending on the length of the passwords, it has to take billions to trillions of guesses anyway. For an algorithm, the special characters mean very little, because they're just another character in the alphabet. 

In short, helping your bot with one or two characters does not make a nontrivial difference in the large scheme of things. ",56whu6,t3_56whu6,emotaylorswift,,Comment,3,0,3
d8mz7tb,2016-10-11 01:30:43-04:00,emotaylorswift,,Cool. Thanks.,56whu6,t1_d8mz26y,enigma_x,,Reply,1,0,1
d8n19vw,2016-10-11 02:46:25-04:00,bchociej,,"I'll add that it would be far worse to restrict a password to a certain specified set of characters, and even worse yet to restrict the length. But in any case, it's best to choose a password scheme where it takes a (relatively) long time (1-2s) to check a password, so that brute force attempts are foiled more readily.",56whu6,t1_d8mz7tb,emotaylorswift,,Reply,2,0,2
56whlw,2016-10-11 00:58:25-04:00,Koyhaku,Why are there variations on software stability across systems,"Lets say we have two PCs running windows 10 with competent users keeping their system's drivers, and software reasonably updated. Both systems have modern hardware. Each person wants to run an application, lets say a game. System A can run the game for long periods, no problem, pretty much never gets a crash. System B crashes randomly, or at a set period of time. Why does behavior like this happen in software development. Why can't I just make a java application which honestly runs everywhere, or at the very least a properly managed system?",,,,,Submission,5,0,5
d8mzab0,2016-10-11 01:33:04-04:00,ase1590,,"two major factors:

1. hardware differences: person A may have an NVidia graphics card which uses one driver while person B uses AMD which has a totally different driver. both may have bugs, just in different spots

2. Hardware defects:  Over time, certain parts may fail in odd ways. Memory is often the most common one, but even a core on the CPU can fail for cycle and cause instability. 


You can make a Java progam that runs everywhere. As soon as you start using things like OpenGL/DirectX/Vulkan/OpenCL etc then you're at mercy of the quality of code written for whatever hardware you're trying to access. ",56whlw,t3_56whlw,Koyhaku,,Comment,4,0,4
d8n0ar6,2016-10-11 02:09:00-04:00,Koyhaku,,"In case 1, wouldn't everyone who has an NVidia card or an AMD card experience the same bug? and for case 2, is hardware failure/malfunction really a huge problem?

It seems like it's really infeasible for someone to build something like a stable game on a lot of systems, yet something like StarCraft was really stable on any computer I played it on. But then you have something like Space Engineers which seems like it will never be stable. I fail to see how someone can build something robust enough to catch errors, like what you stated, and continue to run on most systems. That wasn't really part of my question but I guess that's what I was curious about. Anyway thanks for the answer.",56whlw,t1_d8mzab0,ase1590,,Reply,1,0,1
d8n0jsi,2016-10-11 02:18:36-04:00,ase1590,,"In case #1, no. To make matters more complicated, Nvidia and AMD have multiple video cards that have multiple architectures that each require their own driver. So while one game might work fine on an Nvidia GTX 670 card, it may not work correctly on the GTX 960. The blame for this would rest entirely on Nvidia for releasing broken drivers.

In order for developers to mitigate this, its important to have close relations with the graphics card companies. If you have enough money and status, its also possible to get a dedicated support line to get the engineers to rapidly modify their drivers to find any bugs you have while testing your game on their hardware. 
Indie games often do not have this luxury and will have to simply work around known issues. 

It also largely comes down to the talent of the coders. If the programmers are familiar with doing thorough unit tests on multiple hardware configurations, a lot of the issues can be caught before release. However, this is something that requires a team to pull off effectively.


Example: The GTX 970 card from Nvidia actually had a bit of a design flaw. it was sold as a 4 GB graphics card. However, the last 500 MB of space on it used RAM that had a much slower clock speed, and would perform about 80% slower, should anything fill it up enough to use it. This cause a lot of heavy VRAM using games to have very low frames because of this hardware flaw. A class action lawsuit was later filed because of this. ",56whlw,t1_d8n0ar6,Koyhaku,,Reply,3,0,3
d8myyy7,2016-10-11 01:22:34-04:00,worst,,You are asking why software crashes?,56whlw,t3_56whlw,Koyhaku,,Comment,3,0,3
d8mzm46,2016-10-11 01:43:58-04:00,brennanfee,,"Because it's never just software.  Software and hardware are linked.  If one is bad it can effect the other.

Even if you had two computers, exactly the same equipment, running exactly the same software you could still have issues due to workmanship issues in the components of the two systems.  Maybe the video card on one of them is bad or the hard drive in one of them has some bad sectors that data was written to.  Who knows.",56whlw,t3_56whlw,Koyhaku,,Comment,2,0,2
d8n59e6,2016-10-11 06:33:56-04:00,PM-ME-YO-TITTAYS,,"Generally there are differences between the systems. There are so many variables, that one is bound to be different, be it a piece of hardware, some installed software, how much disk space has been used etc etc.",56whlw,t3_56whlw,Koyhaku,,Comment,1,0,1
56sonf,2016-10-10 12:00:33-04:00,caffeine_depend,How would I use a truth table to show this formula is true?,,,,,,Submission,9,0,9
d8mh0sb,2016-10-10 18:00:35-04:00,RatherPleasent,,"Left side : p and (q or r)

Right side: (p and q) or (p and r)

They're equivalent logically as the end result is either p and r, or p and q. 

Should make a truth table, 0s and 1s, to prove this even so: since that's what you asked. 

Here's a full truth table showing the left hand side of the if and only if, ~~you do the right.~~ I did the whole thing boy.

[Paste](http://pastebin.com/hf6sgsqT)

If you're curious as to how I got the sequence of zeroes and ones, I started at 0 and used binary addition of 1 to each layer. So the end result is 7 in binary and the beginning is 0. 
",56sonf,t3_56sonf,caffeine_depend,,Comment,5,0,5
d8m0elp,2016-10-10 12:17:33-04:00,enigma_x,,"Do you understand what the statement means? Can you translate it to words?

It says ""Left hand side is true if and only if the right hand side is also true.""

You have three variables which means your truth table will have 2^3 rows. Fill each possible value for the variables first. Isolate each individual chunk of the statement and try to arrive at the truth value for that. For instance, (P v Q) is true for all cases where P or Q is true, this is the case irrespective of the value of R. 

If you fill the table like this you should be able to prove the statement. ",56sonf,t3_56sonf,caffeine_depend,,Comment,7,0,7
d8m1maz,2016-10-10 12:42:38-04:00,OffensiveCanadian,,"> truth table will have 3^2 rows.

I think you may have meant 2^3 rows.",56sonf,t1_d8m0elp,enigma_x,,Reply,2,0,2
d8m1ylg,2016-10-10 12:49:32-04:00,enigma_x,,Yes! Thanks for catching that. Fixed it. ,56sonf,t1_d8m1maz,OffensiveCanadian,,Reply,3,0,3
d8mnnyj,2016-10-10 20:39:06-04:00,SayYesToBacon,,2 Raised to 3rd,56sonf,t1_d8m0elp,enigma_x,,Reply,2,0,2
d8m951r,2016-10-10 15:15:01-04:00,camelCaseGuy,,"As said by /u/enigma_x, just fill the table of truth.

Remember this is an _iff_ operator. That means that it holds an _if_ operator for both sides. i.e. **p _iff_ q** means **p _if_ q** and **q _if_ p** both hold true. 

So basically you decompose the _iif_ in two _if_ 's operation, one ""forward"" and one ""backwards""... sort to speak.

EDIT: iif -> iff",56sonf,t3_56sonf,caffeine_depend,,Comment,2,0,2
d8n1ikh,2016-10-11 02:56:18-04:00,okapiposter,,"You mean [**iff**](https://en.m.wikipedia.org/wiki/If_and_only_if), right?",56sonf,t1_d8m951r,camelCaseGuy,,Reply,2,0,2
d8n710u,2016-10-11 07:57:46-04:00,camelCaseGuy,,"Yess. Thank you, Edited.",56sonf,t1_d8n1ikh,okapiposter,,Reply,2,0,2
d8naqh0,2016-10-11 09:54:25-04:00,okapiposter,,Happy cake day by the way. \o/,56sonf,t1_d8n710u,camelCaseGuy,,Reply,2,0,2
d8nartg,2016-10-11 09:55:26-04:00,camelCaseGuy,,Thank you!,56sonf,t1_d8naqh0,okapiposter,,Reply,1,0,1
56q52n,2016-10-09 23:52:54-04:00,universaltoilet,Pep 8 Help,I wanted to know how to use assembly language to create text in a pep 8 simulator. I pulled up the ascii table but I need some guidance on what object code I should be inputting to create words. Thanks,,,,,Submission,0,0,0
56ooaj,2016-10-09 18:29:59-04:00,RZurch,"What is the best way to learn a programming language (books, codecademy, YouTube)?",,,,,,Submission,10,0,10
d8l6303,2016-10-09 20:35:32-04:00,zSilverFox,,"All of the above. Read about some of the basics. Turn to videos for demonstrations and clarification. Then apply all that by *doing* it. Take on challenging projects,  do the exercises in online courses or your books. You truly understand something by applying it and not being afraid of failing. You grow when you try hard things and make failures on the way.

Most importantly of all though, explore different learning resources. Learn what works best for you. Everyone learns differently, but you'll never know your style until you explore and try different things.",56ooaj,t3_56ooaj,RZurch,,Comment,5,0,5
d8l1hcz,2016-10-09 18:42:57-04:00,tyggerjai,,"Writing code. After that, personally, I think books are best, but find what works for you. ",56ooaj,t3_56ooaj,RZurch,,Comment,7,0,7
d8lu2xx,2016-10-10 09:48:38-04:00,BonzoESC,,"Completely agreed, especially if you've got a specific goal in mind. You'll learn more in an hour of trying to make something work well enough to get paid for it than you will in a day of reading or watching videos. ",56ooaj,t1_d8l1hcz,tyggerjai,,Reply,2,0,2
d8mcqtx,2016-10-10 16:29:09-04:00,mehum,,"While this is true, what is the best type of guidance? Anyone can fire up Eclipse, but then what?",56ooaj,t1_d8lu2xx,BonzoESC,,Reply,1,0,1
d8l2qbe,2016-10-09 19:13:42-04:00,AlexDGr8r,,"It really depends on what's easiest for you. What you've listed as examples basically matches up to all the types: 

* Books - Reading (my preferred technique) is perhaps more detailed and you can easily set your own pace and perhaps skip certain parts.
* Codecademy - This is the more interactive approach to learning how to code. Basically, learn by doing.
* YouTube - And then this is the more visual way of learning. If you see someone do it, you can do what they did. Monkey see, monkey do.

So just try them out and see what works best for you.",56ooaj,t3_56ooaj,RZurch,,Comment,4,0,4
d8lm35r,2016-10-10 03:18:54-04:00,meetyouinthewaves,,"It also depends on your level. If this is your first time you should read books first. If you already have some experience then you can find online introduction tutorials (text or video) with working examples you can try yourself, and then read books or the official documentation of the language/framework you're learning. Without the support of books or documentation you end up writing bad code, learning the best practices is important as learning the technology itself.",56ooaj,t3_56ooaj,RZurch,,Comment,2,0,2
d8lofzf,2016-10-10 05:34:19-04:00,Vadoff,,"It depends on your programming experience level and learning style. If you already know the basics, then jumping right into the official reference basics for the programming language can be the fastest way.

For me, to just start coding in that language and looking up anything I don't know is the fastest way.",56ooaj,t3_56ooaj,RZurch,,Comment,1,0,1
d8lu3bj,2016-10-10 09:48:56-04:00,pope4president,,"To program suckily, and keep programming, until you suck less.",56ooaj,t3_56ooaj,RZurch,,Comment,1,0,1
d8mtupx,2016-10-10 23:01:48-04:00,dhyanp11,,one of these posts again.,56ooaj,t3_56ooaj,RZurch,,Comment,1,0,1
d8l2v3f,2016-10-09 19:17:01-04:00,RZurch,,Thanks!,56ooaj,t3_56ooaj,RZurch,,Comment,1,0,1
d8l2vlh,2016-10-09 19:17:22-04:00,RZurch,,Thank u!,56ooaj,t3_56ooaj,RZurch,,Comment,1,0,1
56lroh,2016-10-09 07:08:54-04:00,heavenetica,Why is 4G more effective than standard fibre optic broadband at buying Glastonbury tickets?,"Glastonbury festival tickets were released this morning at 9am, and every year the website appears to crash with most people stuck connecting and never getting a response. The next stage, once it connects, is a landing page that automatically refreshes every 20 seconds, waiting to go through to the main page where it's possible to buy tickets.

The idea is that you buy them in groups of six, so we had six people on different internet connections trying on both normal ADSL / fibre optic broadband (Sky, Virgin Media and University library) and on their mobile phone internet (4G). I was on the university internet with speed test results of 130Mbps download, 120Mbps upload and roughly 15ms ping. Almost everyone, including me, got through to the landing page several times on 4G but didn't get anything on their laptop, and we ended up getting tickets through a 4G connection on a phone. Last year we didn't get through, but I heard about other people that got through on 4G.

Does anyone know why it's much more likely to get through on 4G? I thought that it would be primarily based on internet speed but surely 4G is not faster and more reliable than a fibre optic broadband connection. Are there any other factors that increase chances of getting through?

This is the most relevant subreddit that I could think of, if anyone knows anywhere else that would be more relevant then let me know. Thanks",,,,,Submission,5,0,5
d8kdpoz,2016-10-09 08:49:49-04:00,Syde80,,It's all conjecture... But they likely have load balancers for distributing the load to various servers.  I'll guess that they have different servers for hosting the mobile vs desktop versions of the site.  The load balancers detect you are on mobile and direct you to the mobile servers.  Those servers were probably not under as much load as the desktop ones.,56lroh,t3_56lroh,heavenetica,,Comment,18,0,18
d8ko201,2016-10-09 13:46:41-04:00,-Hegemon-,,"That might make sense if they used a browser from the phone.

The page type is defined by the user agent, not by the IP range.",56lroh,t1_d8kdpoz,Syde80,,Reply,1,0,1
d8kohro,2016-10-09 13:56:01-04:00,Syde80,,"For sure, but OP didn't say they were using a desktop /laptop on 4g connection.  I figured it was a safe assumption they were doing it on a mobile considering they referred to it as phone internet.",56lroh,t1_d8ko201,-Hegemon-,,Reply,2,0,2
d8lmlkk,2016-10-10 03:45:05-04:00,heavenetica,,"Yes that's correct I was using the browser on my phone, but surely the server would only get the user agent from a device after it has made the connection. What I was seeing on my laptop and most of the time on my phone wasn't even making the connection.",56lroh,t1_d8kohro,Syde80,,Reply,1,0,1
d8kfzky,2016-10-09 10:16:05-04:00,ldpreload,,"Lots of possibilities. One is that if most of the people trying to get to the festival's website were on broadband, the connections from the broadband providers may have been overloaded. It's like how you can occasionally get somewhere faster in rush hour by taking city streets instead of the highway: usually you can go much faster on the highway, but if the highway is completely congested, the low-speed roads will be a better choice.

One of the interesting things about mobile internet is that it often does a bunch of weird routing with your traffic while still on the 4G provider's internal network, before it reaches the actual internet (so that you can do things like maintain your IP address while you take the train across the country). So it's entirely possible that your connection first reached the internet in a completely different geographic location. If you and your university mates are in a major city, and the 4G-to-internet connection is somewhere in the countryside, you might be literally approaching the website from different physical directions, and therefore different physical connections.

There's also the possibility that the desktop website was trying to deliver a bigger / more resource-intensive web page, and the servers couldn't keep up with that, so auto-refreshing on mobile was more likely to work than auto-refreshing on desktop.",56lroh,t3_56lroh,heavenetica,,Comment,4,0,4
d8ki3fp,2016-10-09 11:20:15-04:00,heavenetica,,"Thanks for the insight, that makes a lot of sense",56lroh,t1_d8kfzky,ldpreload,,Reply,1,0,1
56ljae,2016-10-09 05:30:12-04:00,BRINGBACKTHEMACHINE,How did/do anti-aircraft missile 'jammers' work?,,,,,,Submission,0,0,0
d8mcg7z,2016-10-10 16:23:01-04:00,dandrino,,"Radar basically works by sending out a radio pulse and timing how long it takes for it to return to calculate the target's range. Jammers work by sending a much stronger radio signal directly at the radar antenna to push the target reflection signal way below the noise floor (think of it like trying to visually track a firefly at night when someone is shining a floodlight in your face). More sophisticated radars take steps to mitigate this (e.g. side lobe cancellation), which means that there are also even more sophisticated radar jammers to counteract those countermeasures. Some very sophisticated jammers will even ""impersonate"" a real signal in order to trick the radar into thinking that no jamming is even occurring at all.",56ljae,t3_56ljae,BRINGBACKTHEMACHINE,,Comment,2,0,2
56icpo,2016-10-08 14:45:22-04:00,saltedcaramel_,What are some of the burning questions in CompSci?,"If someone were to ask you what are the biggest/most important unanswered questions in Computer Science, what would you tell them? ",,,,,Submission,14,0,14
d8jhrt0,2016-10-08 14:50:04-04:00,fladam123,,"Probably the P vs NP problem, you can read about it here : https://en.wikipedia.org/wiki/P_versus_NP_problem

It is also one of the most popularised problems in computer science, so you may have already heard of it.",56icpo,t3_56icpo,saltedcaramel_,,Comment,13,0,13
d8jxob6,2016-10-08 21:34:29-04:00,lordvadr,,"And it's worth adding that quantum computing is an extension of that. Should the NP problem be solvable in essentially constant time, P==NP. But when that conjecture was presented, the idea that instead of polynomial time, it could be effectively constant wasn't even considered (not that constant time isn't polynomial time).",56icpo,t1_d8jhrt0,fladam123,,Reply,-2,0,-2
d8ka839,2016-10-09 05:27:21-04:00,HelleDaryd,,"Note that quantum computing per se does not change it and that there is a new quantum equivilent problem (and I am whiffing on the name of that problem here, I'll look it up later today or tomorrow, it's a lazy Sunday).",56icpo,t1_d8jxob6,lordvadr,,Reply,2,0,2
d8jwiay,2016-10-08 21:03:32-04:00,UncleMeat,,"Not the biggest, but the one I am most interested in.

Software complexity is growing much faster than our ability to understand out software. This is a problem for correctness but it is a HUGE problem for security. Formal methods scale to programs that are millions of times smaller than real-world programs that actually matter. Concolic execution is great but at a fundamental level it is still just testing.

*Is there a better way of understanding the behavior of a computer program beyond testing inputs or using formal proofs of correctness*?",56icpo,t3_56icpo,saltedcaramel_,,Comment,10,0,10
d8jnmvw,2016-10-08 17:15:23-04:00,albatrek,,"P vs NP is the big one, and the answer to it solves a bunch of smaller, more specific, problems. All of these could be called open problems in CS. See Wikipedia's list for more: https://en.wikipedia.org/wiki/List_of_unsolved_problems_in_computer_science

And here are some of the problems that an answer to P vs NP would solve: https://en.wikipedia.org/wiki/List_of_NP-complete_problems (the really cool thing about these is that it goes both ways - solving P vs NP answers these, but solving any one of these would also answer P vs NP!)",56icpo,t3_56icpo,saltedcaramel_,,Comment,8,0,8
d8jxqcb,2016-10-08 21:35:49-04:00,thepobv,,All kinds of ethics questions with AI that are interesting... and many of those have no right answers like philosophy ,56icpo,t3_56icpo,saltedcaramel_,,Comment,4,0,4
d8k9l34,2016-10-09 04:46:40-04:00,adbJ114,,"For example is it even possible to write up a universal code of ethics for AI, as human ethics vary widely from culture to culture and throughout time. ",56icpo,t1_d8jxqcb,thepobv,,Reply,2,0,2
d8l04gb,2016-10-09 18:10:59-04:00,crookedkr,,"As you say though, these are more ethics or philosophy than CS.
",56icpo,t1_d8jxqcb,thepobv,,Reply,1,0,1
d8len1p,2016-10-09 23:19:51-04:00,thepobv,,I believe that comp Sci encompasses many things besides just the technical core... and some of the things cannot be untwined ,56icpo,t1_d8l04gb,crookedkr,,Reply,1,0,1
d8jynt9,2016-10-08 21:59:29-04:00,tbrownaw,,"The ones that are hard to even *ask* coherently, let alone rigorously. The ones about the practical aspect involving people, rather than just the pretty abstract mathematical aspect.

There are a lot of people claiming that functional programming is The Answer. The answer to what? Well, apparently either it makes you more productive, or makes it harder / impossible to write bugs, or something.

I haven't seen this a much lately, but there's been a fair bit of noise about computer science being essentially unteachable, where either you can (and will) learn it easily on your own, or you're missing some critical innate ability or other.

.

So:

* What skills are needed, and what's the best way to teach them? Are any of them like (human) languages, where they're supposedly much harder to learn past a certain age, or are professors mistaking prior experience for ability?
* How do you characterize what problems a language or programming style is good for? What is the *simplest* way that mostly works, to record this and look it up when needed?
* How do you characterize how error-prone a language or programming style is? How do you untangle this from average programmer skill? How do you characterize the learning curve?
* How do you characterize technical debt? A clean codebase makes future development easier, but how much easier and how do you define ""clean""?

Right now, things like this are the subject of arguments and anecdotes, with little in the way of useful studies or even clearly defined aspects *to* study.

Look at the studies that have been attempted about Test-Driven Development. Look at the resulting arguments about them. Under laboratory conditions it doesn't help... but it's obviously good and holy, so the results can be explained away by how real-world conditions differ. It's apparently a way to ""get one over"" on management and deadlines, but any other methods for that or for getting management on board, are quickly poo-poohed and discarded. What's up with all that?",56icpo,t3_56icpo,saltedcaramel_,,Comment,2,0,2
d8k01dx,2016-10-08 22:36:38-04:00,albatrek,,"Big yes to problems that are hard to ask coherently.

I can't count how many times a good definition of a problem was the hardest part of solving it. So many proofs are obvious once you specify a bunch of definitions.",56icpo,t1_d8jynt9,tbrownaw,,Reply,1,0,1
56i1lu,2016-10-08 13:41:44-04:00,fladam123,f(x) = ((x^2)/2) for Z->Q,"Hi, 
We are looking at how functions can be total/surjective/1-1/bijectional right now in my theory of computation module, I had a conversation with my lecturer but the response wasn't really enough to clear up a query I have, I'll do my best to outline it:

Taking the function in the title as an example, for all x that are even we get a value for f(x) in Q that could be reduced, for example if x=4, f(x)=16/2, or 8/1 depending on how you look at it. 

What I'm asking is, would this function be bijectional if we define the set Q as any number ((p/q) were p belongs to Z and q belongs to N), assuming that 16/2 is a distinct rational answer from 8/1? 

If we don't assume 16/2 is distinct from 8/1 here, then does f(x) even map correctly for Z->Q for this function when even elements in the domain can map to several values in the range?",,,,,Submission,3,0,3
d8kcgfn,2016-10-09 07:47:40-04:00,X7123M3-256,,"A function is *surjective* if for every element in the codomain, there is an element in the domain that maps to it. In other words, the preimage of the codomain is the entire domain. A function is *injective* (or 1-1) if no two elements in the domain map to the same thing: i.e f(x)=f(y)=>x=y (it is sometimes easier to show the equivalent statement that  x≠y=>f(x)≠f(y)). A function is bijective if it is both injective and surjective.

This function is not surjective, because -1 is in the codomain, but no input will produce that output (square numbers are always positive). It also isn't injective, because f(-1)=f(1)=0.5. Therefore, it is not bijective either.

> assuming that 16/2 is a distinct rational answer from 8/1? 

If you are talking about the set of rational numbers, these are not distinct elements, they are two different representations of the same thing. Rational numbers can be defined as [equivalence classes](https://en.wikipedia.org/wiki/Rational_number#Formal_construction) of the set of ordered pairs of integers. But it isn't necessary to think of them as such - just note that if you cancel the 2 from 16/2, you get 8/1.

> then does f(x) even map correctly for Z->Q for this function when even elements in the domain can map to several values in the range?

Can you give an example of where that would be the case? The even number 4, for example, maps to 4^2 /2=8 which is a well-defined value. If you take any integer, square it, and divide it by two, you're going to get a rational number out.

An example of a ""function"" that isn't well defined would be something like f:Q->Z given by f(p/q)=p+q, because then 2/1 and 4/2, which represent the same number, would give different results under this mapping. If we were talking about the set Z×Z, where the pair (2,1) and (4,2) are distinguished, then this would be a function (indeed, it is just addition on the integers).",56i1lu,t3_56i1lu,fladam123,,Comment,2,0,2
56g1d1,2016-10-08 03:45:23-04:00,throwfaraway2310,Big data vs machine learning course?,"If I can only take one of these two courses, which would be more useful for a future software developer? How would these classes differ? Would machine learning be more theoretical while big data more practical?",,,,,Submission,4,0,4
d8j3zm4,2016-10-08 08:05:11-04:00,kittttttens,,"this really depends on the school. you should try to talk to other people in your program who have taken the classes, since they could vary a lot between schools.

FWIW, i took a graduate-level big data class a few years ago. it was mostly a discussion-based class, where people would present on their research or we would read papers and talk about them. it was interesting, but not terribly useful in the long run.

my machine learning class was much more technical (the usual programming/theory assignments, exams, etc) and the workload was a lot more, but i would say in the long run it's been more useful.",56g1d1,t3_56g1d1,throwfaraway2310,,Comment,2,0,2
d8j5fsw,2016-10-08 09:10:39-04:00,pewpsewp,,"Interesting, my experience was about the opposite haha. My big data class was extremely practical for modern data use cases I've seen in industry so far and we had assignments implementing things in hadoop, storm, and spark which are still very heavily used today. 

Machine learning class was basically a deep dive into svm which while interesting is not extremely useful with all the ml libraries available. 

I now work in data engineering using mainly hadoop and spark implementing some machine learning and the big data class was more useful to me. I would agree in getting the syllabus for both and thinking which you are more likely to use professionally. 

e: I took both these classes in 2015. ",56g1d1,t1_d8j3zm4,kittttttens,,Reply,2,0,2
56dfd3,2016-10-07 16:18:13-04:00,tedster,"Is it possible to keep track of used ""tickets"" without storing them all.","I'm trying to implement a way to keep track of used tickets. A ticket can be any kind of value (in my case it's a UUID).

The obvious solution would be to simply store all the tickets, check against that set whenever I want to know if a ticket is used.
The problem is that I'm going to create a lot of tickets. I'd like to know the most space efficient way to store them.

My gut tells me that it should be possible to create some kind of checksum, given a set of tickets, with the ability to tell me if a given value is in the set.

Is this possible? Or is there any other way to efficiently (space wise) answer the question ""Is this ticket already used"" without storing them all?

EDIT:
Some more context,

I'm trying to keep track of commands sent to my component. So each client sends a ticket (UUID) with the command. If something goes wrong, and the client don't know if the command reached the component it can resend the command with the same ticket, if the server has already processed the command with the given ticket it should ignore it (trying to get one step closer to ""only once delivery"").
",,,,,Submission,13,0,13
d8idhji,2016-10-07 16:32:18-04:00,reisub_de,,"You might want to take a look at a Bloom Filter:

https://en.wikipedia.org/wiki/Bloom_filter

It allows you to distinguish between ""maybe in the set"" and ""definitely not in the set"" ",56dfd3,t3_56dfd3,tedster,,Comment,12,0,12
d8iihb5,2016-10-07 18:28:04-04:00,veeberz,,"I never heard of this! I'm intrigued. I'm wondering how this could be used in OP's case, since you can't remove elements from the set. And there's a possibility of a false positive.

If you maintain a set of used tickets this way, you can see if a given ticket is definitely not used or maybe used. On the other hand, you don't want to maintain a set of unused tickets this way since you can't remove an element from the set. So then OP would have to maintain a set of used tickets, and check a ticket against that. Then in the case of a false positive, a decision would have to be made whether to treat it as used or unused. Not ideal, especially since the probability of a false positive increases with each new element added to the set.

How would you address that? Really curious, and I appreciate your post! I love these clever solutions",56dfd3,t1_d8idhji,reisub_de,,Reply,3,0,3
d8iiy6u,2016-10-07 18:39:37-04:00,ncsucodemonkey,,"You usually use a BF as a gate to a more expensive check, like on secondary storage. It's useful for times when the answer to a *contains* query will most likely be 'no'. 

For example, I used once used one to guard against a bad actor in another system I didn't own. It was trying to determine if a value was a valid key in our system, but most of the time it was asking about trash strings that weren't valid. So I was able to load our thousands of keys into a compact BF and save a DB query for 99% of invalid keys. Got a nice performance boost for a little bit of memory.",56dfd3,t1_d8iihb5,veeberz,,Reply,7,0,7
d8iiyaq,2016-10-07 18:39:41-04:00,Natanael_L,,"https://en.wikipedia.org/wiki/Merkle_tree

> In cryptography and computer science, a hash tree or Merkle tree is a tree in which every non-leaf node is labelled with the hash of the labels or values (in case of leaves) of its child nodes. Hash trees allow efficient and secure verification of the contents of large data structures. Hash trees are a generalization of hash lists and hash chains.

> Demonstrating that a leaf node is a part of the given hash tree requires processing an amount of data proportional to the logarithm of the number of nodes of the tree: this contrasts with hash lists, where the amount is proportional to the number of nodes

Edit: although this is what OP really wants: https://tools.ietf.org/html/draft-hallambaker-compressedcrlset-00",56dfd3,t1_d8iihb5,veeberz,,Reply,2,0,2
d8j4579,2016-10-08 08:13:14-04:00,tedster,,"I think this could be a really fitting way to solve my problem.
I should have provided some more context..

I'm trying to keep track of commands sent to my component. So each client sends a ticket (UUID) with the command. If something goes wrong, and the client don't know if the command reached the component it can resend the command with the same ticket, if the server has already processed the command with the given ticket it should ignore it (trying to get one step closer to ""only once delivery"").

The bloom filter is perfect, because it could be a sliding window (only the latest commands needs to be taken into account, the older ones are most probably fine)

Will update my original question with this..",56dfd3,t1_d8iihb5,veeberz,,Reply,2,0,2
d8j1t2r,2016-10-08 05:48:20-04:00,tedster,,Look like this is what I'm after. Thank you! Will look into it,56dfd3,t1_d8idhji,reisub_de,,Reply,1,0,1
d8iivbf,2016-10-07 18:37:36-04:00,mrmnder,,"Bloom filters are a good start, but how much is a lot here? A million GUIDs only take 16MB.

I would suggest a [Trie](https://en.wikipedia.org/wiki/Trie), but with GUIDs it wouldn't be very effective because of the by design randomness of the GUIDs.",56dfd3,t3_56dfd3,tedster,,Comment,5,0,5
d8ifd79,2016-10-07 17:14:17-04:00,nemec,,"Is it more efficient to instead track *unused* tickets? Then any valid ticket that's *not* unused has obviously been used. The list of used tickets is going to grow forever, while your unused ticket list will probably fluctuate depending on usage.

Combine that with two additional measures:

* Ticket expiration - if your unused tickets expire after some time, you can continually purge the ""unused"" table of dead tickets.
* Cryptographically signed tickets - use something like a JWT that allows you to securely validate that *you* created the ticket (rather than a completely fabricated ticket) and you can distinguish between ""used"" and ""fake"" tickets since neither will be in your DB.",56dfd3,t3_56dfd3,tedster,,Comment,3,0,3
d8j1s3p,2016-10-08 05:46:33-04:00,tedster,,"In my case the unused tickets are all possible UUID:s, it's quite the list..",56dfd3,t1_d8ifd79,nemec,,Reply,1,0,1
d8ij2r0,2016-10-07 18:42:53-04:00,Natanael_L,,http://hackingdistributed.com/2014/05/16/macaroons-are-better-than-cookies/,56dfd3,t1_d8ifd79,nemec,,Reply,1,0,1
d8ir78d,2016-10-07 22:25:37-04:00,Rangsk,,"Just use a database. Make sure you index the UUID column for fast lookup. That's what they're made to do.

Are you sure you are generating so many UUIDs that you can't just store them? If you generate 1000 UUIDs per second, 24 hours a day for a year, that's less than 500GB, which won't even fill a cheap harddrive.",56dfd3,t3_56dfd3,tedster,,Comment,3,0,3
d8j1sqz,2016-10-08 05:47:45-04:00,tedster,,The point is to get rid of the database and keep it in memory instead. Not because of size but because of the extra transaction to the database.,56dfd3,t1_d8ir78d,Rangsk,,Reply,1,0,1
d8j9dg8,2016-10-08 11:19:43-04:00,Rangsk,,"I've read your edit. This kind of problem was solved ages ago with the TCP algorithm. There's also lots of research on how to efficiently implement reliable UDP. As long as you have a 2-way communication, these same methods should apply to you.

A simple implementation: Assign an incrementing value as a packet ID. Every message sent to the other end contains in the header the largest consecutive ID received from them (if 1, 2, and 4 have been received, then the header would have 2, since 3 has not been received yet). You only need to store duplicates of messages which are larger than the latest ID received on the other end, and resend them after some time if the other end hasn't reported them as received. You also want a ""heartbeat"" message that you can send at some interval in case nothing gets sent in a while (so the other end remains up to date with your latest received ID).

An alternative even simpler implementation, based on TCP: after every packet received, reply with a simple ACK packet with the ID of the packet. Store and resend unreceived packets periodically until you get an ACK.

In all these cases, using an incrementing ID also has the benefit of allowing you to strictly order their arrival. If you receive 1, 2, then 4, hold onto and don't execute 4 until you receive and execute 3. It also allows for easy storage and lookup in a simple array structure. Have one ""base ID"" integer which is the highest consecutive ID already processed, and then use a ring buffer structure to store the needed packets. Finding where to store or lookup a packet is then just:

    packet_index = (packet_id - base_id + ring_start) % ring_size:

Feel free to search for the various ways TCP and reliable UDP is implemented for more information.",56dfd3,t1_d8j1sqz,tedster,,Reply,1,0,1
d8jiq4n,2016-10-08 15:13:39-04:00,tedster,,"My component handles the messages asynchronously, so there's no order, only commands that are sent and possibly received.
Solving the ""once and only once delivery""-problem is, as far as I know, impossible. 

What I'm trying to do here is to tighten the gap so the risk of sending a message twice is reduced.",56dfd3,t1_d8j9dg8,Rangsk,,Reply,1,0,1
d8ikoyb,2016-10-07 19:24:03-04:00,Natanael_L,,"Look at this

https://tools.ietf.org/html/draft-hallambaker-compressedcrlset-00",56dfd3,t3_56dfd3,tedster,,Comment,1,0,1
d8j1ttg,2016-10-08 05:49:43-04:00,tedster,,Look very interesting! Do you know of any implementations?,56dfd3,t1_d8ikoyb,Natanael_L,,Reply,1,0,1
d8j2cjv,2016-10-08 06:24:49-04:00,Natanael_L,,"Not yet, but I could try to look for it.

Edit: by the way, what do you think of using something like this to generate your tickets? 

http://hackingdistributed.com/2014/05/16/macaroons-are-better-than-cookies/

You can securely encode a bunch of metadata to prove the age and permissions and validity of a ticket. Then you can use a Merkle tree hash to store used tickets. 

Then you just cycle through secret keys occasionally when all old tickets have expired, and can drop those old Merkle tree hashes. ",56dfd3,t1_d8j1ttg,tedster,,Reply,1,0,1
d8iu6gv,2016-10-07 23:51:50-04:00,veritasserum,,"If you want 100% certainty, I don't think so.  The only way to bound such a set (AFAIK) is to limit how far back in history you need to keep track.

A fairly efficient way to organize a list of such things would be hash chains.",56dfd3,t3_56dfd3,tedster,,Comment,1,0,1
d8j1vht,2016-10-08 05:52:56-04:00,NullEgo,,"Depends a lot on your exact use case.  What will go wrong if you do generate the same ID?

However, you could just use UUID and assume it won't collide.  Whatever you write is going to be obsolete and gone before there is a decent chance of a UUID collision.  Figure out what you'll do in case it happens, but you should be more worried about your hair spontaneously catching on fire.",56dfd3,t3_56dfd3,tedster,,Comment,1,0,1
56b1h1,2016-10-07 08:13:20-04:00,cybergla,What is happening exactly when my computer does not recognise a USB flash drive or hard disk?,"I've observed this happen on both Windows and OSX systems. My computer will usually just not recognise that the drive is there even though it's plugged in and the light is blinking. I can't see it in any disk utility software. Usually a reboot fixes the problem, but I'm not sure whats happening here. Is it a problem with the OS or the drive itself? Can someone give a technical explanation of why this happens?

I think it usually happens when I forget to ""Safely remove hardware"" or eject it, but thats only conjecture.",,,,,Submission,3,0,3
d8hsjfk,2016-10-07 08:51:30-04:00,andybmcc,,"The ""safely remove hardware"" feature is just to ensure that buffers are flushed for writes.  That has nothing to do with recognizing the device, unless not flushing the buffers actually puts the device in a bad state.

The light blinking on your device probably just indicates that it has power.  That doesn't necessarily mean that it is properly communicating with your computer.  If it fails to communicate vendor, product identification, etc, your computer doesn't know what drivers to associate with it, so it won't work.",56b1h1,t3_56b1h1,cybergla,,Comment,2,0,2
56aq3l,2016-10-07 06:35:28-04:00,kralkop,Help with choosing MAJOR!,"Greetings, I want to become software developer. I live in Turkey and Turkey does not have Computer Science bachelor education except 1 money greedy bad/mediocre university. We have Computer Engineering education too common and a few software engineering which are bad copy of Computer Engineering. What course should I follow, I can do masters after bachelor.I can even do PhD if it's worth it, so please guide me.",,,,,Submission,3,0,3
d8hsma3,2016-10-07 08:54:04-04:00,andybmcc,,"The difference between Computer Science and Computer Engineering is blurred at a lot of universities.  In general, CS is more math and theory, while CEG is focused on application.  They are both valid paths for a software developer and have a lot of overlap.",56aq3l,t3_56aq3l,kralkop,,Comment,1,0,1
d8i6yxd,2016-10-07 14:16:26-04:00,litepotion,,I agree. You can even toss in electrical engineer in the bunch. Yes we do know hardware but we do software as well both low level and high level programming.,56aq3l,t1_d8hsma3,andybmcc,,Reply,1,0,1
569zeb,2016-10-07 02:02:20-04:00,squirreltalk,Is it possible to check logical equivalence between complex propositions that use different atomic propositions?,"We can easily check that  P -> Q is equivalent to Q v ~P. But, can we show that P -> Q is equivalent to R v ~S, in any sense? I ask because next semester I will teach some undergrads logic, and ask them to translate natural language into statement logic, and of course I don't really care what variables they choose for their propositional atoms.

This seems like the kind of thing that might be relatively easy for a human to spot (when I graded by hand last semester, I think I could), but I'm having difficulty thinking of how to program it. It doesn't seem as straightforward as ""move left-right across both formulas, and change one formula's atoms to match the other formula's."" Because that wouldn't work with the above example: P -> Q is NOT equivalent to P v ~Q.....

Thoughts?",,,,,Submission,6,0,6
d8hw7mi,2016-10-07 10:30:08-04:00,respeckKnuckles,,"So let's say you have a complex propositional calculus formula f1 which contains atoms a1, a2, ..., am, and another formula f2 with atoms b1, ..., bn. The question you're asking is: Is there some equivalence mapping from the atoms of f1 to those of f2 such that f1 <-> f2?

You can create a formula e, that is a disjunction of every possible mapping between the atoms ai and bj:

e = [(a1 <-> b1) ^ (a2 <-> b2) ^ ...] v [(a1 <-> b2) ^ (a2 <-> b3) ^ ...] v ...

and then your task becomes to show e 
^ (f1 <-> f2). However, typically for even somewhat small values of n and m, the formula e will be HUGE.",569zeb,t3_569zeb,squirreltalk,,Comment,2,0,2
d8ieuq9,2016-10-07 17:02:14-04:00,squirreltalk,,"Interesting. I think I understand this idea. Thanks for the pointer.

Now to introspect and consider whether the implementation time of this is worth the time savings of grading. =)

EDIT: Actually /u/respeckKnuckles, what do you mean 'show e ^ (f1 <-> f2)'? Do you mean show that that conjunction is satisfiable, or prove it from a set of premises?

EDIT: Nevermind, I did something inspired by your suggestion but a little different. I constructed a dictionary of all the possible mappings between the atoms ai and bj, and then, for each mapping, replaced the atoms in one formula so it used the same atoms as the other formula, and then checked for equivalence between those two. This worked for a few tests.",569zeb,t1_d8hw7mi,respeckKnuckles,,Reply,1,0,1
d8hm637,2016-10-07 03:27:13-04:00,dandrino,,If you could match the corresponding symbols between the two statements couldn't you do this via a truth table?,569zeb,t3_569zeb,squirreltalk,,Comment,1,0,1
d8hmgrq,2016-10-07 03:43:11-04:00,squirreltalk,,"But how would I match the symbols up? Not sure how that'd work.....

If it's something like p>q versus r>s, that's easy enough, but see my example in my op....",569zeb,t1_d8hm637,dandrino,,Reply,1,0,1
d8hnnok,2016-10-07 04:52:26-04:00,matt_bishop,,"Yes, it is possible, but you don't want to program it from scratch. It's called formal equivalence checking, and it's a great application of CNF SAT. (Which, by the way, is NP-Complete.) I think it's kind of fun (though challenging), and depending on how easy you find it to axiomatize a problem into F.O. or existential S.O. logic, it could be almost as much work as just marking by hand.

There's a free tool called MiniZinc that is helpful to quickly express the problem in a way your computer can solve it. If you're up for more of a challenge, you can probably get better performance from your solver by putting your formula into a ground expression yourself.",569zeb,t3_569zeb,squirreltalk,,Comment,1,0,1
567gz2,2016-10-06 16:16:01-04:00,moonsfang16,Finding google data and inputting it in a spreadsheet,"I'm working on a project where I have to find a large amount business names and addresses, then verify, then input them into an excel spreadsheet. Is there any way to simplify this process? I have a small amount of programming knowledge (I haven't actually built anything just yet).

In case it helps to break it down the steps are:
* Search for business via google
* Find business name and address
* Input business name
* Check address with what's already in the spreadsheet
* Input address if different",,,,,Submission,5,0,5
5630b7,2016-10-05 21:28:12-04:00,universaltoilet,Need help with logic gate homework,,,,,,Submission,0,0,0
d8fvlen,2016-10-05 21:39:10-04:00,newdingodog,,What do you need help with?,5630b7,t3_5630b7,universaltoilet,,Comment,2,0,2
d8fvs2s,2016-10-05 21:43:54-04:00,tyggerjai,,"What have you tried, where are you stuck?",5630b7,t3_5630b7,universaltoilet,,Comment,1,0,1
d8fws8j,2016-10-05 22:09:26-04:00,universaltoilet,,"I'm stuck on c and d from number 1. And all of number 2, sadly.

This is what I have so far for problem 1.

http://imgur.com/a/yVYQX

Edit: For 2a my answer is: (AxB) OR (A+B), 2b I got: A' ⊕ (AxB), 2c I got: A' x (A⊕B) and I probably will need some input on 2d. ",5630b7,t3_5630b7,universaltoilet,,Comment,1,0,1
d8go1ox,2016-10-06 13:06:50-04:00,GangstaEater,,"Idk. But im taking digital electronics at mt school which is kinda like that. But for 1a, it says (A+B)*(B+C). I saw you put 3 AND gates. A+B means OR as I've learned it. So instead for your first 2 gates, use OR gates. Then i guess for the tables, try working thru your circuit with 1's and 0's 1 being High and 0 being low.
I'm taking Digital Electronics in college too. I kinda like it. :D",5630b7,t1_d8fws8j,universaltoilet,,Reply,2,0,2
562poa,2016-10-05 20:19:42-04:00,litepotion,"Using library says: Identifier ""bool"" is undefined?",,,,,,Submission,0,0,0
d8fsun2,2016-10-05 20:31:09-04:00,icehaunter,,"Bool type exists in C99 but not in C89/90. Looks like your IDE is set up with an older compiler standard. Considering using another one, checking build flags or dig towards stdbool.h
Generally speaking, you can define your own bool, of course. 

For reference: http://stackoverflow.com/questions/1608318/is-bool-a-native-c-type",562poa,t3_562poa,litepotion,,Comment,7,0,7
d8ftdey,2016-10-05 20:44:02-04:00,litepotion,,Oh okay that makes sense. So I added the definition to the header and it works fine. ,562poa,t1_d8fsun2,icehaunter,,Reply,1,0,1
d8fsf4i,2016-10-05 20:20:30-04:00,litepotion,,For reference im trying to use a library made for the MPU5060 for an stm32 shown from here: https://github.com/Harinadha/STM32_MPU6050lib. I know its something simple but Im not entirely sure why I have these errors.,562poa,t3_562poa,litepotion,,Comment,1,0,1
562jx7,2016-10-05 19:43:23-04:00,juandanielgr,I have a question about how to build a back-end Twitter-fed data analytics platform,"I want to build a platform that will be able to filter, extract, organize, and eventually analyze data derived from key words, hashtags, and specific user-addressed messages/Tweets on Twitter. -

More specifically the data will concern complaints/denunciations by civil society about the problems that affect them (insecurity, infrastructure, corruption,etc) to create 'smarter' and more 'efficient' democracies by helping public policymakers have the best available data to do some actual good when making decisions.

The database for this platform would ideally also have geo-maapping/referencing features to filter data by region. How this might be best extracted from Twitter, I do not know.

Long story short, I have completed market studies and interviewed private/public sector contacts that have convinced me that there is value for this product both in terms of supply and demand.

I am an economics major, and inasmuch as I understand markets and behaviors, I do not know how to build platforms, databases, coding, and/or how to make my desired algorithms come to life by giving me the insights I want.

Essentially, what are the skills/background needed to develop what I have in mind?


Your advice is greatly appreciated, and it is my hope that this project comes to life!

Hopefully we will be able to maintain a thread/discussion about the platform and have interesting ideas exchanged.

Thank you,
",,,,,Submission,3,0,3
d8fzel3,2016-10-05 23:13:35-04:00,sanimalp,,"Gnip already did that and then got bought by twitter. You are too late. You can search ""social analytics"" to find all the other companies also doing it. 

The spring framework has a tutorial on consuming the twitter fire hose. If you know how to use a git repository and have api keys for twitter, you can do it too! ",562jx7,t3_562jx7,juandanielgr,,Comment,2,0,2
d8gavll,2016-10-06 07:41:43-04:00,juandanielgr,,"Thank you for your input. I have been checking out Gnip and will definitely check out social analytics, spring frameworks, twitter hose, APIs, and git repositories.

Our business model will be more focused on a narrow subset of tweets, though, and will distinguish itself by focusing on political/civil society undertones found in 3rd world countries that do not speak English and thus require a good understanding of the local language, problems, trends, (basically the environment).

Essentially I mention twitter because the initial idea was to develop both front-end and back-end, however I was thinking that using twitter as the front-end instead would prove an advantage in terms of saving design/building costs and would avoid the problem of potential users being reluctant to use a platform/application they have never seen before and that has zero track record.",562jx7,t1_d8fzel3,sanimalp,,Reply,0,0,0
d8ggr6q,2016-10-06 10:35:03-04:00,nighthawk648,,People in a third world country don't have time or resources to tweet about their issues. The ones tweeting from those areas will generally be well off,562jx7,t1_d8gavll,juandanielgr,,Reply,0,0,0
d8gtp5e,2016-10-06 15:00:44-04:00,juandanielgr,,"That is where you are wrong. 

Latin America has surprisingly high tweeting rates, check Cuba and Venezuela for example, where people are only able to access objective information via these networks in the face of total media hegemony enforced by the state.",562jx7,t1_d8ggr6q,nighthawk648,,Reply,1,0,1
d8fuvz5,2016-10-05 21:21:18-04:00,umib0zu,,"> I have completed market studies and interviewed private/public sector contacts that have convinced me that there is value for this product (... but) I do not know how to build platforms, databases, coding, and/or how to make my desired algorithms come to life by giving me the insights I want.

How did you do market research for a product that you can't access the feasibility of? I can do market research and find people in a desert will pay top dollar for water, but if it will cost $18B to build a water distribution system from the nearest water source and I don't take it into account, my analysis is flawed. 

[Learn to code first](https://www.coursera.org/specializations/scala) before you embark on a grand scheme that's not feasible.",562jx7,t3_562jx7,juandanielgr,,Comment,1,0,1
d8gazjz,2016-10-06 07:46:25-04:00,juandanielgr,,"Thank you for your input. My market study was conducted by evaluating different individuals across regions/socio-economic strata to assess their fulfilled or unfulfilled desires/expectations when denouncing problems that affect them. I have strong evidence that civil society does not believe their problems are being adequately reviewed or solved by policymakers, therefore there is an strong willingness to become part of a platform/movement that will enable publicly elected officials to become better at their job.

Public sector also is interested in having tools that will essentially have data collected voluntarily from their constituents (i.e. a free market solution) so that they can better understand what problems should be addressed and thus maximize their political capital and have a better shot of winning elections come next round.

However, what I do NOT have, is a business model, as I still do not know what is the best way to build my platform and therefore I have no idea of operating costs, potential revenues, etc. This is what I am trying to figure out right now.

As mentioned before, our platform will be more focused on a narrow subset of tweets, though, and will distinguish itself by focusing on political/civil society undertones found in 3rd world countries that do not speak English and thus require a good understanding of the local language, problems, trends, (basically the environment).

Essentially I mention twitter because the initial idea was to develop both front-end and back-end, however I was thinking that using twitter as the front-end instead would prove an advantage in terms of saving design/building costs and would avoid the problem of potential users being reluctant to use a platform/application they have never seen before and that has zero track record.

I hope this is clarifying and look forward to continuing the conversation with you.",562jx7,t1_d8fuvz5,umib0zu,,Reply,0,0,0
d8gqb3q,2016-10-06 13:52:12-04:00,umib0zu,,"> what I do NOT have, is a business model, as I still do not know what is the best way to build my platform and therefore I have no idea of operating costs, potential revenues, etc.

You're not getting my point, but you're saying what I'm saying. I'm saying your market research is step 1 of 5 of a real business plan. People aren't going to care about your research unless you have some sort of background in the type of product you're building. Rather than explain what others are saying to you, that it's already been done before with the GNIP project, I want to address the main issue: that you don't know about the necessary technology to access whether your project is feasible.

Which brings me to my point, again. Learn to code! If your project is really awesome and you're really motivated, maybe you'll make it happen. At worst, you just get hired by a company to work on their project.",562jx7,t1_d8gazjz,juandanielgr,,Reply,2,0,2
d8gtsvr,2016-10-06 15:02:49-04:00,juandanielgr,,"You mention fair points, internet stranger.

Thank you for your time replying to my post, and I shall take your advice at full value.",562jx7,t1_d8gqb3q,umib0zu,,Reply,1,0,1
560ult,2016-10-05 14:04:00-04:00,SkyewardSword,Which program lifecycle phase is occuring when I invoke a program from the command line along with its command-line options?,"Say I had a python script named foo.py, which had the following switches / flags / command-line options / whichever you want to call them:

`--version`: State foo.py's version number

`--verbose`: Run the script in verbose mode

`--silent`: Run the script in silent mode

`--abort-after=X`: Abort program run after X seconds

When I'm invoking foo.py from the command line, i.e. opening the terminal in its directory and typing `python foo.py --verbose`, but before I press enter to start runtime, which program lifecycle phase is occuring?",,,,,Submission,5,0,5
d8fgj6m,2016-10-05 15:49:32-04:00,MikeBenza,,"This sounds like you're asking a homework question, but I think maybe you're misunderstanding something because the program hasn't started until you press enter.

When you do, the OS finds the path to the python executable, loads that into memory, looks up what else it depends on and loads those things in to memory.

Once everything is loaded, it starts executing the python executable.  That will read the command lines, load `foo.py`, and parse / compile it.  Once it's compiled it will start to execute the code (in `foo.py`) that it just compiled.  

Hopefully that'll guide you down the right path to either figure it out, or ask a more specific question.",560ult,t3_560ult,SkyewardSword,,Comment,2,0,2
d8gl790,2016-10-06 12:09:12-04:00,SkyewardSword,,"I understand that the program hasn't started before pressing enter. There are more program lifecycle phases then just installation time, runtime, link time, and load time, however. There's also edit time, compile time, and distribution time, which are totally separate from when the program starts.

This question was based on my own research so I could have misunderstood what I found. I'm doing a computer science project and I'm currently drawing up the specification for the design of my ""software"" (it's crude software but still software). Since it's going to be a command-line program with a single function, but will include different switches to customise how it performs that function, I wanted a convenient way to refer to the segment of time that I specfied in the question. I suppose I could write up my own definition for ""invoke-time"" to refer to in the project, but I would prefer to use an actual, already-established definition if one exists.",560ult,t1_d8fgj6m,MikeBenza,,Reply,1,0,1
d8fc1im,2016-10-05 14:18:26-04:00,cowmandude,,I guess link time(https://en.wikipedia.org/wiki/Link_time)? In some ways you're linking the console to the executable by typing the command.,560ult,t3_560ult,SkyewardSword,,Comment,0,0,0
d8fg4p2,2016-10-05 15:41:21-04:00,MikeBenza,,"That is not what linking is.  Linking refers to connecting (in a loose sense) function calls from calls in one code module to the appropriate code in another module, then packaging that up into a single executable file.",560ult,t1_d8fc1im,cowmandude,,Reply,1,0,1
d8fp4j6,2016-10-05 18:57:51-04:00,cowmandude,,I know what linking is. What part of the life cycle do you think its in? It's certainly after installation time but before load time. I suppose you could just say this is happening outside of the software life cycle.,560ult,t1_d8fg4p2,MikeBenza,,Reply,1,0,1
5609yy,2016-10-05 12:16:29-04:00,Tebbathy,Reccomendations for a CS Msc January intake. UK,Does anyone know any unis that will accept me this late? Ive heard some do january intake. Also considering studying elsewhere in europe.,,,,,Submission,1,0,1
d8ft4nw,2016-10-05 20:37:55-04:00,ViSoRoX,,Newcastle is great and accepted me super late,5609yy,t3_5609yy,Tebbathy,,Comment,2,0,2
d8ft7qo,2016-10-05 20:40:01-04:00,Tebbathy,,thanks ,5609yy,t1_d8ft4nw,ViSoRoX,,Reply,1,0,1
d8fkq46,2016-10-05 17:16:40-04:00,UKCSTeacher,,"Very rare for unis to offer January starts in the UK, particularly for a masters. Are we talking UG or PG masters though? 
",5609yy,t3_5609yy,Tebbathy,,Comment,1,0,1
d8flvg2,2016-10-05 17:41:36-04:00,Tebbathy,,yeah. post grad. i read somewhere that they were starting jan intake more to attract internationals as its more common for them. But ive yet to see any.,5609yy,t1_d8fkq46,UKCSTeacher,,Reply,1,0,1
55zjr7,2016-10-05 09:50:24-04:00,2xga,Question about barrier primitive (OS),"Hey guys! I'm taking an Operating Systems class and I stumbled into something that I can't figure out...

This is a pseudo-code for a barrier primitive:

    # rendezvous
    
    mutex.wait()
      count = count + 1
    mutex.signal()
    
    if count == n: barrier.signal()
    barrier.wait()
    
    # critical point

I know that this is not a correct solution, since it can create a deadlock.
But the book asks about one specific case where all the threads can go through the barrier...
Can anyone tell me how this can happen?

Thanks in advance for your help!
",,,,,Submission,1,0,1
d8fc5nb,2016-10-05 14:20:40-04:00,dxk3355,,"I'm pretty rusty on this, but I'm guessing it has to do with multiiple cores or CPUs.",55zjr7,t3_55zjr7,2xga,,Comment,2,0,2
55zbii,2016-10-05 08:56:45-04:00,UmmmmmNick,Help on switching from Java to C,I am a first year college student and I took AP Computer Science in high school. Before college started I tested out of the intro class. However next semester I will be in a higher level class and I don't know C because I learned Java in high school so I was wondering if anyone has any websites or videos that can help me learn the basics and differences of C and Java,,,,,Submission,6,0,6
d8f71r5,2016-10-05 12:37:34-04:00,dxk3355,,I'd forget Java while learning C or any new language for that matter. Don't trying a learn the differences in languages like you asked. Start from the basics when learning a language with Hello World and work your way up.,55zbii,t3_55zbii,UmmmmmNick,,Comment,8,0,8
d8ey7n9,2016-10-05 09:17:00-04:00,RonMexico69,,"Honestly the biggest things will be learning pointers and dynamically allocating memory space.  I highly recommend buying ""pointers on c"" by Kenneth Reek and reading it if you feel that you will. ",55zbii,t3_55zbii,UmmmmmNick,,Comment,4,0,4
d8f9oau,2016-10-05 13:30:54-04:00,UmmmmmNick,,Thanks for the advice does anyone know any resources that could help me learn C,55zbii,t3_55zbii,UmmmmmNick,,Comment,1,0,1
d8fk85b,2016-10-05 17:06:00-04:00,randcraw,,"Kernighan and Ritchie's (K&R) ""The C Programming Language"", second edition (ANSI C) is precise, wastes few words, and it's still the best intro to C I know.  

If you plan to master C, I'd also recommend Harbison and Steele's ""C: A reference Manual"", PJ Plauger's ""The Standard C Library"", and Peter van der Linden's ""Expert C Programming: Deep C Secrets"".  Then to update to the C99 and C11 standards, check out Klemens' ""21st Century C"", esp. 2nd edition.",55zbii,t1_d8f9oau,UmmmmmNick,,Reply,3,0,3
55y9zm,2016-10-05 02:58:24-04:00,FUZxxl,Is there an algorithm for permuting an array with duplicate elements in linear time such that each permutation is equally likely?,"I know about the Fisher-Yates shuffle. Is there a similar algorithm for when some elements appear multiple times? If it helps, I can provide input as an array of pairs (*e*, *n*) where *e* is an element and *n* is how often it appears. I can even provide this array sorted in any possible way. In my current use case (chess programming) elements can appear at most twice, but I'm also interested in a solution where elements can appear more often.",,,,,Submission,9,0,9
d8ewdgn,2016-10-05 08:16:42-04:00,okiyama,,Must admit that I don't see why Fisher-Yates wouldn't work,55y9zm,t3_55y9zm,FUZxxl,,Comment,1,0,1
d8ewvm2,2016-10-05 08:34:30-04:00,FUZxxl,,"Yes indeed, it does work. Though not for my specific application: I use Fisher-Yates to encode a permutation (where the code is the series of random values that went into generating the permutation at hand) and for my purpose it is important that the mapping is bijective. Overcounting permutations is thus not a good thing. Perhaps I should have stated that in the first place.",55y9zm,t1_d8ewdgn,okiyama,,Reply,1,0,1
d8exa6g,2016-10-05 08:48:03-04:00,okiyama,,"I see, that makes sense. I'll have to think this one over some more. Are you trying to randomize the back ranks for Chess960?",55y9zm,t1_d8ewvm2,FUZxxl,,Reply,1,0,1
d8ezjie,2016-10-05 09:53:42-04:00,FUZxxl,,"I'm trying to make an endgame tablebase for a chess variant. For this purpose, I need to assign a code to each position. I do that by first encoding what pieces partake in the given position, then I  use a combinatorical code to describe the occupied squares and finally a permutation code is used to describe in what order the chosen pieces occupy these squares. To keep the tables small it is of utmost importance that no part of the coding space is wasted.",55y9zm,t1_d8exa6g,okiyama,,Reply,1,0,1
d8ewnjy,2016-10-05 08:26:43-04:00,_--__,,"Can't you just treat duplicate elements as unique and perform a Fisher-Yates? Yes, you are overcounting permutations that are the ""same"", but you are also overcounting the total number of permutations by the same factor so it all cancels out.  An equivalent approach would be to run a modified Fisher-Yates where you select the elements with probability weighted according to the number of occurrences it has.  E.g. with ABBCCC, your first choice would select A with probability 1/6, B with probability 2/6, and C with probability 3/6.",55y9zm,t3_55y9zm,FUZxxl,,Comment,1,0,1
d8ewx4s,2016-10-05 08:35:59-04:00,FUZxxl,,"> Can't you just treat duplicate elements as unique and perform a Fisher-Yates? Yes, you are overcounting permutations that are the ""same"", but you are also overcounting the total number of permutations by the same factor so it all cancels out.

You are of course right, see my answer to /u/okiyama for my thoughts on this.

> An equivalent approach would be to run a modified Fisher-Yates where you select the elements with probability weighted according to the number of occurrences it has. E.g. with ABBCCC, your first choice would select A with probability 1/6, B with probability 2/6, and C with probability 3/6.

While this seems possible I don't see how this is going to work in linear time.",55y9zm,t1_d8ewnjy,_--__,,Reply,1,0,1
d8exao0,2016-10-05 08:48:29-04:00,_--__,,"> While this seems possible I don't see how this is going to work in linear time.

Well it's pseudo-linear (i.e. linear if the number of occurrences is written in unary)",55y9zm,t1_d8ewx4s,FUZxxl,,Reply,1,0,1
55xuf4,2016-10-05 00:36:42-04:00,northintersect,How good was this Introduction to Bloom Filter Tutorial?,,,,,,Submission,14,0,14
d8erle0,2016-10-05 04:02:10-04:00,Cro_no,,"I liked it! Definitely got a casually explained vibe from the video. The explanation was pretty straight forward and easy to follow. Personally I found that it was a great review, I feel like I have a better understanding of what bloom filters are now after not having dealt with them in over a year.

Some questions that viewers might have though:

What is the practical application of a bloom filter? What is it mostly used for?

How do we calculate the percentage of assurance that an element is present?

The production quality looked good to me too. The diagrams were clean and the animations looked nice. It's great to see someone putting in work to make videos like these instead of videos of someone talking over a screen of code. Keep it up!",55xuf4,t3_55xuf4,northintersect,,Comment,2,0,2
d8etls7,2016-10-05 06:06:20-04:00,northintersect,,"Thank you for the feedback! Yes I was definitely going for the Casually Explained Vibe. I will create a more detailed Explanation for the Practical Uses of Bloom Filter in the future. 

You just supplied me with 1 week’s worth of Creative High!",55xuf4,t1_d8erle0,Cro_no,,Reply,2,0,2
55xabp,2016-10-04 22:21:19-04:00,Fawxhox,Solving Time Complexity Problems,"
How to Solve Time Complexity Problems (self.compsci)
submitted 14 minutes ago by FawxhoxSige [+3]
I have my midterm Friday for Data Structures, and I'm still kind of lost on solving time complexity problems. Like if an algorithm is O(NlogN) or O(N2logN), it takes X seconds to do Y data, how long to do 2Y data, or 4Y data.
I can't seem to find any answers on Google, but I'm guessing i'm just not using the right keywords",,,,,Submission,2,0,2
d8equ27,2016-10-05 03:20:06-04:00,IGetConfused,,"https://en.m.wikipedia.org/wiki/Big_O_notation

The wiki will help you. I believe the term to Google would be Big O Notation. ",55xabp,t3_55xabp,Fawxhox,,Comment,1,0,1
d8flr1y,2016-10-05 17:38:53-04:00,Fawxhox,,"I tried reading up on Big O Notation, but reading up on it, it doesn't seem to really help to solve NLogN problems, giving a sample size and time. I figured out what you need to do however in case anyone else is wondering. N is linear so doubling the size doubles the time, etc. N^2 is exponential so doubling the set takes 2^2 as long or tripling the set takes 3^2 time as long, etc. LogN takes one more second per double of the size. Those are just some things you can memorize, but the actual formula is: T(n) = k* (Big O-ness).

So for example if it takes 10 seconds to do 1024 things, with an NLogN algorithm to do 2048 things it would take: 

T(n) = k* n*Log(n)

10 = k * 1024*(log1024) --> 10 = k*1024*10

1=k*1024 --> k = 1/1024

So T(2048) = 1/1024 *2048 * log (2048)

T(2048) = 2*11

T(2048) = 22

And you can use this formula to solve similar problems for any type of Big O-ness (n, nlogn, n^2, etc) and any time/ data size difference. Hopefully this helps someone else, not sure if this type of problem just isn't very common, but I couldn't find the answer anywhere online.
",55xabp,t1_d8equ27,IGetConfused,,Reply,1,0,1
55wu54,2016-10-04 20:42:56-04:00,TheCommador,Can someone explain how sandboxing works?,"Hi everyone,

I'm working on a research project in the computer science department of my university. Could I get some help understanding how sandboxing of programs works exactly? 

For instance, how would a program prevent a sandboxed program from using excessive amounts of CPU, and memory resources? How does a sandbox restrict permission of files, and file systems?

Any further reading or other resources are appreciated. Thanks!

Edit:
Some more information because I am a fool for not including it right away. 

I need to be able to harden a Raspberry Pi from malicious activity. This Pi will be running the Raspbian operating system, a Linux distribution.",,,,,Submission,10,0,10
d8ecqry,2016-10-04 20:52:03-04:00,teraflop,,"""Sandboxing"" is more of a buzzword than a precise term. Do you have questions about a specific implementation or technique? 

The glib answer is, the operating system already gets to make all the decisions about how a process interacts with the rest of the system, so you can just take a subset of those resources and call it a ""sandbox"". E.g. it would be valid to refer to FS permissions as a sandboxing mechanism.

As an example: the Linux kernel has two different features, cgroups and namespaces that are used together to implement so-called containers. Tools like Docker provide a friendlier interface on top of these low-level primitives.",55wu54,t3_55wu54,TheCommador,,Comment,2,0,2
d8eogdj,2016-10-05 01:33:25-04:00,TheCommador,,"I suppose my question is, ""how does one write a program that will allow another program to run inside of it, whilst restricting that second program's resources and privileges."" 

A vague and difficult to answer question, I know, and I appreciate your help.",55wu54,t1_d8ecqry,teraflop,,Reply,2,0,2
d8fg4vr,2016-10-05 15:41:28-04:00,lordvadr,,"Well, you don't really do that...write a program that will allow another program to run inside of it.  Basically you tell the operating system to ""impose these limits on me"", then then run the program with the new limits.

As others have said, sandboxing can mean a lot of things, so a lot of it depends on which resources you want to limit, and what you want that limit set to.

An simple example is chroot.  You tell the operating system ""make this directory my root filesystem"", and then do what you want to do.  Now, chroot has had all sorts of vulnerabilities over the years (ways to break out of it), and requires some work, but it was used as a way to stick a program, such as an FTP daemon or a DNS daemon into it's own little corner of the filesystem, so that even if a bug could be exploited to read any arbitrary file, it can only read the files inside it's chroot environment.

There are other mechanisms to limit memory, file size, other things.  One of the problems is that they all kinda worked a little differently, or could be overridden if you didn't dump enough privileges. Look into setrlimit and it's predecessor ulimit.

Virtualization and containers take the idea to the extreme, and allow things like sandboxing network interfaces and the like.

Also, lot of available limiting mechanisms have weird units.  For example, you can't say, ""10%"" for a CPU limit.  You give it in seconds, and the program accumulates seconds as it runs and as a percentage of active time on the CPU...a simple example is a program running for 10 seconds at 10% cpu utilization has accumulated 1 second of CPU time.

Can you give us an example of the kind of resources you want to limit, and why you think they need to be limited?",55wu54,t1_d8eogdj,TheCommador,,Reply,1,0,1
d8emwgx,2016-10-05 00:39:36-04:00,BonzoESC,,"> For instance, how would a program prevent a sandboxed program from:

tl:dr: the OS kernel or a VM approximating one, probably

Most of this comes down to the kinds of restrictions and checks you can build with privilege levels: https://en.wikipedia.org/wiki/Privilege_level

> using excessive amounts of CPU

This is a feature of a scheduler, like you'd find in a multiprocess OS like UNIX or Windows, or a runtime system that provides ""green threads"" like the Java or Erlang VM. Since I'm most familiar with it, I'll give a rough idea how the Erlang VM schedules processes: each scheduler has a list of processes it wants to run. Each process is a bunch of steps called ""reductions,"" i.e. reducing the expression `1 + 1` to `2,` or reducing `gen_tcp:send(SomeSocket, SomeData)` to the list of subexpressions that make up the `gen_tcp:send/2` function. After a given number of reductions for a given process, the scheduler quits reducing/executing that process and goes on to one that hasn't run recently. You could restrict the amount of wall clock time a process gets by seeing how many reductions it will use in a given time interval, and limiting it to less than that.

For more about how OS schedulers work, you could start by researching how the Linux kernel does it: https://en.wikipedia.org/wiki/Completely_Fair_Scheduler is one scheduler implementation they've used.

> … memory resources? 

A program gets memory either by being allocated it at startup, or through allocation at run time. On some platforms, especially microcontrollers that only run one program at a time, your startup memory is all you have. If you access memory that doesn't exist, the computer might halt and require a power cycle, it might execute a fault handler that you can provide, or it might do 
some ""unspecified behavior.""

The more interesting case is run time allocation. Most C runtimes for modern computers and OSes provide a `malloc` function, Java has the `new` keyword. Both of these will first see if there's any allocated but currently unused memory, and if possible, return that. If not, the runtime asks the OS for memory. Sometimes the OS says no, if say that process or user already has too much memory allocated. `malloc` returns NULL in this case, `new` will raise some kind of `Exception`, and that's that.

> How does a sandbox restrict permission of files, and file systems?

Working with files and filesystems involves the operating system: I don't want to be able to write a program that can trash my filesystem, or even know what kind of filesystem it's running on. Instead, the OS provides system calls for opening, reading, writing, and closing files. When a program invokes these syscalls, the OS can check to see if the program is allowed to, based on who's running the program, what the program is, etc. If it's not allowed to write 10TB of data to some other user's home directory, the syscall fails.

All of these work different ways with different sandboxing technologies. Docker uses [Linux cgroups](https://en.wikipedia.org/wiki/Cgroups), which are implemented in the Linux kernel. The sandboxing provided by the Java VM is implemented mostly by the VM, since one of the design goals was that it should work the same both on Solaris UNIX and Windows 95, the latter of which didn't really have any OS features to support sandboxing. Amazon EC2 is believed to run on [Xen](https://en.wikipedia.org/wiki/Xen), which uses a special kernel called a ""hypervisor"" that restricts what guest domains are able to do.",55wu54,t3_55wu54,TheCommador,,Comment,2,0,2
d8eooen,2016-10-05 01:41:59-04:00,TheCommador,,"I appreciate your reply, this is some of the best information in the thread. Right now I'm still trying to process some other information, is it alright if I put a pin into this for now, and ask questions to you later?

Thanks again, your reply is really helpful!",55wu54,t1_d8emwgx,BonzoESC,,Reply,1,0,1
d8g64hn,2016-10-06 03:14:18-04:00,lordvadr,,This is a solid answer. Props.,55wu54,t1_d8emwgx,BonzoESC,,Reply,1,0,1
d8ecuxy,2016-10-04 20:54:48-04:00,josh_legs,,"So I think you're probably talking about virtualization. 

Generally it is pretty simple to set up a ""virtual machine"" on your compute that runs an OS of your choice. Then inside that virtual machine, you can fairly safely do whatever you want because everything on the machine is ""sandboxes"" -- e.g., cut off from your actual computer. When you create the virtual machine you set how much of any given resource the machine could use at any given time (e.g., CPU, ram, hard drive, etc). If somebody goes wrong you still have pretty good control over the virtual machine and it won't affect the stability of your ""host"" machine (the operating system you run the virtual machine inside of). Hope that at least makes some sense. I'd read up a bit more on virtual machines because I think it will answer a lot of your questions. ",55wu54,t3_55wu54,TheCommador,,Comment,1,0,1
d8eojx1,2016-10-05 01:37:05-04:00,TheCommador,,"Hey I appreciate your reply. 

I've added some more info in my original question, if you need some clarification I can certainly try to provide to the best of my abilities.

I am somewhat familiar with VM's and their uses. Is all ""Sandboxing"" done in a virtual machine? I suppose it would have to be, right? In order to restrict CPU resources.

Thanks for contributing, and if you do learn something I would be pleased to hear what you have to say. ",55wu54,t1_d8ecuxy,josh_legs,,Reply,1,0,1
d8etbq0,2016-10-05 05:49:05-04:00,ISvengali,,"The CLR and JVM have ways to sandbox apps in them by controlling access to system resources, so I would search for how they do it in addition to the VM stuff.  

I built a little test MUD in Scheme back 1000 years ago where the users could use eval to run their commands.  For this, I sandboxed what environment they had access to which was something the scheme runtime I used allowed me to do.  This was similar to what the CLR and JVM do.  

Unless my memory is failing me, which is possible, Quake II had a full x86 interpreter to safely sandbox dlls that people wrote for gameplay.  The real games' dlls could run at full speed natively.  This may just have been something Carmack was toying with, as I cant find references to it now.  

As /u/teraflop said, its more of an umbrella term for providing a restricted programming environment to folks, and theres many reasons and ways to do it.  ",55wu54,t1_d8eojx1,TheCommador,,Reply,1,0,1
55wn12,2016-10-04 19:58:42-04:00,finessseee,Learning to program on your own vs having a professor teach you? Which would you say is a more efficient way of learning how to code?,,,,,,Submission,3,0,3
d8eb576,2016-10-04 20:14:39-04:00,craiig,,Both at the same time?,55wn12,t3_55wn12,finessseee,,Comment,9,0,9
d8eicei,2016-10-04 22:46:29-04:00,cravenspoon,,"Seconded. In HS I did Java classes, and then did the same programs in C++. In my first coding classes in college (which is where I'm still at) I'll try to do the project in another language as well.  

I don't know how long term beneficial that is, but I figure knowing at least the basics in a few common languages will help, as well as forcing you to actually ""get"" the code you're making",55wn12,t1_d8eb576,craiig,,Reply,2,0,2
d8evk52,2016-10-05 07:44:50-04:00,_--__,,"You should look at working in different programming **paradigms** - e.g. if you are learning in java/C/C++ (imperative languages), try working in Haskell/ML (functional languages).  It teaches a completely different mindset/approach to problem solving.  Ultimately, if you are proficient in just one language per paradigm, it is reasonably easy to pick up others (i.e. learning Java and C++ in parallel is a bit like reading Hunger Games and Divergent - different, but really kind of the same)",55wn12,t1_d8eicei,cravenspoon,,Reply,1,0,1
d8er6t0,2016-10-05 03:39:21-04:00,crookedkr,,Depends on your learning style.,55wn12,t3_55wn12,finessseee,,Comment,1,0,1
55u6yb,2016-10-04 12:01:05-04:00,EntangledAndy,Worried I'm not learning what I need to know in a computer organization class. How can I rectify this?,"I'm currently taking a computer organization class using the 2nd edition of ""Computer Systems: A Programmer’s Perspective"" by Byrant and O’Hallaron, and the lecturer is not particularly effective at teaching this subject. What should I know about computer organization so that I am sure I have the necessary knowledge in this subject matter?",,,,,Submission,1,0,1
d8fumch,2016-10-05 21:14:38-04:00,arbitrarycivilian,,"If the professor isn't effective, then just read the textbook. It's an excellent introduction to computer systems. You should be familiar with every chapter inside.",55u6yb,t3_55u6yb,EntangledAndy,,Comment,2,0,2
d8dqfsh,2016-10-04 12:49:13-04:00,dxk3355,,"Just looking at the chapters in that book it's a pretty high level view of the computer.  My school had two versions one was offered by the computer science department like yours.  The other was the computer engineering department and was the third course in a sequence after digital circuits and assembly.  In the CE you would basically built a simple CPU on whiteboards using AND, OR, MUX, and other gates.  

In your case there's a lot to cover in that book so the class sounds like a grab bag of ideas instead of unifying idea.  I guess if would remember anything it's that the CPU is still a Turing machine but each piece of the machine is actually rather complex today.  For example even though the CPU can only execute a single instruction at a time, in practice there's a pipeline of instructions that need multiple substeps.  Or even though the CPU should be the one writing and reading from memory, DMA allows a device direct memory access in order to save processing power and increase speed.",55u6yb,t3_55u6yb,EntangledAndy,,Comment,1,0,1
55u569,2016-10-04 11:51:39-04:00,slvrfn,How to accomplish converting to/from image and audio?,"Hello! I am exploring the idea of making a program that can convert an image to audio, and vice versa. With this you can make audio visually editable!
Some great examples of other programs that accomplish this are [Photosounder](http://photosounder.com/), [AudioPaint](http://www.nicolasfournel.com/?page_id=125), and [Harmor](http://www.image-line.com/plugins/Synths/Harmor/).
I am not asking for an actual implementation (although that would be pretty cool), just trying to get pointed in the right direction of any algorithms/technologies that might be uesful in re-creating their finctionality",,,,,Submission,5,0,5
d8dsd6b,2016-10-04 13:28:18-04:00,lordvadr,,"So, you can rename an raw audio file to .bmp and vice versa, however, I know that's not what you're asking...you're asking for meaningful images and sounds.  Because you're basically going to get static and noise if you start doing that.

So you're going to need to spend some real time understanding the mathematics behind sounds...or music particularly.  This is not going to be an easy project, and it's not going to be possible until you understand these type of things. Filters are your next subject...learn the mathematics on how filters work, what they do and why.

Start with a fourier transform, which is mostly what a lot of the demos I saw on your mentioned sites are.  That will at least get you to a meaningful image.  And understanding that math will allow you to understand what you're looking at in the image.  You can do the tranform backwards too.   What you're going to need is a way to represent time because sound<->video is a more meaningful conversion, or a common way to represent this is what's called a waterfall display.  [This](http://oona.windytan.com/posters/dialup-final.png) is one of my favorite images on the web, which is the spectral display of a dialup modem (not exactly music).",55u569,t3_55u569,slvrfn,,Comment,2,0,2
55nyz8,2016-10-03 10:44:55-04:00,KrustyKrab111,How do I go about finding eulerian cycles to build a genome from DNA kmers in python?,"I understand how the algorithm works but I have trouble implementing it in python. I have a class that defines each vertex in the graph and has the memory locations of the next and previous memory locations. But I'm confused how to go about finding even a single cycle. 

Can you guys help me out?
 Thanks",,,,,Submission,4,0,4
d8cgel1,2016-10-03 14:40:29-04:00,thewataru,,"Which algorithm for finding eulerian cycle are you implementing? Could you recite, how you understood it? Then I could help you to implement it.",55nyz8,t3_55nyz8,KrustyKrab111,,Comment,5,0,5
55lvgl,2016-10-02 23:29:30-04:00,caqweasd,"First course in Databases, having difficulty understanding example of Relational Algebra","Problem:  http://puu.sh/rvUdh/77baf74cd6.png


Schema: http://puu.sh/rvUit/743284ea1d.png


I'm having trouble understanding the red circled part from the first link. What is the 3x cartesian product of identical Tags doing? I assume it might have to do with us wanting to find 3 specific items, but why is the 3x product necessary?

Any help is appreciated, thanks

",,,,,Submission,3,0,3
d8bu5nj,2016-10-03 02:31:54-04:00,theobromus,,"If you notice the Tag table associates 1 tag with each item. All of the tags for an item will be multiple rows with the same CN of the item, but each of the different tag strings.

The problem is to find the 1 best item with all three of the tags.
In order to do this, you have to join the Tag table on 3 times, since you're looking for one row in the Tag table that matches each of the three query tags. The first instance of the Tag table is restricted to the first query string, the second to the second, and the third to the third. Thus the cartesian product matching those predicates includes exactly the catalog numbers that match all three of the tags.",55lvgl,t3_55lvgl,caqweasd,,Comment,1,0,1
55jri9,2016-10-02 15:21:39-04:00,benes_the_menace,"Parity checks, Hamming codes, and efficiency?","Hi! I just can't understand this homework question. It says:

""Which is more efficient for sending 7-bit data words: 1-bit even parity check; 1-bit odd parity check; (7, 4) hamming code?"" 

If I understand correctly (and there's a very good chance I don't), we can rule out the (7, 4) hamming code straight away, because you can only encrypt 4-bit words with those. So that leaves either the odd or even check. But why would it make a difference? As far as I can tell, using an odd or even check is simply a matter of convention or preference.

Can anyone shed some light on this subject?",,,,,Submission,1,0,1
d8b71ax,2016-10-02 16:11:28-04:00,dmazzoni,,"Unless I'm missing something, your analysis seems totally correct. Maybe it's a bit of a trick question and the answer is that even and odd parity checks are equally valid.",55jri9,t3_55jri9,benes_the_menace,,Comment,1,0,1
d8b85p2,2016-10-02 16:35:33-04:00,ErieSpirit,,Efficient in terms of error rate or data throughput?,55jri9,t3_55jri9,benes_the_menace,,Comment,1,0,1
55ac8u,2016-09-30 18:04:04-04:00,moonsfang16,Can you embed a function in a client-side programming language that could be executable on the server-side?,"I'm starting on my journey through CS and I've thought of something that I'd like to learn about. Could someone with access to edit a client's side source code such as JavaScript, embed or nest functions within in it that could effectively run in a language like PHP when a request is sent to the server? I would assume this would be implausible, but I wanted to hear from someone with some more knowledge on the subject.",,,,,Submission,2,0,2
d88ymq8,2016-09-30 19:29:35-04:00,Merad,,"Sure. Source code is just text. Compilers/interpreters are just programs. A browser can easily send text back to a server, the web server program can easily invoke another program (or use other methods) to compile and execute the text. In general this is a spectacularly bad idea, but it's 100% possible. ",55ac8u,t3_55ac8u,moonsfang16,,Comment,5,0,5
d88wjl7,2016-09-30 18:32:25-04:00,tolos,,"If you have access to the server, yes. see /r/linux /r/sysadmin  

If you don't have access to the server ... those types of things are typically avoided. see /r/netsec or [this](http://blog.portswigger.net/2016/04/adapting-angularjs-payloads-to-exploit.html) for an example.  

Unless you meant something more literal. see https://ideone.com/ or google ""code online""",55ac8u,t3_55ac8u,moonsfang16,,Comment,2,0,2
d891as8,2016-09-30 20:43:56-04:00,dandrino,,"It's certainly possible but behavior like this is frequently undesirable and if it is possible on a site it is usually a (rather serious) security hole. What would basically have to happen is for you to effectively perform some GET/POST request with specially crafted contents on a server that executes the code locally on its machine.

Sites that do explicitly allow this usually execute the code in a well controlled sandbox. For example, you can compile/execute Go code at https://tour.golang.org/welcome/1 but you won't be able to use it to do anything malicious.

A somewhat related example of what you are talking about is [SQL injection](https://en.wikipedia.org/wiki/SQL_injection).",55ac8u,t3_55ac8u,moonsfang16,,Comment,2,0,2
d89ppn8,2016-10-01 12:46:46-04:00,sanimalp,,"It sounds a lot like ""remote method invocation"" to me. ",55ac8u,t3_55ac8u,moonsfang16,,Comment,2,0,2
55aaq8,2016-09-30 17:54:47-04:00,newcsfasttrkporj,"I'm new to CS and want to get started on personal projects, but can't wrap my head around how to actually start","So, I'm new to CS, coming in from a different background, so my actual time spent doing my CS degree will be about 2 years, so a lot less time to get internships, get the hang of coding, etc. I wanted to start on my projects now, having just finished the Intro to Programming course and almost being done the Intro to CS course, but how and where to actually start is still baffling me. I don't know what to do at basically any step. I don't know how to edit open source projects. I don't know git or anything like that. Are there any resources to learn any of these things? Step-by-steps? Walkthroughs?",,,,,Submission,17,0,17
d8978vq,2016-09-30 23:38:37-04:00,cdrootrmdashrfstar,,"[Learn Git in 30 Minutes](http://tutorialzine.com/2016/06/learn-git-in-30-minutes/)

[Python ""guide"" and documentation.""](http://docs.python-guide.org/en/latest/)

[MakingGamesWithBen](https://www.youtube.com/user/makinggameswithben/videos?sort=da&view=0&flow=grid) -- this C++/Game Development tutorial is really got me started around two years ago when I first started programming. It taught me about a lot more advanced concepts than my courses were covering, and gave me real world experience in a project that wasn't a homework assignment (and honestly gave me an edge over all the other students).

Big tip from a now-sophomore who was in your position coming into college, *GO TO OFFICE HOURS*. Seriously, you pay for your professors to... profess, so go and sit in their offices while you write code for a personal project or homework: bug them WHENEVER you have a question, no matter how ""dumb"" it feels, and get everything you answered.

The normal way I create personal projects is that I find something I do a lot (think: repetitive work or tasks done slowly/inefficiently) and attempt to program some sort of automation tool or some sort of helper/utility to speed up whatever process that is.

From [another post](https://www.reddit.com/r/programminghelp/comments/53orw2/why_cant_i_push_through_learning_the_basics_what/) I made about this topic:
> 
> I find it fun to build things, so that's what I did!
> 
> You can make:
> 
> * a reddit bot
> * a command line tool for defining words 
> * a script that makes your favorite api easier to use (also called an api wrapper) 
> * an implementation of an image compression algorithm
> * recreate your favorite games/mini games 
> * a webserver then host your own website on a raspberry pi (after creating the frontend of the website of course)
> * a tool to take screenshots
> * a program that automates signing up for college courses
> * a program that automatically signs you up for specific gym classes when they open up weekly or reserve a study room for you weekly 
> * a home dashboard that displays useful information, load it onto a raspberry pi, then connect it to an external screen for a futuristic dashboard 
> 
> ...actually, come to think of it, those things are what I've done and am continuing to do! 
> 
> There's tons of things you can build, you just gotta be a little imaginative and have the dedication to push through once you inevitably reach the difficult part of the project. 

In addition to that list, I'm now creating a bot (in Python 3) that will automatically reserve study rooms at my college's library for me at times between classes every morning... see how automation makes life easier, and all it took was a little imagination to figure out the idea?

[Here's a link to my Github](https://github.com/seanpianka?tab=repositories) if you're curious about my projects, how they're written, etc. Also, I have 13 public repositories, but 32 total repositories. You can always have little pet projects that never get finished, but make the effort to complete some of them and push through the tough times... then you're free to leave some of them unfinished! :P

Anyways, hopefully I didn't ramble too much here. If you want any help, feel free to message me on Google Hangouts (PM me for my e-mail and I will **gladly** spend whatever time you need to help you and answer whatever questions you want any time).

Okay, good luck on your CS adventure.
",55aaq8,t3_55aaq8,newcsfasttrkporj,,Comment,3,0,3
d88vydf,2016-09-30 18:16:51-04:00,ar21,,"Check out the sidebars, wikis, and FAQs for r/learnprogramming r/programming r/cscareerquestions there should be some great resources in there and also try searching those subs for beginner projects because this is a fairly common question with newer people",55aaq8,t3_55aaq8,newcsfasttrkporj,,Comment,2,0,2
d88vymx,2016-09-30 18:17:04-04:00,iRobinHood,,"It seems that you just learned what programming is and you want to be able to work on an big project.  It's like just learning to swim using the doggie paddle and now you want to jump from the top board on the Olympic pool.

Have you written any simple command line programs or utilities?  Have you tried writing a simple game on your own.  Have you used source versioning software at school or home?  Remember that Google is one of your major tools that you will be using to find manuals, videos, ask question, and training courses.

You can start [here](https://git-scm.com/docs/gittutorial) and then Google for more detailed information.",55aaq8,t3_55aaq8,newcsfasttrkporj,,Comment,1,0,1
d898szy,2016-10-01 00:29:59-04:00,greeniguana6,,"First off, learning Git is really easy. There are some good guides in this comments section, or you can check out the sidebar. Once you learn Git and can do some basic object-oriented programming or web development, just push everything you make to Github/Bitbucket and you'll have a pretty decent portfolio to start applying to places with.",55aaq8,t3_55aaq8,newcsfasttrkporj,,Comment,1,0,1
d89njfd,2016-10-01 11:49:56-04:00,high_side,,"There are tutorials for everything. Some bad, some good. Might be worth starting with one your class projects and just adding/modifying to start out with. If you are new it might be better to explore a small sandbox rather than a crapton of github code.",55aaq8,t3_55aaq8,newcsfasttrkporj,,Comment,1,0,1
5585md,2016-09-30 10:30:06-04:00,Quakerz1,Internship on cloud computing and virtualization in IT,"Hey guys, I'm not 100% certain this is the right sub, so if I should post it somewhere else let me know!

I got a few questions - I'll be entering an internship soon, arranged by my university. I've got my eye on a few, but this one stands out.

There's an internship that is focused on IT through cloud computing and virtualization. Now, from my limited understanding, it should deal with distributed systems, computer networks and such, which is exactly what I'm looking for.

 What I'd like to know, is how would those areas be applied to IT?

",,,,,Submission,1,0,1
556ko8,2016-09-30 01:57:39-04:00,abdulazizaaa,computer hardware question ?,"Your friends just learned that you are studying computer hardware. They feel that their computer is slightly out of date. They want to upgrade the processor, but not the motherboard. Prepare a list of at least 5 questions you should ask them before you know what CPU they can choose or whether it is feasible to upgrade their system. And explain why?
",,,,,Submission,0,0,0
d8809tt,2016-09-30 02:26:41-04:00,tyggerjai,,"1) Why are you bothering me?

2) Can't you ask the salesman at the PC shop, since that's their job?

3) Why not just get a Mac, they're shiny.

4) What do you even *do* besides surf porn?

5) Profit!",556ko8,t3_556ko8,abdulazizaaa,,Comment,9,0,9
d8fuo2u,2016-10-05 21:15:53-04:00,arbitrarycivilian,,But what if I need to prepare my computer for /r/60fpsporn...,556ko8,t1_d8809tt,tyggerjai,,Reply,1,0,1
d87zpol,2016-09-30 02:01:26-04:00,UKCSTeacher,,This question is much better suited to /r/buildapc/ ,556ko8,t3_556ko8,abdulazizaaa,,Comment,4,0,4
d885dgl,2016-09-30 07:11:09-04:00,Loco-D,,Sounds like a homework question....,556ko8,t3_556ko8,abdulazizaaa,,Comment,4,0,4
556070,2016-09-29 23:07:23-04:00,St_OP_to_u_chin_me,What's the most common way to share code on stack over flow?,"codeshare.io?

For VBA or excel data.
Thanks.",,,,,Submission,2,0,2
d87uxy5,2016-09-29 23:21:52-04:00,ase1590,,"[github gist](Http://gist.github.com) 

If you want something really bare bones, used the browser based paste for for[ sprunge](http://sprunge.us/) or whip up a script to upload it for you from command line, if you're into that sort of thing. ",556070,t3_556070,St_OP_to_u_chin_me,,Comment,1,0,1
553jdj,2016-09-29 14:08:38-04:00,adbJ114,File systems relation to directory structures,"I know that file systems differ from OS to OS, and that different file systems determine things like max file size, security, and compression. But how do file systems relate to directory structures?

For example, Mac uses the HFS+ fs and has a UNIX-like directory structure with folders like Applications, Library, User, bin, dev, etc. at its route. Windows is NTFS and uses the C:// drive as its route, with Windows and Program Files above it. But do the file systems themselves dictate this structure? 

Could you hypothetically format an HFS+ file system with a Windows-like directory structure? Are there qualities to the different directory structures that lend themselves to certain file systems? Or is it completely incidental, with each OS's file system shipping with the OS's file structure because they both happen to be what that OS uses?",,,,,Submission,1,0,1
d8au65k,2016-10-02 11:27:57-04:00,None,,"File systems only store directories and other things like files, symbolic links and devices.

Drive letters aren't part of a file system, and are just places where file systems appear in that operating system. So, the same file system that is C: in Windows 7 could be mounted in /media/Win7 in Ubuntu.

Path separators are also an OS thing, not a file system thing. So, for the same file system you would use backslashes in Windows and forward slashes in Linux.

The directory hierarchy and names are up to whatever is using the file system, except that different file systems have different limitations. NTFS is typically case-insensitive, and various characters are prohibited in names in Windows. Actually, you can put most prohibited characters in NTFS and make it case insensitive, but that causes problems in Windows. This makes NTFS inappropriate for a Linux file system. FAT had severe limitations, with 8 character names and a 3 character file extension, though a way was developed to use long file names there.

File systems can also allow for various special things like symbolic links, named pipes and devices. Some file systems, like FAT, don't allow these, and only allow files. Again,  you need a file system which allows the things you need, and stuff would break if you tried to use an inappropriate file system for an OS.

File systems also associate various data with files, like permissions and alternate data streams. Different file systems have different permission models, so for example Windows is designed to work with NTFS permissions and Unix/Linux are designed to work with Unix permissions. Windows also has features which make use of NTFS extended attributes, and they wouldn't work with a different file system.

Finally, you also need to consider the stability, reliability and performance of a file system in a particular OS. While the OS may support many different file systems, some are the most stable, reliable and most optimized. So, even if you managed to run Windows on HFS+ or Linux on NTFS, it wouldn't be as reliable and wouldn't perform as well as Windows on NTFS and Linux on ext4.",553jdj,t3_553jdj,adbJ114,,Comment,3,0,3
d8bowi7,2016-10-02 23:25:29-04:00,adbJ114,,This clarifies a lot of things for me. Thanks for the information.,553jdj,t1_d8au65k,None,,Reply,2,0,2
d882361,2016-09-30 03:55:58-04:00,bchociej,,"A filesystem usually ""knows"" (stores to disk) at least basic name and directory information for its files, but it doesn't necessarily store them in any specific or OS-prescribed way. It's up to the filesystem *driver*--the software that tells the OS how to use the filesystem--to present the directory information to the OS in whatever way is necessary. You can (and people often do) write filesystem drivers to support a filesystem in many different OSes. ""C:\foo\bar.txt"" on Windows could become ""/mnt/somedrive/foo/bar.txt"" in Linux, or whatever else the OS needs, as long as the driver can translate the bytes stored on the storage medium into some kind of meaningful path information.

So, the ""on-disk"" format never really locks a filesystem into a particular directory scheme. It's more of a conversation where the OS says ""hey filesystem driver, can you please get me C:\foo\bar.txt from your filesystem?"", and the filesystem driver has to figure out where to get that data.",553jdj,t3_553jdj,adbJ114,,Comment,1,0,1
552me0,2016-09-29 11:10:31-04:00,ramblerman,Why is the best solution here to use a stack?,"https://www.hackerrank.com/challenges/ctci-balanced-brackets

I get the stack solution, but I can't get past the tests with my own solution.

My assumptions are that any string of uneven length is by default not balanced.

And A balanced string is always symmetrical meaning I only need to check first against last, 2nd against 2nd to last etc.

Is there an error in that line of thought?",,,,,Submission,1,0,1
d877ttc,2016-09-29 14:18:18-04:00,PM_ME_YOUR_SHELLCODE,,"You can have strings that are balanced but not symmetrical.

    (())()

I don't see any rules that would indicate the brackets will always be nested if balanced.",552me0,t3_552me0,ramblerman,,Comment,2,0,2
552asb,2016-09-29 10:01:49-04:00,Sororita,looking for a woman with a career in computer science for an interview.,"I am doing an essay on women in computing for a college class and I need to interview an expert in the field. I focus mainly on motivations and possible bias encountered. I only have 10 questions so it is short and easy to do, if you are interested please let me know.",,,,,Submission,2,0,2
d870tjs,2016-09-29 11:51:44-04:00,minimim,,You could try asking on systers@ietf.org,552asb,t3_552asb,Sororita,,Comment,1,0,1
d871k1t,2016-09-29 12:07:25-04:00,Sororita,,Thanks!,552asb,t1_d870tjs,minimim,,Reply,1,0,1
5512cn,2016-09-29 03:28:08-04:00,Cheeseologist,Question about Little Man Computer model,"Does a Little Man Computer store data and instructions simultaneously? For example, if it's performing an addition, is the data found in the given mailbox location part of the memory required to perform the instruction?

In case anyone wants to know why I'm asking this question, here's some context. I'm supposed to determine how many mailboxes a 16-bit binary LMC can have, given that it has the same op codes as the regular model (in binary). I know that the op codes will be four bits in length (9 = 1001), but I can't figure out the number of mailboxes. If data doesn't count as being part of the 16 bits (to be honest, I don't quite understand what a 16-bit machine refers to), then I would guess that the mailbox addresses can be 12 bits in length (and hence 4097 mailboxes). But if the data counts as part of the 16 bits, then I have no clue what the answer might be.

If anyone could help me sort out my confusion, I'd greatly appreciate it.

Also, I wasn't sure if this is an /r/askProgramming or /r/askComputerScience question, so I'm asking it here and there.",,,,,Submission,1,0,1
550ouw,2016-09-29 01:14:40-04:00,TBoarder,Master Theorem question - I've hit a block with Regularity Conditions involving lgs,"I'm having a problem with figuring out Regularity Conditions for an online class that I'm taking.

So the base equation that I need to figure out is: 

a\*f(n/b) <= c*f(n)

a = 3, b = 3 and f(n) = n^3 lgn

My work is as such:

3 * (n/3)^3 * lg(n/3)^3 <= c * n^3 lgn

3 * n^3 / 9 * lg(n^3 / 9) <= c * n^3 lgn

The n^3 cancels out on both sides, and following the properties of logs, I get:

1/3 (lgn^3 - lg9) <= clgn

I bring the ^3 in lgn^3 down and solve for the constant lg9, giving me:

1/3(3lgn - 3.17) <= clgn

Distribute the 1/3:

lgn - 1.06 <= clgn

Rearrange to get the constant alone:

lgn - clgn <= 1.06

Factor out the lgn:

lgn(1-c) <= 1.06

Here is where I hit my block.  The professor's notes just skip to an answer for her example problem without explaining HOW she got her answer.  I know that c has to be less than 1 for the Master Theorem to even work here, but I have now clue how to solve for c here... Any help would be deeply appreciated.  Thanks!

(And hopefully the formatting here is readable... I can answer any questions if it's not...)

Edit:  I am not looking for somebody to just give me an answer... I am trying to hash out the whys and the hows here.  My *guess* is that c will equal 1 (based on the professor's example), which would make this fall between the bounds of Case 2 and Case 3 of the Master Theorem because c has to be < 1.  0 * lgn equals 0, but there is a possibility that there is a c < 0 multiplied by any lgn that will be > 1.06?  I'm sorry if this is out of bounds for this subreddit, since I'm getting downvotes here...

Edit 2:  Thanks to u/_--__ for pointing out my stupid math mistake.  My final answer came out to be 

lgn(1/9 - c) <= .176, making (I believe) the equation true for c = 1/9.  Thanks for the help!",,,,,Submission,3,0,3
d88a1yu,2016-09-30 09:54:33-04:00,_--__,,"> My work is as such:  
> 3 * (n/3)^3 * lg(n/3)^3 <= c * n^3 lgn  
> 3 * n^3 / 9 * lg(n^3 / 9) <= c * n^3 lgn

I am quite lost here, you seem to have made several errors:

1. Not sure why you have lg(n/3)^3
2. (n/3)^3 = n^(3)/27 not n^(3)/9
3. lg(n/3)^3 is not equal to lg(n^(3)/9) [or lg(n^(3)/27) for that matter]",550ouw,t3_550ouw,TBoarder,,Comment,2,0,2
d88ele3,2016-09-30 11:41:25-04:00,TBoarder,,"Oh, wow... Yes, I'm dumb.  I put the n^3 into the equation, and then obviously had a brain-fart and cubed the lg as well.  Let me try to rework this with the new numbers... I'm an idiot :(",550ouw,t1_d88a1yu,_--__,,Reply,1,0,1
550gkx,2016-09-29 00:06:13-04:00,Th782,Schedule Server to lower SQL ram at certain hours of day?,"Is there a command or a way to set a schedule on an office server that dictates/lowers ram usage on that box during certain hours of the day?

Ideally, I would like to lower SQL's ram amount for a set period of time to allow for a back up to run and not hog up system resources that are causing SQL to run very slow, if at all. After the backup, I'd like to set the ram back to normal. 

Thanks. ",,,,,Submission,0,0,0
54zo60,2016-09-28 20:50:38-04:00,throwfaraway2310,Is databases course worth taking?,"I want to get a software job after graduating and heard that databases is very important to know. But I also heard that it is easy to learn and pick up quickly.

Is my opinion generally correct? If it is hard to fit it into my schedule, would it be worth dropping another cs class for databases or should I just self study it?

edit: you guys have convinced me",,,,,Submission,11,0,11
d86bbo7,2016-09-28 21:01:16-04:00,JonL13,,"There's a lot to learn in a database class, and it is an incredibly valuable skill for quite a few CS professions. I'd say it's one of the more important classes you could take, if you can fit it in your schedule you can talk about taking the class in interviews when it's brought up. ",54zo60,t3_54zo60,throwfaraway2310,,Comment,5,0,5
d86hhde,2016-09-28 23:42:40-04:00,not-just-yeti,,"As a prof: I'd place DB pretty high on the list of course priorities.

Learning SQL is a good skill, but that's the part you can also pick up easily through self- teaching.

But there's also DB design, which is good reinforcement of general programming /data design, which students often need more practice on, and good individual feedback about. That's v.hard to get out of tutorials.

Finally, if the prof is good, there'll be many DB-specific pearls of wisdom & experience, that self-study will miss.",54zo60,t3_54zo60,throwfaraway2310,,Comment,4,0,4
d86hopu,2016-09-28 23:48:44-04:00,thatwasntababyruth,,"Learning how to use a database is easy, yes. You can pick that up on the job just fine. A databases class isn't about that, though, it's about how they work underneath. Until you understand how common types of databases work under the hood, you will be sub-par at using them and modeling your data. It simply isn't possible to do good data modeling without a solid foundation on normal forms, b-trees, non-relational storage methods and sql theory.

Honestly, databases is probably the most important class you can take in a CS undergrad course. My bachelor program didn't let you graduate without it.",54zo60,t3_54zo60,throwfaraway2310,,Comment,3,0,3
d86dtan,2016-09-28 22:06:31-04:00,assface,,You definitely want to take a database course. You will run into databases throughout your entire career no matter area you choose so it's good to have an understanding of how they work.,54zo60,t3_54zo60,throwfaraway2310,,Comment,2,0,2
d86gjdq,2016-09-28 23:15:41-04:00,esmith327,,"If you're working in software, you'll likely deal with databases more often than not. Even when you're not dealing directly with them, many concepts will carry over - structuring data, normalization, indexing, etc. I'd consider basic database familiarity to be essential. ",54zo60,t3_54zo60,throwfaraway2310,,Comment,2,0,2
d86spw4,2016-09-29 08:29:05-04:00,Bottled_Void,,"I covered databases in a more general module, so I didn't take the specific course.


If you have been taught some basic SQL and table relationships already I'd say it's okay to give it a miss.",54zo60,t3_54zo60,throwfaraway2310,,Comment,1,0,1
54yfzw,2016-09-28 16:27:43-04:00,kgjettaIV,EVBPlus2 and Dragon12-P-USB microcontroller trainers worth anything?,"A local tech college is selling a bunch of both of these and I'm trying to determine if they would be worth picking up to resell.  I can't find much on these so I was hoping I could get an idea of if they are worth anything.

Thanks",,,,,Submission,1,0,1
54vpta,2016-09-28 06:38:07-04:00,fatcat561,Getting onto a CS Msc with non CS Degree and no experience? What can i do to improve my chances,"Ive done a game degree 2:1 which we did a little java script thats it for relevant experience.
What would be the most valuable (and time efficient) thing that i could do to improve my chances? i.e. code academy or do some open source coding but i really have no idea",,,,,Submission,4,0,4
d85ghx1,2016-09-28 09:45:47-04:00,videoj,,"[Scott Young](https://www.scotthyoung.com/blog/myprojects/mit-challenge-2/) did a full MIT CS degree in one year, based on the [MIT's Opencourseware](https://ocw.mit.edu/index.htm).  Doing your own version of this would help your chances.",54vpta,t3_54vpta,fatcat561,,Comment,4,0,4
d85d2j4,2016-09-28 08:00:52-04:00,razeal113,,"I'm on my phone so i cannot link you to anything at the moment.  But there are suggested curriculum's for this exact type of question.  Meaning someone who wants to know everything that a BS in CS would give you via the internet for free.

If you are looking to get a Masters degree in CS , than for the most part you will need a good base in most computer science topics.  This can be achieved online though.  Look to MOOCs like coursera , udacity, etc : most of them offer free courses on various subjects, and their instruction is top notch.  

So start by looking up a suggested curriculum, than finding those courses on something like udacity or coursera.  If you are taking the free courses you probably wont have any projects to do and only have little quizzes, therefore, you may need to challenge yourself by attempting some small project that is self imposed based around the subject you are currently learning.  This will have the extra benefit that you can use it as an example project later for interviews, and demonstrations of your understanding and skill.

Lastly, when you run into any problems use a combination of reddit and stack overflow : there are great communities in each for any type of STEM based question you could possibly have.  

most masters degrees will have a focus, but will expect you to study a base of topics.  For CS this would be things like: Computer Graphics, Algorithm design, AI, networks, etc  with a focus (and thesis) on a specific topic from one of those general topics.  ",54vpta,t3_54vpta,fatcat561,,Comment,3,0,3
d85ex10,2016-09-28 09:03:15-04:00,RuthBaderBelieveIt,,https://github.com/open-source-society/computer-science,54vpta,t1_d85d2j4,razeal113,,Reply,2,0,2
d85dk02,2016-09-28 08:18:52-04:00,fatcat561,,thank you thats interesting. Im wondering how reputable MOOCs are going to be in the UNIs eyes though..,54vpta,t1_d85d2j4,razeal113,,Reply,1,0,1
d85nv5f,2016-09-28 12:24:34-04:00,razeal113,,"I really have no idea how universities view MOOCs (as far as admission criteria goes)  .  Many programs offering a masters in CS don't require you to have a BS in CS.  As long as they think what you have learned is sufficient to meet their standards thats all it takes.  

What i originally said was how you could get the knowledge for free online that one would expect to get in a CS degree.  If you want to know how to get passed the application phase , thats a different question.  

Many master programs don't require you to have a BS in CS, as long as they feel that you either have the minimum knowledge needed, or are willing to take a few BS courses before you start your masters.  

If you are worried that MOOCs might not meet their requirements , i would contact the Universities that you are considering directly and explain your situation and see if they have specific advice for you .  

Another thing that you will need is , typically, 3 reference letters written on your behalf.  These are usually from former professors stating that you are a good student and would make a good grad student.  I believe that you can also get employers to write these for you , though i have no experience with that.  


As a final thought about MOOCs , Georgia tech , which is considered to be in the top 10 if not the top 5 CS programs in the world, has an online masters program.  So they obviously believe that teaching online is a valid method of education : as well as the fact that major universities (havard, Gatech, MIT, standford, etc) all offer online courses via coursera and others, so they at least value it enough to put it out there.
 ",54vpta,t1_d85dk02,fatcat561,,Reply,2,0,2
d85q6yu,2016-09-28 13:10:51-04:00,fatcat561,,thank you this has given me new hope despite some rather negative responses to the same question in the past,54vpta,t1_d85nv5f,razeal113,,Reply,2,0,2
54vp2w,2016-09-28 06:30:30-04:00,PMMEYOURHELP,"I'm getting the following digital resources for free from my college, How do I use them well?","Hello Everyone, 
Now you all have your email id with suffix as @####.ac.in
Here are some of the benefits for which you should apply.
Benefit no. 1:
You have 10TB of online Google Drive storage. You can make use of that if you have low hard disk storage. (Generally useful for MAC users)
Benefit no. 2:
Github Education Pack:
1. Make a account on Github.com with your institute email id.
2. Go to http://education.github.com/ and apply for student pack with the same account and #### email ID. 
- Your application will be approved in 1-2weeks and these are the benefits you will get from it:
- $50 Hosting credit on Digital Ocean
- .me domain from namecheap
- $15 credit on AWS (You will get $115 as #### is in list of AWS Educate programme as well)
- Personal plan free on dnssimple for 2 years.
- Github Plan with unlimited private repositories.
- $25 credits on hackhands
- Developer account (normally $49/month) on orchestrate while you're a student
- 15K emails/month on sendgrid 
- Waived transaction fees on first $1000 in revenue processed on stripe
- 1 month free access on udacity on any nanodegree programme (worth $199)
Benefit no. 3:
You will soon receive a email from Microsoft Dreamspark where you can get 100+ paid microsoft products for free.
Benefit no. 4:
Get any PAID IDE from Jetbrains for free. 
https://www.jetbrains.com/student/



PLEASE EXPLAIN WHAT CAN I DO with the above mentioned? 

~aspiring software developer",,,,,Submission,1,0,1
d85flyf,2016-09-28 09:22:56-04:00,INCOMPLETE_USERNAM,,"Fixed formatting:

> 
> Hello Everyone, 
> 
> Now you all have your email id with suffix as @####.ac.in
> 
> Here are some of the benefits for which you should apply.
> 
> Benefit no. 1:
> 
> You have 10TB of online Google Drive storage. You can make use of that if you have low hard disk storage. 
> (Generally useful for MAC users)
> 
> Benefit no. 2:
> 
> Github Education Pack:
> 
> 1. Make a account on Github.com with your institute email id.
> 
> 2. Go to http://education.github.com/ and apply for student pack with the same account and #### email ID. 
> 
> - Your application will be approved in 1-2weeks and these are the benefits you will get from it:
> 
> - $50 Hosting credit on Digital Ocean
> 
> - .me domain from namecheap
> 
> - $15 credit on AWS (You will get $115 as #### is in list of AWS Educate programme as well)
> 
> - Personal plan free on dnssimple for 2 years.
> 
> - Github Plan with unlimited private repositories.
> 
> - $25 credits on hackhands
> 
> - Developer account (normally $49/month) on orchestrate while you're a student
> 
> - 15K emails/month on sendgrid 
> 
> - Waived transaction fees on first $1000 in revenue processed on stripe
> 
> - 1 month free access on udacity on any nanodegree programme (worth $199)
> 
> Benefit no. 3:
> 
> You will soon receive a email from Microsoft Dreamspark where you can get 100+ paid microsoft products 
> for free.
> 
> Benefit no. 4:
> 
> Get any PAID IDE from Jetbrains for free. 
> 
> https://www.jetbrains.com/student/
> 
> 
> 
> 
> PLEASE EXPLAIN WHAT CAN I DO with the above mentioned? 
> 
> ~aspiring software developer",54vp2w,t3_54vp2w,PMMEYOURHELP,,Comment,3,0,3
54urwa,2016-09-28 00:51:03-04:00,yzhang1337,Learning Python and MATLAB simultaneously?,"Hi guys!

As someone who has taken a single course in Python and am familiar with loops, dictionaries, (basics?) etc.. and has some experience with MATLAB just from a data analysis perspective but mainly worked within GUIs..........

I want to learn both simultaneously! MATLAB for research purposes, Python for personal/fun purposes.

Is there any proper way to go about this? Is there a high degree of transfer between both? 

Lost but very interested person looking how to best approach learning how to be comfortable with code!

Thanks :) ",,,,,Submission,3,0,3
d8568jy,2016-09-28 01:56:19-04:00,_karmawhore,,"In my experience with both languages, there's not a lot of commonalities.

MatLab is very unique compared to your ""typical"" languages, in my opinion.

However, it's a great idea to dive into whatever it is you're interested in. My suggestion would be to start personal hobby projects in both language and learn as you go.",54urwa,t3_54urwa,yzhang1337,,Comment,3,0,3
d85g9fj,2016-09-28 09:39:56-04:00,mackaber,,"If you have no restrains in using another thing instead of MATLAB, I recommend you learning Sage instead --> http://www.sagemath.org, Sage is similar to MATLAB in its purpose, but uses Python as it's language instead.

I have nothing against MATLAB, but I always prefer using open software and another advantage is that you can use it online here --> http://cloud.sagemath.com/ and even work collaboratively like with Google docs...",54urwa,t3_54urwa,yzhang1337,,Comment,1,0,1
d86lu7s,2016-09-29 02:25:03-04:00,Cheeseologist,,You should try asking /r/askprogramming. They might have answers too.,54urwa,t3_54urwa,yzhang1337,,Comment,1,0,1
54u15h,2016-09-27 21:34:58-04:00,rockitman12,Building mouse-sharing software exclusively via a hard link (cable) and custom protocol?,"I just posted this on StackOverflow, but it looks like it's quickly on it's way to being closed.

""I've spent hours trying to get even a brief introduction on creating a mouse sharing network, searching for the ""How To's?"" and ""How do they work?"" All I've come up with is endless pages pointing to [Synergy](http://synergy-foss.org) and [Share Mouse](http://www.keyboard-and-mouse-sharing.com).

One reason for my trouble is likely that I'm not familiar with the appropriate jargon to get me to the right places. The best I've gotten is that they usually (or always?) use a virtual KVM desktop, but beyond that I've hit a wall.

I am trying to set up the sharing between a Windows PC and Mac Pro. The two systems share multiple screens, and different screens can be dynamically allocated to either system, on an as-needed basis.

I would like to try to roll my own mouse sharing software (along with file, etc), but with a few extra caveats over the current options available:

* #####BOTH systems can immediately take the roll of server (or share the load at all times)

    Currently, mouse sharing software assigns one system as a server, and the rest as clients. I won't always be using both systems, and might shut one off at a moment's notice (or kick the power cord, or whatever). With current mouse sharing, if the server gets shut off, then the clients are all without keyboard and mouse.

    For my own software, I'd like to maintain uninterrupted functionality at all times.

* #####No WiFi -> Connection exclusively through a hard link (cable)

    Again, current options can allow some finagling to allow crossover cables to be used between the systems. This has the drawback that only one system is truly connected to the internet, and the other is piggybacking off of it.

    For my own, I would like both systems to have ethernet modem connections at all times (again, in the case that I'm using one or the other, or maybe one is down). The particular connection is not a huge concern - I'm happy to connect the two through a crossover cable, or USB, or thunderbolt or whatever.

    An additional reason for the hard link between the two is speed. I don't want any lag between the two screens when the cursor crosses over, and I would like to have the most optimized environment for sharing clipboard and drag/drop files. WiFi is undesirable, and I'm sure not all connection options are created equal.

* #####No Virtual Machines (If possible...)

    In the interest of not including more than is absolutely necessary, I was wondering if there can be a more direct protocol created between the two systems, opposed to having to mount a virtual environment, and pass everything through that? This goes back to speed - I can't imagine that a virtual environment would be anywhere near as quick as a direct system-to-system protocol.

My reason for wanting to roll my own is to get the exact functionality I seek - with nothing more and nothing less - as well as to improve my coding prowess. Whether or not this is the case, I also feel like I have a safer setup when the two systems are not sharing the same internet connection or communicating through WiFi; hard links are reassuring, and I have more control on what can be permitted through the connection.

I'm sorry that I don't have any code to include in this, but I honestly don't know the first place to begin. There doesn't seem to be a lot of readily available resources on the topic, and my ignorance with this kind of coding/integration is a major handicap.

Any pointers, topics to look into, existing similar project and so on, are all welcome. I'll take anything I can get.",,,,,Submission,1,0,1
d852wou,2016-09-27 23:54:53-04:00,gstuartj,,"I don't have low-level details for you, but I feel like I recall Synergy's original virtual desktop code/input tracking being based on a gutted VNC server/client with the display parts removed.

> * BOTH systems can immediately take the roll of server

I don't think this is possible purely in software. If your keyboard and mouse are plugged into the server, it has to be operating to send the mouse/keyboard inputs to the clients. Without the server sending those signals you have no data stream, so no input. You would need an independent piece of hardware to accomplish this in effect, but that hardware would still be acting as a server.

> * No WiFi -> Connection exclusively through a hard link (cable)

You can point the software at an address that can be routed only via the ethernet interface. You can also specify an interface/route when you're making your network calls. (That could vary by platform/tool.) I think a safer way to handle this might be to use encryption so you don't have to worry about the network. If you send it over SSL/TLS or similar then it doesn't matter much if you use WiFi/a shared connection.

> * No Virtual Machines (If possible...)

Virtual machines aren't necessary. I'm wondering if you might be confusing [KVM](https://en.wikipedia.org/wiki/Kernel-based_Virtual_Machine) (Kernel-based Virtual Machine) with [KVM switches](https://en.wikipedia.org/wiki/KVM_switch), which are generally used to share monitors and input devices across machines. Maybe your research indicated you needed a virtual KVM switch, not a virtual machine? You could even consider buying a hardware KVM switch.

Either way, good luck with your project.
",54u15h,t3_54u15h,rockitman12,,Comment,2,0,2
d8olu60,2016-10-12 06:44:51-04:00,rockitman12,,"Thank you for your input! Sorry for the late reply.

I won't be taking this project on for a while, but I've been thinking about the ""either take control as server"" thing, and the best option I've come up with is to use a third *computer* solely as the server.

A Raspberry PI (or similar) is super cheap, and can be used as the server. That way, it won't matter if either other computer is turned off or removed. Don't include the WiFi module on the server unit, and you've got yourself an offline (i.e. secure) solution.",54u15h,t1_d852wou,gstuartj,,Reply,1,0,1
54txak,2016-09-27 21:09:45-04:00,cronos844,Absolutely lost as to what to do.,"So I'm trying to program a textbox into displaying ""a b c d e f g"" all across and down when I click a button. From what I can tell from microsofts' website, this means I have to use a char, but I can only get one character inside it. Is there another way to pad it?",,,,,Submission,1,0,1
d84x147,2016-09-27 21:14:51-04:00,tyggerjai,,"Probably better in /r/learnprogramming. It would be useful, when you post there, to include language, platform, and a link to the code you have (on gist or pastebin) with enough info to actually compile and run it, or at least a more extensive description of what you've tried. ",54txak,t3_54txak,cronos844,,Comment,5,0,5
d84y509,2016-09-27 21:42:55-04:00,cronos844,,"
Visual Studio 2015 C#",54txak,t1_d84x147,tyggerjai,,Reply,1,0,1
d84yblx,2016-09-27 21:47:50-04:00,tyggerjai,,"/r/learnprogramming

gist/pastebin

Are you using Windows forms? Have you discovered Textbox? ",54txak,t1_d84y509,cronos844,,Reply,1,0,1
d84yfk9,2016-09-27 21:50:42-04:00,cronos844,,Windows form and I have a textbox on it with buttons.,54txak,t1_d84yblx,tyggerjai,,Reply,1,0,1
54t6xx,2016-09-27 18:20:21-04:00,Emperor_Duckbutter,Anyone with experience with serial ports? I'm having trouble communicating with a Roomba and I'm not sure where I'm going wrong.,"I have a Roomba Red that I'm trying to control via the Serial Command Interface. It was manufactured in 2006, so it must have one. The Roomba has a 7-pin female connecter, but I used an 8-pin cable connected to a Silicon Labs CP210x USB to UART bridge (basically a Roostick) and installed the driver from the SL website. From my Device Manager, I set the port's baud to 57600 bps as that is what the Roomba communicates with. I'm not very familiar with terminals and code, so after trying a serial connection with PuTTY (not sure if my syntax was actually sending the serial operation codes), I began trying RealTerm. I get no response with that as well despite having all the right parameters for parity and flow control. Please, if there is something fundamental I am not understanding or doing, I would appreciate advice or a sense of direction. It could even be that the computer I'm using, a Optiplex 360 with Windows 7, isn't capable of that high of bit rate. Thank you.",,,,,Submission,3,0,3
d85287x,2016-09-27 23:34:14-04:00,TheGrue,,"Any PC capable of running Win7 won't have any issue with serial at 57.6kbps.  

I don't know much about the interface for the Roomba, but I have encountered plenty of cases when working with serial ports where the pinout of the remote device is not the same as a standard RS232 DB9.  

Pinout is a mapping of which physical wires in the connector are used for PC Transmit, PC Receive, signal ground, etc.  Even in some cases where the device expects a DB9 connector, the pinout isn't the same as a standard DB9 line from the PC.

After a touch of googling, it seems that the pinout for Roomba is a bit different, and you will probably have to craft your own converter, thankfully it appears that Make has you covered:
http://makezine.com/2008/02/29/how-to-make-a-roomba-seri/

(P.S. This isn't really a Computer Science question - CS is more about applied mathematics than actual physical computers or specific applications.  But we're all geeks here, so I hope this helps.)
",54t6xx,t3_54t6xx,Emperor_Duckbutter,,Comment,4,0,4
d8694ww,2016-09-28 20:04:06-04:00,Emperor_Duckbutter,,"Thanks for your response and help! Yeah, even though the Roomba has 7 pins, I've been recommended 8 by people who managed to hack theirs. The UART-USB bridge and the port the driver made should have been enough to connect the interface to a terminal like PuTTY or tool like RealTerm, so I'm inclined to think either the interface isn't working properly or the bridge. If it's the bridge, then I definitely will need to use that link you shared.

Edit: thanks also for the advice about this sub, I wasn't sure what a good candidate would be to share potential software and/or hardware issues.",54t6xx,t1_d85287x,TheGrue,,Reply,1,0,1
54shc5,2016-09-27 15:57:30-04:00,Specialisedshape,Getting onto an CS Msc (masters) without a relevant degree possible?,"i have a 2:1 BA honours in computer games and animation.

What do you think they would be looking for in an application that might sway them despite me not having a cs degree??",,,,,Submission,2,0,2
54smik,2016-09-27 16:25:37-04:00,Rooooooook,Automating extremely manual process?,"Question for you compsci geniuses.  
I have an extremely manual process that I have to go through quite often at work. Bare with me while I try to explain this.
I manually have to populate unique information in a website (case management [ServiceNow/Salesforce type system])hundreds of times because I cant figure out how to automate something that has unique info every time.

Essentially I am doing this step by step:    

In Firefox, Clicking Create Case.    

Copying information from Excel and pasting it in a field. (info changes on every case; column A: line 1) (When doing this the next time, it's Line 2)   
  

Copying information from Excel and pasting it in a field. (info changes on every case; column B: line 1) (When doing this the next time, it's Line 2)    

Clicking enter button  

Copying information from Notepad or Word and pasting it in a different field. (DOES NOT CHANGE on case to case basis)  
  
Clicking enter button  
  
Clicking Save button  
  
Done  
  
I initially thought about just setting up a simple macro in macro express as I have almost no coding knowledge, but that doesnt help me with the fact I have unique information that needs to get entered depending on the case. I have to run down the list 1 by 1 doing this.  
  
Does anyone have a solution or something someone with very little to no coding experience can handle?",,,,,Submission,6,0,6
d84nfzg,2016-09-27 17:17:01-04:00,umib0zu,,"Depends on what you define as solution. You probably aren't going to get a fix that is a simple plug-and-play since it's likely your company is behind a firewall, or your website is password protected. Yes you can automate this process by coding, but you'll likely need to code it yourself or hire a developer.

If you're willing to learn how to code, [start with this book](https://automatetheboringstuff.com/). If you're willing to hire someone out, PM me!",54smik,t3_54smik,Rooooooook,,Comment,6,0,6
d84uxxq,2016-09-27 20:22:29-04:00,heyitsvini,,"Hey! I'm interested in doing something similar to OP's idea but simpler. Also I don't have any knowledge on coding, but here's what I wanted to do:
1- Computer would automatically turn on everyday at a specific time (searched poorly about that. Is that really possible? By BIOS settings?) 
2- A macro or something automatically opens and it would open a game, type my password and click a couple of buttons. 
3- It would stay turned on for about 70 minutes and then turn off (automatically). 

Is there any software that like that? Do you have any ideas? ",54smik,t1_d84nfzg,umib0zu,,Reply,1,0,1
d84vz85,2016-09-27 20:48:38-04:00,Nelgraf,,"I really like https://autohotkey.com/ for this kind of problem. You need a little bit of scripting knowledge, but nothing too crazy. It's really good at tasks like ""click this x/y coordinate on the screen and then type this, after that go click this button"". additional benefit is you can bind the macro to a hot-key combination to run it quickly and often. 

here is the tutorial to get started - https://autohotkey.com/docs/Tutorial.htm ",54smik,t3_54smik,Rooooooook,,Comment,2,0,2
d84n4mt,2016-09-27 17:09:54-04:00,metaobject,,"It depends on which courses you took in your undergrad.  If they decide to admit you, and they feel your deficient in an area or two, you may have to take a few courses to fill in your background, etc.

Have you taken an Algorithms and Data Structures course?  How about your math background?  Up to Calc3?  Stats?  Numerical Analysis?",54smik,t3_54shc5,Specialisedshape,,Comment,1,0,1
d84v6da,2016-09-27 20:28:21-04:00,Walf9,,Algorithms and Data Structures course's are key to computer science. ,54smik,t3_54shc5,Specialisedshape,,Comment,1,0,1
54noe0,2016-09-26 19:41:09-04:00,St_OP_to_u_chin_me,Should I upgrade my Dell 6420 to Windows 10?,"I use it for personal experimentation with windows because I mainly use Mac. So like simple programming stuff and it's not for gaming or anything fancy really.

I just needed a PC that didn't belong to work that I could screw around with. That was cheap.

Oh it's currently Windows 7 Pro 64
I'm afraid it will be slower.",,,,,Submission,0,0,0
54mt7k,2016-09-26 16:44:20-04:00,jnzq,C++ Macros Question,"Can someone please explain the difference between a macro and a function

I understand that a macro is preprocessed before compilation in that [what?] goes in and replaces [is it every macro instance?] with [what kind of value?]. Whereas a function is simply compiled as it is and runs as is. Is this correct?

Could someone help me fill in the brackets to help me understand the details of what really happens under the hood?",,,,,Submission,6,0,6
d838ftq,2016-09-26 17:51:55-04:00,dandrino,,"Macros are just text substitution at compile time. Functions are blocks of code that at runtime are branched-to and executed.

For instance:

    #define ADD(a, b) (a + b)
    int add(int a, int b) { return a + b: }
    std::cout << add(2, 3) << std::endl: // Results in a call to the add function (i.e. follows C function call conventions).
    std::cout << ADD(2, 3) << std::endl: // Results in ""std::cout <<  (2 + 3) << std::endl"" being compiled, no function call occurs

Does that make sense?
  ",54mt7k,t3_54mt7k,jnzq,,Comment,3,0,3
d83eu4w,2016-09-26 20:26:46-04:00,jnzq,,Oh okay. That helps a lot. Thanks!,54mt7k,t1_d838ftq,dandrino,,Reply,1,0,1
d844itb,2016-09-27 10:31:20-04:00,lordvadr,,"It's actually part of the C preprocessor, which is what expands...well...all the preprocessor macros (like #include, or #define, or #if etc).  The c++ preprocessor has a couple features that the C one doesn't, but they're otherwise essentially identical.

> before compilation in that [what?] goes in

The preprocessor

> is it every macro instance?

Yes.

> what kind of value?

Usually the literal value, but there's support for function-like macros.

----

    #define DEFAULT_VAL 4
    
    cout << ""The value is "" << DEFAULT_VAL << ""\n"":

becomes:

    cout << ""The value is "" << 4 << ""\n"":

But, there are function like macros...

    #define MAX(a,b) (b<a?b:a)

    cout << ""Max of 3 and 4 is "" << MAX(3,4) << ""\n"":

becomes:

    cout << ""Max of 3 and 4 is "" << (4>3?4:3) << ""\n"":",54mt7k,t3_54mt7k,jnzq,,Comment,2,0,2
d836peq,2016-09-26 17:12:30-04:00,MCPtz,,"Seriously do an internet search on this question.

https://stackoverflow.com/questions/4990362/what-is-the-difference-between-a-macro-and-a-function-in-c

https://stackoverflow.com/questions/653839/what-are-c-macros-useful-for?noredirect=1&lq=1",54mt7k,t3_54mt7k,jnzq,,Comment,0,0,0
54mn14,2016-09-26 16:12:26-04:00,mrmovealot,"(UK) 25 yr old wanting to study computer science, is it possible or even worth it?",,,,,,Submission,9,0,9
d833vzk,2016-09-26 16:12:30-04:00,mrmovealot,,"I scraped a pass in A-level Maths, Biology and Chemistry (when I was 18). I'm 24, almost 25 years old. I've got mental health and physical health issues but despite that I really would like to have a great IT/computer science career (be that in networking, developing, database administration...). I've heard a few negative things about IT certifications which put me off a little and I think a degree would allow me to switch roles in the fields more easily as compared to specific certs.

I'm just lost. Should I resit my A-level exams in June? Should I apply for a foundation degree? Also, i'm worried about being 29 and still in university with no friends (shout out to social anxiety).

Any advice would be much appreciated.

Thank you. 

(If i've posted to the wrong subreddit, please redirect me to a more fitting one.)",54mn14,t3_54mn14,mrmovealot,,Comment,7,0,7
d8345tt,2016-09-26 16:18:08-04:00,DementedPanda69,,"Anyone and everyone can make friends at university, and they do. There are societies for literally everything so many opportunities to make friends. 

As for the course go for it, I know every university is different but the university i'm currently at provides so much support, they teach you everything starting at the very basics so if you have no other IT qualifications you would be fine. And usually universities let you switch courses within the first few weeks. 

It really depends on your own ambition and standards as to whether you want to resit your A-levels or not.. it would help you get into a better university and if that is just an extra year, personally i would do it. But obviously you are your own person. 

Good luck bud",54mn14,t1_d833vzk,mrmovealot,,Reply,6,0,6
d83fxpo,2016-09-26 20:52:32-04:00,ThePoundDollar,,"> Anyone and everyone can make friends at university, and they do. There are societies for literally everything so many opportunities to make friends.

Just like to add that this isn't as easy as it sounds for people with Social Anxiety. The fear of being part of a group where you're essentially forced to socialise can be very scary. There's a fear of rejection too, which is also a big put off.

Not trying to put OP off or anything, just wanted to make this a little clearer. Sometimes it's a matter of having the willpower and courage to take a leap and well, basically, just...wing it. If it goes well, hooray, if not, maybe it'll work out another time. But again, taking that leap is A LOT to ask for someone with SA.

Me personally? I haven't gotten to taking that leap yet, but maybe someday I will. Right now, I'm just happy on my own most of the time.

For u/mrmovealot, yeah, there's bound to be tons of societies at the university he goes to. More importantly, there's bound to be a society that he's interested in, which makes making friends so much more easier and natural. So I say go for it, you don't know what you're missing out on!",54mn14,t1_d8345tt,DementedPanda69,,Reply,2,0,2
d834bwc,2016-09-26 16:21:38-04:00,mrmovealot,,Thank you for that. I feel much more positive about it now.,54mn14,t1_d8345tt,DementedPanda69,,Reply,1,0,1
d83fvfo,2016-09-26 20:51:07-04:00,NearSightedGiraffe,,"While I went straight from high school, many of my mates at uni took 1-15+years off before starting. There are plenty of ways to meet people and make friends at uni. Because I have hung around for a second degree, I have had to go through the process a couple of times- once without the benefit of the same school year- and it always feels awkward to me, as I am not a fan of talking to strangers. Nevertheless, as the above poster said, there are clubs for all parts of interests that make it easier. Also, something as simple as asking how people are going on assignments, or asking if people want to meet up to work on a project together, is a start. Many of the friends I have I met in early years by working on a group project for uni with them still keep up with each other. We got a long, and now keep up separately from that topic we all passed back then. 

Also, note that not everyone at uni is inclusive. This does not matter- there are so many people here that you only need to get along with a couple of the majority who are inclusive to be set.

Tl/dr: uni is a mixing pot that makes it easy to connect with people on interests regardless of age difference. 

Edit: grammar",54mn14,t1_d833vzk,mrmovealot,,Reply,3,0,3
d841y6k,2016-09-27 09:24:03-04:00,amishbrobot,,"I started my Computer Science degree at age 26....I'm in my final year now and will be 29 when I graduate.

I've always been interested in Computer Science but haven't had much succes at math or teaching myself to program in the past.  Learning to program here was fantastic, it's a completely different experience when you have tutors and peers to help you.  I sometimes struggle with the the math but the more you do it, the better you get :)

Making friends isn't hard at all, I would suggest living in the university halls of residence in your first year as this was what formed the foundation of my friendships here.  While its true that some people might treat you slightly differently for being older (not necessarily in a bad way), you will find that the people who really matter don't :)  I noticed you said you have social anxiety, I cannot know how bad it is for you but I get nervous around groups of strangers too.  Alot of people on my course are fairly introvert, so I'm sure you will find people similar to yourself that would rather a have small group of similar minded friends than a larger, more intimidating group

I'm not going to tell you whether or not you should go for it, as it is a huge life decision, taking into considering things such as student debt etc.  All i can tell you is that I wasn't happy with the way my life was going at 26, I always wanted to work in this industry so I took the leap of faith and I am so glad I did.

I had some of the same concerns you do, and still have concerns about gaining employment at 29 when I leave.  But I think this is one of the rare industries where your skills matter more than anything, I know lots of people who are great at what they do and don't even have degrees!

Good luck in your decision mrmovealot, and if you have any other questions for someone who was in a similar position to what you are now, I would be happy to answer",54mn14,t1_d833vzk,mrmovealot,,Reply,2,0,2
d83c88f,2016-09-26 19:22:50-04:00,Aganomnom,,"Okay, soo0o. I don't know about the resitting, or whether it's worth it to just dive straight on in. It would depend on the university you want to go to.

But getting there to study next year or the one after? 

Totally, definitely, absolutely, YES!

I've known a lot of people at uni. From 17 through to mid 40s for undergrads (and I'm sure there are older folks, but that just who I was friends with!)

If you worry about your age and *you* let it become an issue, then it might be. But nobody else will give a crap.

As ever, it's a great time to go do things. And those ""things"" is a great place to meet new interesting people. Clubs and societies are the single best thing about uni.

Basically: yes. Do it. Join whatever you want to. Make friends. Enjoy!

I would do it all again now (at 26), without a second of doubt.",54mn14,t3_54mn14,mrmovealot,,Comment,5,0,5
d840q2h,2016-09-27 08:46:11-04:00,regimental,,"I'm 37, a full time parent to 3 kids and I work part time at Lidl. i'm just about to start my final year of a computer science degree. 
Whats your excuse?",54mn14,t3_54mn14,mrmovealot,,Comment,4,0,4
d841e53,2016-09-27 09:07:33-04:00,GunslingerJones,,"I'm 25 now and about to move up to Sys Admin at my company. I attained a B.S. in Computer Science at my local state university.

I did not go for any certs or any extra exams/degrees/etc. So, in my PERSONAL opinion, spending the extra time and money on certs is not that important. Do it, go get your degree, it pays off in this field unless you already have established connections.

Don't be afraid of going to school. I am a total introvert, but I made friends very easily. Anybody can in a college environment. I was shooting shit with 40+ year olds in my classes who were there for the same reason as yourself, being 29 when you graduate isn't a big deal so don't sweat it.

Another strong reason to attend a university is to network and meet people who are also in your field of study. I probably wouldn't be in the position I am now if my fellow CS buddy hadn't pushed me to apply for a tech job while still in college which kickstarted my career. You'll make friends that can help you get where you want to be (but don't forget to return the favor!). 

Also, enjoy your college experience, I wish I had indulged more. I commuted and really didn't stay on campus for anything other than classes. It is my biggest regret. The environment is teeming with things to do and learn, and it will really help if you make yourself more approachable and social - what better place is there for that than a campus full of likeminded people!?

Trust me, you want to break out of your shell and work on your social anxiety. I had it bad too, and still do at times, but nothing sells yourself like being a good talker. Don't underestimate how much a potential employer judges your social skills during an interview. Good luck!",54mn14,t3_54mn14,mrmovealot,,Comment,2,0,2
d83x3ea,2016-09-27 06:01:05-04:00,pope4president,,"Sure.  I became a software engineer at age 39.  I studied business in college, and previously worked as a headhunter (aka nothing remotely close to programming).  It turns out the trick is...  learn how to program.  It's a grind, but you can do it if you want it bad enough.",54mn14,t3_54mn14,mrmovealot,,Comment,2,0,2
d8b6af7,2016-10-02 15:55:50-04:00,gyroda,,"A bit late, but always check out /r/cscareerquestions. 

It's not too late. You can make friends no problem, just join a bunch of societies. With postgrads, PhDs and so on you won't be that out of place. ",54mn14,t3_54mn14,mrmovealot,,Comment,1,0,1
54kgro,2016-09-26 09:04:15-04:00,Zombieklopper,How much of the latency is owed to each network layer?,How big is typically the role of each layer in regards to latency? Are there any studies which layer delays the most in standard/optimal environments? E.g. apart from physical distance is the medium access or the routing the slower layer?,,,,,Submission,11,0,11
d82t9on,2016-09-26 12:30:41-04:00,-Hegemon-,,"Do you mean a simple data transfer or involving processing?
Because if you don't just transmit data but also need to process it on layer 7, every calculation goes out of the window.

",54kgro,t3_54kgro,Zombieklopper,,Comment,4,0,4
d8407rg,2016-09-27 08:28:40-04:00,Zombieklopper,,Just the data transfer itself. Let's say from layer 4 downward.  ,54kgro,t1_d82t9on,-Hegemon-,,Reply,1,0,1
d83rtoz,2016-09-27 01:25:10-04:00,TheTarquin,,"The short answer is: it depends heavily on the network and application, but that it's usually bound by either application-layer rendering, or number of round trips (hereafter RTTs), reduced by the amount of parallelism that's possible.  RTTs these days often can't be made faster, (networks have gotten smart and fiber isn't getting any faster) and so have to be reduced in number.  While protocols have some control over this, it's often application-layer concerns that determine the number of RTTs and how many can be processed in parallel.

Some things that are usually not bottle necks are encryption/TLS overhead (especially on modern networking stacks and modern processors), ""fetch"" attempts for intelligently cached resources, network fragmentation, routing, DNS, etc. except in pathological edge cases that should be diagnosed separately.

The long answer is: https://www.amazon.com/High-Performance-Browser-Networking-performance/dp/1449344763/ref=sr_1_1?ie=UTF8&qid=1474953613&sr=8-1&keywords=high+performance+browser+networking",54kgro,t3_54kgro,Zombieklopper,,Comment,2,0,2
d840aro,2016-09-27 08:31:39-04:00,Zombieklopper,,"Thanks for the answer! Do you have any numbers for wifi networks without application processing, just the transfer? How important is the routing vs mac protocol?",54kgro,t1_d83rtoz,TheTarquin,,Reply,2,0,2
d84a72g,2016-09-27 12:37:36-04:00,TheTarquin,,"They'd both be totally swamped by higher layers (e.g. a single TCP handshake will take MUCH longer than either).  Considering individual residential routers can reliably route up to 1Gb/s

ARP resolution usually takes on the order of single milliseconds and doesn't need to happen that often unless your gateway changes a bunch.

Honestly: both will be miniscule once you consider the entire OSI stack, and both are probably on the order of a few milliseconds or less.

But they'll also change heavily depending on network topology and usage.

You can profile most of this stuff on your own network if you want, using some tools like wireshark (for ARP), FlowScan (not sure if there's new hotness these days, been awhile since I used it), or just writing your own light-weight bash script and using the time(1) command on Linux, if you don't need super accurate fidelity.",54kgro,t1_d840aro,Zombieklopper,,Reply,1,0,1
54i7w6,2016-09-25 21:14:29-04:00,1KillerMidget,Help!,"I'm a senior in high school and I've been trying to research the difference between cs and informatics but I can't find many sources telling them apart. I'm looking for a degree that will better suit me for a career in information/data security analyst, computer systems analyst, or cyber security. Many college websites that I've  visited have conflicted each other, saying informatics, cs, or cis are the best degrees for those careers.",,,,,Submission,3,0,3
d824gef,2016-09-25 22:04:31-04:00,ZeroCool2u,,"Anything you can do with Informatics, you can do with CS. The other way around is not true. Not that informatics is easy, but CS is more difficult. ",54i7w6,t3_54i7w6,1KillerMidget,,Comment,3,0,3
d8254ei,2016-09-25 22:21:30-04:00,1KillerMidget,,So it'd probably be better to go for a cs degree?,54i7w6,t1_d824gef,ZeroCool2u,,Reply,2,0,2
d826tc1,2016-09-25 23:05:05-04:00,ZeroCool2u,,"In general, yes. Of course, I'm biased to an extent, but in my honest opinion, I'd still say this is a fairly objective argument. ",54i7w6,t1_d8254ei,1KillerMidget,,Reply,3,0,3
d82bbpo,2016-09-26 01:35:56-04:00,GreyMX,,"Doing a CS degree will ensure that you're more well-rounded, which is a better position to be in. During your CS degree, you'll be able to choose courses that focus on informatics and cyber security, so you'll still be able to ensure that you're well prepared for your desired career.

In my (admittedly limited) experience, Informatics is usually treated as a minor to supplement other majors, or it's like a CS-lite degree for people who want to do data science but might not be able to handle the behemoth that is Computer Science. If you consider yourself capable, then you should choose CS.

However, to be fair, my experience might not reflect the way it is everywhere. To be certain about your decision, it's always a good idea to inquire about the specific programs at the specific universities that you would be considering.",54i7w6,t1_d8254ei,1KillerMidget,,Reply,2,0,2
d82aqqf,2016-09-26 01:12:32-04:00,achNichtSoWichtig,,amazing job with your titel bro :-).,54i7w6,t3_54i7w6,1KillerMidget,,Comment,-2,0,-2
54hyki,2016-09-25 20:13:33-04:00,chick3234,ELI5: Affinity Propagation,"Hey, I am trying to self teach myself about clustering algorithms and I understand most of them, however I cannot wrap my head around how affinity propagation works, could somebody please explain to me in simple terms how it works?",,,,,Submission,3,0,3
54fcjs,2016-09-25 10:37:34-04:00,NokiaAnotherOne,What kind of math should I know before going for a CS degree?,"Hello, in about a year I will be starting my CS degree and my knowledge of math is not poor but I'd say it's below average for a person who wants a CS degree. I'd like to know what i should learn before going for it and if you have any helpful links please put them in. Thanks! :)",,,,,Submission,12,0,12
d81buja,2016-09-25 10:49:33-04:00,systemnate,,"I was out of school 7 years before I went to college. Typically calculus would be the first math to take, but I had to take algebra, trig, and pre-cal first to get up-to-speed since it had been so long.",54fcjs,t3_54fcjs,NokiaAnotherOne,,Comment,15,0,15
d81f1vj,2016-09-25 12:17:57-04:00,ifwinterends,,"Linear algebra is the most important if you'll be doing any kind of 3D stuff (transforms, matrices, dot products, cross products). At my school that was taught in Calc 3.",54fcjs,t3_54fcjs,NokiaAnotherOne,,Comment,11,0,11
d81dr8q,2016-09-25 11:43:47-04:00,xiroV,,"Don't worry. I'm pretty poor at math too, but I'll be finishing my B.Sc. soon. Somewhat I've managed to get through all the mathematical courses someway or another. If you want to prepare ahead, I would say that the most important area is probably algebra (especially Linear Algebra), Statistics/Probability and maybe Discrete Math.

Calculus is probably helpful too, but to be honest we had the Calculus course on our first year, and have only used a small part of it since then.

Disclaimer: I'm from Denmark so the degree of math may vary from your country.",54fcjs,t3_54fcjs,NokiaAnotherOne,,Comment,8,0,8
d81fyq2,2016-09-25 12:41:23-04:00,i_throw_rocks_,,"My program includes:

Precal I & II 

Cal I, II, III 

Linear Algebra 

Probability and statistics 

Discrete structures 


I did all of these once I started college. Prior to college, I had only taken algebra and geometry. Make sure you are extremely comfortable with algebra as you will be using it in most, if not all, of the above classes.

Check your program on your school's website. It should have a list of required classes and any prerequisites you may need prior to entering the program.",54fcjs,t3_54fcjs,NokiaAnotherOne,,Comment,6,0,6
d81lcf4,2016-09-25 14:41:24-04:00,FemaleAndComputer,,"Did you have to take a math placement test yet? Often unis make you do this to find out how many classes you need to take before calculus. I was out of school several years when I started back for cs, so really lacking in math dept. Just studied for a week or two before my placement test to review all the math I did in high school. Was able to get a good enough grade to go straight to calc 1.

Figure out what math class you have to take first, and study everything up to that. If you have to do a placement test, study everything up to trig/pre-calc if you don't want to be forced to take algebra and trig again. I don't love math classes, so naturally I wanted to limit the number of math classes I had to take. Also, wanted to decrease the cost of my education by avoiding the pre-reqs.

Good luck. Utilize your local library. I borrowed so many math books from them right before I started college.",54fcjs,t3_54fcjs,NokiaAnotherOne,,Comment,5,0,5
d81lvoo,2016-09-25 14:53:34-04:00,ThatsOkayToo,,"I dropped out of HS, then went and got my engineering degree nearly 20 years later. I had to re-do all math, starting with Math 70 (which was like algebra). Once I got up to speed, my first real math class in college was Calculus. So if you are up to Trigonometry before college, you will be fine.",54fcjs,t3_54fcjs,NokiaAnotherOne,,Comment,3,0,3
d82368s,2016-09-25 21:32:54-04:00,None,,">What kind of math should I know before going for a CS degree?

Arithmetic ",54fcjs,t3_54fcjs,NokiaAnotherOne,,Comment,2,0,2
d81uojw,2016-09-25 18:04:20-04:00,None,,"Have you watch this MIT course?

https://www.youtube.com/watch?v=L3LMbpZIKhQ

Side comment: Man, you guys are lucky when your CS degree is integrated with its mathematical requirement. In my uni mathematical knowledge application seems to be non-existence (only a little, like *very* little), even still so little on Algorithm course (e.g. we don't do proofing). Or is this normal?",54fcjs,t3_54fcjs,NokiaAnotherOne,,Comment,1,0,1
d82ljmr,2016-09-26 09:35:55-04:00,andybmcc,,"Going into it,  you probably want a very solid foundation in Algebra, and ideally a basic understanding of Calculus.  You'll probably be tested and placed appropriately.  Don't fret, it's not a big deal if you have to do some catch-up courses.  Worst case it extends the time to get your degree a bit, as they may be pre-requisites for required classes.  Not a big deal, a lot of people who come back after being out of school for a while need to take those courses as well.  It's just the nature of the beast.",54fcjs,t3_54fcjs,NokiaAnotherOne,,Comment,1,0,1
d828bbb,2016-09-25 23:48:18-04:00,autoshag,,"you really don't NEED any math for CS. I'm a 5th year CS student at the UofA, and did a year internship in industry, and all the math we needed was basic arithmetic. now having said that, CERTAIN areas will need some math. and certain maths will be useful. In my machine learning classes we use a lot of linear algebra, though it's nothing beyond a first or second year lin algebra class. We also use calculus, but again nothing beyond a first year calc class (just derivatives). if you want to do anything with graphics you'll probably need more linear algebra. 

What's more important is being familiar with logic. which isn't so much math in itself, but a way of thinking. Any good CS degree should teach you most of the math that you need. ",54fcjs,t3_54fcjs,NokiaAnotherOne,,Comment,1,0,1
54daol,2016-09-24 22:20:17-04:00,Jon_dhc,Drones in computer science research project ideas (UAV),"Hi, I'm currently on third semester of computer science in college. I want to do a research project about drones. The thing is, I don't really know how could I work in the project as a CSC student. I know the basics of programming but I'm willing to learn whatever is needed. One of my professors suggested Image Processing, or using the drones for 3D modeling of a place. I would like more ideas like that. I would get resources from the university and also I would do the research for a period of 2 years at least. Could you please give me some suggestions? Research topics involving drones. Sorry if I made some mistakes, English isn't my mother tongue. 
Thanks :)",,,,,Submission,3,0,3
d80wzlf,2016-09-24 23:17:32-04:00,AlexDGr8r,,"In my capstone course that I'm in right now, there is a group of students that are working on a project with drones and cattle herding. If I understand it, they basically are trying to keep track of cattle using GPS and drones. I wish I knew more details for you, but that's all I know.",54daol,t3_54daol,Jon_dhc,,Comment,3,0,3
d80y80w,2016-09-24 23:55:47-04:00,Jon_dhc,,"This is useful, it would be interesting to program the drones and it is actually involving image processing I think. I will mention the idea to my professor. Thank you! ",54daol,t1_d80wzlf,AlexDGr8r,,Reply,1,0,1
54cqxs,2016-09-24 19:51:35-04:00,asdfqwertylol,Regular expression X such that L(X)=L(r○X∪s),"How can I find a regular expression X such that the L(X)=L(r○X∪s), i.e. both X and r○X∪s have the same language? Whatever I chose X to be, I concatenate r in front of it, so I can't think of any way of the two having the same language, how can I approach this? Btw in this case r, s and X are all regular expressions and ○ stands for concatenation and ∪ for union. Thanks!",,,,,Submission,3,0,3
d80yfe7,2016-09-25 00:02:24-04:00,_--__,,"Observe that L(A\*) = L((ε∪A)○A\*) for any regular expression A, so if L(A) contains ε, then L(A\*) = L(A○A\*).  This should give you enough of a clue about how to solve it.",54cqxs,t3_54cqxs,asdfqwertylol,,Comment,2,0,2
54c9po,2016-09-24 17:52:50-04:00,Ractor85,C++ Employer Given Quiz - Help!,"Hello, I'm a freshman in a CSE program that started this fall. After a recent job fair, a company that I am very interested in sent me an email, saying that they were interested in me for an internship but that I would first need to take a quiz in C/C++.  
  
They are going to email me a text file with six questions in it in a few days, I have sixty minutes to answer all the questions and email them back. They said some people put the answers directly in the text file, others send a full program with test cases and the   
  
I have pretty much no experience in C or C++, but I have experience taking classes at a sophomore level last year in java (with stuff like LinkedLists), and I have a couple of personal projects in java. I have lots of java experience, but with this test format, what kind of things should I brush up on in C++ before I take the test?
  
  
I already have a minGW compiler working, using Notepad++. I have done a couple of basic programs, with things like loops and if statements. What else do you think I need to know? Is anyone familiar with this sort of test?  
  
Thanks so much! ",,,,,Submission,6,0,6
d80miwr,2016-09-24 18:16:23-04:00,PerpetuallyScrolling,,"One massive difference between Java and C++ is pointers and memory management. You'll most likely want to look up both C style (malloc/free) and C++ style (new/destroy). 

Its been a while since I've used Java so I can't remember all the differences but here are some things you might want to look up templates, preprocessor directives, and how typing works (when you do/don't need to typecast).

There are probably things I'm forgetting but I'll update this post if I think of anything else.
 ",54c9po,t3_54c9po,Ractor85,,Comment,3,0,3
d80mm61,2016-09-24 18:18:44-04:00,Ractor85,,"Great, thanks - I will.  
 I'm not really sure what to expect from a quiz like this, though. If the answers can be typed right into the text file or written as a full program, I'm currently thinking that it might be something like solving the quadratic formula, or other really basic stuff? Maybe just to filter out those not really interested?",54c9po,t1_d80miwr,PerpetuallyScrolling,,Reply,1,0,1
d80n3yx,2016-09-24 18:32:05-04:00,PerpetuallyScrolling,,"I haven't taken a quit like this before so this is only a guess but since it's only 10 mins per question I imagine it would be some rather basic stuff unless it's a big name company that expects the best of the best. Depending on the type of job it might be less about the specifics of C++ and more about general programming related problem solving done in C++.

Good luck, I hope it goes well for you!",54c9po,t1_d80mm61,Ractor85,,Reply,1,0,1
d80o4f7,2016-09-24 18:59:12-04:00,Ractor85,,"That's what I'm thinking as well.  

It's not really a big name company, but it is competitive, so I'm not sure. Anyway, thanks a lot for the help!",54c9po,t1_d80n3yx,PerpetuallyScrolling,,Reply,1,0,1
d80ujmn,2016-09-24 22:02:58-04:00,dragonnyxx,,I'd argue that for C++ it's even more important to understand RAII and smart pointers than new / delete. Modern C++ programs will have little to no manual memory management.,54c9po,t1_d80miwr,PerpetuallyScrolling,,Reply,1,0,1
d80vou7,2016-09-24 22:37:44-04:00,PerpetuallyScrolling,,My university really needs to step up their curriculum. I've taken a couple C++ classes and they never even mentioned RAII or smart pointers. I think its important to learn the basic bare bones stuff like new / delete but it would be nice if they actually taught us things that are relevant in today's job market.,54c9po,t1_d80ujmn,dragonnyxx,,Reply,2,0,2
d83jgze,2016-09-26 22:01:13-04:00,Ractor85,,"For anyone interested, there was some really simple functions and some pointer stuff on it. I think I did rather well. Thanks everyone",54c9po,t3_54c9po,Ractor85,,Comment,1,0,1
548zn1,2016-09-24 02:02:56-04:00,ebolanurse,Looking for resources for using VIM through putty? Just starting out.,"I'm trying to train myself on VIM. Particularly through my schools remote terminal via putty. 

I want to change the features and set defaults on the server but all the 'how-to's' seem to teach under the presumption that VIM is operating on the users local computer.

1. Does anyone have a good resource for customizing vim through putty.
and
2. Can someone tell me what the technical terms are for what I'm describing? Like I said I'm new and I'm not even really sure what key words I should be searching to answer my particular question.",,,,,Submission,6,0,6
d7zv8e6,2016-09-24 02:10:23-04:00,chasecaleb,,It doesn't matter whether you're running it locally or via ssh. It's the same thing. ,548zn1,t3_548zn1,ebolanurse,,Comment,11,0,11
d8007ev,2016-09-24 07:15:15-04:00,tolos,,"Some of this may not be new, but I'm going to start with the basics:  

- Your user account has a home folder, which is given an alias as the tilde character (~)  
- Files prefixed with a dot/period are hidden by default, but you can show them with `ls -a` (or `ls -la`)  
- Vim will look in a few places for a config file, but you just need to work with the one in your user folder. The path is `~/.vimrc`. This may or may not already exist.   
- If you're new to vim, I'd setup your config with another editor .... nano should be installed (`nano ~/.vimrc`)  
- Check out vimtutor to learn commands and things
- Google vim cheat sheet and find one you like. Make your own cheat sheet, but just write down commands you have difficulty remembering.  
- Don't worry about plugins or vim's weird tabbing system for now  

Having said all that, if you just want to learn vim, I'd install it locally. There are windows binaries (look for [gvim](http://www.vim.org/download.php#pc) or [direct link](ftp://ftp.vim.org/pub/vim/pc/gvim80.exe) ). There's nothing different about running vim remotely or locally other than the files you can access -- well, technically other differences, like `!` commands, but you shouldn't have to worry about that.",548zn1,t3_548zn1,ebolanurse,,Comment,4,0,4
d7zw4eo,2016-09-24 02:57:07-04:00,xiongchiamiov,,"Pretty much everything is just the same. Any config files you change will be the ones on the computer you're running vim on.

Personally, I find the chrome secure shell add-on to be significantly nicer than putty.",548zn1,t3_548zn1,ebolanurse,,Comment,1,0,1
547j16,2016-09-23 19:04:11-04:00,mrwnmonm,what do top universities CS students study?,"Hi,  
i am from Egypt, i was graduated from CS college here in Egypt  
but it just worthless, i find myself don't know fundamental subjects, and don't really understand what i am dealing with  
so i decided to restudy what i should have learned in the college

so what institutions like MIT or Harvard CS departments are teaching?  
if there is something like a list of classes and books used for them, please refer me to it  

thanks",,,,,Submission,17,0,17
d7zix4w,2016-09-23 19:15:33-04:00,MagnificRogue,,"MIT [Open Course Ware](https://ocw.mit.edu/courses/electrical-engineering-and-computer-science/) is a fantastic resource for this. A lot of the content is free, some of which include lectures and assignments.",547j16,t3_547j16,mrwnmonm,,Comment,17,0,17
d7zl6wy,2016-09-23 20:20:48-04:00,mrwnmonm,,"thanks for your reply  
what does ""Graduate"" level means? does it mean that this courses are for students who are already graduated, and doing master degree for example?",547j16,t1_d7zix4w,MagnificRogue,,Reply,7,0,7
d7zlh90,2016-09-23 20:29:27-04:00,TheBlackVista,,Yes. ,547j16,t1_d7zl6wy,mrwnmonm,,Reply,4,0,4
d7zugnv,2016-09-24 01:34:26-04:00,zorkmids,,"A few from (old) memory: data structures, algorithms, CS theory, operating systems, databases, programming languages, artificial intelligence.  Supporting subjects often include electrical engineering (e.g. digital circuit design), discrete math (e.g. logic, algebra, set theory), calculus, various sciences like physics, and various engineering classes.  Hope this helps.  

Note that there's a big difference between education and skills.  You don't need all that to practice CS.
",547j16,t3_547j16,mrwnmonm,,Comment,7,0,7
d806fyu,2016-09-24 11:14:51-04:00,rhiaaryx,,"You don't need all of it, but learning at least one programming language and how to use data structures/algorithms is pretty essential (if I had to pick from that list).",547j16,t1_d7zugnv,zorkmids,,Reply,3,0,3
d7zmf4p,2016-09-23 20:58:10-04:00,epakai,,"Harvard has this [guide](http://cdn.cs50.net/guide/guide-14.pdf) which gives a good overview of the general study on pages 6 and 7. Page 10 has a further breakdown of courses that are more focused.

Searching for degree plans can yield decent lists. I'm going to reference the University of Texas at Dallas [CS Degree plan](http://catalog.utdallas.edu/now/undergraduate/programs/ecs/computer-science) (undegraduate/Bachelor's degree) because it's a straightforward list and they also have brief summary's of each course if you mouseover the links which might be helpful if you're trying to get a handle on what topics are covered (and I'm a UTD student).",547j16,t3_547j16,mrwnmonm,,Comment,5,0,5
d7zqio1,2016-09-23 23:07:04-04:00,mrwnmonm,,thanks very much,547j16,t1_d7zmf4p,epakai,,Reply,2,0,2
d806hkb,2016-09-24 11:16:07-04:00,rhiaaryx,,There's a coursera course starting right about now on data structures and algorithms from Princeton. I'd link but I'm on mobile. That'd be a good place to start.,547j16,t3_547j16,mrwnmonm,,Comment,1,0,1
d807m88,2016-09-24 11:46:41-04:00,Filmore,,"Harvard.... cs? Yeah they must not be teaching you much. 


Just FYI, a huge portion of the US college system value  comes from a combination the alumni network and the culture imprint it leaves on you.",547j16,t3_547j16,mrwnmonm,,Comment,-2,0,-2
546soo,2016-09-23 16:27:39-04:00,LostInOttawa,Proving 1^k + 2^k... + n^k is Big Omega(n^(k+1)),"Was just in class and the professor was proving the statement in this post's title. 'k' is treated as a constant.

He went about it by using the formal definition (finding a constant 'c' and a number 'n0' such that 1^k+2^k...+n^k is always greater than n^(k+1)). I couldn't follow what he did but it seemed to involve repeatedly halving the sum and ending up with something like c(n/2)^k.

If anyone has any insight on how to approach this problem please let me know, I'm drawing blanks and the teacher wasn't clear in conveying their approach.

Thanks",,,,,Submission,5,0,5
d7zgbg1,2016-09-23 18:05:33-04:00,PlayMeOut,,"Forgive the formatting...  

Let's just assume k = 1 for the sake of our first argument. The point your professor is making is:  
1 + 2 + .... + (n-2) + (n-1) + n < n^2  
= (0 + (n)) + (1 + (n-1)) + (2 + (n-2)) + ... (m + (n-m)) < n^2  
= n + n + n + ... n < n^2  
= (n/2) * n < n^2  
= n^2 / 2 < n^2  
The definition of Big O is that you can find a constant c, that for some value of n (n0): 
 c*f(n0) <= g(n0)  
And for every value of n > n0, c * f(n) < g(n)  
So... Looking at my derived equation f(n) = n^2 / 2, and g(n) = n^2  
We can see quickly that if c = 1 , n^2 / 2 < n^2 for all n > 0 (i.e. n0 = 0). So we can say that f(n) = 1 + 2 + ... + n is BigO(n^2 ). This can easily be extrapolated to the general case of \^k like your professor showed.  
edit: formatting
",546soo,t3_546soo,LostInOttawa,,Comment,2,0,2
d7zhpf6,2016-09-23 18:42:10-04:00,LostInOttawa,,"Thanks but I wasn't having trouble with Big O, but rather Big Omega, proving that 1^k + 2^k... + n^k is >= to c*n^(k+1).

On mobile right now but will post an edit with he solution I came up with later.",546soo,t1_d7zgbg1,PlayMeOut,,Reply,1,0,1
d7zi7n0,2016-09-23 18:55:51-04:00,PlayMeOut,,"Oh right... Sorry. Well that can be proven as well with the same line of thinking since Big Omega is defined with a reversed inequality to Big O (i.e. c * f(n0) >= g(n0), and c * f(n) > g(n) for n > n0).  
Using my same equations above, instead of c = 1, set c = 4.  
4 * n^2 / 2 = 2 * n^2 > n^2 for all n > 0  
The generalization to a power of k is fairly trivial, and you should be able to figure it out (and it is worth the exercise if you can't see it clearly). ",546soo,t1_d7zhpf6,LostInOttawa,,Reply,2,0,2
d7zew9l,2016-09-23 17:29:29-04:00,Zombi_Sagan,,"I'm on mobile, and don't have my textbook, but it reminds me of mathematical induction from my calculus class a little bit. That's the best I got. ",546soo,t3_546soo,LostInOttawa,,Comment,1,0,1
d7zpzjv,2016-09-23 22:50:07-04:00,jjhjhhj,,I think if you integrate that series you get c=1/2 as the coefficient. ,546soo,t3_546soo,LostInOttawa,,Comment,1,0,1
d80d470,2016-09-24 14:05:07-04:00,_--__,,"Using the [AM-GM inequality](https://en.wikipedia.org/wiki/Inequality_of_arithmetic_and_geometric_means) and [Stirling's approximation](https://en.wikipedia.org/wiki/Stirling%27s_approximation), you have:

(1^k + 2^k + ... + n^(k))/n ≥ (n!)^(k/n) ≥ (c\*n)^k

and the result follows by multiplying both sides by n.",546soo,t3_546soo,LostInOttawa,,Comment,1,0,1
541e11,2016-09-22 17:40:28-04:00,throwfaraway2310,Which class to take?,"If I have to decide between one of the following two cs courses to take, which one would be more important for a solid general background that will still be useful down the line?

computer architecture or networks?

Also, what about between big data and databases?",,,,,Submission,6,0,6
d7y3vzl,2016-09-22 18:42:19-04:00,Bottled_Void,,"Really, it depends what you like. What you want to go into. All modules are useful, they don't usually have one that's a waste of time.


Computer Architecture vs Networks:


Well networks is pretty useful in more day to day things. But seting up sockets and IP routing and stuff is probably something you'll run into anyway.


Computer Architecture is great for having an understanding of what is happening inside your computer. So you'll have a better idea of what's going on under the hood. Really, if you don't learn this in a course, you'll probably never really know about it. Maybe because it's less important.


Big Data vs Databases


I presume big data is about large sets of data, transfer of data and data mining. Databases probably covers database structure and maybe some SQL queries. I never did databases as a module, but I picked it up pretty well. Between the two, I think databases would be more useful, but is decidely dull. Big data might give you some insight into solving some tricky problems that we're starting to run into like scaling.


There is also another way to think about it:

Which do you think you'll get the better grade in? Learning new things on your degree is great and all. But your grade is something that catches employer's eyes pretty quick. And really, computer science should always be a foundation for building more knowlege on. It's not going to teach you everything you'll ever need to know.",541e11,t3_541e11,throwfaraway2310,,Comment,1,0,1
d7ybvkv,2016-09-22 22:01:35-04:00,not_an_evil_overlord,,"I'd say architecture and databases but it's really dependent on your background, what you want to get into, and who is teaching the courses.",541e11,t3_541e11,throwfaraway2310,,Comment,1,0,1
d7z2tdc,2016-09-23 13:04:57-04:00,crookedkr,,"Definitely databases, ""big data"" is just buzzwordy data analysis. 

Architecture and Networks is more of a tossup. I've done both and I think networks has more day to day use (job interview questions, getting data from one place to another in some why), but architecture gives a better understanding of how and why programs work the way they do. ",541e11,t3_541e11,throwfaraway2310,,Comment,1,0,1
540yuz,2016-09-22 16:15:23-04:00,bigolpardy,Unsure if my complexity analysis is correct,"Attempting to find a Big-O estimation of this code chunk:

    int a[][] = new int[m][n];
    int w = 0;
    for (int i = 0; i<m; i++) {
        for(int j = 0; j<n; j++) {
            if ( a[i][j]%2 == 0) {
                w++;
            }
        }
    }


I made an esimation and simplified: O(m)*O(n)*O(1) => O(mn)

It looks like all cases will be O(mn) because it doesn't matter if the O(1) operation executes or not, is this correct? Or are there best/worst/average cases?

Appreciate any insight!

Thank you",,,,,Submission,2,0,2
d7y0clm,2016-09-22 17:18:34-04:00,Majiir,,"Think of it this way: the entire if statement, whether the block runs or not, is O(1). You have to index into an array, then another one, then modulo division, and then a comparison... all just to find out if you even need to run the block. Running the block itself is O(1), so it does not change the complexity (although it does change wall-clock time).",540yuz,t3_540yuz,bigolpardy,,Comment,3,0,3
d7y2ujd,2016-09-22 18:17:07-04:00,bigolpardy,,"Well appreciated, I thought I had it but the answer seemed too simple. ",540yuz,t1_d7y0clm,Majiir,,Reply,1,0,1
d7z8ch7,2016-09-23 15:01:57-04:00,dzendian,,It is indeed O(m * n),540yuz,t3_540yuz,bigolpardy,,Comment,2,0,2
53wwxq,2016-09-21 22:38:37-04:00,choojack,Currently studying for my Bachelors in CE. Looking to get my Masters in finance afterwards. Good or bad idea?,"I'm currently going to school for a degree in CE and want to obtain a masters degree afterwards. I was originally going to school for Finance and I feel like a masters in it would really impact my career in a positive way. Remove a ceiling possibly?

Do you think this is a good idea or a bad one?

Thanks",,,,,Submission,0,0,0
d7x5thg,2016-09-22 02:32:04-04:00,pencan,,"Depends what you want to do. Seriously, I can't be more helpful than that. Do you want to go into technical tracks or management? Away from engineering entirely? ",53wwxq,t3_53wwxq,choojack,,Comment,4,0,4
d7xilko,2016-09-22 11:04:33-04:00,choojack,,"I hope to be able to get into the higher levels of management after working in the technical field and gaining some experience there.

It's my goal to get to the highest executive position possible for whichever company I work for.",53wwxq,t1_d7x5thg,pencan,,Reply,1,0,1
d7xocmh,2016-09-22 13:05:10-04:00,pencan,,"That's company dependent, as well. For example, many many IBM VPs have PhDs. Other companies that I've worked at have had VPs with BS. In many cases an MBA  will get you a single managerial promotion. 


If you want to get high at a tech company, you have to do good work. That sounds obvious, but what I mean is being a shitty tech employee + MBA means nothing. That's why I would advise going into the field, making a name for yourself and then pursuing a Master's, on the company dime if possible. ",53wwxq,t1_d7xilko,choojack,,Reply,2,0,2
d7xx98l,2016-09-22 16:10:55-04:00,choojack,,Thank you for your input.,53wwxq,t1_d7xocmh,pencan,,Reply,1,0,1
d7xjpyq,2016-09-22 11:28:34-04:00,umib0zu,,"Lofty goals, but I don't think you realize the point of a CS degree. People get CS degrees because they like CS and don't want to get into management or finance. I can't really say your goals are a bad idea, but many engineers and computer science trained people aren't saying they want management roles. Honestly, if you're worth your salt as a developer, you can make 6 figures right after college as an engineer at a ton of companies. If you're already on track to be a top earner, why go through further, expensive schooling just for a slight bump in pay and respect? If you're really into finance, just stick with a finance degree since it will be easier, or get a comp sci minor. [Computer science/engineering is hard,](http://blogs.wsj.com/economics/2013/07/08/math-science-popular-until-students-realize-theyre-hard/) so putting yourself through the degree for a role you can get without it would be a waste.",53wwxq,t1_d7xilko,choojack,,Reply,1,0,1
d7xxgce,2016-09-22 16:15:07-04:00,choojack,,I appreciate your response. I don't agree with everything you said but I do see where you're coming from. Thanks,53wwxq,t1_d7xjpyq,umib0zu,,Reply,1,0,1
d7xrz6v,2016-09-22 14:20:41-04:00,jalagl,,"I can talk about my experience, I have an undergraduate degree in Computer Science and Master Degrees in Computer Science, Enterprise Strategy (MBA-ish), and Project Management with emphasis in IT projects. Plus I have PMI's PMP credential.

These have given me a lot of opportunities and an interesting profile. My official title is ""architect"" and I lean more towards the technical side. However I also manage the technical team and work constantly with our customers translating their needs into requirements and system design. I also work with project managers to define scope, breakdown the work, assign resources, make estimates, etc. And I also design the systems and do quite a bit of coding (less so nowadays), mostly framework/infrastructure, and I assign tasks to the rest of the team.

I like where I am right now. I am in a high position in the company , while still being in touch with the technical side.

That being said, I have several coworkers that are brilliant developers but frankly suck as soon as they need to manage someone else. ",53wwxq,t1_d7xilko,choojack,,Reply,1,0,1
d7xxceu,2016-09-22 16:12:48-04:00,choojack,,"Wow congratulations, that really sounds ideal. Thanks for the response!",53wwxq,t1_d7xrz6v,jalagl,,Reply,1,0,1
d7xjn04,2016-09-22 11:26:51-04:00,elykl33t,,"In one of my first CS classes my freshman year someone asked the professor what a good degree to pair with CS would be.

""Well, if you can swing it, some kind of a business degree would give you a huge amount of potential. I personally would put a bullet in my brain, but that's just me.""

Of course that was talking about double majoring, but still.",53wwxq,t3_53wwxq,choojack,,Comment,1,0,1
d7xxae7,2016-09-22 16:11:35-04:00,choojack,,Shoot if I wasn't already working 40+ hours a week I would double major in a heartbeat lol. Thanks for the input. ,53wwxq,t1_d7xjn04,elykl33t,,Reply,1,0,1
53wdlp,2016-09-21 20:32:54-04:00,zSilverFox,How are spelling checks and autocorrect implemented?,"I'm guessing regex comes into play,  but what else is involved?",,,,,Submission,2,0,2
d7ww2xh,2016-09-21 21:34:09-04:00,LastElemental,,"I'm not too sure myself, but I highly doubt regexes are used that much. Most likely they're a combination of tries and fuzzy searching.",53wdlp,t3_53wdlp,zSilverFox,,Comment,6,0,6
d7xf0nv,2016-09-22 09:41:33-04:00,lordvadr,,"Here's a high-level overview of aspell:

http://aspell.net/0.50-doc/man-html/8_How.html",53wdlp,t1_d7ww2xh,LastElemental,,Reply,1,0,1
d7xks2l,2016-09-22 11:50:47-04:00,LastElemental,,"That is really interesting, I should do more research into stuff like this",53wdlp,t1_d7xf0nv,lordvadr,,Reply,2,0,2
d7x3n5v,2016-09-22 01:01:28-04:00,xiongchiamiov,,"A simple approach would be to calculate an edit distance, like [Levenshtein distance](https://en.wikipedia.org/wiki/Levenshtein_distance?wprov=sfla1), between the word and every word in your dictionary and show the user those that fall within a predefined boundary.",53wdlp,t3_53wdlp,zSilverFox,,Comment,6,0,6
d7x5uv9,2016-09-22 02:33:52-04:00,DeeJay250,,"It doesn't seem at all trivial to calculate the Levenshtein distance between your word and every word in the dictionary, and then to do this for all words

Why not take a dictionary and implement a trie with it? Then you could at least do the spelling part pretty trivially O(n)",53wdlp,t1_d7x3n5v,xiongchiamiov,,Reply,1,0,1
d7xjr2k,2016-09-22 11:29:12-04:00,xiongchiamiov,,"I didn't say it was trivial (or efficient), I said it was *simple*. Introducing a trie would make it less simple, and is the sort of thing you'd implement after you have the simplest possible version working first.",53wdlp,t1_d7x5uv9,DeeJay250,,Reply,1,0,1
d7xmjnv,2016-09-22 12:27:51-04:00,apendleton,,"You're on the right track. Doing a from-scratch edit distance calculation for every word in the dictionary is expensive, so you do want to pre-build a datastructure to be able to cheaply find the words you're looking for. Tries aren't great for spellcheck because if your typo is at the beginning of the word it's pretty inefficient to search for possible corrections. Turns out there are datastructures specifically designed for this search task, though, where you can put in a word and say, ""give me all the words within Levenshtein distance 2 of this word"" relatively cheaply. An example is the [BK tree](https://en.wikipedia.org/wiki/BK-tree).",53wdlp,t1_d7x5uv9,DeeJay250,,Reply,1,0,1
d7x9jnl,2016-09-22 06:09:58-04:00,nawap,,"Peter Norvig has a [great article on the topic](http://norvig.com/spell-correct.html)

It basically comes down to edit distance and simple machine learning.",53wdlp,t3_53wdlp,zSilverFox,,Comment,4,0,4
d7xtwwp,2016-09-22 15:01:00-04:00,xerxesbeat,,"[Soundex](https://en.wikipedia.org/wiki/Soundex) is one example.  It's much less complex than most spelling checks would be in practice, but it's fundamentals are easy to understand.",53wdlp,t3_53wdlp,zSilverFox,,Comment,1,0,1
53reg4,2016-09-20 23:58:01-04:00,mad_scientist42,What would be the structured data output of an unstructured data input?,"I've been given an assignment to impose structure on an unstructured data by clustering them based on topic. 

I understand, that unstructured data is the form in which humans basically give their output in, like simple text, music, emails etc. 

And structured data is what you enter in a database, which describes each field i.e. a computer knows what the field is. 

However, I am still unclear on what the output would be given an unstructured data? For example an email is given, what should be its structured output? 

I could not find any examples on this on Internet. Any help, links would be appreciated.

",,,,,Submission,1,0,1
d7vwnjk,2016-09-21 08:31:57-04:00,okiyama,,">And structured data is what you enter in a database, which describes each field

So the output would be what you would enter into a database for the email.",53reg4,t3_53reg4,mad_scientist42,,Comment,1,0,1
53r7es,2016-09-20 23:06:39-04:00,finessseee,Did you have to take Calc based Physics in college while pursuing your CS degree?,or can I just take regular Physics? ,,,,,Submission,8,0,8
d7vkp6k,2016-09-20 23:34:08-04:00,F0064R,,Depends on school I'd assume. Many don't require physics at all.,53r7es,t3_53r7es,finessseee,,Comment,7,0,7
d7w1nmp,2016-09-21 10:41:28-04:00,None,,Yes physics 1 and 2 with calc 1 and 2. Also 3 if you do computer engineering. Instead of CS.,53r7es,t3_53r7es,finessseee,,Comment,4,0,4
d7vvsun,2016-09-21 08:01:26-04:00,dxk3355,,Calculus based.  They dropped it a few years later and watered down the science.,53r7es,t3_53r7es,finessseee,,Comment,2,0,2
d7w0ymu,2016-09-21 10:26:07-04:00,PM_ME_YOUR_SHELLCODE,,"It depends on the school, check out the class requirements for the program you're looking at.

The university I went to only required that it be a lab-based Physics class but I didn't have to be the calculus based one.",53r7es,t3_53r7es,finessseee,,Comment,2,0,2
d7wfjcq,2016-09-21 15:22:38-04:00,metaobject,,"No, but I wish I'd taken a physics class as one of my science reqs.  I had to learn quite a bit on my own after I graduated for my jobs (atmospheric science related, 3D visualization, physical modeling of aerosol advection, etc)

I took up to Calc 3 and Numerical Analysis, so I had the math background to learn some things on my own.",53r7es,t3_53r7es,finessseee,,Comment,2,0,2
d7w6qvs,2016-09-21 12:26:23-04:00,sunemori,,"We had to meet general science credit requirements. Physics was an option (bio andchem were the others). It was ""calc-based"" physics, but being as the better physics classes were reserved for physics majors, everyone else got lumped into large lecture halls where the professors were just trying to make the class as easy as possible and push people through. We pretty much skipped everything that involved calc. Calculus 1 and 2 were required anyway, though.",53r7es,t3_53r7es,finessseee,,Comment,1,0,1
d7w9ovr,2016-09-21 13:25:20-04:00,deelowe,,"Depends on the school and the state. A lot of schools just ask you to meet the core requirements, which would allow you to opt for biology, chemistry, etc... instead.",53r7es,t3_53r7es,finessseee,,Comment,1,0,1
d7wgltr,2016-09-21 15:43:18-04:00,Meganomaly,,"I had to, yes.  Calculus I and University Physics I (with a Lab), then Calculus II and University Physics II (with Lab).",53r7es,t3_53r7es,finessseee,,Comment,1,0,1
d835k1c,2016-09-26 16:47:33-04:00,jnzq,,"Really depends on your school. At my school, computer science doesn't require physics at all, whereas software engineering does. But there's really no difference between the two majors.",53r7es,t3_53r7es,finessseee,,Comment,1,0,1
d7vn42s,2016-09-21 00:48:03-04:00,EpicSolo,,"""Regular"" physics is a waste of time",53r7es,t3_53r7es,finessseee,,Comment,-4,0,-4
d7w87m0,2016-09-21 12:55:38-04:00,SuburbanFilth,,what makes you say that ?,53r7es,t1_d7vn42s,EpicSolo,,Reply,1,0,1
d7wbe0q,2016-09-21 13:59:25-04:00,EpicSolo,,"I don't think it is going to teach you anything worthwhile let alone advancing your knowledge beyond what you have already seen in high school. Memorizing several formulas and then applying them mindlessly in applicable problems is not physics nor is it any helpful for your education. There is a reason why mostly life science people take the ""regular"" physics.",53r7es,t1_d7w87m0,SuburbanFilth,,Reply,3,0,3
53qcbs,2016-09-20 19:37:04-04:00,lseactuary,How to make a 'swap' to computer science?,Studied statistics at university and currently self-teaching Python. Not finding it very useful in terms of a career. Should I go back to university and study computer science (BSc) as the Masters seem to require previous comp science knowledge? ,,,,,Submission,5,0,5
d7vpcky,2016-09-21 02:14:31-04:00,ACoderGirl,,"There's a few options. For one thing, I'm under the impression that a masters doesn't necessarily require previous knowledge in that field, but rather you can take accelerated courses to get up to snuff, possibly with the requirement of having shown skill in the field already (eg, by being self taught).

The other option is to just go full self taught. Build a portfolio (that's the important part!) and just apply to jobs. If they say that they require a CS degree, apply anyway. Plenty of jobs really just want a good programmer and it doesn't matter how they got good. That may mean you'd need to spend a lot more time learning, first. Although that's assuming you want to do programming. If you want to do research in the field, it's more complicated.

Out of curiosity, I'm curious why you say your statistics degree isn't useful in terms of a career. Do you simply mean it's not what you enjoy? Because a stats degree should certainly be *very* employable. Even more so with a masters in it (if you wanted to go that route instead of a masters in CS).",53qcbs,t3_53qcbs,lseactuary,,Comment,3,0,3
d7vucw7,2016-09-21 06:57:23-04:00,lseactuary,,"When I have been researching MSc computer science courses, they often have required pre-requisites which is why I was confused. I am definitely picking up Python in order to build up a portfolio - just didn't want to get auto-rejected because typically those jobs ask for a MSc / PHD in computer science or something technical like maths/physics. I'm not sure - I've been applying like crazy and I'm getting no looks. Maybe I'm looking in the wrong place. I work at a top tech firm and even internally it is a big struggle to move as I don't have an engineering / computer science background. :( ",53qcbs,t1_d7vpcky,ACoderGirl,,Reply,2,0,2
d7vpjam,2016-09-21 02:22:53-04:00,Jerome_Eugene_Morrow,,"Advanced degree in a field like informatics, data science, or machine learning might be a better path than returning to university. I know a number of math and stats folks who have made the transition. People who can code and implement statistical models are few and far between. ",53qcbs,t3_53qcbs,lseactuary,,Comment,1,0,1
d7vudq2,2016-09-21 06:58:34-04:00,lseactuary,,Very interesting. At my firm the level of 'statistical models' is very low (which is surprising given its a popular tech company) mostly because the data is so difficult to get / messy. Hence I am learning Python so I can start building models over data that does exist / is not too messy etc. Will take time though. ,53qcbs,t1_d7vpjam,Jerome_Eugene_Morrow,,Reply,1,0,1
d7w3fwo,2016-09-21 11:19:29-04:00,Jerome_Eugene_Morrow,,I went into a data science PhD after a biology degree so the flexibility is certainly there. Most of my classmates were math or stats folks before they started and it's worked to their advantage. ,53qcbs,t1_d7vudq2,lseactuary,,Reply,1,0,1
d7vs39y,2016-09-21 04:42:35-04:00,blufox,,"Masters in Computer science is not hard, even if you don't have previous CS experience. It is not worthwhile to go back for a BSc. Rather, go for a Masters, and you will be asked to take the pre-requisite courses for any advanced subject. Just take those, and you will be good.",53qcbs,t3_53qcbs,lseactuary,,Comment,1,0,1
d7vufd0,2016-09-21 07:00:51-04:00,lseactuary,,Understood on MSc vs BSc. The MSc wouldn't start until 2017 though (at which point I'm like 27 yo) and would have to leave my full time job. Still worth it? ,53qcbs,t1_d7vs39y,blufox,,Reply,1,0,1
d7vwsv7,2016-09-21 08:36:47-04:00,blufox,,I did my masters in CS (shifting from Civil Engg) when I was 27. I think it was worth it (though I had experience in IT before starting masters). It also depends on what you want to do after.,53qcbs,t1_d7vufd0,lseactuary,,Reply,1,0,1
d7vyihu,2016-09-21 09:26:33-04:00,crookedkr,,"Skip the Bsc there is no point once you have an undergrad. You should be able to get into and excel in an Masters program with a stats background. Most programs I'm familiar with allow you to take a mix of upper level CS undergrad classes and regular grad classes. It will be easier to just do grad only but if you feel like you are missing something background-wise you can take a (harder) undergrad class in it. At the very worse you might have to avoid certain classes. For example, a grad operating systems class, depending on how technical it is, would be considerably harder than a grad ML class given your background. 

As far as age, you can get a MS in <2 years so 27 is not even close to too late.",53qcbs,t3_53qcbs,lseactuary,,Comment,1,0,1
d7x933j,2016-09-22 05:40:43-04:00,lseactuary,,"Researched yesterday. Seems like I take the preliminary courses at Stanford/Harvard from Jan next year. And then the Masters from Sept 2017 (and the courses are a pre-requisite / at Stanford they can contribute to the Masters). Will crack on with applications, and meanwhile, solidify my Python experience at my company. :)",53qcbs,t1_d7vyihu,crookedkr,,Reply,1,0,1
d7x9cdg,2016-09-22 05:57:19-04:00,crookedkr,,It doesn't look like Harvard has a masters in computer science. Columbia might be another school to check out in addition to Stanford. ,53qcbs,t1_d7x933j,lseactuary,,Reply,1,0,1
d7xjwss,2016-09-22 11:32:34-04:00,lseactuary,,They have something called S.M Computer Science (which is a Masters). That's what they told me anyway :D ,53qcbs,t1_d7x9cdg,crookedkr,,Reply,1,0,1
d7xm5iq,2016-09-22 12:19:43-04:00,crookedkr,,"They used to have a terminal masters but [I don't think they do anymore](https://www.seas.harvard.edu/computer-science/masters-cs-requirements). They have a ""Computational Science and Engineering"" masters which is in a different department (IACS) from CS but has a fair amount of overlap with traditional CS degree (you can take some CS courses that will fulfill certain requirements). Harvard's SM is for PhDs (""on the way"" as they say) and for undergrads (BA + SM in 4 years).",53qcbs,t1_d7xjwss,lseactuary,,Reply,1,0,1
d7yo471,2016-09-23 06:09:19-04:00,lseactuary,,Good to know! Looks interesting -- data structures and algorithms especially :),53qcbs,t1_d7xm5iq,crookedkr,,Reply,1,0,1
53lsao,2016-09-20 00:46:23-04:00,finessseee,How did you do in calculus while pursuing your degree in CS?,I absolutely suck at this shit. Good at programming.. terrible at math (calc specifically). Hby?,,,,,Submission,15,0,15
d7ugc0l,2016-09-20 07:45:51-04:00,YourFavoriteBandSux,,I liked Calc 2 so much I took it twice!,53lsao,t3_53lsao,finessseee,,Comment,37,0,37
d7u9i9z,2016-09-20 01:55:10-04:00,TheCommador,,"Sophomore here. Calc one and two, both C's. 

Certainly not a bright spot on my transcript, but nonetheless, I'm still kicking.",53lsao,t3_53lsao,finessseee,,Comment,8,0,8
d7uv0s6,2016-09-20 13:43:04-04:00,greeniguana6,,"Don't worry man, almost every single CS student, including myself, has struggled with calculus, even if the programming and CS courses are a breeze. Just get through it and try to remember that your roommate will probably get sick of you complaining about calculus real fast.",53lsao,t3_53lsao,finessseee,,Comment,4,0,4
d7u9tt2,2016-09-20 02:08:51-04:00,midnight_mission21,,"Junior here. I did really well in calc 1 (A), pretty well in calc 2 (B) and I had a hard time with calc 3 (C)",53lsao,t3_53lsao,finessseee,,Comment,3,0,3
d7uacep,2016-09-20 02:32:09-04:00,cdrootrmdashrfstar,,"(Hopefully) similar story here -- sophomore (about to be junior), got 98 in calc 1, feeling ~90-85 in Calc 2 (taking it currently) , hopefully will do really well in Linear Algebra/Calc 3.",53lsao,t1_d7u9tt2,midnight_mission21,,Reply,1,0,1
d7w8bji,2016-09-21 12:57:50-04:00,SuburbanFilth,,What did you struggle with in calc 3 ?,53lsao,t1_d7u9tt2,midnight_mission21,,Reply,1,0,1
d7waugc,2016-09-21 13:48:32-04:00,midnight_mission21,,"A couple of things. First of all, I thought that calc 1 and 2 were a lot more interesting than calc 3, so I had a hard time focusing on the actual content.

The hardest individual part of that course for me was probably understanding how to set the upper and lower bounds for triple integrals.",53lsao,t1_d7w8bji,SuburbanFilth,,Reply,1,0,1
d7uf80p,2016-09-20 06:54:53-04:00,Flerpinator,,"I failed calculus 101 three times. But I stuck with it and took as many math electives as I could through my CS degree, and now I'm the guy everybody in the office goes to when they need some mathematical consulting. ",53lsao,t3_53lsao,finessseee,,Comment,2,0,2
d7xbl4l,2016-09-22 07:52:38-04:00,KatsTakeState,,This exactly. You learn it eventually if you don't give up. At some point you've seen almost all the types of problems they'll serve.,53lsao,t1_d7uf80p,Flerpinator,,Reply,2,0,2
d7ufw11,2016-09-20 07:27:08-04:00,epakai,,"I took Calc 1 twice then Calc 2 twice at community college. Final grade in Calc 2 was passing but not transferrable.

5 years later I took pre-Cal, Calc 1 and 2 at university with all As.

In general I've found 1 term isn't enough for me to learn a complex subject well. I had the same problems in Linear algebra and Discrete math although I haven't failed them.",53lsao,t3_53lsao,finessseee,,Comment,2,0,2
d7uht1u,2016-09-20 08:39:28-04:00,BonzoESC,,"I sucked at it and retook each of them at least once. Do the homework, do more practice work, and you'll at least pass it. ",53lsao,t3_53lsao,finessseee,,Comment,2,0,2
d7ujb4q,2016-09-20 09:24:44-04:00,Pardomatas,,terribly!,53lsao,t3_53lsao,finessseee,,Comment,2,0,2
d7uyaqe,2016-09-20 14:50:24-04:00,Fazaman,,"I was great at math... until Calc 1. I took it three times, then had a couple other post-calc math classes. One was statistics, and I forget the other one.

Good luck!",53lsao,t3_53lsao,finessseee,,Comment,2,0,2
d7wgcf3,2016-09-21 15:38:14-04:00,lead999x,,"Economics major and CS/AM minor here. Read the damn textbook, it tells you what everything is and how to do the problems and usually gives examples. If you still don't get it do what you would as a programmer. Get Calculus I for dummies and read it.  Go on Khan Academy and watch the videos. Sal Khan does a great job at explaining everything and you can post questions on that site too. Also there's reddit's math subs where if you post a question there will be at least 20 M.S. or PhD mathematicians of every race, gender, and creed lining up to answer  your questions.

Honestly my best advice is just think of math as a being another programming language or tool in your tool kit and do what you must in order to master it. And just like a programming language trial and error, practice, and writing a whole lot of broken proofs is how you're gonna get good at it. And trust me I sucked at this stuff when I first started but what I've learned is that math is just another subject and if you work at it you will be able to master it. 

-An upperclassman ",53lsao,t3_53lsao,finessseee,,Comment,2,0,2
d7uco5h,2016-09-20 04:36:00-04:00,RuthBaderBelieveIt,,"I did my degree in the UK so our grading system is different to what you're used to but I managed to fail 2 of my maths units then retake the test to up my mark to the minimum grade (3rd class or 40%). Fortunately one of those was in the first year which in the UK doesn't count towards your final mark.

When I got to the final year of my Degree I just picked all the non maths units so I still ended up with a 2:1 which wikipedia helpfully informs me is the equivalent to a 3.33-3.67 GPA

I sucked at maths but still got a good degree overall you just have to play to your strengths with the other classes you take to compensate for the lower marks in the one's you're not so good at.",53lsao,t3_53lsao,finessseee,,Comment,1,0,1
d7uklr4,2016-09-20 09:59:03-04:00,Zackeezy116,,I got an A. ,53lsao,t3_53lsao,finessseee,,Comment,1,0,1
d7utspe,2016-09-20 13:17:55-04:00,andybmcc,,"It's my opinion that a lot of these courses don't teach enough real-life application of calculus.  I absolutely despise rote memorization style teaching.  If I can't figure out *why* I'm learning something, I'm somewhat unmotivated.  That being said, after Calc 4, I ended up taking some Diff Eq classes and that clicked a bit more.  Depending on the domain you work in, that calculus knowledge may actually be put to good use.

If you're not a fan of math, you're going to have a rough ride.",53lsao,t3_53lsao,finessseee,,Comment,1,0,1
d7uu8go,2016-09-20 13:26:54-04:00,yendrdd,,"Calc 2 & 3 (skipped Calc 1 from the AP test), and Differential Equations with all A's. I'm pursuing a math minor/double major.",53lsao,t3_53lsao,finessseee,,Comment,1,0,1
d7v0y13,2016-09-20 15:43:32-04:00,wescotte,,It's all about finding the right professor. Try and sit in a lecture by each prof and find the one that works best for you. After that its not so bad as long as out in the effort to actually do the problems.,53lsao,t3_53lsao,finessseee,,Comment,1,0,1
d7v2vs5,2016-09-20 16:22:43-04:00,sullage,,"Java developer for an insurance company here.

I took ap calc in highschool then tested out of calc A-B in college. Calc B-C and diff eq were pretty hard. I very nearly failed linear algebra.",53lsao,t3_53lsao,finessseee,,Comment,1,0,1
d7u9rv9,2016-09-20 02:06:33-04:00,kurtms,,uhh computer science major/math minor here so needless to say I did pretty well in calc. In all fairness though I had a really good high school teacher for calc and have always had an affinity for math.,53lsao,t3_53lsao,finessseee,,Comment,1,0,1
d7uldla,2016-09-20 10:18:10-04:00,KronktheKronk,,"I can't imagine how you can be good at programming and not calculus.  Solving your calculus problems is literally the application of a differentiation/integration algorithm to some mathematical phrase.

Take some time and figure out the algorithm if you need to.  If you can (ignoring the giant table of differentiation/integration results for a given phrase), then you should have no problem answering any question. ",53lsao,t3_53lsao,finessseee,,Comment,-6,0,-6
d7uuxv9,2016-09-20 13:41:26-04:00,greeniguana6,,"Well for starters, it's a lot easier to be motivated to learn programming because a beginner can understand the practical applications of programming and aim to become proficient enough to be able to make whatever they can imagine.

Calculus is a series of tedious math classes you have to take in your first two years of college that aren't nearly as applicable or accessible for a beginner CS student.

Also this is just me personally but while I didn't find memorizing the rules of calculus to be that hard, I can only truly be good at it once I understand exactly why a rule/method exists and works, and the whys and hows of calculus are a lot more abstract than the whys and hows of programming concepts.

I'm still in calc 3 right now so maybe there's some big grand finale that will all of a sudden make all of calculus make sense to me and I'll be proven wrong, but I figured I should chime in because I find calculus much more difficult than programming.",53lsao,t1_d7uldla,KronktheKronk,,Reply,2,0,2
d7uvffe,2016-09-20 13:51:22-04:00,KronktheKronk,,"That's the lamest set of excuses I've ever heard.  There's no doubt in my mind that you have no idea how programs work beyond the top level layer of whatever language you like to use.  For some reason you can just accept programming at face value but you can't accept that x^2 dx is 2x without someone showing you every step of the calculus done to get there?  Ludicrous.

The rules of calculus are neither numerous nor difficult.  Passing the classes with ease is no more difficult than applying the very finite rules set to the numbers and accepting the outcome.  The people bitching about how hard math is on this subreddit probably just refuse to apply themselves to anything beyond remedial in difficulty.

I strongly suspect they'll run in to the same problems when they start programming anything beyond remedial in difficulty.

 ",53lsao,t1_d7uuxv9,greeniguana6,,Reply,-2,0,-2
d7uz1jb,2016-09-20 15:05:33-04:00,greeniguana6,,"You honestly don't understand why someone would want to conceptually understand the power rule instead of just blindly following it? What's even the point of taking calculus then? 

Also, I'll eat my foot if I have to regularly evaluate integrals as a sysadmin. ",53lsao,t1_d7uvffe,KronktheKronk,,Reply,1,0,1
d7uz8rj,2016-09-20 15:09:37-04:00,KronktheKronk,,"Congratulations, you missed the point entirely.
",53lsao,t1_d7uz1jb,greeniguana6,,Reply,0,0,0
d7uzplg,2016-09-20 15:19:02-04:00,greeniguana6,,"And you missed an earlier point of mine, accessibility, calling it a lame excuse. It's pretty simple to understand that the quicker a beginner sees applicable results from their skills, the more motivated they'll be to continue and be willing to teach themselves outside of class above and beyond the course material.

I'm glad to hear that you think the rules of calculus are few and simple to remember, but I certainly don't think so and I'm willing to bet there are other CS students and graduates that will agree. Summing up the reason someone has difficulty in one subject and not another as ""refusing to apply themselves to anything beyond remedial in difficulty"" is a bit harsh, don't you think?",53lsao,t1_d7uz8rj,KronktheKronk,,Reply,2,0,2
53lp17,2016-09-20 00:19:42-04:00,TheBusDriver00,"Need any help possible from you fellow redditors, considering college.","Wow, I will keep it as short as possible, I already wrote a lengthy text, but deleted it. Please read me out. I came to TX about 3 years ago from Nepal. I slacked off and have horseshit GPA (2.8 or 9), but I am doing everything I can now to improve it, have gotten 3.8 in grade 11 and taking mostly AP classes right now, to raise it as high as I can. Predicting will get more than 4.0 this trimester. But my overall will still remain poop. Already took the SAT and got about 970 (Literally did not try and guessed mostly on the math section). Going to take it again. My mom said she will kick me out if I don't get around 1400. I am lazy and don't study much. (I feel like if I actually studied I could been around 5% of my class right now). Going to take it about 2 more times.
Now the real question. I am researching colleges, and want to major in Computer Science (am taking a computer science course in senior year right now). I honestly don't know if i like this field or not, but I like a lifestyle where I can do work at home and do something else in free time, like working out and family time and other crap, and still earn a lot of money, (figured CS would fit me). I don't think I will get into A&M and UT Arlington, and trying to go to community college, but need your advice fellow redditor, on anything I need to know before deciding to major in CS. I want to transfer into Universities after CC, but don't know where. And I also don't know which is the best CC for CS field in TX. And also I cannot afford to be in debt, since my parents are in debt :/  
I want to move out of home after Graduating High School, and live out. I promise you I am motivated at first and want to be the best at everything I can, and learn everything, but am lazy and get demotivated really quick. 
So my main question is what should I know before majoring in CS. What ever information possible. I feel like I am failing in life and really need help. I am a ambitious person but get demotivated. And did I mention my parents always fight, and it is really stressful for me. I want to live as a happy person, but don't see any options. 
Any feedback will be terrific. I don't want to regret anything in my life so any feedback will be taken seriously.
Thank you :]",,,,,Submission,1,0,1
d7u7hw9,2016-09-20 00:43:15-04:00,IGetConfused,,"First, sorry to hear about your parents. I grew up in a similar situation. I'm also from Texas. I'll give you the short of it. After reading your post, I'd have to say, no. I don't believe it'd be for you. However, I'm not sure there is anything out there like what you're looking for. I could be mistaken, but you're looking for something where you don't have to work, don't have to sacrifice, and don't have to try hard.  Sadly, I don't think computer science is that. This field is always adapting and changing. Literally, every day something new comes out. This means you've got to constantly learn, and adapt. Sure, there are nice perks of the field, but it's not rainbows and sunshine. There are nights you won't sleep because you've gotta get code ready for production, or even for a class. I'll leave it there and I can explain more if you want. 

Austin community college is the best CC for comp sci from what I've been told. However, you won't be prepared for a 4 year college if you go to a CC. Truthfully, the 4 year college isn't as bad as people say. Sure, there is debt. But there are ways to handle that. And if you apply yourself even slightly you can get scholarships, grants and good loans. It's not as bad as you're thinking. 

You should build an app. I could help you through the basics if you want one evening and then you can decide for yourself. ",53lp17,t3_53lp17,TheBusDriver00,,Comment,2,0,2
d7u88od,2016-09-20 01:07:48-04:00,TheBusDriver00,,"Thank you very much for the feedback.
I think we got on the wrong foot here, its not like I don't want to work hard, its more like I get demotivated, and on the flip side I get motivated really easily too. And I kinda think I know why I am being demotivated too, think its because of the kind of environment I live in. Parents fighting, frustration, everyone financially dependent on me, yelling at me for certain things, and on top of that I don't fit this kind of household, I am a laid back person, most of the times i try not to get bothered by these things, but my parents want me to study my ass off day and night and get a good SAT score, but I don't feel like working that hard. And i am a social person, who has friends and outside life. So basically I hate my situation right now, I don't know if this happens to others or not but I always dreamt of being that urban kind of person who lives in a appartment, is stylish and has a cool life, goes to work on time and is very happy person. To put it short I am extremely social, and I also like to a sort of a tech person that my siblings can look up to and respect. And for reference I am taking Java right now. 

And the Austin college looks really expensive for a CC (10k), Please suggest which will be the best for me, I want as much exposure to CS as much as possible, as much experience as possible. I want to learn and do practical stuff.

And yes I would love to learn how to make an app. Please tell me when will be the appropriate time for it.And I actually really like computer science, I am just not exposed to the potential of it yet, and what can be achieved with it.
I am sorry for being weird lol, I never actually opened up to anyone in a while like this.

Thank you.",53lp17,t1_d7u7hw9,IGetConfused,,Reply,2,0,2
d7uis1x,2016-09-20 09:09:51-04:00,IGetConfused,,"I'll message you this evening. We can build something quickly if you'd like. 

Also, for a school in Texas the best ranked CS school is UT (from what I remember) with the UT sister schools being close to follow. Texas Tech isn't bad. A&M is a great school in general and has an excellent engineering program. So, from what I remember UT>UT sister schools (UT Dallas) >A&M > Texas Tech. 

At my company, and most companies in the Austin area, Texas State is a frowned on CS school. Doesn't mean they don't produce good computer scientists. Just it's not often and we are thorough when interviewing a Texas State grad. There are other private colleges that are good. But they are expensive. 

I don't know what your budget is, but school isn't free. I can give you advice and try to demystify some of the things about college if you'd like. It's not as bad as everyone says about tuition. We can discuss that later. I'll pm you my email here in a bit. ",53lp17,t1_d7u88od,TheBusDriver00,,Reply,1,0,1
d7vbou7,2016-09-20 19:43:52-04:00,TheBusDriver00,,"Thank you for helping me out. 
Ya I am thinking of either applying to A&M and get a Bachelors from there, or study in a community college at first and then try going to UT Austin. And also if you don't mind answering, I am aware that there are several jobs for majoring in computer science, but I want don't know what should I do, I mean like app development, site development, software engineer, and other things. And I also like doing Hardware stuff, so are there jobs out there that hire people with both software and hardware stuff, or should I just focus on one type of stuff? And Is IT hardware of software? And how many languages should I learn, to be a really good developer? 

Could you also give me some insight on some sites or resources that I can use to learn programming, I don't feel like waiting for college, and thank you again for the help.",53lp17,t1_d7uis1x,IGetConfused,,Reply,1,0,1
d7uums4,2016-09-20 13:35:06-04:00,andybmcc,,"You're in for a rude awakening.  If you want to be an independent person, that requires fiscal responsibility, which implies that you work.  If the reason you want to pursue a CS degree is solely for the fact that you think it will pay well and you can be lazy, you're not going to last long.  Is there something else you're passionate about that you would actually fully apply yourself to?",53lp17,t3_53lp17,TheBusDriver00,,Comment,1,0,1
d7vbznu,2016-09-20 19:51:31-04:00,TheBusDriver00,,"Yes, I don't know if you can call it passonate even when you have no experience with it. I have literally no experience with hardware whatso ever, but I still like the idea of building computers and fixing computer parts and doing physical work, But I am even more interested in being that all rounder perfect person, in terms of computer. I want to know everything there is in terms of computer weather it be Hardware or Software. 

I am truly sorry for the uninformative post. Its not like I hate programming, I think its just that I don't have enough exposure to it, and so I am discouraged that I wont last long and will waste my college carrier, and that scares me even more since everyone in my family is financially dependent on me and looks forward to the day I graduate and work and get tons of money. Naturally I could have done medical stuff, but I like tech stuff so, that is why I peeked inside the world of tech. 

If you have any other critiques or information please feel free to inform me, I really appreciate it.

Thank you.",53lp17,t1_d7uums4,andybmcc,,Reply,1,0,1
53l96j,2016-09-19 22:26:00-04:00,BlahBoy3,Tips for undergraduate research?,"I'm a freshman in college (studying CS), and I just found out about a really cool research opportunity in cybersecurity and the IoT.  Essentially, I'll be working with a couple of graduate students on their Masters' project, which will involve breaking down an IoT system (not sure what yet), and studying it's security mechanisms/finding vulnerabilities.  It sounds like a great way to learn and gain some experience; that being said, these students are well-ahead of me in terms of their knowledge in the field.  
Bottom line: how can I, with my limited experience, be more of an asset to the team, rather than a detriment?  And are there any good resources that I need to see before meeting up with them again to ensure that I can effectively contribute?",,,,,Submission,1,0,1
d7u2l44,2016-09-19 22:34:31-04:00,distortedlojik,,"Be honest and upfront with them. I have been on both sides of the coin when it comes to undergraduate research and you are much better off just saying that you don't know something than to just beat your head against it trying to, more than likely, reinvent the wheel. I am much more inclined to just assume you know something unless you stop me and tell me that you don't, so don't be afraid to be vocal. It may take a little bit of time to figure out the work they need done which matches up with your experience and interests. 

I have had a few different students associated with my projects in grad school and I tell them essentially all of the above. I also worked on a few research projects (REUs) as an undergrad and spent way too much time being quiet while trying to create solutions to things which already had very well defined answers. ",53l96j,t3_53l96j,BlahBoy3,,Comment,5,0,5
53kjwy,2016-09-19 19:42:24-04:00,newl_survivor,Is it common for masters research assistants to get tution for free and a stipend?,,,,,,Submission,7,0,7
d7tx6dq,2016-09-19 20:29:11-04:00,GreyMX,,"In my experience, this would be very very unusual for a master's student. Usually they will get either a discount on tuition (a waiver if you're really lucky) **or** hourly employment. A Graduate Research Assistantship with a complete tuition waiver **and** a stipend is usually only available for doctoral students. But again, this is only based on my experiences with the two universities that I've been involved with.",53kjwy,t3_53kjwy,newl_survivor,,Comment,9,0,9
d7u4f74,2016-09-19 23:16:57-04:00,newl_survivor,,Were these smaller or larger universities? Does that seem to matter? ,53kjwy,t1_d7tx6dq,GreyMX,,Reply,3,0,3
d7u54q3,2016-09-19 23:34:36-04:00,GreyMX,,"Both are ranked in the top 10 for graduate-level engineering and computer science in the world (UIUC and Georgia Tech). I did aerospace engineering at UIUC, so I don't necessarily know how their Computer Science department operates, but I imagine it would be similar to Aerospace. At Georgia Tech I'm doing Computer Science, and I can tell you for absolute certain that masters students do not get both a full waiver and a stipend here unless there are truly extraordinary circumstances.",53kjwy,t1_d7u4f74,newl_survivor,,Reply,5,0,5
d7ugz4y,2016-09-20 08:10:57-04:00,pencan,,I'm at UIUC for an MS ECE with a full tuition waiver and stipend for TA. Would be same for RA,53kjwy,t1_d7u54q3,GreyMX,,Reply,3,0,3
d7upuho,2016-09-20 11:55:39-04:00,MagnificRogue,,"Reporting in UIUC MS CS student, full tuition waiver and stipend for TA as well.",53kjwy,t1_d7ugz4y,pencan,,Reply,3,0,3
d7uh1zn,2016-09-20 08:13:48-04:00,GreyMX,,"That's interesting. I didn't know anyone in Aerospace with that deal, but I suppose ECE is probably big enough to afford that.",53kjwy,t1_d7ugz4y,pencan,,Reply,1,0,1
d7u2b09,2016-09-19 22:28:10-04:00,distortedlojik,,This has been my experience as well at two different universities. ,53kjwy,t1_d7tx6dq,GreyMX,,Reply,2,0,2
d7utcxd,2016-09-20 13:08:41-04:00,andybmcc,,"I went to a state university and received a tuition waiver and monthly stipend as a research assistant.  There were other ""research assistants"" under the same professor that did not.  I co-authored accepted conference papers and brought in some money through a military partnership associated with my research.

EDIT: Master's student.",53kjwy,t3_53kjwy,newl_survivor,,Comment,3,0,3
d7uwtdr,2016-09-20 14:19:59-04:00,newl_survivor,,Were you a masters student?,53kjwy,t1_d7utcxd,andybmcc,,Reply,1,0,1
d7uwvfd,2016-09-20 14:21:11-04:00,andybmcc,,Yuppers.,53kjwy,t1_d7uwtdr,newl_survivor,,Reply,2,0,2
d7wggat,2016-09-21 15:40:21-04:00,newl_survivor,,Is there any expectation by the school if they find you that you would pursue a phd there afterwards if they find your masters?,53kjwy,t1_d7uwvfd,andybmcc,,Reply,1,0,1
d7wh61z,2016-09-21 15:54:07-04:00,andybmcc,,"Not really.  Most PhD students switch schools, unless you're somewhere like Berkley, MIT, etc.  At that point you're going to have a good grasp on what narrow concentration you want to pursue and you'll go somewhere that fits.  That means funding, labs, projects, and other experts in the narrow domain.  It's highly unlikely that you'll bang it out from BS to PhD all at the same school.",53kjwy,t1_d7wggat,newl_survivor,,Reply,2,0,2
d7usihh,2016-09-20 12:51:02-04:00,RobToastie,,"Depends on the school. You can get tuition + stipend at UMD as a masters TA / RA, however PhDs are prioritized for those positions.",53kjwy,t3_53kjwy,newl_survivor,,Comment,1,0,1
d7uwrqo,2016-09-20 14:19:04-04:00,newl_survivor,,"How competitive is it for a masters student? Any idea percentage wise?

What type of masters students are given the position?",53kjwy,t1_d7usihh,RobToastie,,Reply,1,0,1
53i1sy,2016-09-19 11:19:57-04:00,DaWylecat,How many languages are software engineers expected to have mastered?,"I'm currently pursuing a degree in Comp Sci and want to focus in on Software. Because I am relatively new to the world of programming in general, I was just wondering how many languages your average software engineer needs to know.

I know that the quantity of languages is generally less important than how good you are with the languages you do know, however its still something I would like to know and be ale to prepare for.

If there are any software engineers on this sub reading this, could you list out which languages you know/use and then rate how advanced your knowledge on that language is  on a scale of 1-10?


This would be extremely helpful to me and other young aspiring software engineers on knowing which languages to brush up on and be proficient in. Thanks.",,,,,Submission,11,0,11
d7t99bs,2016-09-19 11:51:42-04:00,MCPtz,,"You should be able to pick up any language when you're done with a standard 4 year university degree.

Here's a link with language popularity:

http://www.tiobe.com/tiobe-index/

No surprises there. Java, C, C++, C#, Python.

Adapt to your surroundings.

If you're into scientific computing, you'll probably need to get into Matlab and scientific python.

If you're working with marketing, you'll probably need to be able to make interactive graphs, maybe on a webpage, maybe not.

If you're working on embedded or robotics, it'll likely be C and C++ and the ability to deal with limitations.

Just be able to adapt.",53i1sy,t3_53i1sy,DaWylecat,,Comment,15,0,15
d7t8zn9,2016-09-19 11:46:11-04:00,not_an_evil_overlord,,"Relative noob here (worked for a year, back in grad school now so correct me if I'm wrong) but my experience was that it depends on the job you're working. If a job needs someone familiar with Hadoop and image processing techniques and no one is available to work that job then they either pay to get someone qualified or hire someone who is already qualified. The number of languages isn't as important as the tools and tactics you're familiar with. Do you know how to work with hadoop? hive? pig? azure? aws? spark? git? cobertura? junit? mockito? Do you know what regression tests are? How would you best test for performance on a large pre-existing project with no knowledge of implementation? Sure, putting ""Python, C++, Java, etc."" on your resume may be a neat addition. However, you're probably going to be entering pre-existing projects that call for usage of specific libraries/technologies that you'll either have to know or learn.

As for the last part. I've been ""programming"" for the last 7 years (still a noob) but here's my level of experience with each. I can't really put it on a scale of 1-10, so I give how long I've been working with the language instead:

Python - 5 years. If I need something done with a dataset (remove all words between the first and second ',' in a csv file that aren't addresses) or a quick script that I'll run a bunch this is my go-to.

C++ - 6 years. I'm a weirdo that loves C++. Don't ever get to work with it anymore though.

C - 6 years. Learned C after C++ because of my undergrad's weird curriculum.

Java - 7 years. I don't particularly enjoy Java but I keep running into it for some reason.

HTML/PHP/C#/Javascript - 3 years. I don't do web development but I figured I should have these ""under my belt"".

x86 Assembly - 5 years. I never want to see this (or smali) again. It was fun but also extremely frustrating to work with.

Bash - 7-? years. I've been writing bash scripts for myself for a long time.

There's more I'm sure... but the languages you know doesn't really matter. It's projects you've done and specific technologies you've had experience with. ",53i1sy,t3_53i1sy,DaWylecat,,Comment,3,0,3
d7tb9jy,2016-09-19 12:32:26-04:00,lorddimwit,,"The number of languages isn't really that important. It's more important to understand the fundamentals of a paradigm well enough that you can pick up languages in that paradigm fairly quickly.

I don't know Ruby very well, but I'm reasonably sure I could pick it up pretty quickly and write at least halfway decent code since I already know Python and some Smalltalk. I won't be a Ruby expert, but I won't be floundering.

You should instead pick some relatively popular language in each of the broad divisions: a scripting language (Python, Ruby), a lower-level systems language (C, C++), and if you're targeting web development, JavaScript. Don't worry about the more esoteric languages unless you want to learn it for fun or are targeting an industry where they're used heavily (if you want to work in telecom, Erlang is useful: if you want to work in AI, you should learn Lisp or Prolog or both, etc).

Don't worry about being able to say ""I know Ruby, Python, Lua, Prolog, OCaml, C, Erlang, Smalltalk, and Lisp"" on your resume. That just makes me think you don't know any one of them particularly well. :)

EDIT: You should also be proficient in a shell language for whatever platform you're targeting: Bourne shell, PowerShell, etc.",53i1sy,t3_53i1sy,DaWylecat,,Comment,3,0,3
d7tbjgg,2016-09-19 12:38:02-04:00,Madsy9,,"I think the question is framed the wrong way. It's not about knowing X number of languages, but feeling secure and adept enough to pick up a new language whenever required, and learn it fast.

While some languages are a better fit to some problems than other languages, there is no silver bullet. It's always a compromise between abstraction levels / supported semantics, performance overhead, amount of syntax sugar and flavor of type systems. Once you are familiar with all the different semantic constructs languages have, learning a new one on the job boils down to learning new syntax. Languages that truly introduce brand new concepts are exceedingly rare.

And this is why you should turn down job positions that look for ""java developers"" or ""C# developers"" in my opinion. In order to keep work interesting one should work with well-rounded people who excel at problem solving and don't look themselves blind on a single tool or technology X, Y and Z",53i1sy,t3_53i1sy,DaWylecat,,Comment,3,0,3
d7tirs3,2016-09-19 15:05:52-04:00,uber_kerbonaut,,"I will only say I've mastered something if it's been the primary thing I've been writing for the past few years. So I don't think you can ever really be a master of more than one. In all the others, you're either rusty, or a completely unexposed.",53i1sy,t3_53i1sy,DaWylecat,,Comment,2,0,2
d7trtrj,2016-09-19 18:18:43-04:00,combuchan,,"You should ultimately be able to write good code in any language within a reasonable amount of time.

Eg, I started at a Python shop a month ago having no knowledge of Python and in that month I write better Python then the non-software engineers that were here for a year before me.",53i1sy,t3_53i1sy,DaWylecat,,Comment,1,0,1
d7uuasc,2016-09-20 13:28:13-04:00,andybmcc,,"It really depends.  You should be able to pick up on other languages that follow a similar paradigm to those that you are intimately familiar with.  e.g. Say I know Lisp fairly well, I should be able to pick up on other functional languages such as F#, Scheme.  If I know C++ well, I should be able to pick up on other OO/imperative languages like Ruby, Python.  And so on.

That being said, it really depends on the domain.  I work in the embedded world, and having a solid foundation of C/C++/Assembly is pretty essential.  If you're a web developer, that shit changes so fast, I don't even know what the goto technologies are right now, to be honest.  Last time I did anything on the web, it was CGI scripting in Perl, if that tells you anything.  I'm guessing PHP, Node.js, and a bunch of things I don't know about now.",53i1sy,t3_53i1sy,DaWylecat,,Comment,1,0,1
53hlsy,2016-09-19 09:45:40-04:00,MindBodyDisconnect,"Hey guys, what does it mean for us that China will be taking over the oversight of ICANN?",,,,,,Submission,0,0,0
d7t4y45,2016-09-19 10:16:27-04:00,HelloYesThisIsDuck,,"It means you should read the article again. There is absolutely **nothing** that says China or Russia will be taking control of ICANN.

> The **misguided freakout** over ICANN

> there’s no obvious place for Russia or China to take control. 

This article is two years old, and the internet hasn't collapsed.",53hlsy,t3_53hlsy,MindBodyDisconnect,,Comment,6,0,6
53gf1m,2016-09-19 03:04:15-04:00,har777,How good is the Grenoble INP Master of Science in Informatics ?,"The program link can be found here: http://www.grenoble-inp.fr/masters-/master-of-science-in-informatics-at-grenoble-mosig-482439.kjsp?RH=INP_EN-FOR-MASTERS

The course structure looks extremely promising but I couldn't find any quality information on the program from students.",,,,,Submission,4,0,4
53fad1,2016-09-18 21:30:12-04:00,taehyun778,What will be the output of: int x = 128 ?,I literally have no clue and I can tell this is a really basic question. Pls don't flame lol.,,,,,Submission,0,0,0
d7sljw3,2016-09-18 21:53:00-04:00,prowler57,,There is no output from this statement. It's just assigning the value 128 to the variable x.,53fad1,t3_53fad1,taehyun778,,Comment,1,0,1
d7spzf6,2016-09-18 23:43:49-04:00,taehyun778,,Hm. I thought an output was just the message that you get back to confirm what you did? So wouldn't x=128 be the output? ,53fad1,t1_d7sljw3,prowler57,,Reply,1,0,1
d7sz9zz,2016-09-19 07:04:39-04:00,PM-ME-YO-TITTAYS,,"Depends on what programming language you are using and where you are typing the ""int x = 128"". If you just write that into a text file, and do nothing else, there will be no output. I'm guessing you're talking about using some sort of interactive shell where you type in a line of code and it echos stuff back to you. If so, why not try entering this and seeing what happens?",53fad1,t1_d7spzf6,taehyun778,,Reply,1,0,1
d7t4zxc,2016-09-19 10:17:39-04:00,taehyun778,,Yeah I don't understand where to get that. Do I have to download a program to get something like that?,53fad1,t1_d7sz9zz,PM-ME-YO-TITTAYS,,Reply,1,0,1
d7t5aqu,2016-09-19 10:24:44-04:00,PM-ME-YO-TITTAYS,,"In short, yes.

What exactly are you trying to achieve? Are you trying to learn a specific language? Are you doing some sort of course?",53fad1,t1_d7t4zxc,taehyun778,,Reply,1,0,1
d7ty3jp,2016-09-19 20:50:48-04:00,taehyun778,,"My class is learning Java, but we haven't really been explained how it works. For example, is the defining of variables, say as a double, something specific to Java? Or is it part of a standard every program uses?",53fad1,t1_d7t5aqu,PM-ME-YO-TITTAYS,,Reply,1,0,1
d7u9zbo,2016-09-20 02:15:42-04:00,PM-ME-YO-TITTAYS,,"It's fairly standard. Some languages do it slightly differently,but the idea is the same.

I did a class on Java at university, and it was useless. I learnt a lot more in a day a few years later by following an online tutorial. I suggest you try the same. You'll need to install Java, then you can use notepad to write some code and run it on the command line. Once you get the hang of that, install an ide which will do some of the work for you and you can run your code more quickly. There are a few ides around, but might be best to wait and see if there's one your course uses. Google for a tutorial and go from there, Java has loads of good learning material online. If you're still stuck, I don't mind answering some questions.",53fad1,t1_d7ty3jp,taehyun778,,Reply,1,0,1
d7ve0be,2016-09-20 20:42:34-04:00,taehyun778,,Thank u !,53fad1,t1_d7u9zbo,PM-ME-YO-TITTAYS,,Reply,1,0,1
d7sltjw,2016-09-18 21:59:44-04:00,LastElemental,,As a heads up if you need help learning programming there is r/learnprogramming full of people willing to help along with others seeking help,53fad1,t3_53fad1,taehyun778,,Comment,1,0,1
d7spzw5,2016-09-18 23:44:08-04:00,taehyun778,,"Ahh, I thought this was a reddit for the same purpose. Didn't look hard enough. Thanks.",53fad1,t1_d7sltjw,LastElemental,,Reply,1,0,1
53f2hh,2016-09-18 20:36:39-04:00,gandalf-the-gray,Am I the only one who calls flash drives USBs?,,,,,,Submission,0,0,0
d7sla30,2016-09-18 21:46:18-04:00,bokonator,,I'm with you on calling flash drives USBs..,53f2hh,t3_53f2hh,gandalf-the-gray,,Comment,1,0,1
d7slytg,2016-09-18 22:03:27-04:00,LordPasserine,,I've always called them USBs.,53f2hh,t3_53f2hh,gandalf-the-gray,,Comment,1,0,1
53exu7,2016-09-18 20:06:01-04:00,taehyun778,Is it fine that I'm just using my calculator to convert decimals to binaries?,In a beginner level computer science course at my high school.,,,,,Submission,1,0,1
d7shghc,2016-09-18 20:09:59-04:00,tyggerjai,,"In the words of my digital design lecturer - ""you need to know how to do this, but if you ever find yourself doing it by hand, you've made some pretty poor life choices. """,53exu7,t3_53exu7,taehyun778,,Comment,3,0,3
d7sifi5,2016-09-18 20:35:16-04:00,taehyun778,,I don't understand what that even means..,53exu7,t1_d7shghc,tyggerjai,,Reply,-1,0,-1
d7sil0r,2016-09-18 20:39:12-04:00,tyggerjai,,"You need to understand how to convert between binary and decimal - you need to understand why binary is, and how it maps to decimal. What you don't need to do is do it by hand - writing a program to do it for you is generally covered in your first week of learning C, and so long as you understand how and why it works, it is foolish to avoid the tools that can do it for you. ",53exu7,t1_d7sifi5,taehyun778,,Reply,1,0,1
d7soxe7,2016-09-18 23:16:30-04:00,nano_singularity,,"It's understandable if you're doing this in order to get the answer but most exams (assuming your College does the same protocol) won't allow you to use a calculator to identify binary/hex/decimal values. In addition to finding the answer you must understand it, you cannot go throughout your life by just punching in some numbers without knowing the depth behind it. ",53exu7,t3_53exu7,taehyun778,,Comment,1,0,1
d7sq2hr,2016-09-18 23:45:54-04:00,taehyun778,,"Really? I'm in high school, so my teacher just wants me to get the concept, but that's suprising. How are you supposed to do the math in your head for hex? Couldn't converting that possibly require you to do 16*9 in your head?",53exu7,t1_d7soxe7,nano_singularity,,Reply,0,0,0
d7srbyp,2016-09-19 00:23:50-04:00,SolarShrieking,,Which is 144. I was required to learn the multiplication tables up to 25 in middle school/high school.,53exu7,t1_d7sq2hr,taehyun778,,Reply,1,0,1
d7sribv,2016-09-19 00:29:41-04:00,taehyun778,,"Or what about having to do 15^9? That's possible in the hex system, right?",53exu7,t1_d7srbyp,SolarShrieking,,Reply,0,0,0
d7srss5,2016-09-19 00:39:33-04:00,SolarShrieking,,"You'll have to forgive me, as I am not fluent with the hexadecimal system. I cannot do that in my head, no matter how much I wish I could.",53exu7,t1_d7sribv,taehyun778,,Reply,1,0,1
d7t6zqk,2016-09-19 11:03:36-04:00,andybmcc,,"It would benefit you to be able to convert hex byte values (two hex digits) in your head.  It's not that difficult.  Just do what you do for everything, break the problem down into easier steps.

I'd see 0x90 and think, ok, it's 16*9.  That's 10 * 9 + 6 * 9, hey those are both really easy, it's 90 + 54 = 144.  I can do that much faster than accessing a calculator.

If I had the hex representation of a 32-bit IEE754 float, I wouldn't even bother trying to figure out what the value represented.  In that case, a tool makes sense.",53exu7,t1_d7sq2hr,taehyun778,,Reply,1,0,1
d7spt6y,2016-09-18 23:39:11-04:00,albatrek,,"Force yourself to do it by hand for a little while, to make sure you can. Once you really understand how it works and why, using a tool is fine.
I'd say it's a good idea if you have 0-15 for decimal/binary/hex memorized (or can figure them out fast enough to be equivalent to memorization). You end up needing it more often than you might think.",53exu7,t3_53exu7,taehyun778,,Comment,1,0,1
d7sqk9j,2016-09-18 23:59:46-04:00,zanidor,,"How big of a number are we talking?

As the saying goes, ""your calculator moves at the speed of your fingers: your brain moves at the speed of light.""

For large binary numbers a calculator is probably faster, but I'd say it's good to be able to convert quickly in your head at least up to 16.",53exu7,t3_53exu7,taehyun778,,Comment,1,0,1
d7srkye,2016-09-19 00:32:06-04:00,taehyun778,,"We're talking really small, like 10010 into decimal form. I just started, so it seemed strange that my teacher was asking me to do 100 of these in one night. But I think the consensus is that at a certain point, you should have things memorized. Sort of like the unit circle for sin, cos, and tan. ",53exu7,t1_d7sqk9j,zanidor,,Reply,1,0,1
53dvms,2016-09-18 16:04:44-04:00,Rigamortis818,What is the Purpose of Calculus in Computer Science?,"Title is my only real question. 

I'm very new to the compsci world, and I find it all very interesting.  

I'm currently a Senior in high school, and plan on majoring in compsci, so I am just beginning to learn more and more about computers, programming, etc.

Anyways, i just don't really know what the application for Calculus is in compsci.  I am decent at math, and I will be able to pick up Calculus as long as I study hard, but I just dont understand where it comes into play for computer science.  

Can someone please explain to me like I'm 5? 

Thanks!",,,,,Submission,14,0,14
d7s8su3,2016-09-18 16:44:57-04:00,panderingPenguin,,"It is applied frequently in the subfields of artificial intelligence and numerical methods. Maybe a couple others that I'm missing as well. That being said, discrete math tends to be more broadly useful in the CS world, although most CS programs (in the US at least) include calculus as a result of wanting to be ABET certified.",53dvms,t3_53dvms,Rigamortis818,,Comment,19,0,19
d7sacxp,2016-09-18 17:19:39-04:00,Rigamortis818,,"I'm sorry man I really don't understand what you just said.  

I literally have almost no knowledge of this type of stuff as far as calculus goes, so I'm really not sure what you mean by all this",53dvms,t1_d7s8su3,panderingPenguin,,Reply,12,0,12
d7sbflg,2016-09-18 17:43:32-04:00,arichi,,"* Calculus is used in some subfields, such as Artificial Intelligence

* Discrete Mathematics -- using whole numbers, set theory, logic, number theory, graphs, etc -- is more useful.  You'll probably take a class like that early in your school time.

* Most CS programs in the U.S. require calculus because of an accreditation requirement. ",53dvms,t1_d7sacxp,Rigamortis818,,Reply,18,0,18
d7smt9t,2016-09-18 22:25:00-04:00,Doughboy72,,"Wow whoever the fuck downvotes someone needing clarification, what an asshole :/",53dvms,t1_d7sacxp,Rigamortis818,,Reply,10,0,10
d7spxmg,2016-09-18 23:42:28-04:00,Rigamortis818,,"Lol for real... I didn't know why I was getting downvoted... I literally don't know anything about how Calculus and all this computer stuff works, I'm very new to it all, and trying to gain more of an understanding.

Thank you though, lol",53dvms,t1_d7smt9t,Doughboy72,,Reply,7,0,7
d7scaci,2016-09-18 18:03:52-04:00,jpflathead,,"Will you need it to code html and css? No.

Would you like to work in any sort of real world computer application?

+ vision applications
+ any application involving biology
+ flight controls
+ rocketry
+ automated control systems
+ landing probes on asteroids
+ 3d graphics (eg determining volumes of odd shapes)
+ CAD
+ 3d games
+ almost literally any scientific or engineering field
  + weather
  + strength of materials
  + minimize material used
  + acoustics
  + medicine
+ finance (as opposed to accounting)
+ statistics

All of these kinds of applications will involve implementing algorithms that are based on calculus, the mathematics of continuous functions. Even if by the time you see them, they have been redescribed as simple mathematical functions, to truly understand where they are coming from will certainly involve calculus.

I'm not saying I use calculus everyday, far from it. I am saying if I want to learn how something actually works, the papers and books and research will often rely on calculus.

",53dvms,t3_53dvms,Rigamortis818,,Comment,12,0,12
d7sggjn,2016-09-18 19:44:28-04:00,Rigamortis818,,"Ah ok man this makes more sense. 

I'm not quite sure if I'll do any of *those* applications for it, but I will definitely not under value the importance of Calculus, so that I can use it for these things if necessary. 

Thanks man. ",53dvms,t1_d7scaci,jpflathead,,Reply,2,0,2
d7sjq9b,2016-09-18 21:07:45-04:00,drummer_ash,,"You're going to be at uni/college for a while, so be prepared for your intended career path to change. I entered uni studying games, and now, 3.5 year later I'm arranging to enter an honours degree in data science. 
I don't mean to make it sound like a bad thing. As you learn more about the field you'll end up finding your strength and inspiration, so you may end up choosing a path where calculus is not very relevant, but you may also end up on a path where it is. With this in mind, I'd recommend taking every single subject very seriously, so you have the best chances and opportunities. ",53dvms,t1_d7sggjn,Rigamortis818,,Reply,6,0,6
d7sk0cy,2016-09-18 21:15:05-04:00,Rigamortis818,,"Yup exactly.

I don't know what specific field I will go into, so I might as well become as good as I can in every single category.",53dvms,t1_d7sjq9b,drummer_ash,,Reply,3,0,3
d7sh4of,2016-09-18 20:01:26-04:00,fostermatt,,"My first internship was just after I had finished (barely) Calculus 2. We were working on an software for a device that would be attached to a kettlebell. The device would be used track the position and speed of the kettebell during a workout. The hardware only had an accelerometer. So first thing I did after thinking ""How am I going to use this?"" answered that question handily. ",53dvms,t3_53dvms,Rigamortis818,,Comment,7,0,7
d7sb0y5,2016-09-18 17:34:30-04:00,Deathnerd,,"Even if you never use it, Calculus will teach you how to deconstruct a complex problem and solve it. All math will, really, but that's what I took away from my two Calculus courses. That and a greater appreciation of math and the world around me",53dvms,t3_53dvms,Rigamortis818,,Comment,7,0,7
d7sg733,2016-09-18 19:38:02-04:00,Rigamortis818,,"Ah I see. 

I'm just somewhat concerned, because I am ok at math, but Calculus just seems super difficult, and if I'm going to potentially base my career on something I really struggle with, then I'd be in trouble lol. ",53dvms,t1_d7sb0y5,Deathnerd,,Reply,2,0,2
d7sgsdc,2016-09-18 19:52:43-04:00,YourFavoriteBandSux,,"We all got through it, too.  Don't sweat it.  Just remember to go to class and go to office hours.",53dvms,t1_d7sg733,Rigamortis818,,Reply,5,0,5
d7shgab,2016-09-18 20:09:51-04:00,Rigamortis818,,Sounds good thanks man,53dvms,t1_d7sgsdc,YourFavoriteBandSux,,Reply,3,0,3
d7sj977,2016-09-18 20:55:57-04:00,Deathnerd,,"And practice and do the homework! Even if it won't be graded, for the love of God do the homework! You'll build good habits and be a better student all around",53dvms,t1_d7shgab,Rigamortis818,,Reply,6,0,6
d7sjyvf,2016-09-18 21:13:56-04:00,Rigamortis818,,"Yep haha.  

If it's going to be needed for my job, I will definitely do the homework, even if it isn't being checked.  

Cause at the end of the day, the homework isn't for the teacher, but for me. ",53dvms,t1_d7sj977,Deathnerd,,Reply,2,0,2
d7sreho,2016-09-19 00:26:05-04:00,1stonepwn,,"Definitely do your homework! I passed Calc 2 by the grace of a generous grading policy, but I would've done a lot better if I had the extra practice from doing the homework.",53dvms,t1_d7sj977,Deathnerd,,Reply,2,0,2
d7sohdi,2016-09-18 23:05:30-04:00,wackyvorlon,,"Calculus is actually not that bad. Your car does calculus. The odometer takes the integral of the position with respect to time, the speedometer takes the derivative. 

You'll spend the first two maybe three weeks learning about limits and how it actually works. The rest of what you learn is how to integrate and differentiate different functions. They're basically short cuts. You learn the power rule, the derivative of x^n is nx^(n-1). You can figure that out using limits, but it's faster to learn the result. Most of it is doing that. ",53dvms,t1_d7sg733,Rigamortis818,,Reply,4,0,4
d7sq5di,2016-09-18 23:47:59-04:00,Rigamortis818,,"Lol well I'm sure once I learn what all of those terms such as derivatives and limits mean, I'll be good to go.

It's just that as of right now, it was as if you typed half of that paragraph in Chinese, due to my limited understanding. 

I really just need to brush up on my Algebra II, because my teacher for that was absolutely terrible, and had no intent to actually make us learn.  

Anyways, I'm sure as I start to brush up on that, and introduce myself to Pre Calc, and how it all works, I'll probably be ok.  It's just that as of right now, Calculus just seems overwhelming due to all the fancy words that I don't know. ",53dvms,t1_d7sohdi,wackyvorlon,,Reply,2,0,2
d7ssmxz,2016-09-19 01:09:44-04:00,wackyvorlon,,"A derivative tells us how fast something is changing. Normally, to find how fast something changes you do rise divided by run. So say I'm in a vehicle. At t=0 seconds I'm at 0 meters. At t=5 seconds, I'm at ten meters. We take the change in position (10 meters) and divide it by the change in time (5 seconds). 10/5 is 2. So I'm moving at 2 meters a second. 

This is easy and straightforward, but that's an average over 5 seconds. What if I'm actually *accelerating* during that time? Then at no point during those five seconds is my speed actually 2m/s. So instead of measuring over 5 seconds, we measure over four. The change in distance will be smaller too. We keep going, measuring the change in distance over time using smaller and smaller durations and we get closer and closer to knowing what speed I'm going at a particular instant. But we want to know the speed I'm going at one instant. How long is an instant? Well, it's 0 seconds. And the change in position is zero too, because I haven't actually had any time to move. 0/0 is useless to us. This is where limits come into play. At an instant, the change in position is zero. But we can keep measuring over smaller and smaller durations, as small as we like so long as they are bigger than zero, and we will get closer and closer to a certain defined number. That number is the limit as the change in time approaches zero. 

In many ways the integral is the opposite of the derivative. Instead of telling us how fast something is changing, it tells us how much it has changed. ",53dvms,t1_d7sq5di,Rigamortis818,,Reply,3,0,3
d7st2qb,2016-09-19 01:26:48-04:00,Rigamortis818,,"Dang man that was a lot to handle, but it does make sense. 

I will definitely read more into it, but that was a great way to give me an idea of what it is",53dvms,t1_d7ssmxz,wackyvorlon,,Reply,2,0,2
d7st9e7,2016-09-19 01:34:17-04:00,wackyvorlon,,"When you study calculus, you end up learning the work of many staggering geniuses. It really is awesome and incredibly powerful. ",53dvms,t1_d7st2qb,Rigamortis818,,Reply,3,0,3
d7stpfq,2016-09-19 01:52:34-04:00,Rigamortis818,,"Yeah it sounds pretty amazing, I'm already excited to learn more",53dvms,t1_d7st9e7,wackyvorlon,,Reply,2,0,2
d7smqvx,2016-09-18 22:23:20-04:00,None,,[deleted],53dvms,t3_53dvms,Rigamortis818,,Comment,7,0,7
d7spsnp,2016-09-18 23:38:50-04:00,Rigamortis818,,Oh ok thanks man I'll look into it more,53dvms,t1_d7smqvx,None,,Reply,2,0,2
d7sbxlb,2016-09-18 17:55:14-04:00,Syde80,,"I'm the kind of guy that I have to understand ahead of time why something is useful to me for me to learn it well.  If I don't see the knowledge as being potentially useful I have a very hard time retaining it.

1st day of calc 1 in university... Prof: ""yeah I don't know why they make you comp sci guys take this class, you are never going to use it"".  FML.  Did not do well in the class. It was so theoretical and learning concepts.  Oddly enough... I also had to take calc 2, which actually talked about the applications of all the concepts and I did so much better.  I remember basically nothing about it, because the career path I chose does not make use of it... It's just funny how knowing the usefulness of something can have a heavy influence on learning.",53dvms,t3_53dvms,Rigamortis818,,Comment,4,0,4
d7sgdee,2016-09-18 19:42:21-04:00,Rigamortis818,,"Yup that's exactly my mindset.  

If I understand the importance of the concepts, and why I will need them, then I will be better at learning them. ",53dvms,t1_d7sbxlb,Syde80,,Reply,2,0,2
539vz4,2016-09-17 20:11:05-04:00,er_thang,What is the consensus or general opinion on Mr. Robot?,[Mr. Robot.](https://en.wikipedia.org/wiki/Mr._Robot_(TV_series) I'm sure people have heard about it. What is the general opinion on this show from computer scientists and programmers? I'm quite curious. And if there's probably a better subreddit to direct this question to then go ahead and suggest it. I'd just like to know how the type of people this show portrays actually feel about it. Thanks,,,,,Submission,19,0,19
d7r7ko1,2016-09-17 20:57:49-04:00,CelticJoe,,"Portrayal of actual coding and social engineering techniques? Best ever on film if not always 100% realistic and sometimes still over simplified some.

Portrayal of ""hacker culture"" / character and risks? Typical Hollywood power fantasy and or paranoid nightmare. It's basically fight club with computers so... Yeah. 

Beyond that it's pretty much pure personal opinion and conjecture. I love it, a lot of techs I know do, some dont and nitpick it to death. Not really sure anyone can claim to make an official consensus.",539vz4,t3_539vz4,er_thang,,Comment,24,0,24
d7raznh,2016-09-17 22:30:53-04:00,HatchCannon,,Fight Club with computers.... That is the best description of that stereotype I have ever heard. Bravo!,539vz4,t1_d7r7ko1,CelticJoe,,Reply,7,0,7
d7radlm,2016-09-17 22:14:48-04:00,Yartch,,"I've noticed they have good detail on the actual stuff getting typed into the computers. In one episode, Elliot is trying to gain access to some data servers or something, and it shows him type a chmod command. They feature Kali Linux a lot too, which is definitely the standard when it comes to hacking/pentesting.",539vz4,t3_539vz4,er_thang,,Comment,7,0,7
d7r8th4,2016-09-17 21:32:40-04:00,magikker,,"The consensus at work is that the first handful of episodes are ""must watch"" for our field because they're probably the most accurate portrayal that has every been on TV. The consensus at work is also that it quickly got pretty wierd after that. Most of my colleagues gave up on it. ",539vz4,t3_539vz4,er_thang,,Comment,8,0,8
d7r8ycy,2016-09-17 21:36:24-04:00,Kimano,,"Yeah, I think the tech in them is very well portrayed.

The amount you could get away with in real life/the character behavior, not so much.",539vz4,t1_d7r8th4,magikker,,Reply,5,0,5
d7r672v,2016-09-17 20:18:47-04:00,gride9000,,https://i.redd.it/irk5cwqm8fdx.jpg,539vz4,t3_539vz4,er_thang,,Comment,4,0,4
d7s0wzf,2016-09-18 14:03:53-04:00,admiral_asswank,,What's this referring?,539vz4,t1_d7r672v,gride9000,,Reply,2,0,2
d7sj6tj,2016-09-18 20:54:19-04:00,gride9000,,Have you ever cried during sex?,539vz4,t1_d7s0wzf,admiral_asswank,,Reply,0,0,0
d7rwmms,2016-09-18 12:31:47-04:00,wischichr,,"IMHO the tech/computer stuff is pretty nice, but the plot is average at best.",539vz4,t3_539vz4,er_thang,,Comment,1,0,1
d7ridds,2016-09-18 02:34:44-04:00,jokoon,,"He is a script kiddie, not a hacker in the black hat sense.

The whole drug and punk movement is lame.

Real computer security is complex. So far all this guy does is finding passwords. It's light years from what you hear about real stuff like Snowden or stuxnet.

Mr robot is not worth watching.
",539vz4,t3_539vz4,er_thang,,Comment,-22,0,-22
539qmd,2016-09-17 19:31:25-04:00,BwRevival,Is every arithmetic progression regular?,"I'm currently studying Sipser's Intro to Theory of Computation. In problem 1.36 you are asked to prove Bn = {a^k| k is a multiple of n} is regular. Since this is basically an arithmetic progression in the unary system (i.e. some symbol concatenated X number of times to represent X), is it true that any arithmetic progression in any base is also a regular language (can you prove/disprove this theorem)?",,,,,Submission,2,0,2
d7r8bmw,2016-09-17 21:18:54-04:00,billdroman,,"Yes! In fact, for any arithmetic progression of naturals a_1, a_2, ... and any base b, the language of values in a_i written in base b is regular. The unary example is just a special case.

I'll give you a hint for a proof. Suppose the difference between consecutive values in the arithmetic sequence is n. The arithmetic sequence is basically the set of naturals that are equal to a_1 modulo n, right? (To be precise, it's equal to that set except that some finite number of the smallest elements might not be included.)

Try proving that for any base b, modulus n, and remainder k, the set of naturals that are k mod n written in base b is regular. Set up your finite automata with one state for each remainder mod n and see what the right transitions are.",539qmd,t3_539qmd,BwRevival,,Comment,1,0,1
534ylf,2016-09-16 20:22:59-04:00,TheMemberBerries,"I am just beginning my CS journey, and I'm doing it all from Youtube and Google.","I'll be using this thread to guide my journey through the land of the computer. Where should the journey start and what does the road map look like?


-Thanks for the suggestions!",,,,,Submission,7,0,7
d7q7li9,2016-09-17 00:23:17-04:00,Binjanka,,"I found that following a pre-defined course impossibly difficult. At university/school, there's a reason for me to do well and stay on track. But when I was trying to teach myself, if I didn't immediately see the purpose to what is was learning, or if it was too difficult, I just gave up and skipped it. 

The opposite is true when I'm working on a project and can see the light at the end of the tunnel. I may be much more short-sighted and less motivated than everybody else, but if you can't push yourself through a course, try finding a project that requires skills that you currently don't have.

Good luck!",534ylf,t3_534ylf,TheMemberBerries,,Comment,5,0,5
d7q1eq8,2016-09-16 21:10:39-04:00,tyggerjai,,This thread is probably still relevant: https://www.reddit.com/r/learnprogramming/comments/3btnh6/the_opensource_computer_science_degree/,534ylf,t3_534ylf,TheMemberBerries,,Comment,5,0,5
d7qy3fk,2016-09-17 16:42:05-04:00,djangodjango,,http://sijinjoseph.com/programmer-competency-matrix/,534ylf,t3_534ylf,TheMemberBerries,,Comment,2,0,2
53459u,2016-09-16 17:11:27-04:00,jspacecadet,What to put on Github?,"I'm a computer science student, but since I'm only in my first year I've only been taking prerequisites (no actual coding). I've been using Codeacademy (and a bit of Free Code Camp) to learn, and I'm wondering what I should be putting up on Github. Would it be a good idea to put my Codeacademy projects up? I'm hoping to get some freelance web design jobs. ",,,,,Submission,9,0,9
d7pvs33,2016-09-16 18:33:36-04:00,phrz,,"Github may be used de facto as a sort of tech resume, but you shouldn't focus on picking and choosing what to put up there. Use it to back up and manage your projects (because that's what it's for), and over time your ""portfolio"" will grow naturally as you do more projects and branch out into more tools/frameworks. ",53459u,t3_53459u,jspacecadet,,Comment,8,0,8
d7pvzb0,2016-09-16 18:38:50-04:00,jspacecadet,,"cool, thanks! ",53459u,t1_d7pvs33,phrz,,Reply,2,0,2
d7pypzp,2016-09-16 19:53:46-04:00,combuchan,,"What /u/phrz said, but just make sure that when you actually start showing it to people that could be hiring you, you only have reasonably good code on there--might be a good idea to start paying the few dollars a month for Github when that time comes and marking some of your less than stellar stuff as private.  

I've seen bad portfolios and it seriously hurts the candidate when the market is competitive.  ",53459u,t1_d7pvzb0,jspacecadet,,Reply,2,0,2
d7pw82r,2016-09-16 18:45:12-04:00,None,,What do you use git or another source code version control system for?,53459u,t3_53459u,jspacecadet,,Comment,2,0,2
d7q05gy,2016-09-16 20:34:38-04:00,gyroda,,"Helps you undo changes, work on multiple features without having them fuck with eachother and, most importantly, allows more than one developer to work on the same file without overwriting eachothers' changes. ",53459u,t1_d7pw82r,None,,Reply,2,0,2
d7qqn2k,2016-09-17 13:29:52-04:00,EpicDangerFish,,"I had a recruiter tell me that the only thing you can put on github that will hurt your chances of being hired is stealing someone else's work. Other than that there are no negatives. Use it to learn basic git commands, back-up work, and host any personal projects you have no matter how small. ",53459u,t3_53459u,jspacecadet,,Comment,1,0,1
d7qvh9i,2016-09-17 15:33:56-04:00,sleepdeprecation,,"Unless you get to okay to put the web design jobs on github and have the source be available for anyone to see/use, I would definitely not put those on GitHub (or at least, not make them publicly available).

I personally just put whatever projects I'm working on in my own time that I don't mind have the source be public (so most of them) on github.",53459u,t3_53459u,jspacecadet,,Comment,1,0,1
52zyfm,2016-09-15 23:00:00-04:00,zootsuitman,Would a programmer be willing to answer a couple questions for a class,"What responsibilities do you have?

What does a typical work-day look like for you?

What do you anticipate doing for work in 5 years?

What should you have studied more in college?

What causes you the most grief on projects?

What skills should a new hire have?

What out of class experience would you recommend?

How did you find your current job?

Edit: thanks to everyone you really saved me here",,,,,Submission,6,0,6
d7ou3lf,2016-09-16 00:14:13-04:00,okmkz,,"What responsibilities do you have?

What does a typical work-day look like for you?

Show up at 9 for a quick daily meeting where we connect with the rest of the team. From there it can be pretty variable, some days are spent in meetings, planning features and assigning tasks with very little actual code being written. Other days are pants-on-head crazy, spent putting out fires and writing code that makes you feel kind of dirty. Some days still I'll just watch YouTube for an hour or two because, well, because sometimes I get lazy. 5ish is around when people will start to head home, but other times you've got a launch coming up and, well, these bugs aren't gonna squash themselves.

What do you anticipate doing for work in 5 years?

I think I'm comfortable enough with the engineering side that I'll avoid the management path and go for something in systems architecture. Or games. Both would be fine.

What should you have studied more in college?

Algorithms and data structures, linear algebra

What causes you the most grief on projects?

Executives who don't respect the process

What skills should a new hire have?

Communication skills are an absolute must. If you can't communicate what you need help with then you're gonna have a bad time. Sometimes you have to work with assholes, so it helps with that as well.


What out of class experience would you recommend?

Experiment with new technologies. Do a game jam. Go to a local meet up and talk to other devs. Always be learning and creating.

How did you find your current job?

I don't think I can give a valuable answer here.",52zyfm,t3_52zyfm,zootsuitman,,Comment,3,0,3
d7ox1gf,2016-09-16 02:03:58-04:00,combuchan,,"- Help automate the management of thousands of internet servers for online games.
- Show up around lunchtime for a 15-minute standup.  Deploy and verify any servers if it's my turn. If I'm supporting other engineers, I try to get to that stuff but it depends on priorities which my boss has got me.  In any event, eventually work on infrastructure code into the night.  
At my last job, I was a build and release engineer managing the systems that run software tests automatically for a customer relations webapp, redoing a lot of code to make that happen faster and more efficiently, and handling the release process.  

- Probably much the same, just deeper into it.  Chasing stock options, perhaps.

- Tossup between music and various algorithms, the latter just to pass dumb whiteboard questions in interviews.

- The code I was hired to fix and build upon.  It does the job, but it's not designed.
- An insatiable thirst for knowledge and desire to stay relevant (even if it's just on the job), else you'll probably not last long. 
- Knowing your way around a Linux server, because engineers who can also run them are in high demand.
- Referred by a friend",52zyfm,t3_52zyfm,zootsuitman,,Comment,2,0,2
d7oy77f,2016-09-16 02:59:14-04:00,PvsNP_ZA,,"What responsibilities do you have?

* I work as a programmer in pharma. It's my job to analyse and process pharmacokinetic data (numbers) from blood/urine samples and generate new numbers that I pass on to other pharmacokinetic experts. Luckily I don't have to work with the samples themselves, just their concentration values. :) The job is boring, but often difficult, with a lot of responsibility and oversight from the authorities. I'm one of the most experienced analysts in the company worldwide, so I help other people out (from all over the world) regularly throughout my day.

What does a typical work-day look like for you?

* I come in at 08h00, check emails. See if anything went wrong somewhere with one (or more) of my projects. Do some SAS/SQL programming, do some pharmacokinetic analysis with specialised software. Help other people with advice. Occasionally have a meeting. Try to avoid paperwork as far as possible. Leave at 16h00 or 17h00, unless I need to work overtime.

What do you anticipate doing for work in 5 years?

* Pharma data, pharma data, pharma data. Same shit, different day. Sometimes the analyses can be difficult and complex, but it's in no way exciting or fun. It's not like some great app or website that looks amazing when it's done. It's black numbers on a white page, regardless of the difficulty. I guess the work we do helps save lives through new medicine, though, so there's that.

What should you have studied more in college?

* I started working in pharma when I was busy with my final year of undergrad. Pharma programming uses very little software engineering concepts: it's more of a scientific environment, so think of the way mathematicians/physicists write programs. They just want to get the job done (obviously correctly), but without worrying about agile programming or the waterfall model or that nonsense. Pharma programming is usually SAS-based, so your standard university programming languages don't really apply. Honestly, maths and statistics are probably more directly attributable to my field. But a good foundation of programming concepts is essential to quickly picking up SAS.

What causes you the most grief on projects?

* People who don't do their jobs and check their output before sending it to me. Incompetence. Newbies without proper oversight. Managers/executives who make timelines they know full well we can't adhere to without overtime.

What skills should a new hire have?

* Willingness to work hard. Logical and critical thinking. Good programming foundations. Quick learner. Before hiring someone, I check that they're a passionate programmer, not just someone who claims to have done a lot on their CV. I ask them to solve logical problems, and if they're pharma programming veterans, I give them easy coding problems they should be able to solve. People make grandiose statements on their CV about years of experience, but can't even determine whether something is a prime number or not? If that's the case, their CV goes into the shredder.

What out of class experience would you recommend?

* You mean extra-curricular? I'd say just get as much experience as you can. University gives you a good foundation, but it's vital to get work experience. I worked as a junior dev during my holidays, and it ended up paying off. In a sea of fresh graduates, I already had some experience on my CV.

How did you find your current job?

* Via a friend already working for the company.",52zyfm,t3_52zyfm,zootsuitman,,Comment,2,0,2
52zhra,2016-09-15 21:04:10-04:00,gentlejaws,"Is it possible for a computer to ""compensate for random occurrences?""",,,,,,Submission,21,0,21
d7orgpi,2016-09-15 23:00:34-04:00,ldpreload,,"I don't think this is a claim about what a _computer_ is doing so much as a claim about how the program on the computer _was designed_. If you're writing, say, a computer vision app that is looking for a military installation in a foreign country via satellite images, occasionally you'll see something similar to the target by chance. So you program it to look at several photos taken at nearby times, track the object and look from another angle, etc. and see if it consistently looks like the target object or not. This is mostly as statistics or algorithm design question&mdash:if computers didn't exist, and you were sending photos back to human analysts (and the US and USSR both did exactly this during the Cold War with human analysts), you'd design your processes so you compensate for these random occurrences, and you don't start bombing as soon as one analyst thinks they see something suspicious. So, yes, you could have a computer program do the same thing.",52zhra,t3_52zhra,gentlejaws,,Comment,16,0,16
d7one43,2016-09-15 21:18:49-04:00,RamsesA,,"There isn't a lot of context to go on here, but the closest thing to ""accounting for random occurrences"" would probably be rejecting a null hypothesis in statistics. Not strictly a CS question, but yes, you can do this with a computer.",52zhra,t3_52zhra,gentlejaws,,Comment,11,0,11
d7onu05,2016-09-15 21:29:36-04:00,gentlejaws,,"Thanks, it was all the context given and I wanted to know if the writers BS'd it.",52zhra,t1_d7one43,RamsesA,,Reply,2,0,2
d7osxch,2016-09-15 23:38:55-04:00,no_moon_at_all,,"It's not clear what they're talking about, and it probably isn't this, but you might be interested in error-detecting and even error-correcting codes, such as [Hamming code](https://en.wikipedia.org/wiki/Hamming_code).

Hamming code is a kind of error-detecting code, a system for figuring out if a group of numbers has been changed in some small way since the group was created by adding more numbers that can be used to verify it.

For a simple example, if we want to communicate the binary number 10 without losing any data to single-bit errors, we can simply double the number of bits to 1010 and say that if the first two bits aren't identical to the second two, there must have been a transmission error and we should re-send the message. (This won't work if there are too many errors in the message.) There are ways to make this basic concept more efficient and powerful, even to the point of indicating exactly which bit out of all of them was changed.

There are a number of answers on this topic at [this Stack Overflow question](http://stackoverflow.com/questions/388599/how-does-the-hamming-code-work), so maybe one will make sense for you.

Also interesting: [Shannon-Hartley theorem](https://en.wikipedia.org/wiki/Shannon%E2%80%93Hartley_theorem), [radiation resistant computers](https://science.nasa.gov/science-news/science-at-nasa/2005/18nov_eaftc/).",52zhra,t3_52zhra,gentlejaws,,Comment,8,0,8
d7pd6cf,2016-09-16 11:55:59-04:00,UntrustedProcess,,"This is a thing that has to be considered in avionics software and hardware.  The higher you go, the more you are exposed to radiation that randomly flips bits.",52zhra,t1_d7osxch,no_moon_at_all,,Reply,1,0,1
d7ppxij,2016-09-16 16:17:29-04:00,ericGraves,,"And your general wireless router, where bit error rates can get to near 33%. ",52zhra,t1_d7pd6cf,UntrustedProcess,,Reply,1,0,1
d7ostyg,2016-09-15 23:36:19-04:00,respeckKnuckles,,"Denoising autoencoders come to mind. In a sense, they compensate for normal, random noise in inputs, so long as that noise follows a known distribution.",52zhra,t3_52zhra,gentlejaws,,Comment,2,0,2
d7plyqv,2016-09-16 14:55:35-04:00,dokushin,,"From the perspective of the computer, user input is a random occurrence.",52zhra,t3_52zhra,gentlejaws,,Comment,2,0,2
d7pnbsv,2016-09-16 15:23:25-04:00,sandwichsaregood,,This strikes me as oddly philosophical. It's pretty amusing to consider the user as a very poor quality random number generator with a large amount of internal state.,52zhra,t1_d7plyqv,dokushin,,Reply,1,0,1
d7pmuop,2016-09-16 15:13:49-04:00,sandwichsaregood,,"Well like the others here are saying this is kind of a vague statement and this is more a question of engineering (albeit still pretty interesting IMO), but it's definitely possible for computers to compensate for random occurrences. In addition to the error correction schemes others have mentioned, I think another mostly unrelated topic to examine is [control theory](https://en.wikipedia.org/wiki/Control_theory), which is all about designing automatic schemes for controlling systems with random influence.

A very good example is the famous [PID controller](https://en.wikipedia.org/wiki/PID_controller), which is a system for automatically controlling inputs a system that is subject to external influences (think controlling the throttle in cruise control on your car, that's one application of PID). [This video](https://www.youtube.com/watch?v=FDSh_N2yJZk) shows a robot that has been programmed to balance itself via a PID feedback mechanism: you can see the guy kicks and pushes the robot repeatedly and it is able to correct and maintain balance. If that's not ""compensating for random occurrences"" then I dunno what is.",52zhra,t3_52zhra,gentlejaws,,Comment,1,0,1
52z4i4,2016-09-15 19:39:28-04:00,finessseee,What's a good alternative from CS major?,I can't pass calculus so I'm just gonna minor in CS so I don't have to take it and major in something else. Any advice?,,,,,Submission,2,0,2
d7ordm8,2016-09-15 22:58:28-04:00,panderingPenguin,,"If you want to work in software engineering or study CS further in academia, then just pass calculus.  No excuses.  It's one damn class.

If you don't want to do either of those things, then you're probably better off in another major anyways.  

Tl:dr: don't let one class dictate the rest of your life.",52z4i4,t3_52z4i4,finessseee,,Comment,8,0,8
d7otfl3,2016-09-15 23:53:38-04:00,systemnate,,"This. It's tough, but you can do it. It's kind of like programming in that you just have to put in a lot of time to become good at it. Check out PatrickJMT, Khan Academy, and Paul's   Online Math Notes, and perhaps the book Calculus Lifesaver. Just study a little bit at a time and try and solve a bunch of problems. If you don't know the answer you can use something like Wolfram Alpha to get the answer. Seriously check out the resources above. You will be glad you did it.",52z4i4,t1_d7ordm8,panderingPenguin,,Reply,2,0,2
d7osyg3,2016-09-15 23:39:46-04:00,finessseee,,"Well it's pre calc 1,2 then calc 2,2.. but you're right. Sorry for the excuses. ",52z4i4,t1_d7ordm8,panderingPenguin,,Reply,1,0,1
d7om5sa,2016-09-15 20:49:22-04:00,sobolewskiking2,,That's completely depends on what you want to do. What's your goal?,52z4i4,t3_52z4i4,finessseee,,Comment,2,0,2
d7oqupa,2016-09-15 22:45:27-04:00,RonMexico69,,"A good friend of mine studied information systems and works as a developer making good money. He had to learn a lot on his own time though. Keep in mind there is a stigma among developers in the industry that the material that you learn in school is what qualifies you as a good candidate for jobs, for whatever reason. That is, you need to know algorithms and lower level CS concepts to get hired, even if you are just building web apps at many places. 

I work in consulting, so this might not be relevant for all developer jobs, but so long as you get a semi-relevant degree and can prove in an interview that you are a good developer, despite not having a CS degree, you will be fine. Keep in mind though that there will be bias in interviews against you, and companies will use it as an excuse to try to pay you less.

Edit:
If you enjoy coding, I recommend to pursue it, even if you just get the minor. Build a web app and learn git. Just start with something really basic and add features when you can. It's a great to bring up in an interview. If you can talk about what you did, you become an easy hire instead of a questionable one.",52z4i4,t3_52z4i4,finessseee,,Comment,2,0,2
d7or8xh,2016-09-15 22:55:17-04:00,finessseee,,Does information require calculus? That's honestly the sole reason I'm not pursuing CS.. I'm a great programmer.. shitty math student. ,52z4i4,t1_d7oqupa,RonMexico69,,Reply,1,0,1
d7orgzy,2016-09-15 23:00:47-04:00,RonMexico69,,"It depends on what you do. I build web apps. I've never been asked about calc nor have I needed to use it. I barely got through calc 2 in college, was a 3.0 student, no internships, and I got a job for a large consulting firm in the D.C. area making like 75k out of school. Not bragging or anything, just for reference that if you can get through your degree from a good school, it does open doors.",52z4i4,t1_d7or8xh,finessseee,,Reply,2,0,2
d7p1803,2016-09-16 05:56:37-04:00,Irony238,,I was going to suggest Maths but this does not seem to be an option.,52z4i4,t3_52z4i4,finessseee,,Comment,1,0,1
d7p6l8b,2016-09-16 09:28:30-04:00,dpears,,"It all depends on what you want to do with it.
I would recommend math since number theory and a lot of other principles both apply in computation.
Philosophy might be a decent choice, though it doesn't ""prepare you for the real world"" as much as other degrees. Philosophy largely covers logic (necessary for deductive argument) and demonstrates to employers your ability to think critically about problems.
A lot of my peers studied IT or IS (I'm CS) and so they develop the skills to develop software, but working with them is sometimes frustrating because they don't fully understand the complexity of problems, or how to approach them in an elegant and efficient way. If you want to make websites or manage hardware or lead a team on a project, either IT or IS might be a good choice.
Ultimately, I would just buckle down and pass. I failed calculus twice but really wanted a CS degree. Take an extra semester (degrees don't have dates on them) and really focus on calculus. Form a study group, get a private tutor, and talk to you TAs often. I have no doubt you can do it.",52z4i4,t3_52z4i4,finessseee,,Comment,1,0,1
d7p7ut9,2016-09-16 10:00:46-04:00,andybmcc,,"Most people that I encountered struggling with CS ended up in MIS (Management Information Systems), which is more of a business-side deal.  People who still struggled with that just went into business management from there.  Check for MIS.",52z4i4,t3_52z4i4,finessseee,,Comment,1,0,1
52t0hj,2016-09-14 18:02:43-04:00,NOLES320,Transition Table of the Finite State Machine,"I need to create a transition table of the finite state machine for simple BNF grammar that looks like this:

                          <S>  ->  a<B>a
                          <B>  ->  b | b<B>

I have been given part of the table already and it looks like this:

                  | a | ?
                 ---------|----------|----------
                    S | A1 | ?
                    A1 | err | ?
                    B | ? | ?
                    A2 | ? | ?


Any help completing this table would be greatly appreciated as I have had no luck trying to understand it on my own.
",,,,,Submission,3,0,3
d7n8wtp,2016-09-14 21:27:29-04:00,crookedkr,,"Can you write the language that is produced by this grammar? Is it regular, context free, or unrestricted?

If you don't know the answer to these questions try to make a few strings that are part of the language and see what they look like.  ",52t0hj,t3_52t0hj,NOLES320,,Comment,1,0,1
d7n9ynh,2016-09-14 21:54:02-04:00,NOLES320,,"It is described as regular or type 3. I know the strings it produces would start with an ""a"" then one or more ""b's"" followed by another ""a"". So something like ""aba"" or ""abbba""",52t0hj,t1_d7n8wtp,crookedkr,,Reply,1,0,1
d7na6fo,2016-09-14 21:59:33-04:00,crookedkr,,"great so draw a FSM that represents the same language. From there the transitions are the same, just add them to the table.",52t0hj,t1_d7n9ynh,NOLES320,,Reply,2,0,2
52rh6z,2016-09-14 12:58:57-04:00,AaronWardHere,What is Ontology in Sentiment Analysis?,"I heard that ontology means a set of keywords under a domain for easier sentiment analysis. For example: keywords such as battery, display, audio within the domain of a smartphone. Is that it? Or am I missing something here?",,,,,Submission,7,0,7
d7msnfn,2016-09-14 15:17:19-04:00,andybmcc,,"I think you mean semantic analysis.  Semantics is dealing with the meaning.  But, yes, ontology is referencing a formal naming scheme for entities and relationships.  You can have a domain ontology.  For example, the word ""glasses"" would represent something different in the domain of optometry (eye wear) compared to bar tending (something you drink from).",52rh6z,t3_52rh6z,AaronWardHere,,Comment,2,0,2
52mxcx,2016-09-13 17:08:28-04:00,AR96,What intrigued you about computer science that made it your career?,"So I'm a freshmen in college and I've been interested in compsci ever since I heard the word ""hacker"". I may be leaning more towards the security aspect but I also kinda understand the coding aspect as well. I also feel like majoring in electrical engineering but Im still undecided. Algorithms kind of scare me but I know with explanation I'll be able to comprehend it. I know this sounds like rambling, but I just want an input from another source. Thanks :D",,,,,Submission,17,0,17
d7lnzmm,2016-09-13 18:51:55-04:00,Eracoy,,"I just graduated with a BS in CS and am now working as a software engineer. Algorithms was my primary focus, and I agree that they can seem scary at times. However, anything a computer, or you for that matter, do to solve a problem is an algorithm. I think of algorithms and data structures as our bread and butter, but that all they are is a way of writing the process that you might use to solve a problem or compute an answer. They are ways of understanding a problem. Each is like a tool on your tool-belt or a memory of how someone else once solved the problem.

I have some background in the EE side of it, but am pretty flaky when it comes to security. I have been interested in CS ever since I picked up a TI calculator and typed in an example program from the manual. You sound like you have a similar deep interest, and that is a great thing, to be interested in what one might potentially do for a job. That is what brought me to seek a career in CS: that I  was interested in the topic and could see myself getting paid to use it. That's my recommendation to you, find your interest in the field and seek that path. It is a very open field when it comes to potential. You could go into literally any industry in some capacity. Seek internships and read stuff on your own that interests you. Nothing helps you understand what it's like to be a Computer Scientist like actually doing computer science activities like coding and practicing your tool-belt of skills.",52mxcx,t3_52mxcx,AR96,,Comment,9,0,9
d7lxl4k,2016-09-13 22:51:28-04:00,combuchan,,"I fell into it.

Writing webpages with a little bit of server side includes was fun in the mid 1990s and spending money in the late 1990s.

Then I tried to start my own a'-la-carte ISP around 2000 and needed a signup form. So I started coding for ""real"" in PHP.

I wanted to be in systems administration and IT, but at the first real job as a student worker my boss told me to ""brush up on my PHP"".

I didn't hang around at the university long, but I tried to start a webapp in 2001 that I ultimately burned out on.  Again, lots of PHP.  Then, in lieu of sysadmin jobs, I ended up working in web programming for well over a decade.  

I didn't get that sysadmin role until years later until I got into devops.  If I didn't learn to code well as a web programmer, I wouldn't be at the job I always wanted, and my career would have sucked.  Systems administrators that can't code are beginning to die out or otherwise not get hired.

To one of your original points, there will *always* be more jobs for programmers than there will be for electrical engineers.",52mxcx,t3_52mxcx,AR96,,Comment,8,0,8
d7ltzxz,2016-09-13 21:20:41-04:00,Reinhold_Messner,,"I've gone through Engineer and PA, now a Solutions Architect. I like solving puzzles. ",52mxcx,t3_52mxcx,AR96,,Comment,3,0,3
d7lxlml,2016-09-13 22:51:51-04:00,pharma_joe,,"I think any good CS problem is overwhelming by nature, and can sometimes seem impossible. With experience you learn to distill or dissect a huge problem into more manageable chunks, then slowly the bigger picture emerges. It is very rewarding IMHO: practicing  patience and persistence will help, and the rewards of overcoming something that at first seemed impossible is a great feeling!",52mxcx,t3_52mxcx,AR96,,Comment,3,0,3
d7mfr9s,2016-09-14 10:48:33-04:00,Randolpho,,"I came into the field backwards. I was originally a physics student. 
While I'd been familiar with programming from a younger age (copying basic games from the backs of magazines, Logo, that sorta thing) I had my first formal programming class as part of my Physics degree. C -- aced it, it was easy stuff. 

The next year, while dealing with some home life issues (newborn stress, etc.) I found myself utterly unable to cope with differential equations and relativity. I looked fondly back on my programming courses and... well, I switched majors to Computer Science.  

I still kinda wish I'd stuck with physics, but hey, I make a good living now. 
",52mxcx,t3_52mxcx,AR96,,Comment,2,0,2
d7lp2c3,2016-09-13 19:18:55-04:00,spongebue,,"I've always enjoyed building things, but until I got a big boy job in programming, I couldn't afford the raw materials that are inevitably required. I think I enjoy programming partly for the puzzle it brings, but also because I can create all kinds of things with a single computer. Generally, I don't need to order anything new to make a new program. And when I really miss working with something physical instead of software, I can still do it as a hobby or whatever, whether I'm doing a home improvement project or restoring an antique radio.

I think generally, software and hardware (EE) people do their own thing. You can do a lot with your career with just one or the other. But if you really want to boost your skills, maybe you can get a minor in EE?  And if you like it enough, consider what additional classes you need for the major.",52mxcx,t3_52mxcx,AR96,,Comment,1,0,1
d7lvz88,2016-09-13 22:10:03-04:00,None,,"An internal combustion engine and the software engine for Sonic are similar.

I'm trying to figure out why I care about internal combustion engines.",52mxcx,t3_52mxcx,AR96,,Comment,1,0,1
d7mx1fn,2016-09-14 16:43:54-04:00,Jerome_Eugene_Morrow,,"Computer programs are just fascinating to me. They're like these stupid little living things that you give one really specific job, and then they go ""okay!"" and they just do that job forever. Then you get them some friends to work together, and they do more complicated jobs together. And then you teach them how to read in from sensors, whether that means an actual heat sensor or an internet data stream, and then they can interact with the world around them.

It's just always been amazing to me that we have the power to make that happen.",52mxcx,t3_52mxcx,AR96,,Comment,1,0,1
d7vk3pu,2016-09-20 23:17:57-04:00,sullage,,I wrote a vb program to boot people off aol. Hooked ever since.,52mxcx,t3_52mxcx,AR96,,Comment,1,0,1
52jd5f,2016-09-13 04:06:45-04:00,Kubanreka,"Made device for reading human thoughts, but it is impossible to publish the discovery. Russia."," Made device for reading human thoughts / human mind reading machine / Brain computer interface. In particular, I have created a perfect Speech Generating Device for people with Amyotrophic Lateral Sclerosis / ALS. About the problem look : Jack Gallant, Tom Mitchell and Marcel Just; John - Dylan Haynes, Andrea Stocco and Rajesh Rao, human mind reading machine. Discovery is not published. ",,,,,Submission,0,0,0
d7ksx60,2016-09-13 06:20:40-04:00,worst,,Ok?,52jd5f,t3_52jd5f,Kubanreka,,Comment,5,0,5
d7kt1ob,2016-09-13 06:28:34-04:00,None,,Generally questions end with a question mark.,52jd5f,t3_52jd5f,Kubanreka,,Comment,3,0,3
d7lbnqi,2016-09-13 14:28:33-04:00,andybmcc,,I am interested in integrating this with my tachyonic antitelephone. Tom. Dick. Harry.,52jd5f,t3_52jd5f,Kubanreka,,Comment,3,0,3
d7kvko5,2016-09-13 08:22:34-04:00,zedoriah,,Post the source on github.,52jd5f,t3_52jd5f,Kubanreka,,Comment,2,0,2
d7kw679,2016-09-13 08:42:48-04:00,Kubanreka,,Thank you. ,52jd5f,t1_d7kvko5,zedoriah,,Reply,1,0,1
52ivy8,2016-09-13 01:23:21-04:00,Vector112,"[Algorithmic Complexity] Problem 3.3 of the Manber text ""Introduction to Algorithms: A Creative Approach"""," Hello all, 

(and apologies to mobile reddit users, if the powers like 2^3 = 8 don't show up right).

I was looking through the drill exercises of the third chapter of [the book by Manber](https://www.amazon.com/Introduction-Algorithms-Creative-Udi-Manber/dp/0201120372) and found the following problem difficult. Let me first regurgitate **the problem**.

> Prove, by using [the theorem below], that: n*(log of n to base 3)^5 = Big_O(n^1.2 ).

[Per tradition, I'll refer to *g*(n) as it is usually presented in other texts, i.e. the estimated run time represented by *f*(n) = Big_O(*g*(n)). I'll also write the log of n to a base b as ""log_b(n)"".]

The **theorem** mentioned is this: 
> For all *c* > 0, all *a* > 1, and all monotonically increasing *f*(n), (*f*(n))^c = Big_O(a^*f*(n) ). In other words, an exponential function grows faster than does a polynomial function.

Just to remind the reader, taking log of n to any base (greater than one) results in Big_O(n). Additionally, one can add a couple of different *f*(n)'s and the complexity class of the sum is equal to the addition of the respective *g*(n)'s; multiplication works out to the same effect.

**Major events in my attempt**: I tried working the problem backwards and forwards, but wasn't sure if either approach brought forth any reasonable fruit. Working a bit forwards, one can see that n * (log_3(n))^5 = (n^1/5 * log_3(n))^5. However, after making additional calculations based directly off of the above theorem, I got to a *g*(n) of 3^n^1/5 * n.

After some frustrating repeats of working forwards, I then worked backwards from the given *g*(n) of n^1.2 . You can decompose this to n^1/5 * n, which can come from multiplying the complexity classes of the functions n^1/5 and n *or* log_b(n), in particular with b = 3. However, per the theorem, raising the product of n^1/5 and log_3(n) to the fifth power would bring me back to the *g*(n) from the above paragraph.

**Conclusion**: I'm at a loss, at this point. I feel like I'm missing something, like the fact that a quadratic *f*(n) is not just in Big_O(n^2 ), but also in Big_O(n^3 ), and that I can reduce the time complexity by ""just"" replacing the 3^n^1/5 term with n^.2, but that just feels artificial.

Please help me, and even if you can't help and have read this far, thanks for taking the time to read through this post, which might be hard to read. If it's too dense, I'll re-iterate in the comments.",,,,,Submission,1,0,1
d7knvrb,2016-09-13 01:48:07-04:00,_--__,,"First observe that you only have to show that (log_3(n))^5 is O(n^(0.2)) [why?]  Next, why does this mean you only have to show log_3(n) is O(n^(0.04))?  Finally, apply the theorem with f(n) = log_3(n).",52ivy8,t3_52ivy8,Vector112,,Comment,2,0,2
d7komq8,2016-09-13 02:19:37-04:00,Vector112,,"Oh, so...

The simplest way to prove *g*(n) is n^1.2 is to take advantage of the n that's already in *f*(n). So really I have to just show that (log_3(n))^5 is O(n^(1/5)). Additionally, since the product of multiplying log_3(n) five times is a member of the complexity class of n^(1/5), then log_3(n) itself is a member of the complexity class of n^(1/25), which is n^(0.04).

So if I just let the n alone and take advantage of the fact that b^log_a(x) == x^log_a(b) , then I can say the following:

log_3(n) = Big_O(a^log_3(n) ) = Big_O(n^log_3(*a*) ). The appropriate *a* here about 1.0449, which satisfies the restriction *a* > 1, albeit by a slim margin. This means that log_3(n) is approximately Big_O(n^(0.04)). Hence n*(log_3(n))^5 = Big_O(n^(1.2)). Halmos.

Thanks for the help. ~~Also, what syntax do you need to type in in order to get the exponents to not affect the adjacent characters?~~ Nevermind, I found out how.

I don't know why I ended up picking a route that ""seemed to work"" but was too stubborn to see that there was another, simpler option. I stuck with the same attempt for like a good hour and a half, to no avail.",52ivy8,t1_d7knvrb,_--__,,Reply,1,0,1
52h6hd,2016-09-12 18:42:56-04:00,finessseee,Does anyone know where I can pursue a computer science degree abroad?,,,,,,Submission,4,0,4
d7k7s0p,2016-09-12 18:53:41-04:00,tyggerjai,,"It would help to know where ""abroad"" is for you, but the obvious places are:

a) The UK, Canada or Australia if you're from the US

b) The US if you're not.",52h6hd,t3_52h6hd,finessseee,,Comment,6,0,6
d7k8x4k,2016-09-12 19:21:31-04:00,finessseee,,good call.. I'm from the US.,52h6hd,t1_d7k7s0p,tyggerjai,,Reply,3,0,3
d7liygq,2016-09-13 16:55:43-04:00,heyitsvini,,Germany! Most (more than 90%) courses are free and excellent quality. Take a look at [daad.org](http://daad.org) for more information.  ,52h6hd,t3_52h6hd,finessseee,,Comment,3,0,3
d7kooj7,2016-09-13 02:21:47-04:00,zorkmids,,"There are loads of good options in the UK.  Also Check out University of Melbourne, ANU, and UNSW in Australia.  

Here's a good list:
http://www.topuniversities.com/university-rankings/university-subject-rankings/2015/computer-science-information-systems#sorting=rank",52h6hd,t3_52h6hd,finessseee,,Comment,1,0,1
d7krxfg,2016-09-13 05:19:31-04:00,Yannnn,,"I'd start with looking at countries that you're not currently living in :) 

But seriously, what are you looking for? Be more specific? ",52h6hd,t3_52h6hd,finessseee,,Comment,1,0,1
52gly7,2016-09-12 16:50:44-04:00,wolfenado,Advice for transitioning into computer science,"Background:
I'm a biology major who's recently attained an MS in biology with a strong emphasis on bioinformatics. I currently work at a medical research institution doing data analysis on next generation sequencing data. Primarily I work in R, but have decent python and bash experience.

I'd like to transition from data analysis to software development. My question is what path would be best to do so? I've considered going back for a BS in computer science, attempting to get into a computer science masters program, or just focusing on teaching myself new languages. 

Any input on what the best course of action would be is appreciated!",,,,,Submission,3,0,3
d7kimqs,2016-09-12 23:12:59-04:00,None,,[deleted],52gly7,t3_52gly7,wolfenado,,Comment,1,0,1
d7kpxjy,2016-09-13 03:22:16-04:00,None,,[deleted],52gly7,t1_d7kimqs,None,,Reply,1,0,1
d7lft4o,2016-09-13 15:52:26-04:00,wolfenado,,"Thanks for the reply!

I'd never thought about going for a bioinformatics degree. My worry is that the course content would be repetitive since my current MS program focused heavily on the bioinformatics aspect of things even though it's technically a biology degree.

I'd definitely lean more towards a software engineering degree than computer science (I had not been able to figure out what the exact difference was until now). 

I guess where I'm at is that I feel like I have the baseline knowledge to teach myself the skills I would need to be proficient as a developer. However, job postings seem to require a degree in the field and I don't want to get overlooked because my degree isn't exactly what's being looked for.",52gly7,t1_d7kimqs,None,,Reply,1,0,1
d7li4ad,2016-09-13 16:39:00-04:00,None,,[deleted],52gly7,t1_d7lft4o,wolfenado,,Reply,1,0,1
d7lk4t7,2016-09-13 17:20:57-04:00,wolfenado,,"Yeah I mean I don't think you're being a huge jerk hah I'm literally asking about the necessity of a degree in the field. 

All I know is that CS vs SE wasn't distinct at my university so I was going with the knowledge that a base CS degree (as it was in my school) is sufficient knowledge to get a job. I'm definitely aware that R won't get me far. However, I have a decent base in Python and bash scripting.

I'm not trying to find a loophole. I understand it's not going to be easy and take a lot of work. I'm not above going back for another BS.

That said, CS/SE from what I have heard isn't quite as rigidly structured as far as the need to get a degree goes. For example, my sister who is in the CS field doesn't seem to think going back to school would be necessary and I could teach myself a lot over the next year or so. I was looking for other opinions on that subject. ",52gly7,t1_d7li4ad,None,,Reply,1,0,1
52g2xg,2016-09-12 15:12:29-04:00,Zebuuuu,Need some help with a Discrete Math problem!,"I have my first test in Discrete Math this Wednesday, our teacher gave us a past exam as review and i cannot figure out the answer to one of the problems even though i know it is relatively easy, I am just confused on notation. The problem states:

Write the statement: ""No irrational numbers are integers'."" Formally using a quantifier..

If anyone could help me with this it would be appreciated!",,,,,Submission,6,0,6
d7k4rxk,2016-09-12 17:43:27-04:00,Mukhasim,,"We write **Z** to denote the set of all integers, and **Q** to denote the set of all rational numbers. (**Z** and **Q** are the standard symbols for these terms, sometimes written in a fancy font or, especially in more recent books, in boldface.)

Then, we can write the statement as

not ( exists *x* ( ( not ( *x* is-a-member-of **Q** ) ) & ( *x* is-a-member-of **Z** ) ),

where ""not"", ""exists"", ""is-a-member-of"" and ""&"" represent the standard graphical symbols for these operators (¬, ∃, ∈, ^ respectively). I have used more sets of parenthesis than are really necessary, in order to make it very clear how the symbols are to be grouped. Using the graphical symbols (which I didn't use above in case your font doesn't have them), and removing some of the parenthesis, we have:

¬ ∃ *x* ( ¬ ( *x* ∈ **Q** ) ^ ( *x* ∈ **Z** ) ),

The statement ""( not ( x is-a-member-of Q ) )"" says that x is an irrational number (in other words, it is not one of the rational numbers), and the statement ""( x is-a-member-of Z )"" says that x is an integer.",52g2xg,t3_52g2xg,Zebuuuu,,Comment,9,0,9
d7kgt52,2016-09-12 22:28:43-04:00,Zebuuuu,,Thank you very much. I completely understand the notation now. Appreciate the help!,52g2xg,t1_d7k4rxk,Mukhasim,,Reply,2,0,2
d7k5e0n,2016-09-12 17:57:22-04:00,Mukhasim,,"Also, there's a better sub for such questions: /r/learnmath",52g2xg,t1_d7k4rxk,Mukhasim,,Reply,1,0,1
52db8q,2016-09-12 04:18:31-04:00,sumrehpar_123,Which language should I learn before starting university?,"Hey guys. I'm planning on majoring in maths and computer science at university and was wondering if I should learn a programming language before starting the course. If yes, then which one would help me the most? I have a year before I start university so time should not be an issue for me.",,,,,Submission,8,0,8
d7jcza2,2016-09-12 05:35:23-04:00,tyteen4a03,,"Practically any programming languages you are interested in is fine, but Python 3 is usually the best for beginners.

As you are a math major, you might also want to know Matlab and to some extent, R.",52db8q,t3_52db8q,sumrehpar_123,,Comment,15,0,15
d7jklyr,2016-09-12 10:32:54-04:00,jlong1202,,My school started us out learning c. Very useful language but damn if it isn't hard for beginners ,52db8q,t1_d7jcza2,tyteen4a03,,Reply,1,0,1
d7jm1gn,2016-09-12 11:07:07-04:00,Treyzania,,I find that C is a poor choice if you don't have someone (ie professor) holding your hand through it.  Otherwise it can be very good as it can teach you exactly what the processor is doing and exactly how the memory is laid out.,52db8q,t1_d7jklyr,jlong1202,,Reply,3,0,3
d7joadu,2016-09-12 11:57:40-04:00,jlong1202,,"It really is rough 

Op should probably learn bash or. Python jus to get a feel",52db8q,t1_d7jm1gn,Treyzania,,Reply,2,0,2
d7lo5rw,2016-09-13 18:56:15-04:00,Eracoy,,Would you really recommend bash to someone as a first language?,52db8q,t1_d7joadu,jlong1202,,Reply,2,0,2
d7jzjii,2016-09-12 15:52:52-04:00,moeseth,,English,52db8q,t3_52db8q,sumrehpar_123,,Comment,8,0,8
d7jzw88,2016-09-12 16:00:01-04:00,ObeselyMorbid,,My first thought as well,52db8q,t1_d7jzjii,moeseth,,Reply,2,0,2
d7lo7df,2016-09-13 18:57:20-04:00,Eracoy,,http://futurama.wikia.com/wiki/Olde_Fortran,52db8q,t1_d7jzjii,moeseth,,Reply,1,0,1
d7jpxow,2016-09-12 12:33:26-04:00,singham,,No doubt Python,52db8q,t3_52db8q,sumrehpar_123,,Comment,3,0,3
d7juvcs,2016-09-12 14:16:51-04:00,eonbre,,I think in programming is important to learn logic and problem solving more then  picking specific language.Beacuse you can pick up any language fast if you master one i think :D.But C++ is probably hardest to learn.,52db8q,t3_52db8q,sumrehpar_123,,Comment,3,0,3
d7jz67m,2016-09-12 15:45:23-04:00,Jerome_Eugene_Morrow,,"If you know which school you're planning on applying to, find out what their CS intro series is taught in. Some schools do Java, others do Python, others start straight with C. All have their strengths and weaknesses, but it usually reflects what the program wants to emphasize.

If you don't have any experience and want to get familiar with the logic you need to do coding, I'd recommend Python because it reinforces good formatting conventions and is interpreted rather than compiled (so you can screw around at the command line if you want). Some commenters are pushing Python 3, but I'd recommend learning 2.7 since there are a lot of libraries that were never updated to 3.x. In my program 2.7 is the defacto standard, even though a lot of students and professors use 3 in their personal projects. Ultimately the difference isn't that big, though, so go for whatever seems best to you.",52db8q,t3_52db8q,sumrehpar_123,,Comment,3,0,3
d7kfi63,2016-09-12 21:58:05-04:00,ZuuRocks,,"At my program, we started out with Python 3 then C++ for COSC II. I wish we would have began with C++ because we spent maybe a lecture on syntax then jumped into pointers right away",52db8q,t3_52db8q,sumrehpar_123,,Comment,2,0,2
d7jugh1,2016-09-12 14:08:16-04:00,bhuddimaan,,"App development. 
Be it android or iOS .

It may pay partly for your college fund. 

Other than that, Matlab",52db8q,t3_52db8q,sumrehpar_123,,Comment,-5,0,-5
52d8ab,2016-09-12 03:47:17-04:00,_Casey_Anthony,What are the mid-tier internships? (Sophomore in college),"Hi,
I'm in my second year of college and currently looking for summer internships. Right now I'm applying to two major types of jobs:
1. The deam jobs: Google, Microsoft, Facebook, Amazon... The one's that everyone has heard of, the jobs that you can find by searching ""top computer science jobs"" and that I have very very very little shot of actually getting.
2. Local jobs. CS jobs in my hometown and in my college town that I think I have a good shot of getting. I had one of these internships last summer, and while I did gain a lot of valuable experience, I would like to do something better for this summer.
So my question is this: What is the middle tier? Where do I find the jobs that aren't the Google-tier jobs, but that the Google-tier jobs will want to see on my resume come next year when I'm looking for internships again? Obviously nobody is going to advertise themselves as mid-tier, so I'm wondering if anybody has been in a similar place to where I am now, and has any suggestions for searching.

Thanks so much!",,,,,Submission,2,0,2
d7k4bpp,2016-09-12 17:33:24-04:00,Razor_Storm,,"I live and work in SF, and there's tons of startups here. These startups have varying levels of success and renown: from world famous rocketships that have a reputation on par or even better than your top tier list, to small mismanaged broke ideas that will hire anyone who walks through their doors.

Between these there are tons of companies that are doing quite well and have rosy futures but not quite has the clout to attract the best talent. These companies are still usually well known enough that if you were to apply to a better company in the bay area the experience will be a positive in your resume.

Check out angelist http://angel.co/ many startups and larger companies frequently look for talent there. I found my current job there. 


",52d8ab,t3_52d8ab,_Casey_Anthony,,Comment,1,0,1
52csnt,2016-09-12 01:18:50-04:00,chaotic2h,"Sophomore student studying CS, what should be my priorities in getting internships and such?","I've always had a passion for computers and their internal features. However, I spent the first 20 years of my life in a religious Jewish cult, where there wasn't really a chance to devote any spare time to extracurricular activities. 

Now I've broken away and am attending college. I know I want to major in computer science but I feel like I'm so far behind in terms of programming knowledge I'll never be qualified for an internship position. 

This semester I'm taking my first programming class and am actively enjoying it however the programs I'm writing are very basic such as a Fahrenheit to Celsius converter in Java or printing in JOptionPanes (I realize this is truly beginner stuff, yet this is the extent of my knowledge)

I'd like to get an internship this summer but I have no special qualifications of CS knowledge other than a 4.0 GPA. Between 15 credits of classes and 30 hours of work a week I struggle just to stay on top of my classes. Never mind learning how to program on the side and working on personal projects. 

How can I best prepare myself? I am a true beginner and like an idiot when I can't even write a simple program and people in my class have coded video games already and know about four languages?",,,,,Submission,11,0,11
d7jeg7q,2016-09-12 07:02:34-04:00,RHC1,,"My advice would be to just apply and study some interview problems. I didn't start doing internships until junior year because I didn't think I had enough knowledge but I figured out that you can do internships no matter what year you are. I personally like to tell people to aim for learning one new language every year to have some flexibility under your belt, but if you don't have a lot of outside time to do any side work I'd recommend trying to put some of your school projects on Github. Even if they're small you're still a sophomore, they're not going to expect you to know everything. Practice some interview problems and put school projects on GitHub and apply everywhere! Hope this helped!",52csnt,t3_52csnt,chaotic2h,,Comment,3,0,3
d7jfd3e,2016-09-12 07:46:28-04:00,rkaz246,,"I know my company hired a freshman intern. He had a little background knowledge but he wasn't very technically inclined. 
It's great because my company hirea a lot of interns that eventually get hired. So he got to review different code bases, see how the sdlc works in real life and not just on paper and all of that. It couldnt hurt to brush up on the basics of some languages, but if a company may see that you are willing to learn and that will help you a ton. ",52csnt,t3_52csnt,chaotic2h,,Comment,1,0,1
d7jsrut,2016-09-12 13:33:27-04:00,huck_cussler,,"You can apply and try to get an internship.  I applied my freshman year and got one phone interview.  I didn't get the job because I just flat out didn't know the answers to their questions.  I would have known them the subsequent year.

Even if you don't get a job, it's good to start getting used to technical interviews.  It's good to make some connections with employers who might hire you in the future.

I ended up working for a professor in the department over that summer teaching basic programming to junior high and high school students.  It was a lot of fun and beneficial.  It's always good to get on the good side of professors in your department.

I'd say you can best prepare yourself by overdoing your assignments.  Does your temperature converter work for really large input?  Why or why not?  Can you write a program that will automate testing for all such input?  What happens if the input is non-numeric?  What other sorts of testing have you done on it?  Can you use the same basic idea and structure to create a program that will convert numbers in one base to another one (e.g. convert from decimal to binary)?",52csnt,t3_52csnt,chaotic2h,,Comment,1,0,1
d7jy4t6,2016-09-12 15:24:05-04:00,BrokeDiamond,,"You probably won't get an internship this year. It's okay. Most first year CS majors don't, despite what you may hear.

That being said, here should be your priorities in developing your skills as a developer ASAP:

1. Take as many programming classes as you can but maintain high grades. You obviously have some catch-up to do in terms of classwork, but never compromise GPA for that.

2. Join a CS student/project group. Everyone else is suggesting studying interview questions which is okay, but doesn't really help if you can't get an interview in the first place. By joining a CS student/project group you'll have some extracurriculars/projects to put on your resume, and will get some hands-on programming experience.

3. If you have any more time after that, then study interview questions. Alternatively, once you get an interview, you can forget about #2 temporarily to focus on this one.

And of course #0: make friends in high up places. You probably won't be getting a nice Silicon Valley job your first try, but if you know anyone working for a startup or a company that's not traditional tech (medical equipment, aerospace/defense, retail, etc.) and they're probably not returning next summer, then it's simply a matter of asking them to forward your resume to their manager for consideration. Networking always gets better results than trying to be a programming genius.",52csnt,t3_52csnt,chaotic2h,,Comment,1,0,1
528jyr,2016-09-11 09:44:43-04:00,sssssunshine,"My uni shuts down the wifi everyday at midnight and it's back up working in the morning, what must they actually being doing?","The routers are all up and working throughout the night. Even the network is visible in the *""discover networks""* on a system.

Do you know what this actually is? Do they pull on some ""on/off"" switch everyday?",,,,,Submission,19,0,19
d7i6mi2,2016-09-11 10:26:23-04:00,Syde80,,They probably just have time restrictions setup on their firewall or web gateway,528jyr,t3_528jyr,sssssunshine,,Comment,17,0,17
d7i6v5i,2016-09-11 10:33:46-04:00,sssssunshine,,"Hi,

Thanks for the answer, is there a remotely possible chance I can somehow bypass this? You know test weeks are coming up and I'm kind of a night-crawler kind of guy. So, I'd appreciate it if I could study at night as well.",528jyr,t1_d7i6mi2,Syde80,,Reply,8,0,8
d7i8e1v,2016-09-11 11:17:38-04:00,Steve132,,Can you *ask* them?  It seems like a terrible policy imho.  At my university if they did this there would have been riots.,528jyr,t1_d7i6v5i,sssssunshine,,Reply,36,0,36
d7ig00f,2016-09-11 14:10:15-04:00,sssssunshine,,"Yeah man, a pretty terrible policy indeed. The only plausible cause of this action we, as students, are made aware of is that the administrations somehow expects undergrads to 'sleep' in this 6 hour period(wtf). On a side note, our data caps on wifi are restricted down to 10gigs/month as well.

I like your idea, and I'm gonna contact the student council and maybe make an online petition of sorts to try and get as many students involved to get this thing sorted out asap. Thanks :) ",528jyr,t1_d7i8e1v,Steve132,,Reply,9,0,9
d7iy3dn,2016-09-11 20:44:48-04:00,sandwichsaregood,,"10gb/month? Wow, that's *really* limited. Is there some reason they restrict internet access so much, like a conservative religious school or something? Or is this only for wifi?",528jyr,t1_d7ig00f,sssssunshine,,Reply,5,0,5
d7j5viq,2016-09-12 00:00:55-04:00,sssssunshine,,"It is. Though I'd like to clarify as this point, that this is the 'free' usage we get. Move beyond that and start paying for every gb you use. I'm guessing this has more to do with making extra money.

Not a religious school, but conservative? Hell yes. Ohh, did I mention? I'm based in India, and this is one of the 'top rated' engineering schools here. A lot of the policies are pretty ridiculous as well, this is my 7th sem and I can't wait to get my degree and vanish.",528jyr,t1_d7iy3dn,sandwichsaregood,,Reply,6,0,6
d7j2k0e,2016-09-11 22:32:40-04:00,NearSightedGiraffe,,Especially as some course related downloads can be large. Start of the year I used the uni wifi to fownload Matlab (software keys are provided through our course fees)... that was 7.5gb on its own,528jyr,t1_d7iy3dn,sandwichsaregood,,Reply,3,0,3
d7j5xx8,2016-09-12 00:03:02-04:00,sssssunshine,,"Exactly! Updating Windows is a pain in the arse, so is downloading essential softwares like Matlab, Pspice, HFS, Keil, etc etc.",528jyr,t1_d7j2k0e,NearSightedGiraffe,,Reply,2,0,2
d7ifgx0,2016-09-11 13:59:39-04:00,wackyvorlon,,It may be necessary to organize a group if they do not really acquiesce. ,528jyr,t1_d7i8e1v,Steve132,,Reply,6,0,6
d7i8o9i,2016-09-11 11:25:11-04:00,splenetic,,"There's half a dozen different ways they could be doing this. Some will have work-arounds - if they're just blocking http and https then an ssl vpn might work. If they're just blocking dns then using a different dns server might work. But if they're blocking everything then there's no realistic way around it.

",528jyr,t1_d7i6v5i,sssssunshine,,Reply,5,0,5
d7ig3ik,2016-09-11 14:12:11-04:00,sssssunshine,,"Thanks for the answer, pretty solid points.

Just for the record, we tried using a different dns server once, and it didn't work. ",528jyr,t1_d7i8o9i,splenetic,,Reply,2,0,2
d7i9z72,2016-09-11 11:58:37-04:00,Syde80,,"As /u/Steve132 your best option is to make a case and ask them.  Yes there may be ways to get around their restrictions but without knowing their method it's difficult to say how.  Besides, you likely signed something that said you agree to their terms and violating their terms could give you some form of punishment all the way up to expulsion.. I doubt that would be the first strike, but bring banned from their network could be.  You likely don't want this.",528jyr,t1_d7i6v5i,sssssunshine,,Reply,2,0,2
d7ig7qh,2016-09-11 14:14:34-04:00,sssssunshine,,"Exactly! As I mentioned in a previous comment, I'm gonna contact the administration and try to get as many students' support and try to revoke this ancient rule. And yes, you're correct, there are punishments of course. Though some guys still use proxies and get their way around with unlimited access. Thanks for the advice though :)",528jyr,t1_d7i9z72,Syde80,,Reply,1,0,1
d7ieqb0,2016-09-11 13:44:34-04:00,thasquidkid,,You could try changing your MAC address temporarily ,528jyr,t1_d7i6v5i,sssssunshine,,Reply,1,0,1
d7ig8vb,2016-09-11 14:15:13-04:00,sssssunshine,,"I'm not aware of how this is done, can you link me to a good source?",528jyr,t1_d7ieqb0,thasquidkid,,Reply,1,0,1
d7inav7,2016-09-11 16:37:31-04:00,skunkwaffle,,This is when the frats all turn on their porn syphons.,528jyr,t3_528jyr,sssssunshine,,Comment,1,0,1
d7itilv,2016-09-11 18:56:44-04:00,I_AM_TESLA,,What school is this? ,528jyr,t3_528jyr,sssssunshine,,Comment,1,0,1
d7j6874,2016-09-12 00:12:05-04:00,sssssunshine,,"I'm not really sure if I should name it here, but I'm from India, and it is one of the top private engineering schools here.",528jyr,t1_d7itilv,I_AM_TESLA,,Reply,2,0,2
d7j6m2c,2016-09-12 00:24:50-04:00,KrustyKrab111,,Hahaha bro that explains alot... (indian here),528jyr,t1_d7j6874,sssssunshine,,Reply,2,0,2
d7j9tl6,2016-09-12 02:31:50-04:00,hobabaObama,,Jio not available there?,528jyr,t1_d7j6874,sssssunshine,,Reply,1,0,1
d7iggvu,2016-09-11 14:19:30-04:00,thasquidkid,,"Here are the instructions for OSX (and I assume they are similar to Linux, if you use that instead)
http://osxdaily.com/2012/03/01/change-mac-address-os-x/
I will try to find the instructions for Windows.",528jyr,t3_528jyr,sssssunshine,,Comment,1,0,1
d7igppk,2016-09-11 14:24:24-04:00,sssssunshine,,"Thanks for the link, btw I recently switched from Debian to Windows 10. So a windows tut would be great! :)",528jyr,t1_d7iggvu,thasquidkid,,Reply,1,0,1
d7jxen6,2016-09-12 15:08:59-04:00,thasquidkid,,"(Sorry I'm a little late, I actually just went and bought a Windows computer 😐)
Try this: http://www.online-tech-tips.com/computer-tips/how-to-change-mac-address/",528jyr,t1_d7igppk,sssssunshine,,Reply,1,0,1
d7qr4c7,2016-09-17 13:42:19-04:00,sssssunshine,,Thanks a lot man! I'll go through it :),528jyr,t1_d7jxen6,thasquidkid,,Reply,1,0,1
d7ilp54,2016-09-11 16:04:52-04:00,bhuddimaan,,"If you know a pc that ""works"" even in this window....

If you are lucky you can turn it into a proxy.. 

You did not hear from me. ",528jyr,t3_528jyr,sssssunshine,,Comment,-6,0,-6
523xc5,2016-09-10 12:44:18-04:00,boobanies1234,It's baffling to me that a lot of people here use windows over Linux! Any reasons??,,,,,,Submission,0,0,0
d7h7ah4,2016-09-10 14:22:08-04:00,jordanminjie,,"There's an article from Joel on Programming that talks about this. The two OSs are by now pretty much equivalently powerful and the biggest difference between them is their culture.

http://www.joelonsoftware.com/articles/Biculturalism.html

If you give it an honest try, I think you'll be able to understand why some people might happen to prefer Windows, even if you yourself don't.",523xc5,t3_523xc5,boobanies1234,,Comment,5,0,5
d7h7wk5,2016-09-10 14:36:45-04:00,boobanies1234,,"I read the article! Very interesting indeed. But it was written in 2003. Both windows and linux have come a long way. Back then, they wouldn't have imagined microsoft to incorporate the bash shell. It's been 13 years (right?) and the field has change, and it's incorrect to assume that the gap is still the same. Do you have a more current article?",523xc5,t1_d7h7ah4,jordanminjie,,Reply,0,0,0
d7hgh9c,2016-09-10 18:16:30-04:00,jordanminjie,,You don't need to assume anything about the gap. I'm telling you that it's been more or less closed.,523xc5,t1_d7h7wk5,boobanies1234,,Reply,2,0,2
d7i4rce,2016-09-11 09:21:04-04:00,levilisko,,steam,523xc5,t3_523xc5,boobanies1234,,Comment,3,0,3
d7i4rrl,2016-09-11 09:21:31-04:00,levilisko,,and yes i know steam runs on linux,523xc5,t1_d7i4rce,levilisko,,Reply,3,0,3
d7i5vrf,2016-09-11 10:02:17-04:00,boobanies1234,,There's steam for linux,523xc5,t1_d7i4rce,levilisko,,Reply,0,0,0
d7i7ghy,2016-09-11 10:51:31-04:00,levilisko,,"as I promply commented...

You know I could also emulate the 80% of my library that doesn't run on linux or use another windows box and stream the games... I could also  take a chopper to go grocery shopping or go to work everyday by horse - I'm pretty sure that works well for several people around the world, as for me I'm happy to walk the groceries home and take my car to work.

And that's not to say that Linux is bad, it is not, it's a great OS that basically runs the internet, and is probably the basis of the mobile  market nowadays",523xc5,t1_d7i5vrf,boobanies1234,,Reply,2,0,2
d7ipslj,2016-09-11 17:31:36-04:00,gyroda,,"Which doesn't have most AAA games on it. There *is* WINE, but why bother when I'm dual booting both. 

Look, I use Linux and used to have it as my daily OS and I still use it for major pieces of development. But most of the time, on my home PC, I'm not doing anything too major and being able to have steam downloading updates or boot up overwatch instantly is a good enough reason to use Windows. 

There's also the windows specific tools, like certain game engines.  Using some proprietary software on Linux can be annoying (usually remembering how I installed it to later uninstall it is the problem). Add in that when developing *for* Windows you're probably best of using Windows. ",523xc5,t1_d7i5vrf,boobanies1234,,Reply,1,0,1
d7j7948,2016-09-12 00:46:48-04:00,boobanies1234,,"> Add in that when developing for Windows you're probably best of using Windows.

This never occurred to me.I've never considered this aspect of software development. TIL",523xc5,t1_d7ipslj,gyroda,,Reply,1,0,1
d7h3l68,2016-09-10 12:49:20-04:00,thatguywhorows,,Some of us have shit to do. ,523xc5,t3_523xc5,boobanies1234,,Comment,3,0,3
d7h40cg,2016-09-10 12:59:59-04:00,rocketbunny77,,Wrong sub,523xc5,t3_523xc5,boobanies1234,,Comment,2,0,2
d7h7c2x,2016-09-10 14:23:16-04:00,boobanies1234,,What might be the right sub? :P,523xc5,t1_d7h40cg,rocketbunny77,,Reply,1,0,1
d7jlxql,2016-09-12 11:04:43-04:00,andybmcc,,".NET.

Driver support.

DirectX.

Games.",523xc5,t3_523xc5,boobanies1234,,Comment,1,0,1
d7h57mt,2016-09-10 13:29:34-04:00,CoopNine,,"What's the reason for using Linux over Windows on your desktop?  What exactly can you do easier?

I've been a Linux user since the mid 90's... but if you don't think Windows is a better desktop OS, you're ridiculously biased.   Linux on the desktop has been a year out for over 15 years... and it still sucks.  I wouldn't make my mom use it, and I don't have the time to deal with a crappy desktop.  ChromeOS has real potential, but it's not really Linux, and it won't do my job yet.",523xc5,t3_523xc5,boobanies1234,,Comment,-1,0,-1
d7h7sdc,2016-09-10 14:33:54-04:00,boobanies1234,,"> What exactly can you do easier?

Code, configure, contribute to some of the most cutting edge open source technologies (which i frequently do), modify, have multiple window managers, safe package installers, save money, list goes on



 >Linux on the desktop has been a year out for over 15 years... and it still sucks

This means absolutely nothing. Which desktop manager did you use on your Linux machine? Some are meant to be user friendly, while some are meant to be light weight, while some are meant to be non existence. There are hundreds. Don't make trivial statements.



>and I don't have the time to deal with a crappy desktop

Another trivial statement. Some distros are designed to have faster updates and hence are unstable, while some are designed for stability. Like the Debian OS. Servers running debian have record uptimes. They have the most stable OS out there with full support. Again, trivial statements don't help anyone.



>I wouldn't make my mom use it

Absolutely. Infact, i have a tough time teaching my parent to use windows, let along linux. I assumed that this being a computer science threads, that this particular topic wouldn't come up. Apparently I got the wrong thread.",523xc5,t1_d7h57mt,CoopNine,,Reply,0,0,0
d7h8izt,2016-09-10 14:52:10-04:00,CoopNine,,"None of those are trivial statements, they are just apparently not ones you agree with.

I've used just about every distro and WM.    Not a single one is anywhere near as overall functional as Windows.   But there's also serious application gaps.  Find me an email client that works better than Outlook with Exchange servers, or a number of other apps.

I'm not arguing about servers.  I'm saying Linux is not a good platform for most people, including developers' desktops.

Because someone is a software engineer doesn't mean that they want to put up with substandard software.  I need to get work done, and Windows doesn't get in the way.  Linux does, and that is why I don't use it on my desktop.",523xc5,t1_d7h7sdc,boobanies1234,,Reply,4,0,4
d7huqok,2016-09-11 00:48:24-04:00,boobanies1234,,"I'm slowly starting to sense that maybe we do very different work on computers. 

It'll sound silly now, but my work is related to servers, deployment and kernel drivers. Go linux! :P

What kind of work do you need to do? I'm curious because up until this thread, i've always heard of windows getting in the way. I just want an introduction to your circle.",523xc5,t1_d7h8izt,CoopNine,,Reply,1,0,1
d7i4tek,2016-09-11 09:23:18-04:00,CoopNine,,"I'm an application developer, primarily web applications.  SaaS stuff.

We run Linux on our servers that host our Java applications.  .NET of course runs on windows servers, it's not quite the time for .NET to run on Linux yet.  Maybe in a year or two.

Our java developers usually use Eclipse, and deploy to a local app server on their machine.  Our apps run the same on windows as they do Linux since they are .war deployments.

The company we work for uses MS office.  Yeah, you could use Libre office, or Google and be probably OK, but office removes any issue with compatibility.  Outlook is a pretty great email/calendaring tool.  I've tried Linux alternatives, and they're all janky.  It's just a pain to try to run Linux and be able to do everything you need to.  Maybe once everything is hosted, which we will get to, but I'm watching ChromeOS for that.  

We spend a lot of time communicating with product teams, there's word and excel docs, along with Visio.  Again, yep, there's ways around all this, but they can be a pain in the ass. 

If I need Linux, I'll spin up a VM, I can use the same image I use on my servers so I don't run into things like, well I use SUSE on the prod servers, but like Ubuntu on my desktop, so when I want to install this software it's different here rather than there.

I'm a huge proponent of Linux, and love to talk about the benefits it provides, but won't evangelize for its use on the desktop, because it is super situational.  I see where both Windows and OSX have value that Linux isn't providing.  And that's OK.  Linux doesn't need to replace the desktop/workstation OS, and it likely won't, until our requirements for a desktop OS change.   ",523xc5,t1_d7huqok,boobanies1234,,Reply,1,0,1
d7h8cbz,2016-09-10 14:47:35-04:00,boobanies1234,,"I suggest you read this article first. It seems as though you've read a lot of ""Why windows is better..."" articles and none on the counter argument, which seems to make you ill informed. Especially given how loosely you're using the words ""linux on desktop"" etc

[Why arguing that windows is better than linux makes you look silly](https://www.linux.com/news/why-arguing-windows-better-linux-makes-you-look-silly)",523xc5,t1_d7h57mt,CoopNine,,Reply,-2,0,-2
d7h9oos,2016-09-10 15:20:51-04:00,CoopNine,,"I haven't read a lot of articles, I've used both Linux and Windows extensively over my career, this spans from the first slackware distro I downloaded back in the 90's, to elementary which I downloaded a couple months ago.    

You want to talk about trivial statements... that article is full of them.  If you're telling people to run wine and put up with those hassles, you've given up.  That article makes no case for Linux in the slightest.  It's fluff, intended to bolster people who are already biased, because the author has no argument on why Linux would be better for someone to run on their workstation or desktop.

Linux has a place, and that's the server room.   If it works for you, great, but it simply does not for most people.  You do Linux a disservice when you try to shame people into running Linux simply because they are developers.  In the best setups Windows and Linux complement each other.  They both do different things very well.

",523xc5,t1_d7h8cbz,boobanies1234,,Reply,1,0,1
d7hd55i,2016-09-10 16:48:13-04:00,UncleMeat,,Did you just come here to rant?,523xc5,t1_d7h8cbz,boobanies1234,,Reply,1,0,1
d7hurv9,2016-09-11 00:49:27-04:00,boobanies1234,,Either contribute or move along. I'm sick of people like you ,523xc5,t1_d7hd55i,UncleMeat,,Reply,-2,0,-2
d7iptnn,2016-09-11 17:32:17-04:00,gyroda,,The same could be said for this comment. ,523xc5,t1_d7hurv9,boobanies1234,,Reply,1,0,1
d7j77ar,2016-09-12 00:45:02-04:00,boobanies1234,,I'm am contributing towards shutting down unwanted menaces such as yourself. ,523xc5,t1_d7iptnn,gyroda,,Reply,0,0,0
521n0j,2016-09-10 00:56:06-04:00,Citozto,Advice for a CS student,"I want to have an informative interview with a computer scientist, software developer, software engineer, penetration tester, etc. I want to know what its like to be CS major at work. I'd be great if you're in NYC!",,,,,Submission,1,0,1
d7h03ho,2016-09-10 11:18:31-04:00,umib0zu,,Why not jump on IRC? I know at least on freenode there's plenty of chats for your respective interest where you can ask meta questions and banter as well.,521n0j,t3_521n0j,Citozto,,Comment,1,0,1
520elj,2016-09-09 19:39:23-04:00,Microwave_on_HIGH,Vendor-neutral Firewall configuration guide?,"I recently got my Security+ certification and want to configure my firewall to develop working experience with the concepts I've learned.

Problem is, even though I'm familiar with the general protocols, ports, and concepts, I don't have the working knowledge to make good decisions about configuring my host-based firewall.

Is there a guide that gives you pointers and general guidelines, based on your priorities?  All of the guides I've seen so far are usually vendor-specific, and focus strictly on the technicalities (*open this menu, click here, yadda yadda*) rather than about the judgment calls for a secure, reasonable configuration.

Any recommendations would be greatly appreciated, thank you!",,,,,Submission,2,0,2
d7h2wx6,2016-09-10 12:32:04-04:00,boobanies1234,,"Find a textbook on network security.

The first few chaters will be about encryption ect, but you'll later find chapters on firewalls and their configurations",520elj,t3_520elj,Microwave_on_HIGH,,Comment,1,0,1
51zvxy,2016-09-09 17:47:18-04:00,qwerty_danny,Solving Recurrence Relations Algorithmically,"While doing some Big O proofs, I had to find the closed form of  recurrence relations for algorithms defined recursively. I wondered if I could check my answers online and noticed wolfram alpha can do it. 

What process does Wolfram Alpha or similar systems use to find the closed form of a recurrence relation? We typically look for patterns and then say ""ah! therefore it's log(n)"" or something.  Do programmed approaches to the problem differ from this?",,,,,Submission,5,0,5
d7gad3j,2016-09-09 18:49:34-04:00,zefyear,,"I looked into this awhile ago and as I recall it's not tractable for all cases.

On the other hand, SICP has you build a simple computer algebra system that can both solve and simplify in the first two chapters as well as providing a lot of material on simplifying recursive algorithms to their closed-form solution programmatically.",51zvxy,t3_51zvxy,qwerty_danny,,Comment,1,0,1
d7gs99u,2016-09-10 05:48:27-04:00,_--__,,"Linear recurrence relations can readily be solved algebraically (which is where Mathematica/WolframAlpha excels)  - with e.g. matrix analysis or generating functions.  And many recurrence relations you come across in algorithm analysis are linear (though possibly after a change of variable - e.g. taking logs).  There is also the Master Theorem, though I doubt that features in WolframAlpha's default computation path.",51zvxy,t3_51zvxy,qwerty_danny,,Comment,1,0,1
d7gvrkw,2016-09-10 08:56:48-04:00,tolos,,"There are a large class of equations that can be simplified (edit: or can prove no such closed form exists), as outlined in [A=B](https://www.math.upenn.edu/~wilf/AeqB.html) by Marko Petkovsek, Herbert Wilf and Doron Zeilberger.",51zvxy,t3_51zvxy,qwerty_danny,,Comment,1,0,1
51zjbb,2016-09-09 16:36:51-04:00,jnzq,What's the point of discrete math?,"My university requires discrete mathematics as part of my computer science major. I'm taking it right now, and it seems like purely a math class. Even my professor is a math professor who doesn't touch any comp sci related topics.

So my question is why is this subject relevant to our field? What can we use it for?",,,,,Submission,3,0,3
d7g63wu,2016-09-09 17:05:51-04:00,james41235,,"It helps you understand the building blocks of algorithms.  Counting principles, graph theory, the basics of probability, boolean algebra, etc.  These are used later when formalizing algorithms, as well as in more classically solved problems related to computing.",51zjbb,t3_51zjbb,jnzq,,Comment,6,0,6
d7gbay9,2016-09-09 19:14:21-04:00,veeberz,,"You'll find out when you're taking a class on data structures and algorithms! Discrete math is just a catch-all for math done using discrete structures (e.g. set of integers, graphs) rather than continuous (real numbers, curves). Many discrete math classes also throw in logic and proofs to give you the tools you need to solve problems in the future.",51zjbb,t3_51zjbb,jnzq,,Comment,6,0,6
d7gn3x2,2016-09-10 01:10:42-04:00,1stonepwn,,"Yup, my class even threw in regular languages, automata, and some grammars for good measure",51zjbb,t1_d7gbay9,veeberz,,Reply,4,0,4
d7i6t6e,2016-09-11 10:32:06-04:00,veeberz,,"Nice! That was a class on its own for me. But an intro to those topics wouldn't hurt! The class I took had mostly CS people, plus some math majors. I think it's worth mentioning that what I learned in discrete math class was useful in a geometry class I took as an elective. It's called ""Euclidean and non-Euclidean geometries"" or something like that. Through the course we sort of ""rebuilt"" geometry from scratch, starting with incidence geometry and ordered geometry. Discrete math gave me good enough of a foundation to approach that class.",51zjbb,t1_d7gn3x2,1stonepwn,,Reply,1,0,1
51yowf,2016-09-09 13:49:40-04:00,finessseee,"I love programming, but suck at calculus.. Any advice?","Title says it all. I absolutely suck balls at calculus, but I have a passion for programming. It's fun. I love computers and the tech industry. I'm currently pursuing my bachelors in CS. Doing great in everything except CALCULUS.. Idk if I'll ever be able to pass it, but I want to have a career programming. Any advice? ",,,,,Submission,25,0,25
d7fx7ov,2016-09-09 13:53:55-04:00,panderingPenguin,,"Work hard, pass the class, get your degree, probably never worry about calculus again. Sorry, but there's not really a more helpful answer than that. You've just gotta get through it one way or another.",51yowf,t3_51yowf,finessseee,,Comment,47,0,47
d7fxlms,2016-09-09 14:02:12-04:00,superAL1394,,"This. Just gotta grind at it.

That being said I have had to break out the calculus post grad, but now I have Wolfram Alpha.",51yowf,t1_d7fx7ov,panderingPenguin,,Reply,8,0,8
d7g4qzk,2016-09-09 16:35:27-04:00,panderingPenguin,,"Yeah, I did need calculus in some higher level courses like AI, but for the most part, it can be avoided if you don't want to deal with it.",51yowf,t1_d7fxlms,superAL1394,,Reply,2,0,2
d7fzps9,2016-09-09 14:47:03-04:00,huck_cussler,,"Calculus is more or less a ""character building"" class for CS majors.  It will almost never come up in programming unless you decide on one of a few specific programming routes to specialize in.

Find people to study with, even better if they are also CS people.  Get a tutor.  Practice solving problems.  You may believe you understand something but until you do it a few times, you probably don't.

I suck at calculus also.  I like it in theory, as in it still blows my mind to think about what it does and how.  But I super suck at doing it.

Definitely don't change your major to physics.

**edit:** But learn linear algebra.  That's handy to know.",51yowf,t3_51yowf,finessseee,,Comment,10,0,10
d7fzov4,2016-09-09 14:46:29-04:00,jaredub69,,Do you have to take it? My school has the CS degree setup so you can do stats or calc as your main math track. ,51yowf,t3_51yowf,finessseee,,Comment,4,0,4
d7g36a2,2016-09-09 16:00:56-04:00,finessseee,,Yeah it's a requirement for all schools I've looked at where I'm from (NJ). What school do you go to? ,51yowf,t1_d7fzov4,jaredub69,,Reply,2,0,2
d7g4js6,2016-09-09 16:31:01-04:00,panderingPenguin,,"If it's an ABET certified program, I'm pretty sure that it must include calculus",51yowf,t1_d7g36a2,finessseee,,Reply,2,0,2
d7k6ya5,2016-09-12 18:33:51-04:00,finessseee,,Yo what school do you go to where you don't have to take calc Cus I literally can't do this shit lol ,51yowf,t1_d7fzov4,jaredub69,,Reply,1,0,1
d7k7fqa,2016-09-12 18:45:26-04:00,jaredub69,,Portland State. If I'm understanding the BS of CS program correctly calc is not required. ,51yowf,t1_d7k6ya5,finessseee,,Reply,1,0,1
d7g3dsl,2016-09-09 16:05:28-04:00,Human_Transmutation,,I took all my calculus classes at a community college as a transient student while still pursuing my CS degree at a University. If this is an option for you I would highly recommend it. The classes were much smaller and seemed easier in general. ,51yowf,t3_51yowf,finessseee,,Comment,5,0,5
d7h0bo2,2016-09-10 11:24:43-04:00,ForTheBread,,I did this as well. Was even able to swap Calc 3 for Stats which was miles easier in my opinion. ,51yowf,t1_d7g3dsl,Human_Transmutation,,Reply,3,0,3
d7g3ltt,2016-09-09 16:10:26-04:00,zostendorf,,"Hi, I took calc I 3 times and calc II 4 times, all to get my computer science degree. It sucks but there is no getting around it at most universities. 
The best advice I can offer is to interact with the professor and to work with classmates on the math you don't understand. You will struggle, you may cry, but it can be done, I promise.",51yowf,t3_51yowf,finessseee,,Comment,6,0,6
d7gbbou,2016-09-09 19:14:54-04:00,tuxedoorigin,,3 and 2 for me. passed with a C and a D. It can be done.,51yowf,t1_d7g3ltt,zostendorf,,Reply,3,0,3
d7fzd5u,2016-09-09 14:39:33-04:00,Nattfrosten,,"[Watch through these, and do the exercises](https://www.khanacademy.org/math/calculus-home)",51yowf,t3_51yowf,finessseee,,Comment,3,0,3
d7g7epw,2016-09-09 17:35:55-04:00,Fate_Creator,,This. Khan Academy is one of the best resources for math that I've found. Couple it with Wolfram Alpha and you can figure out anything you'll need to know for Calc classes.,51yowf,t1_d7fzd5u,Nattfrosten,,Reply,2,0,2
d7g60o8,2016-09-09 17:03:46-04:00,Aaronvan,,"Check out PatrickJMT on The Tube. His is the best Calculus instruction out there, in my opinion. ",51yowf,t3_51yowf,finessseee,,Comment,2,0,2
d7ggubx,2016-09-09 21:45:20-04:00,qnmaster,,"I was terrible, too. I barely got a D my first time through Calc 2, but was at the top of the class when I retook it. Then I did well in Calc 3, too. Probably because I had 2 turns through Calc 2 and a lot of practice. It's a learned skill, not an innate talent.",51yowf,t3_51yowf,finessseee,,Comment,2,0,2
d7h103w,2016-09-10 11:42:59-04:00,987f,,"Forget the label of ""being bad at calculus."" It is false. You haven't spent as much time in relevant fields to perform as well as your peers. Spend more time on it. ",51yowf,t3_51yowf,finessseee,,Comment,2,0,2
d7fz6x3,2016-09-09 14:35:53-04:00,umib0zu,,[Try this.](https://www.math.wisc.edu/~keisler/calc.html),51yowf,t3_51yowf,finessseee,,Comment,1,0,1
d7gb9n3,2016-09-09 19:13:23-04:00,fromITroom,,"It's said before but take it from somebody in IT department in various industries. Unless you are working on low level programming or compression level shit, calculus isn't gonna get used. That being said, you got to either go to Salman Khan's website or just work hard, cram or practice. One way or another you got to pass it. ",51yowf,t3_51yowf,finessseee,,Comment,1,0,1
d7gjfat,2016-09-09 23:01:46-04:00,pseudoredditer,,"3 things: limits, derivatives, and integrals",51yowf,t3_51yowf,finessseee,,Comment,1,0,1
d7gvri6,2016-09-10 08:56:43-04:00,946789987649,,"go to university abroad, many unis in the UK do not have such a strong focus on the maths like American unis seem to.",51yowf,t3_51yowf,finessseee,,Comment,1,0,1
d7g7w15,2016-09-09 17:47:20-04:00,bit_banger,,"You might be just misunderstanding what Calculus is, as well as software development. They are the same. ""The study of how the change in one thing affects the change in another"".  An oversimplification, but true.  Math will serve you well, I wouldn't recommend dismissing it.",51yowf,t3_51yowf,finessseee,,Comment,-1,0,-1
d7kprps,2016-09-13 03:13:51-04:00,bit_banger,,Let me get this straight.....  I am being down voted for encouraging him to stick with math.....  Fuck off.,51yowf,t1_d7g7w15,bit_banger,,Reply,2,0,2
51vc0v,2016-09-08 23:17:40-04:00,Shwunky,Difference in Accessing SQLite and MySQL,"Can someone explain to me the difference between how SQLite and MySQL access the DB? I've read that SQLite stores your db in the file system, thus making it easy to access, while MySQL stores it in a server (or something like that). However, if MySQL does that, doesn't that mean that you would need internet connection every time you use it? Why can people use it even when offline? 

Thanks",,,,,Submission,1,0,1
d7fisya,2016-09-09 08:08:10-04:00,None,,[deleted],51vc0v,t3_51vc0v,Shwunky,,Comment,4,0,4
d7fs0b6,2016-09-09 12:03:24-04:00,Shwunky,,This makes sense. Thank you!,51vc0v,t1_d7fisya,None,,Reply,1,0,1
d7fwuls,2016-09-09 13:46:18-04:00,Treyzania,,"And to answer your last question, you only need a network connection if your server is on a different machine.",51vc0v,t1_d7fs0b6,Shwunky,,Reply,1,0,1
d7k5zeo,2016-09-12 18:11:02-04:00,BonzoESC,,"Fundamentally, the SQLite backend and the MySQL server access databases the same way: they use OS APIs to read and write one or more files on disk.

The difference is alluded to in the above sentence and other posts. While MySQL separates the ""you call this from your program"" and ""reads and writes files on disk"" parts into a client and server that communicate over a network, SQLite puts them into the same library that generally lives in the same process as your program. While the MySQL client talks to the MySQL server over a socket (can be a UNIX socket, a loopback TCP socket that doesn't need an external network, or a TCP socket to an arbitrary nextwork host), SQLite internally just uses registers and the stack.

SQLite seems like a huge win this way: no network needed, no server, just a public domain library that's already installed in all but one popular OS. Lots of built-in iOS, Android, macOS, and desktop Linux features use SQLite for this reason.

The downside is SQLite is almost entirely unsuited for handling multiple requests at a time. How everyone expects SQL writes to work is that they either happen all at once or not at all (this is an oversimplification of the ACID properties), which basically means that any reads to a SQL database can't touch anything that's being written. In SQLite 3, this means that any writes to a database can only happen when no reading at all is being done. This coördination is done through the filesystem, since that's the only shared state between multiple SQLite programs: filesystems are slow, requiring lots of communication between the program and the OS, and very very slow communication between the CPU and the disk. This is acceptable for phone call history, but not acceptable for a multi-user website. 

On the other hand, MySQL is smarter about coördinating writes and reads at the same time, can perform any synchronization using mutexes and other in-memory constructs (that may require context-switching to the OS but almost certainly won't require filesystem access), and as such is the preferred database for popular online services like Facebook and Twitter.",51vc0v,t3_51vc0v,Shwunky,,Comment,2,0,2
51u8kz,2016-09-08 19:11:25-04:00,velikov,What does this punch card translate to?,,,,,,Submission,17,0,17
d7ez92g,2016-09-08 20:27:07-04:00,thiagobbt,,"I got ""/*I CPQ.SASB"" on both [this site](http://www.masswerk.at/keypunch/) and [this one](http://tyleregeto.com/article/punch-card-emulator). Does it make any sense?",51u8kz,t3_51u8kz,velikov,,Comment,7,0,7
d7ggfs3,2016-09-09 21:34:00-04:00,velikov,,No clue. I think /u/glitchn may be right about the gibberish textbook image. My stats teacher will be proud (maybe). Thanks!,51u8kz,t1_d7ez92g,thiagobbt,,Reply,1,0,1
d7fbdd6,2016-09-09 01:46:14-04:00,glitchn,,"My guess would be that each character isn't related to converting to something like a text string. The first 5 characters could be used as line numbers, followed by a column for continuation marks or some other code to give reading instructions to the reader, and then only the numbers starting after column 7 or 8 would even be code. And the code doesn't need to translate to something readable to us, it could be some sort of command to the computer.

I feel like the only way to know what the card means would be to know what computer it was intended for. Or it could just be gibberish that a textbook maker made up as something that resembles a punch card.",51u8kz,t3_51u8kz,velikov,,Comment,4,0,4
d7ew9sc,2016-09-08 19:13:53-04:00,velikov,,"This was found in a high school statistics textbook. I thought it translated to ""PRSRF3"" (a variable described in the image's accompanying paragraph), but when I tried my hand at it I got ""/<9 378.B1S2"" (using the guide found at the bottom of [this masswerk website](http://www.masswerk.at/keypunch/).
Is there a way to figure out what it says (if it says anything different)?",51u8kz,t3_51u8kz,velikov,,Comment,3,0,3
d7f14jw,2016-09-08 21:10:27-04:00,Jedimastert,,It might be compiled bytecode,51u8kz,t3_51u8kz,velikov,,Comment,3,0,3
51tefn,2016-09-08 16:23:44-04:00,arguenot,"CS student in final semester, need a final project idea","I'm a CS student in my final semester and I signed up for a class called ""final project"". I thought that there would be a list of projects to choose from, or at least consultation or something but there's nothing. So I have to come up with a project to do and apparently there are no restrictions other than it has to take about 3-4 weeks of work to do.

I have no idea what to do. I'm terrible at coming up with ideas. So if someone could come up with suggestions for a project that would be incredibly well appreciated because I need to start this project soon and I feel really stressed out about the situation.",,,,,Submission,12,0,12
d7ep4y8,2016-09-08 16:31:21-04:00,Magoo2,,"What are your passions and/or hobbies outside of programming/computer science? Something you're passionate about is easy to come up with an idea for, and even though every passion/hobby might not be relevant for this project, it's not a bad place to start when trying to think of an idea.",51tefn,t3_51tefn,arguenot,,Comment,10,0,10
d7eq13r,2016-09-08 16:49:55-04:00,arguenot,,"Yeah I've heard that before and genuinely tried but there's just nothing there really. I don't really have many interests, just hanging out and stuff.",51tefn,t1_d7ep4y8,Magoo2,,Reply,4,0,4
d7ff3g5,2016-09-09 04:53:47-04:00,Davehig,,Make an application that helps people find their passion. Then use it to not be boring.,51tefn,t1_d7eq13r,arguenot,,Reply,11,0,11
d7epb5g,2016-09-08 16:34:55-04:00,jamuza,,Exactly what I'd have suggested.,51tefn,t1_d7ep4y8,Magoo2,,Reply,1,0,1
d7f648s,2016-09-08 23:08:13-04:00,SmoothB1983,,"How about making a phone app / web app that matches students to final project ideas (kind of like a dating app)? Then they can use it in future semesters.

You would need a login + authentication, a back end service, and a GUI. That sounds like a decent final semester project.

",51tefn,t3_51tefn,arguenot,,Comment,3,0,3
d7f6adj,2016-09-08 23:12:25-04:00,SmoothB1983,,"Alternatively, I proposed a group final project called Pizza Ace (the group went for something else, and the name was a play on Red Baron). It was basically seamless just for Pizza that cataloged pizza places by toppings, quality, price, distance, and time for delivery. The killer feature was a learning algorithm that used everyone's ordering + feedback to suggest other toppings you might like, or other pizza places you might want to try (for instance if the one you wanted was closed or had a long wait time).",51tefn,t1_d7f648s,SmoothB1983,,Reply,2,0,2
d7f5wij,2016-09-08 23:03:04-04:00,dxk3355,,"Do you have a job lined up for after graduation yet?  If so try and think of something related to that.  Otherwise try and think of something in the field you want to be in.  

All else fails build an android app that displays comic books from cbr format archives.",51tefn,t3_51tefn,arguenot,,Comment,3,0,3
d7f46l0,2016-09-08 22:21:21-04:00,CosmicResonance,,Privacy-related projects could be helpful for the community in general...,51tefn,t3_51tefn,arguenot,,Comment,2,0,2
d7exn7z,2016-09-08 19:47:33-04:00,hemenex,,You probably have access to the projects of previous students. Looking at them should give you some ideas.,51tefn,t3_51tefn,arguenot,,Comment,1,0,1
d7exvs7,2016-09-08 19:53:29-04:00,JudgeGroovyman,,"Something I haven't seen is a way to sort through things based on your preferences

Fork this JavaScript library 
http://lab.hakim.se/reveal-js/#/

and make the following key modifications to make it into a sorting app: 
Make it so it reads a JSON list or a pasted in list and presents two options to the user and you choose which is better and it implements a sorting algorithm to let you sort your list based on your preferences.
Add lawnchair.js integration so it saves state and so you can sort through several lists
Finally: Make it into an app and put it on both app stores
Or make it into a site that a user can add to their home page when they have uploaded a list

Later add in features to allow multi user access to sort a particular list


That's one idea and I hope it helps. I have many more and I can provide more details on this one if you want but id rather not type them in so contact me and you can call via Skype if you want
",51tefn,t3_51tefn,arguenot,,Comment,1,0,1
d7fhjsh,2016-09-09 07:15:27-04:00,daymi,,"It depends on what you want to do. Right now the big problems in industry are:

- reproducible builds: ensure that the program that is running is the one that you actually wrote
- formal verification: ensure that what you meant your program to do is what will actually happen - and nothing else
- hardware verification: does the hardware only do what it is told and nothing else (example: rowhammer) - and does it do it in the time alotted?
- secure communication: ensure that only the intended recipient can read a message you send him",51tefn,t3_51tefn,arguenot,,Comment,1,0,1
d7fl5qb,2016-09-09 09:24:28-04:00,moeseth,,Machine learning project that analyzes Austrlian Stock Exchange 🤔,51tefn,t3_51tefn,arguenot,,Comment,1,0,1
d7iq641,2016-09-11 17:40:01-04:00,gyroda,,"Speak to your lecturers and members of academic staff. 

I literally carpet emailed a bunch of academic staff. I spoke to my head of department who pointed out a member of staff who had a project going. 

That said, my final project needed a supervisor, so I needed the staff help either way. My project was also a much larger piece, we had 4 months (including dissertation and a business plan/academic paper as appropriate).",51tefn,t3_51tefn,arguenot,,Comment,1,0,1
d7fc831,2016-09-09 02:23:18-04:00,Smiley_35,,http://lmgtfy.com/?q=cs+project+ideas,51tefn,t3_51tefn,arguenot,,Comment,-3,0,-3
51rjak,2016-09-08 10:22:09-04:00,therealcatspajamas,Drastic difference in internet speed between two computers,"So I have two computers, one is a surface book with wireless internet, and the other is a custom build with a wired connection. 


Now I always expected the wired connection to be marginally faster, but I've been noticing huge, like 100+ Mbps differences between the two lately, and I can't put my finger on why.

The wired gets 170 Mbps and wireless gets 60-70.

My router is a Ubiquiti UniFy Enterprise which runs through my server.

Any help would be greatly appreciated!",,,,,Submission,0,0,0
d7e7mzo,2016-09-08 10:30:04-04:00,dxk3355,,This isn't ask tech support.,51rjak,t3_51rjak,therealcatspajamas,,Comment,7,0,7
d7e95qm,2016-09-08 11:03:45-04:00,TashanValiant,,"/u/dxk3355 is right, but I guess I can still offer advice?

What is your router listed as? Is it g/n/ac? That will make a massive difference. And do you mean MBps (Bytes) vs Mbps (bits)? The router in question appears to only have wireless n based on Amazon. That max range is about 600 mbps which is roughly 60-70 MBps.

Its just the nature of the beast. Wireless vs wired will almost always see a difference. Most ethernet ports nowadays are 1000 mbps. To keep up, you'd have to buy wireless ac, but even then with the tech and the overhead it causes you still won't see the speeds wired can achieve.",51rjak,t3_51rjak,therealcatspajamas,,Comment,1,0,1
d7ea694,2016-09-08 11:25:19-04:00,therealcatspajamas,,"Thanks!

It's megabits per second via speedtest.net

I originally gave the wrong router info it's actually a ""Ubiquiti Networks Unifi 802.11ac Dual-Radio PRO""

But even on wireless N I should be able to get above 70Mbps correct?",51rjak,t1_d7e95qm,TashanValiant,,Reply,2,0,2
d7eahqi,2016-09-08 11:31:58-04:00,TashanValiant,,">But even on wireless N I should be able to get above 70Mbps correct?

 Depends. Do you have other devices on the network? Are they wireless n? Where are you in relation to the router? 

Wireless is always gonna be slower. I'd personally just stop worrying. 70mbps isn't anything awful. 

Edit: See this post - https://www.reddit.com/r/wireless/comments/3na97l/ac_wifi_vs_gigabit_ethernet/",51rjak,t1_d7ea694,therealcatspajamas,,Reply,1,0,1
d7eawnn,2016-09-08 11:40:50-04:00,therealcatspajamas,,"Yeah I get roughly the same speed when I'm right next to my router, still only about 70. I do have other devices on the network, but not anything active right now",51rjak,t1_d7eahqi,TashanValiant,,Reply,1,0,1
51o7pj,2016-09-07 19:15:39-04:00,NewbProgrammer96,CS HELP NEED ASAP,"Can you show me how to convert, 112 and -112 in 2s comp. I can do Hex, Binary, even Octal but I CAN'T get 2s comp!!! thx ",,,,,Submission,0,0,0
d7dhvrn,2016-09-07 19:32:52-04:00,J2quared,,"2 comp. flip bits add 1 

So 5 is 0101 

-5 is
1010 (add 1)
So
1011",51o7pj,t3_51o7pj,NewbProgrammer96,,Comment,3,0,3
d7di8fl,2016-09-07 19:42:03-04:00,NewbProgrammer96,,THANK YOU! One more question how do you convert  -920.16 to binary? I thought you can't convert negative numbers and decimal numbers to binary? Thanks ,51o7pj,t1_d7dhvrn,J2quared,,Reply,0,0,0
d7dlt36,2016-09-07 21:11:12-04:00,huck_cussler,,"The answer is my favorite answer in computer science ... it depends.

It depends on whether it's fixed point binary, in which case the answer will be similar to above.  Or, if it's represented as a float, that's a whole other ball of wax.

Fixed point: http://www-inst.eecs.berkeley.edu/~cs61c/sp06/handout/fixedpt.html

Float: http://steve.hollasch.net/cgindex/coding/ieeefloat.html",51o7pj,t1_d7di8fl,NewbProgrammer96,,Reply,6,0,6
d7djuxw,2016-09-07 20:23:11-04:00,kakarotsan,,"The negative sign and decimal portion are stored in bits separate from the whole number, if I recall correctly.",51o7pj,t1_d7di8fl,NewbProgrammer96,,Reply,-2,0,-2
51mxi3,2016-09-07 15:06:03-04:00,ebolanurse,Starting my first web scraping project. I'll be gathering a lot of data. What should I use to store it?,"For the sake of discussion, the data I'll be gathering would be very similar to weather data or perhaps traffic data that accounts for individual automobiles on the road.

So basically I expect to create 5k+ objects that record things like direction, speed, and about a dozen or so other qualities. These objects could updated perhaps every 10 minutes<x<1 hr.

I want to be able to track these 24hr/day for at least 6 months(ideally over a year).

I'll most likely using python to take advantage of the big data libraries. 

Any advice on what the storage mechanism should look like?",,,,,Submission,8,0,8
d7dldzx,2016-09-07 21:00:47-04:00,bo1024,,Hard drive? Any reason not to just write the data to disk?,51mxi3,t3_51mxi3,ebolanurse,,Comment,2,0,2
d7dlq2e,2016-09-07 21:08:59-04:00,ebolanurse,,"I mean, that was what I figured but I wasn't sure what the best way to store it would be so that I could go back and analyze the data efficiently.

Tbh, this is probably a really simple answer it's just I'm so new to this I don't really know how to ask the question, ya know?",51mxi3,t1_d7dldzx,bo1024,,Reply,1,0,1
d7e4oxw,2016-09-08 09:15:13-04:00,bo1024,,Yeah for sure.,51mxi3,t1_d7dlq2e,ebolanurse,,Reply,1,0,1
d7e6tw6,2016-09-08 10:10:55-04:00,billdroman,,Use sqlite3. It's a well-designed file-based database with good Python support. Keeping your data in a proper structured store will make it easy to explore and use.,51mxi3,t3_51mxi3,ebolanurse,,Comment,2,0,2
d7dmcck,2016-09-07 21:24:23-04:00,IAmNotNathaniel,,"The absolute simplest thing to do would be just to dump the data out to log files that you can later ingest/manipulate however you want. Each line would be the date, object id, and whatever details you need.

The advantage here is that you might be comfortable dealing with plain text files, as far as backups, moving data around, compression, etc.

MySQL might be more than you need. However! A MySQL db would give you way more flexibility and allow you search against it much more easily. If you had things in logfiles, you may very well one day end up sucking them up and loading everything into a db anyway.

MySQL is fairly simple to learn, is free, you can install it on linux easily, etc. Almost any language you want to use will have a mysql lib with it.

You said you will be using linux, which sort of implies you aren't at the moment - if that's the case and you are more comfortable on Windows or something, I'd recommend installing it there first, just to see if it's something you want to work with. 

Happen to have access to a webhost? Most have all this junk already installed, along with phpMyAdmin (which is a simple web interface for mySql)

Don't be scared by the size of your data set.. I work with a mysql table with 12mil records every day, and that's just small time. Mine's only a few gb in size on disk.

You will want to google for some tips on simple table design - you may find you want a few tables instead of just one. but don't get too caught up worrying about everything - sounds like you don't need tons of relationships everywhere.",51mxi3,t3_51mxi3,ebolanurse,,Comment,2,0,2
d7d6wxo,2016-09-07 15:27:35-04:00,kurtms,,"Unless I'm mistaken this sounds like you should just store it in a database. You could either use a local database or a remote one through heroku, Amazon web services, etc.",51mxi3,t3_51mxi3,ebolanurse,,Comment,1,0,1
d7d73jv,2016-09-07 15:31:30-04:00,ebolanurse,,"You're probably not. I'm pretty green to programming. I understand the broad strokes of how to implement my idea but I've never actually worked with a data set *I* created, instead it's always been from zip files from a lab or something. 

This will also be a part of a resume building project where I get my own server running (which will also run the scraping program) I'll be using linux. Any suggestions for what kind of data base I should use or what keywords I should search to answer that question?",51mxi3,t1_d7d6wxo,kurtms,,Reply,3,0,3
d7d9ofx,2016-09-07 16:24:25-04:00,kurtms,,"Oh okay! I'm kind of the same - third year CS. Id suggest reading up on SQL which is a query language that lets you insert into and manipulate databases. Play around with it on code academy for a while and it'll make some sense. After that you can make a database on heroku (which uses MySQL iirc) for free (up to 10,000 rows iirc) and write a few php scripts to access it through http requests or do it in whatever language your application is (I've only done http requests because it's usually easier). This can save all your data neatly without much fuss (or less fuss than trying to save it as text or whatever).",51mxi3,t1_d7d73jv,ebolanurse,,Reply,2,0,2
51lkse,2016-09-07 10:41:28-04:00,how_tall_am_I,Questions about a human computer interactions class,Taking one this semester and I have no clue what it entails or what the assignments will be like. Is there coding involved? What does a typical assignment in this field/class look like?,,,,,Submission,1,0,1
d7cw3vy,2016-09-07 11:32:44-04:00,thepobv,,"Yo, ask the professor.

Ask the other students who have taken it before.

Read the course description/catalog.

Every school teaches different things... even if it may be the same title.",51lkse,t3_51lkse,how_tall_am_I,,Comment,3,0,3
d7cwc3m,2016-09-07 11:37:48-04:00,how_tall_am_I,,I'm going to when I go to class. It's a first year teacher and the first time this class has been offered at this school it's kind of a trial for the teacher to be fully integrated into the program. So as of right now none of those are available to me. I only ask this because I don't technically need to take this class it's an add on to make next semester easier. I just wanted some sort of thought from people who have taken a course and can give me some insight as to what it will be I'm signing up for. ,51lkse,t1_d7cw3vy,thepobv,,Reply,1,0,1
d7iqnvz,2016-09-11 17:51:06-04:00,gyroda,,"I've taken two classes in that, I even got a paper published for it! 

In one we did a study, mine was based on Fitts' Law comparing the use of a mac track pad to the leap motion (basically comparing if tactile feedback had an impact). 

In the second, we made a thing and wrote a paper on it. I'll PM you a link to mine as it has my real name on it. (Any spelling errors are blamed on my lecturer, who modified it a bit to make it more suitable to publishing than to grading).",51lkse,t1_d7cwc3m,how_tall_am_I,,Reply,1,0,1
d7czl2d,2016-09-07 12:48:48-04:00,somedifferentguy,,"In our university we got a chair for HCI which offers some lectures. Its group is lead by Prof. Borchers and the group is quite successful on the HCI conferences such as CHI etc.

The ""introductory"" course is called Designing Interactive Systems I and a crash course or short version of this course can be seen as a TEDx Talk here:

https://www.youtube.com/watch?v=evoa-ULOb0Y

Basically, you either love or hate HCI. There are those who think it's totally unnecessary bullshit and others who think it's super important and way too underrated.",51lkse,t3_51lkse,how_tall_am_I,,Comment,1,0,1
51jhrq,2016-09-07 00:24:17-04:00,cronos844,[Visual Studios 2015 C#] Is there a way to view and edit the pop up windows in my program?,"So I created a simple program that lets you enter your name click one of two buttons and then another window pops up saying either ""Hello [textbox]"" or ""Goodbye [textbox]"" depending on which button you press. So far, I have not figured out a way to edit the window that pops up, such as adding in another button that let's you sort of have a conversation with the computer.

Is there a way to display and edit the popup window?",,,,,Submission,1,0,1
d7cg3sm,2016-09-07 00:36:48-04:00,SockPuppetDinosaur,,Create a new view and show that instead of a pop-up?,51jhrq,t3_51jhrq,cronos844,,Comment,0,0,0
51c82t,2016-09-05 19:20:46-04:00,poobah34,"Ignoring the problem of physical space (having a case big enough), what complications might arise in building a computer with many terabytes of ram","Edit: Also ignore software problems like operating system support and so on, I'm mainly talking about hardware limitations to having RAM that size",,,,,Submission,15,0,15
d7b8zu8,2016-09-06 03:32:02-04:00,notsomaad,,Actually it is fairly common for off the shelf hardware such as dell or hpe servers or workstations to have up to 2TB of ram. It's very useful in scientific computing.,51c82t,t3_51c82t,poobah34,,Comment,7,0,7
d7bx9pz,2016-09-06 16:32:57-04:00,bit_banger,,Next gen will have ~6TB.,51c82t,t1_d7b8zu8,notsomaad,,Reply,2,0,2
d7bg8c4,2016-09-06 09:32:32-04:00,lordvadr,,"From a hardware standpoint, you'll have heat dissipation and power distribution problems, all of which could be solved with enough R&D.

At the chip level, the memory controller, well, any chip for that matter, can only drive the inputs of so many other chips. So, in server memory, they add what are called registers (not like CPU registers, although conceptually not all that different either).  Registers are just logic gates that output their input.  A simple explanation is say the memory controller output lines can only ""drive"" (drive means forcing the other chip's input to either a ""0"" or a ""1"", which takes a low but not zero amount of current) the address input lines of 16 DRAM chips.  If you drive 16 registers, and each are capable of driving an additional 16 inputs, you can now drive 256 chips--it's a lot more complicated than this, but that's the idea behind ""registered"" memory.  What I'm getting at is that you might have to have several stages of registers even before off-the-shelf registered DRAM modules, which would run your memory latency up quite a bit.  Nothing that couldn't be engineered around, but might require some hacks somewhere.

While it's not the ""biggest factor"", the speed of light is a concern.  Take a look at any motherboard and you'll find traces on the board that squiggle (technical term) back and forth to lengthen that trace.  That's done so that signals on that trace will arrive at the same time as signals on a much longer trace somewhere.  If you're talking about a motherboard something on the order of the size of a coffee table, this will become a bit of an issue as the clock-skew between your nearest DRAM slot at your farthest DRAM slot will be very significant.  This will run your latency up quite a bit as well.  Again, nothing that can't be solved.

Finally, a CPU can only do a finite amount of work.  As your memory space increases, you need more and more CPU power to do something with all that data, so now you're talking about more and more CPU sockets--which is a whole different problem.  Or you can spread your problem across multiple machines, which just scales better.  When you're talking DRAM space that size, and the latency that it would entail, a properly parallelized application could split that up among many machines, and with proper networking technology, have similar latency figures.

Bottom line is that the R&D for a single machine that big just isn't supported by any economic factors.  You could do it, but it would be very expensive and it doesn't scale well.",51c82t,t3_51c82t,poobah34,,Comment,6,0,6
d7b0uih,2016-09-05 22:30:41-04:00,IgnorantPlatypus,,How many TB of RAM are you thinking of? Servers today can be outfitted with 1TB of RAM. A few more years and it will be a few TB.,51c82t,t3_51c82t,poobah34,,Comment,2,0,2
d7bl4pp,2016-09-06 11:39:57-04:00,poobah34,,I didn't know that I guess I could just change it to an arbitrarily large amount of memory below 14 Exabytes (I think that's the limitation imposed by 64 bit words). ,51c82t,t1_d7b0uih,IgnorantPlatypus,,Reply,1,0,1
d7cueqi,2016-09-07 10:53:54-04:00,gyroda,,"Part of it would be latency. The memory would be physically further away, so signals would take considerably more time to pass there and back. ",51c82t,t1_d7bl4pp,poobah34,,Reply,1,0,1
d7awjg7,2016-09-05 20:36:49-04:00,AmateurHero,,"If I'm not mistaken, current CPUs have the hypothetical ability to address far beyond many terabytes of RAM, but there are other reasons we don't use that much RAM for a given system. I may fudge a few things here, as it's been a while since I've studied hardware and operating systems. I'm also going to use metaphors here to break it down.

Generally speaking, there are three tiers of memory. There is the CPU cache (L1, L2, L3, etc.), your RAM, and your hard drive. CPU cache can be thought of as free access (for simplicity's sake). You retrieve something from the CPU cache, and it's as fast as the speed of light. Your RAM is tier 2. Access is at the speed of sound. It costs a few CPU cycles, but it's damn fast. Reading from disk is like Usain Bolt. He's the fastest man alive, but comparatively, he's as slow as molasses. 

Without getting too technical, we limit the size of these memory tiers, because storing too much stuff in each tier causes retrieval time to encroach on other tiers. Notice that the CPU cache is never given in GB(usually)? Reading from the L3 cache is quick, but searching through too much data take too much time. Notice that RAM is never given in TB? Same reason as the CPU cache. As hardware improves, we are able to index and access these tiers of memory with better speed and accuracy. These improvements will allow us to have larger memory pools without sacrificing speed to read and write to them. 

You can have a system with terabytes of RAM, but if you're filling those terabytes with active data to look through, it'll take too long to retrieve what youre looking for. You may as well save money by skipping the RAM and partition disk space in place of it. ",51c82t,t3_51c82t,poobah34,,Comment,2,0,2
51b5tu,2016-09-05 15:42:26-04:00,idratherdrinktea,Affordable Laptop for sister starting compsci degree - Advice please,"Hiya,

I've been doing a bit of searching around for a suitable laptop for my sister who is starting her Bachelors this month in Maths and Computer Science (in the UK). I'm unsure as to what specs I should be looking at . Will she need a particularly beefy laptop for her first year? I don't really know what a computer science degree involves so I'm a bit out of my depth. Our budget is (ideally) max. £400.

We've been looking at a Lenovo on amazon. Would [this](https://www.amazon.co.uk/Lenovo-ideapad-310-15-6-Inch-Notebook/dp/B01HU6E5NY/ref=sr_1_40?s=computers&ie=UTF8&qid=1473077645&sr=1-40) be a good choice? 

Many thanks :)",,,,,Submission,4,0,4
d7axdvv,2016-09-05 20:58:55-04:00,AmateurHero,,"For reference, I have a 2007 HP dv6 [similar in specs to this dinosaur](http://support.hp.com/us-en/document/c01915421). Long story short, I happened to have it in my car when trying to work on an open source project that was not Windows friendly. It was slow. Everyone had a good chuckle at the laptop older than most everyone's professional career. However, it worked fine while the other Windows users had to move to something else. 

You don't need a current or even previous generation processor to program. If the computer works, it's probably good enough. ",51b5tu,t3_51b5tu,idratherdrinktea,,Comment,5,0,5
d7b2qbn,2016-09-05 23:21:29-04:00,DerekZoolanderJr,,"I know it's alot of money, but I had a windows machine for three years in college and then I switched to a Mac, The Mac was many times easier to use and it saved me SO much time and frustration compared to the windows machine. I know it's more money, but if your sister is committed to CS, then a Mac might be a good long-term investment.

Perhaps you could get a cheap one second-hand?",51b5tu,t3_51b5tu,idratherdrinktea,,Comment,2,0,2
d7bpcfr,2016-09-06 13:16:21-04:00,nacholicious,,"I bought a laptop for around 400€ for my degree five years ago, still use it today. Computer science is not a degree that requires beefy computers, as the most challenging thing to run would be an IDE to write code in. 

The RAM looks good, storage too, nothing that I'd think is needed more. As long as it can play youtube well (my current laptop cant), then she should be just fine for everything else",51b5tu,t3_51b5tu,idratherdrinktea,,Comment,1,0,1
d7c7kkk,2016-09-06 20:45:36-04:00,dhyanp11,,Check out [this](http://www.pcworld.co.uk/gbuk/computing/laptops/laptops/hp-pavilion-15-ab150sa-15-6-laptop-silver-10138272-pdt.html?intcmpid=display~RR) one.,51b5tu,t3_51b5tu,idratherdrinktea,,Comment,1,0,1
51b159,2016-09-05 15:16:44-04:00,RobotChikin,How does ReactJS link up with hosting company? (Softalicious),"Hello. I am trying to get into Web Development and bought a domain through Namecheap.com and hosting through CrocWeb.com. CrocWeb has Softalicious, which means one click install with many libraries, but not ReactJS. I want to code my site in React though. 

If I wanted to make a React app, would I just build it on my computer, then upload the files into my hosting company? How, then, do I do my backend. Meaning how do I setup the database. Would I install MySQL using Softalicious? I'm pretty lost lol.",,,,,Submission,0,0,0
d7b44us,2016-09-06 00:04:12-04:00,chasecaleb,,Do more research via Google.,51b159,t3_51b159,RobotChikin,,Comment,1,0,1
518wwv,2016-09-05 07:02:07-04:00,drummyfish,Why does Windows install drivers for my mouse again when I plug it into a different USB?,,,,,,Submission,18,0,18
d7a8qs5,2016-09-05 09:56:46-04:00,CelticJoe,,"Short answer: Plug n Play. It can often mean ""Unplug n Forget"" as well.

The longer answer gets a little sidetracked and theoretical as it has to do with USB being less universally standardized than the name and extremely high adoption rate would suggest. On mobile so I'll skip it for now but let me know if you want a more detailed answer.",518wwv,t3_518wwv,drummyfish,,Comment,12,0,12
d7ackc4,2016-09-05 11:49:43-04:00,Irony238,,I would appreciate a more detailed answer.,518wwv,t1_d7a8qs5,CelticJoe,,Reply,9,0,9
d7avuf3,2016-09-05 20:18:35-04:00,bit_banger,,It happens when manufacturers take shortcuts with the USB spec.  There are mechanisms for a device to properly identify itself and for the OS to just note the port change.  Sadly most devices do not use it.,518wwv,t1_d7ackc4,Irony238,,Reply,2,0,2
d7apavo,2016-09-05 17:18:12-04:00,lichorat,,!RemindMe 1 day,518wwv,t1_d7a8qs5,CelticJoe,,Reply,1,0,1
d7apbh9,2016-09-05 17:18:40-04:00,RemindMeBot,,"I will be messaging you on [**2016-09-06 21:18:34 UTC**](http://www.wolframalpha.com/input/?i=2016-09-06 21:18:34 UTC To Local Time) to remind you of [**this link.**](https://www.reddit.com/r/AskComputerScience/comments/518wwv/why_does_windows_install_drivers_for_my_mouse/d7apavo)

[**2 OTHERS CLICKED THIS LINK**](http://np.reddit.com/message/compose/?to=RemindMeBot&subject=Reminder&message=[https://www.reddit.com/r/AskComputerScience/comments/518wwv/why_does_windows_install_drivers_for_my_mouse/d7apavo]%0A%0ARemindMe!  1 day) to send a PM to also be reminded and to reduce spam.

^(Parent commenter can ) [^(delete this message to hide from others.)](http://np.reddit.com/message/compose/?to=RemindMeBot&subject=Delete Comment&message=Delete! d7apbh9)

_____

|[^(FAQs)](http://np.reddit.com/r/RemindMeBot/comments/24duzp/remindmebot_info/)|[^(Custom)](http://np.reddit.com/message/compose/?to=RemindMeBot&subject=Reminder&message=[LINK INSIDE SQUARE BRACKETS else default to FAQs]%0A%0ANOTE: Don't forget to add the time options after the command.%0A%0ARemindMe!)|[^(Your Reminders)](http://np.reddit.com/message/compose/?to=RemindMeBot&subject=List Of Reminders&message=MyReminders!)|[^(Feedback)](http://np.reddit.com/message/compose/?to=RemindMeBotWrangler&subject=Feedback)|[^(Code)](https://github.com/SIlver--/remindmebot-reddit)|[^(Browser Extensions)](https://np.reddit.com/r/RemindMeBot/comments/4kldad/remindmebot_extensions/)
|-|-|-|-|-|-|",518wwv,t1_d7apavo,lichorat,,Reply,1,0,1
d7c1zq0,2016-09-06 18:23:38-04:00,lichorat,,"Will you give a more in-depth answer? I'd love to learn. But it's okay if you can't, too.",518wwv,t1_d7a8qs5,CelticJoe,,Reply,1,0,1
d7awtoo,2016-09-05 20:44:24-04:00,AmateurHero,,There's actually an official [MS post about this](http://blogs.msdn.com/oldnewthing/archive/2004/11/10/255047.aspx) in their dev blog. The short answer is that Windows is doing its best to accommodate hardware from manufacturers who don't serialize their  devices. ,518wwv,t3_518wwv,drummyfish,,Comment,3,0,3
51753x,2016-09-04 21:43:08-04:00,Lambdabam,Issue with using doubles in Java.,"Hello, I'm a networking major and I have to learn Java. I have an issue in the assignment that I can't figure out. I'm actually finished with the assignment and it compiles and runs fine, but when calculating the tax, it has a lot of zeros behind the answer. Does anyone need to see the source code? Or should I just write out what I have? Thanks for the help!

*edit: Okay, I took screenshots of my code and the results. Thanks for the help! http://imgur.com/a/DhYI1",,,,,Submission,1,0,1
d7a98db,2016-09-05 10:13:15-04:00,dragonnyxx,,"**Short answer:**

Use System.out.printf(""%.2f"", number). This will format a floating point number with two decimal positions.

**Long answer:**

This is not Java-specific, it has to do with floating point numbers in general. You are used to decimal numbers, which have a decimal point followed by some number of decimal digits. You already know that decimal numbers cannot accurately represent some numbers (such as 1/3, which repeats endlessly).

Floating point numbers are similar, except they have a binary point followed by a number of binary digits. Just as with decimal numbers, they cannot accurately represent every value with only a finite number of digits, but the values they cannot represent are different. One of the more problematic cases is that 1/10 is an infinite repeating fraction in binary.

Because of this, math using floating point numbers almost invariably ends up being an approximation. You end up with an answer generally close to, but not exactly equal to, the true answer due to the roundoff error. Java's standard policy is to print out the fewest number of decimal digits necessary to uniquely distinguish the floating point number from its nearest neighbors -- so if the result happened to be equal to exactly 48, it would say ""48.0"", but if it ended up ever-so-slightly larger you might get ""48.000000000003"".

The standard means of dealing with this is to tell Java how many digits you want to print. The easiest way to do this is generally with the ""%.<number>f"" format string in PrintStream.printf() (though DecimalFormat will also work, as /u/HelloYesThisIsDuck suggests).",51753x,t3_51753x,Lambdabam,,Comment,2,0,2
d79t6k6,2016-09-04 22:22:37-04:00,filthywabbit,,"java.math.BigDecimal has a method called stripTrailingZeros, you could try using that. Or just use string manipulation to strip the trailing zeros manually.",51753x,t3_51753x,Lambdabam,,Comment,1,0,1
d79uuoz,2016-09-04 23:12:17-04:00,HelloYesThisIsDuck,,https://docs.oracle.com/javase/7/docs/api/java/text/DecimalFormat.html,51753x,t3_51753x,Lambdabam,,Comment,1,0,1
516yau,2016-09-04 20:55:30-04:00,c0sm0nautt,Technology careers without heavy programming?,"Hey all!
Are there any paths to a technology career that is not heavy in programming?

I am considering going back to school for my second bachelors for one of the following degrees:

Computer Programming and Information Systems: https://www.farmingdale.edu/academics/curriculum/bs-computer-programming-and-information-systems.shtml

Security Systems:
https://www.farmingdale.edu/academics/curriculum/bs-security-systems.shtml

I love working with hardware and software... doing stuff like building computers and cryptocurrency mining rigs. I took a few programming classes in HS and it didn't really appeal to me. However, stuff like command line in Linux I find interesting and fun.

So if I do decided to go back to school this Spring, it will probally take me ~2 years ($20k) and I will be 30 years old when I graduate. Are there many CS careers where I could focus on something like security without getting heavy into programming?

Thanks!",,,,,Submission,3,0,3
d79rvql,2016-09-04 21:43:20-04:00,DoktorJeep,,Look into solutions architect job postings to see if that interests you.,516yau,t3_516yau,c0sm0nautt,,Comment,2,0,2
d79vsdg,2016-09-04 23:40:22-04:00,c0sm0nautt,,Seems pretty interesting. I wish I knew some of the terminology so I could discern exactly what this position does... but seems to be a sort of project manager. ,516yau,t1_d79rvql,DoktorJeep,,Reply,1,0,1
d7a9y6q,2016-09-05 10:35:53-04:00,videoj,,"Some careers that are CS related that don't require major amounts of  programming:

Systems administrator.

Software sales engineer (depends on the product).

QA engineer (some coding, but mostly scripts and test fixtures).

Network engineer.",516yau,t3_516yau,c0sm0nautt,,Comment,2,0,2
d7adzgt,2016-09-05 12:26:57-04:00,TheTarquin,,"You might consider something like support engineering or DevOps: https://en.wikipedia.org/wiki/DevOps

Basically, helping to streamline deployment and maintenance of production software.  There will probably be a little bit of scripting and coding, but it's probably a minority of the job.",516yau,t3_516yau,c0sm0nautt,,Comment,2,0,2
516y1c,2016-09-04 20:53:36-04:00,tornato7,"What is windows actually doing when it's ""preparing to uninstall"" a program?",I was just uninstalling stuff from a Windows 7 machine and each one takes a few minutes to 'prepare' for uninstalling. Now that I know more CS I still have no idea what it's doing all that time.,,,,,Submission,26,0,26
d79v1us,2016-09-04 23:18:10-04:00,Parzival6,,"Windows can be a real mystery, even to people who maintain and develop it.",516y1c,t3_516y1c,tornato7,,Comment,17,0,17
d79r5pu,2016-09-04 21:21:19-04:00,ObeselyMorbid,,"Maybe indexing where stuff is saved in memory / storage,  or checking integrity of files? Not sure, interesting question though. ",516y1c,t3_516y1c,tornato7,,Comment,15,0,15
d79s4j3,2016-09-04 21:50:44-04:00,crookedkr,,This is a great question and I hope someone with some expertise comes through with an answer. My guess would be that it's searching the registry and/or file system for which keys/files need to be deleted or updated. Another guess would be it's copying state so if the uninstall fails it doesn't corrupt the registry or files.,516y1c,t1_d79r5pu,ObeselyMorbid,,Reply,8,0,8
d79tlez,2016-09-04 22:34:43-04:00,tornato7,,"Ah, yeah I noticed that the preparing phase isn't much different in length between large and small programs. So part of it could be backing up the state of the registry no matter what's being uninstalled.",516y1c,t1_d79s4j3,crookedkr,,Reply,2,0,2
d7aot2m,2016-09-05 17:04:59-04:00,BANGcake,,Is it possible it's doing some kind of dependency resolving as well? ,516y1c,t1_d79s4j3,crookedkr,,Reply,1,0,1
d7a60sv,2016-09-05 08:04:41-04:00,SleepingSheeple,,"Scanning for other programs that might have a dependancy on the one about to be uninstalled

Creating a restore point

Checking the validity of the source msi",516y1c,t3_516y1c,tornato7,,Comment,8,0,8
d79wfh2,2016-09-05 00:00:00-04:00,Pseudofailure,,"Do you know what programs you were installing? Most Windows MSI/EXE installers aren't Windows in the trademark sense: that is, they're written by third party developers, so you'd probably find your best answers by finding out the uninstallation program/framework that's actually saying this.",516y1c,t3_516y1c,tornato7,,Comment,3,0,3
d79yx90,2016-09-05 01:31:16-04:00,tornato7,,"I was uninstalling quite a few programs (LogMeIn, Itunes included) through ccleaner and most of these had the exact same 'preparing' window. Almost every installer has a similar step as well. Just wondering in general what the preparation step does. I think I have a better idea of it now though.",516y1c,t1_d79wfh2,Pseudofailure,,Reply,2,0,2
d7a8hrz,2016-09-05 09:47:59-04:00,tafoya77n,,"It maybe something that ccleaner puts in there, granted I have seen it on many other uninstalls.",516y1c,t1_d79yx90,tornato7,,Reply,-2,0,-2
d7abr3u,2016-09-05 11:27:55-04:00,scriptmonkey420,,"CCleaner does not alter the uninstaller, it just runs the one that the application installer put on the system.",516y1c,t1_d7a8hrz,tafoya77n,,Reply,2,0,2
d79x32z,2016-09-05 00:22:27-04:00,jpflathead,,"Creating restore points then deleting them.  
Downloading ads and uploading ""instrumentation"".  
Mining dogecoin.",516y1c,t3_516y1c,tornato7,,Comment,-5,0,-5
515nui,2016-09-04 15:55:10-04:00,Oli_Picard,How do I calculate Offsets of Sectors/Clusters in a file system?,"Hey /r/AskComputerScience

Just wondering how do you calculate Offsets of Sectors/Clusters in a file system? How are things different between using an NTFS system or Ext4 offsets.

Many Thanks
/u/Oli_Picard",,,,,Submission,2,0,2
d79m2zm,2016-09-04 18:46:05-04:00,IgnorantPlatypus,,I'm not quite sure what you're asking. Do you want to know how the filesystem decides where in the media (what offset) a part of a file will be stored at? Or what?,515nui,t3_515nui,Oli_Picard,,Comment,1,0,1
d7ag0ov,2016-09-05 13:19:33-04:00,Oli_Picard,,"Hey, Yup and how to find the offset within a sector.",515nui,t1_d79m2zm,IgnorantPlatypus,,Reply,1,0,1
d7agkoo,2016-09-05 13:33:47-04:00,IgnorantPlatypus,,"The offset within a sector is easy: you take the sector size (e.g. 512 bytes) and you take the file offset mod the sector size: e.g.:

    offset_in_sector = offset_in_file % sector_size

As for what sectors are used in a file, that's completely up to the filesystem implementation. Each filesystem will use its resources differently.",515nui,t1_d7ag0ov,Oli_Picard,,Reply,2,0,2
d7agpk1,2016-09-05 13:37:14-04:00,Oli_Picard,,Super thank you! One last question if thats ok? :-) How do you calculate the file offset?,515nui,t1_d7agkoo,IgnorantPlatypus,,Reply,1,0,1
d7ahfxk,2016-09-05 13:56:25-04:00,IgnorantPlatypus,,"The file offset itself is a parameter to e.g. the `pread` syscall.  Sometimes its implicit and maintained by the OS as part of the file structure: e.g. the `read` syscall starts with an offset of 0, and the offset increases as the file is read, so after a read of `N` bytes the offset is increased by `N`.",515nui,t1_d7agpk1,Oli_Picard,,Reply,2,0,2
51462v,2016-09-04 10:22:51-04:00,inviztj,Question about purpose of router in networking,"Sorry if wrong sub, but since I'm learning this in computer science I figured I should post it here anyway.

Is a router used to forward packets **from one network to another**, or **between computers in a network**?

 When I read about routers online they tend to lump routers together with bridges, hubs and switches which are used to connect computers within a network, yet others say routers are used for connection between networks, and not within a network.",,,,,Submission,4,0,4
d795pd2,2016-09-04 11:14:56-04:00,HelleDaryd,,"The answer is both, depending on the exact definition of router and what layer of the network we are looking at.

Presuming applied and related to lower levels of networking, so we can roughly use the OSI model, switches serve the purpose of routers for layer 2, routers connect layer 2 networks together at layer 3. ",51462v,t3_51462v,inviztj,,Comment,3,0,3
d7963ox,2016-09-04 11:26:59-04:00,inviztj,,"Okay that's something I did not learn about. I guess generally routers are used commonly to forward packets from network to network in big organisations, while in home routers they are used to forward packets between our devices?",51462v,t1_d795pd2,HelleDaryd,,Reply,1,0,1
d79c2v6,2016-09-04 14:11:42-04:00,panderingPenguin,,"Router is actually somewhat of a misnomer for home routers. They generally contain both a switch and a router. The switch handles traffic on your local network, and the router allows you to send packets to other networks over the Internet. It's a bit more complicated than that, but the basic idea is there.",51462v,t1_d7963ox,inviztj,,Reply,2,0,2
d7a0fey,2016-09-05 02:37:14-04:00,inviztj,,I see. Thanks for your help!,51462v,t1_d79c2v6,panderingPenguin,,Reply,1,0,1
d7byhv6,2016-09-06 17:00:18-04:00,andybmcc,,And the difference between a switch and a hub is generally the collision domain.,51462v,t1_d7a0fey,inviztj,,Reply,1,0,1
510wne,2016-09-03 17:30:40-04:00,Aggis,Advice needed!,"I am a 33 year old mom who got sick of working minimum vage jobs and decided to finally go to university and am on the first semester of cs. 

Every morning I go to school as soon as i've dropped my son of at kindergarten and I stay in school until I pick him up again at 4.

When I'm not attending lectures I'm working on homework.

As soon as my son falls asleep I keep studying until I go to sleep around midnight.

The thing is, even with all this studying i'm still at the point where I barely manage to complete homework assignments in time. I often have to skip lectures to finish them.
There is no time left for reading the textbooks.

I know a lot of this is because it's been so long since I've been in school, so my math is really rusty, and because I'm always worried about picking my son up in time/finding time to go grocery shopping/cooking dinner etc.

There is no way I am giving up on this, not just because of not wanting to work shitty jobs for a shitty salary anymore, but also because I really enjoy what I'm learning. It makes me happy.

So I am desperate for some tips on how to be more efficient in my studying, tips on where to find good resources for concise books or videos or just any advice you can give me that you think might help me out.

tl;dr Old hag, tired of being broke, went back to school, have a kid that takes up a lot of my attention, feel like i'm drowning. Please help!",,,,,Submission,5,0,5
d78lhon,2016-09-03 20:35:26-04:00,metaobject,,"I was in a similar situation for some of my undergrad and all of my masters degree.  Although, I must say, it was all definitely made possible bc of my wife being able to help out.  We had two kids and I was working full time for most of my undergrad and all of my grad degree.  If you want it bad, it'll be so worth it when you compete it.  Just don't give up.  It sucked having to stay up until 2-3 AM trying to get my code to work.  But in my opinion it gets easier as you get to higher level classes.  You may start noticing that you're gaining a type of intuition about how things work and you start to see the relationships between the various sub-genres of the CS world.

If you take public trans. you can take notes with you to review.  Some of my classes mirrored other classes at other schools.  For ex: my Intro to Algorithms course closely matched the one offered by MIT and I'd frequently go to theMIT OpenCourseware website and view lectures on topics I needed help on.

When you're studying figure out the best environment for you to be in.  Is it completely quiet?  Do you need headphones playing a certain type of music to drown out the people around you?  I use music a lot for writing code, but not so much for designing the layout of my software.  When I get the design down and understand what needs to be done, music can definitely help me finish faster.

Do you have any friends or family you can lean on so that a few hours on the weekend they can take you child while you study/do homework?  Can you afford a high school aged kid to do this on the weekend?

If you think you're struggling more than usual, do hesitate to go the professor or the TA for help.  You'd be surprised at the kinds of relationships that can come out of these meetings.  I was meeting with my Comouter Vision professor and started chatting about various CV topics, then in my senior year I ended up doing an independent study course that focused on CV and it was really cool.

Are there any resources on campus for people with young children?  Don't hesitate to ask the guidance/adviser organization.

",510wne,t3_510wne,Aggis,,Comment,3,0,3
d790owg,2016-09-04 07:53:56-04:00,Aggis,,"Thank you for your reply!

I have this weekend baby-free and am dedicating it to discrete math study. I don't feel like my professor explains things well enough so i'm trying to just solve as many problems as I can until I reach the ""aha moment"".

I haven't tried to use music, I'll try that, and I'll definitely check out MIT OpenCourseware and see if anything there could help me.

I agree that I do feel like i'm gaining intuition, slowly yes, but it is happening. And I recently realised that the short quotes one of my teachers puts at the top of all assignments are actually jokes, and I get them now, which makes me happy :)

Sadly there aren't any resources for student's with children. My mom has passed and my father isn't very reliable and his grandparents on his fathers side are farmers in the country.

I see now that my post makes it look like i'm a single parent. I have a husband, but he works late most nights and every other weekend, so sometimes after baby's asleep I can sneek out and head back to school, and sometimes when he has a weekend of he'll take him to his parent's so that I can study. It definitely helps, but I just feel like I still need more time to be able to grasp the material. 

After reading through the replies here I do believe that it will become easier as I get further in my studies, and since I really enjoy learning cs I know I'll hang in there.

Again, thank you for your input!",510wne,t1_d78lhon,metaobject,,Reply,3,0,3
d78ozfy,2016-09-03 22:21:48-04:00,Pseudofailure,,"CS usually has two sides to it: Theory and practice. Theory tends to be math, logic and proofs: practice is programming and application of the theory. Im going to answer for the latter, because I'm not great on the former. 

I presume you're in an intro-level programming course in something like C/C++, Java, or Python? If you're learning programming--and you don't have time for extra-curricular practice, as you stated--then I think the best thing you can do is working on practicing mindset and learned skills. Both of which don't really take much extra time. 

In terms of skills, a really important habit to get into is to actually write your own code. That is, if you find a solution to a problem on StackOverflow--which will save your life if you haven't heard of it yet--then don't just copy and paste the answer into your code. If you instead *type the answer by hand*, then you'll actually learn more and develop a better understanding, than if you were to copy and paste code from elsewhere. 

My suggestion for mindset is a bit more abstract. Try to think in terms of algorithms and code occasionally during your day. You don't need to know actual valid code, but you can develop some nice skills by just imagining how you'd ""program"" every-day tasks of your life. Like, even if it sounds silly, thinking about something like, for example, making toast, you can think about how you'd program ""get loaf: for each slice of bread: Put in toaster: wait for toaster to finish..."" Etc. A very large portion of programming is just algorithms, and finding ways to practice the thought process of programming a task will greatly help with your speed and general ability to create good programs. ",510wne,t3_510wne,Aggis,,Comment,3,0,3
d790rxd,2016-09-04 07:58:37-04:00,Aggis,,"That is a great idea! I'll definitely ""code"" every mundane task I do from now on! This is a great way to get in to the thought process of programming!

And yes, i'm learning C++ and feel like a superhero when I write the simplest programs. I love it :)

Thank you for your ideas!",510wne,t1_d78ozfy,Pseudofailure,,Reply,2,0,2
d78uwow,2016-09-04 01:57:08-04:00,TheCommador,,"I'm presently an undergrad in computer science. I applaud your determination, but do remember, academic burnout is real. 

To shed some light into the end of your tunnel.

My beginning classes were some of the hardest I've ever taken. But it seems to get easier as you advance. As another poster had stated, you begin to gain an intuition for it. Feel stupid now? I have good news! Everybody does. It's part of the self loathing nature of computer science. 

Hopefully that makes you feel a little better.

On to advice.

Visit the professors and teaching assistants. VISIT THEM. They will help you. I recognize the value of solving it yourself, but that isn't always practical. Furthermore, sometimes you never will get it on your own. 

Alright, you wish to become more efficient. That's a difficult one, but here's how I learned what I know. I think about it as often as I can stand to. I doesn't have to be correct, and it doesn't always have to be code. But I like to entertain my mind with asking myself questions, trying to answer them, and learning later. 

I hope I have helped some. 

[Here is a really nice link for when you talk about data structures and algorithms. ](https://www.cs.usfca.edu/~galles/visualization/Algorithms.html)",510wne,t3_510wne,Aggis,,Comment,2,0,2
d790w2s,2016-09-04 08:05:00-04:00,Aggis,,"I hope I'm not experiencing academic burnout after a month of school!

I'm just really stressed out trying to manage my time without my son having to feel how stressed out i am. I don't ever want him to feel like i'm in a hurry to get him to bed or like he is in the way.

I do feel stupid now and it does make me feel better to know everyone else does too haha! Thank you :)

I'll try to reach out more to the TA's and asking myself qestions.

Thank you!",510wne,t1_d78uwow,TheCommador,,Reply,2,0,2
d78z8iy,2016-09-04 06:19:56-04:00,Mildeww,,"Couple of things: one, if you only do something for money, you will never improve as fast or be able to do something as well as your peers who pursue it out of passion. 

Two: if you are doing it just to get a job writing code, and your priority isn't about being gaining deep knowledge of a wide amount of topics in computing, university probably isn't the best place for you. If you live in a city, or close to one there are a lot of ""coding bootcamps"" which are intensive programs (I think usually 6 months?) that are designed for people to be able to get a job as a programmer as fast as possible. If you're just trying to be a developer, you aren't going to need to understand or create algorithms that traverse graphs efficiently for example... you'll just need to be able to write a clean function that creates a JSON object or something. Think about what you want, and if the passion isn't there I would recommend not going through four years of university.",510wne,t3_510wne,Aggis,,Comment,2,0,2
d790zqa,2016-09-04 08:10:36-04:00,Aggis,,"Thank you for your input. But as I wrote in my post 

""There is no way I am giving up on this, not just because of not wanting to work shitty jobs for a shitty salary anymore, but also because I really enjoy what I'm learning. It makes me happy.
""

So i'm not just doing it for the money.

I do live in a city, but I live in a city on a tiny Island in the middle of the North Atlantic ocean, and there aren't any coding bootcamps here. There is one program that teaches you how to be a web page developer and it takes two years to get that degree. I might as well take one more year and become a cs :)

Thank you for your input.",510wne,t1_d78z8iy,Mildeww,,Reply,2,0,2
50ysf4,2016-09-03 09:26:14-04:00,saltedcaramel_,Computer Science vs Computer Science and Mathematics,"Does anyone have any advice/opinions on the advantages of completing a joint degree in Computer Science and Mathematics versus just a degree in Computer Science alone? As an undergraduate I have the option to switch to the joint degree and I have been considering making the change for a few weeks now. I understand the main differences to my course itself - having more options to choose maths courses over computing courses, etc. What I'm looking for is advice on whether the joint degree is looked at very differently by employers and whether it would lead me to certain careers over others. I don't want to base this decision purely on how much I enjoy maths as I honestly can't decide whether my enjoyment of the subject is enough reason to change my course. Any help would be appreciated!

Edit: If it makes a difference to know I am in the UK. So instead of picking my classes and eventually choosing a major I start my degree with my equivalent of a major already chosen. This also means I have a set of classes that are compulsory with only a few extra credits for me to choose whatever else I want to take. So I can't really do any equivalent of minoring in maths as I have to choose straight away since it will affect which classes I have to take.",,,,,Submission,9,0,9
d77ymes,2016-09-03 09:43:25-04:00,yes_thats_right,,"What would you like to do for your career?

Computer science and an advanced degree in mathematics would set you up for some very nice opportunities, such as quantitative development (finance).

If I were to do my degree again, I'd choose to add business to my comp sci degree, as this is required by a large number of senior management roles.

If you would just like a pure 'hands on' type of tech role, then all you will need is computer science.",50ysf4,t3_50ysf4,saltedcaramel_,,Comment,3,0,3
d783ujz,2016-09-03 12:23:06-04:00,saltedcaramel_,,"At this point I'm not really set on a career path. I'm open to a lot of options. 

What I am curious about is whether a joint degree with mathematics would open more doors for me so to speak.

The point you make about business is interesting. Do you really think a degree such as compsci with management would make a significant difference?",50ysf4,t1_d77ymes,yes_thats_right,,Reply,2,0,2
d798r9m,2016-09-04 12:42:44-04:00,wolfer_,,"I think there isn't much of a difference between someone who gets good grades in one degree versus someone who does it in two.

Math and computer science degrees do not match up much at all. They'll probably share a mathematical computing course and possibly statistics or cryptography. Otherwise the math courses you take won't relate much.

It's really a question of if you love math and want to learn more about it. Don't feel like you have to get a double degree though.",50ysf4,t1_d783ujz,saltedcaramel_,,Reply,1,0,1
d79degk,2016-09-04 14:47:35-04:00,saltedcaramel_,,I definitely do love maths and want to learn more. However I'm just trying to consider whether missing out on some compsci courses puts me at a disadvantage in terms of my knowledge of the subject.,50ysf4,t1_d798r9m,wolfer_,,Reply,1,0,1
d79eio6,2016-09-04 15:18:00-04:00,wolfer_,,"Computer science is an incredibly broad field. There's a very high chance whatever job you get in it isn't directly related to what you learn. Being good in the field is mostly about being good at self driven learning.

I guess that part of it is pretty related to math. In college math you are often presented with problems you haven't seen before and expected to figure out the solution/proof by understanding the solutions to similar problems you have done before.",50ysf4,t1_d79degk,saltedcaramel_,,Reply,1,0,1
d78cyee,2016-09-03 16:27:26-04:00,yes_thats_right,,"Honestly, it depends entirely on your career aspirations.

I am a senior manager trying to break into the executive world and finding that almost every job I want requires one of the following: MBA, Masters degree or Business degree (not all for the same role).

Technology roles for people with less than 10-15 years of experience will very rarely require these, so it only becomes relevant later in life and if you want to move more into business management/strategy.

The roles I have seen in technology which require any type of mathematics degree are very much a niche, however they are very well paid, more than just about any other technology job if you are good enough.

I suggest going to a few job seeking sites and looking for what types of positions interest you and see what the requirements are. I recommend: indeed.com, dice.com and efinancialcareers.com",50ysf4,t1_d783ujz,saltedcaramel_,,Reply,0,0,0
d78fyy1,2016-09-03 17:52:40-04:00,ebolanurse,,"Not op but I just realized earlier this month that I'm 2-3 classes away from a minor in mathematics, and I have interest in the finance route.

Also though, I'm very interested in developing a skill set that will give me a lot of opportunities to find jobs that I can telecommute with. (my dream is to work from a laptop off of a sailboat)

I get the feeling this conflicts with the finance route, though. Any advice? Other paths that offer high possibility for remote work?",50ysf4,t1_d78cyee,yes_thats_right,,Reply,1,0,1
d78ia7t,2016-09-03 19:00:28-04:00,yes_thats_right,,"I'd suggest having a serious think about your dream and decide how serious you are about it. Is it something you feel very passionate about and would not be happy any other way, or is it something you think would be cool in the same way that owning 100 ferraris would also be pretty cool?

In technology there are many, many different fields, and each is no doubt quite different from one another. I have done the startup thing, the government beauracracy thing and the finance thing so I can talk about those.

If you really want to work from a sailboat (or from a remote location), your choices are going to be very limited. For the first 5-10 years of your career I can only see this being possible if you work for a very new startup, probably somewhere that doesn't have an office and will mostly be paying you in equity. Alternatively, once you have made a good reputation for yourself (e.g. after 10 years of high performance), you could become a consultant which will allow more time away from the office - although probably just one or two days a week.

There are some related fields however where working from home (or a sailboat) would be more acceptable, for example as a recruiter.

Anyway, my advice for you right now is to focus a bit more on what you will be doing rather than where you will be doing it from - unless that is something very, very important to you.",50ysf4,t1_d78fyy1,ebolanurse,,Reply,1,0,1
d796rmt,2016-09-04 11:46:58-04:00,saltedcaramel_,,I'm interested in learning more about the options available for those with degrees in compsci to work in finance. I'm not very aware of the kind of careers I could end up in if I followed this route. Do you have any advice on the best way to read up on this?,50ysf4,t1_d78cyee,yes_thats_right,,Reply,1,0,1
d7997m4,2016-09-04 12:55:03-04:00,yes_thats_right,,"There are literally hundreds of different careers.

I do recommend going to efinancialcareers.com and searching for Technology. Otherwise you could go to a bank website and search for their open tech roles (e.g. http://careers.bankofamerica.com/search-jobs.aspx?c=united-states&r=us you will need to select technology from the career areas menu) or a hedge fund (e.g. http://www.goldmansachs.com/a/data/jobs/americas-technology.html)

Tech roles will basically be in delivery (building technology for the organization), support (making sure everything runs smoothly) or in rare cases for very smart people, in front office actually generating money through trading strategies.",50ysf4,t1_d796rmt,saltedcaramel_,,Reply,2,0,2
d79d54c,2016-09-04 14:40:31-04:00,saltedcaramel_,,I'll definitely spend some time looking around. Thanks for the help!,50ysf4,t1_d7997m4,yes_thats_right,,Reply,1,0,1
d79dr0l,2016-09-04 14:57:08-04:00,yes_thats_right,,you're welcome. Good luck!,50ysf4,t1_d79d54c,saltedcaramel_,,Reply,1,0,1
d780hlm,2016-09-03 10:47:12-04:00,Mildeww,,A lot of universities with less strict graduation policies offer computer science and computer science and mathematics. Strictly computer science probably won't develop your mathematical skills as much as you might want. Remember that cs is basically just applied math. Also getting good at math will improve your quantitative thinking skills. Also math is a lot cooler in college than it was in high school.,50ysf4,t3_50ysf4,saltedcaramel_,,Comment,2,0,2
d7844mf,2016-09-03 12:30:43-04:00,saltedcaramel_,,"I very much enjoyed the advanced level maths I worked through in my last year of high school. That is really what prompted me to start considering a joint degree.

A concern I have is whether I would 'miss out' on some of the computer science knowledge that other graduates would have, considering I'd be swapping out some of the compsci courses I would have taken for maths ones instead.",50ysf4,t1_d780hlm,Mildeww,,Reply,2,0,2
d784s0r,2016-09-03 12:48:18-04:00,Mildeww,,"I had the same problem, so I just added a math minor and kept the cs major. I guess it depends on how much math you want to do. ",50ysf4,t1_d7844mf,saltedcaramel_,,Reply,1,0,1
d781xvy,2016-09-03 11:30:20-04:00,somedifferentguy,,You could also look if it's possible to just minor in maths.,50ysf4,t3_50ysf4,saltedcaramel_,,Comment,2,0,2
d7847sz,2016-09-03 12:33:04-04:00,saltedcaramel_,,"I'm not in America so the system is slightly different. My course is relatively fixed from the beginning, we don't pick a major after some time.",50ysf4,t1_d781xvy,somedifferentguy,,Reply,1,0,1
d784fod,2016-09-03 12:39:07-04:00,dmazzoni,,"In my experience, employers and even grad schools won't care at all about a joint major or double major.

Focus far more on the classes you take, then pick the major that fits.
",50ysf4,t3_50ysf4,saltedcaramel_,,Comment,1,0,1
d784szz,2016-09-03 12:49:01-04:00,saltedcaramel_,,"For my situation, the classes are one of the main concerns for me. It comes down to deciding whether it's worth taking some courses in maths instead of some of the courses in compsci I would have taken by doing just Computer Science",50ysf4,t1_d784fod,dmazzoni,,Reply,1,0,1
d78kx5f,2016-09-03 20:18:34-04:00,Bottled_Void,,"I did Computer Science at Uni (also in the UK). The modules I picked were exactly in line with those of the joint degree with Maths (I picked some pretty heavy maths modules). The only difference was the selection of dissertation.


Of course other universities may not offer these same options.",50ysf4,t3_50ysf4,saltedcaramel_,,Comment,1,0,1
d78yx50,2016-09-04 05:57:39-04:00,saltedcaramel_,,Surely in that case you might as well do the actual joint degree,50ysf4,t1_d78kx5f,Bottled_Void,,Reply,2,0,2
d790w37,2016-09-04 08:05:01-04:00,Bottled_Void,,"Maybe, except I really enjoyed my dissertation. I wrote an artificial neural network. I would have had to do a more math based dissertation instead of that.


This page seems to have some guidelines on single vs joint:


http://www.oxbridgeapplications.com/blog/should-you-apply-for-a-joint-honours-degree/


One point they pick out is that there are generally less places on combined honours.


I suppose my advice would be, it's already pretty hard to do well at University. I coasted through my GCSEs and A-Levels. But there were things I had trouble understanding or remembering completely at University (I could blame bad lecturers, but at the end of the day it's down to you to learn the material despite that). The reason I picked the harder maths modules is because I'd already touched on the others at A-level (I did 2 A-levels in maths) and I thought I'd be bored. In hindsight, it might have been easier on me to go for the easier options, at the end of the day you're only really looking to get a piece of paper. In my work I never touch on infinite sequences, Fourier series or chaos theory.


Either option should give you a big enough foundation to go off and do your own independent learning. But I'd suggest you you don't distract yourself too much while you're doing your degree. That piece of paper means you can stick with a program, learn, understand and produce an accurate body of work. This is what businesses are really looking for, not that you already know how to do matrix transformations.


I suppose my preference is for the single honours because that's what was best for me. I can't say either way which would be the best for you.",50ysf4,t1_d78yx50,saltedcaramel_,,Reply,1,0,1
50uq4r,2016-09-02 14:31:42-04:00,jspacecadet,Struggling with Javascript,"hey all, I'm a computer science major at a community college. I'm not able to take any computer science courses until I finish my prerequisites (will be done by spring semester or next fall), so I've been learning on my own. I learned CSS, HTML, and bootstrap pretty easily, but I've really been struggling with Javascript. I'm using freecodecamp and a few other resources to learn, but I'm starting to get annoyed at how much difficulty Javascript is giving me. Does anyone have any tips or resources that could help me out?",,,,,Submission,4,0,4
d773wqq,2016-09-02 14:51:41-04:00,dxk3355,,"Javascript is a shit language to start with because the only reason it's popular is that everyone supports it, not because it's great.  Trying something more structured like Python or Java to start maybe?  

Besides that, the most basic JavaScript stuff is just modifying the webpage content so work with that first.  Don't start with those fancy Frameworks, only learn with vanilla javascript to start.",50uq4r,t3_50uq4r,jspacecadet,,Comment,3,0,3
d7749xo,2016-09-02 14:59:45-04:00,jspacecadet,,"I've been thinking of switching over to learning Python for a while, I've heard it's a better place to start - I think I'll give it a shot, thanks!",50uq4r,t1_d773wqq,dxk3355,,Reply,3,0,3
d774c94,2016-09-02 15:01:11-04:00,yooman,,"Specific questions will be more helpful, but some general good places to start for javascript:



A Javascript Primer for Meteor - https://www.discovermeteor.com/blog/javascript-for-meteor/

This is my favorite introductory document on Javascript as a language, even though it's geared toward building apps with the Meteor framework it applies mostly to the language itself and is a good read even if you don't plan to use Meteor.

Javascript: The Good Parts by Douglas Crockford - https://www.amazon.com/JavaScript-Good-Parts-Douglas-Crockford/dp/0596517742

Very good and to-the-point book by one of the best JS experts out there.
",50uq4r,t3_50uq4r,jspacecadet,,Comment,1,0,1
d774spi,2016-09-02 15:11:24-04:00,jspacecadet,,thanks! just ordered it! ,50uq4r,t1_d774c94,yooman,,Reply,1,0,1
d77gxb5,2016-09-02 20:29:23-04:00,-SoItGoes,,"The best resources I had at your state were sites like codecademy - things that I could practice code in as a controlled environment. Another great help was to switch into using Linux as my primary operating system. You'll fumble around quite a bit, but that's what learning feels right. ",50uq4r,t3_50uq4r,jspacecadet,,Comment,1,0,1
d773hwz,2016-09-02 14:42:42-04:00,rfinger1337,,"tip 1:  ask a specific question.

find something (no matter how basic) and ask for clarification.  your post isn't specific enough for a helpful answer.",50uq4r,t3_50uq4r,jspacecadet,,Comment,0,0,0
d7748ws,2016-09-02 14:59:09-04:00,jspacecadet,,"I'm looking for resources to help with basic javascript, mostly struggling with nesting and loops right now, as well as arrays and javascript object notation ",50uq4r,t1_d773hwz,rfinger1337,,Reply,1,0,1
d77g17z,2016-09-02 20:02:18-04:00,minesasecret,,"Those things are not really specific to Javascript.. It sounds like you're more struggling because this is your first programming language.

If that's the case I'd recommend just picking up a good programming book.. Javascript is actually not an easy language to learn either, and although I do like it, I'm not sure it's the best choice for a first language. You might be better off with something like Python or C. Some will disagree with suggesting C.

Why don't you try the introduction to CS and programming class on edx by MIT? Alternatively if you're really motivated you could try reading the Structures and Interpretation of Computer Programs which I think is freely available online, but that's probably overkill if you just want to learn to program and you can always save it for later.",50uq4r,t1_d7748ws,jspacecadet,,Reply,4,0,4
50mmgz,2016-09-01 07:01:53-04:00,EruptingVolcanus,Any advice on which computer-related university course I should take?,"Hi. I would love some guidance/suggestions on which computer-related courses would be right for me, as I will be finished my A-Levels in a year and going into university. My concern is that I'm not a huge fan of maths (or science for that matter). Here's some details:


• I have an A in GCSE computing and a B in GCSE maths, but was certainly not eager to take maths it at A-level so I have not.

• I am doing both Computer Science and ICT at A-level currently.

• I have an interest in programming that is growing, and I have completed the Python CodeCademy course and enjoy practicing my new skills now and then.

• I am quite interested in computers and games.

I can give any more information if needed. All advice is appreciated!",,,,,Submission,1,0,1
d767i2e,2016-09-01 21:28:55-04:00,IAmNotMyName,,A-Levels? GCSE? Guessing British? You might want to give a list of what the options are. I have no idea what level in your education process you are at. I'm going to assume equivalent to college freshman. If so you probably won't have many options besides introductory coding practice and computer systems.,50mmgz,t3_50mmgz,EruptingVolcanus,,Comment,1,0,1
d76lqhw,2016-09-02 07:02:35-04:00,EruptingVolcanus,,"British indeed. I can't really give a list of options as there are so many different university courses. But the most common ones seem to be Computer Science, and also courses such as Computer Programming.
Perhaps some British redditors might have experience in the courses over here.",50mmgz,t1_d767i2e,IAmNotMyName,,Reply,1,0,1
d76mo36,2016-09-02 07:46:25-04:00,IAmNotMyName,,No those are the same courses as over here. Computer Programming is going to be your introductory software practice course. It will likely be in C++ or Java. Good option if you want to code. I'd be really surprised if you had all that many options at this point considering required prerequisites.,50mmgz,t1_d76lqhw,EruptingVolcanus,,Reply,1,0,1
50kppj,2016-08-31 21:24:59-04:00,tohearstories,CS senior freaking out about finding an open source java project,"I just started my senior year as a CS major(GPA:3.44, to give you some idea of where I'm at), and Java is my preferred language. I need to find a medium size open source project for my Software Design class. The goal is to find something I can contribute to (like, add a feature) over the course of the semester.

Here's the thing, I'm freaking out. I do very badly with this level of freedom.  Seriously, I don't even enjoy playing open world video games for this reason.  If there isn't a clear goal or narrative to follow I just freeze.  

Can anyone relate to this? Or possibly help me or point me in a direction? Everything I find on github or by googling things like ""open source java projects"" are all completed fully functioning projects that seem way above me.",,,,,Submission,1,0,1
50k6ec,2016-08-31 19:23:09-04:00,SherbTheSuperb,"Starting Computer Science Bachelors in a week, pro tips?"," enrolled in computer science because i have always been fascinated with the seemingly endless applications of the computer. I hope to one day start my own company, preferably in game design (generic, I know). Does anyone have any tips concerning my goal or the program in general, I really dont know what to expect and any advice would be appreciated. Thanks.  ",,,,,Submission,16,0,16
d74ul55,2016-08-31 21:42:36-04:00,TheCommador,,Make friends. Preferably smart friends. ,50k6ec,t3_50k6ec,SherbTheSuperb,,Comment,12,0,12
d74qius,2016-08-31 19:57:04-04:00,mahalo1984,,"It's very easy to learn where you stand by writing programs. Write lots of programs. Don't depend on a grade to tell you how you are doing, depend on the programs you can write.

It's a doing skill more than a knowing one.

It's a complex craft. Start now and don't stop. If you get ahead, it will make classes easier.

It's a very rewarding trade. Best of luck!",50k6ec,t3_50k6ec,SherbTheSuperb,,Comment,13,0,13
d74r5s6,2016-08-31 20:13:41-04:00,quzR,,"Get a good base of programming languages. Then find your favourite (and look at what jobs want) and then learn that one (or a couple) in more depth.

Find out (by trying, you'll most likely be forced into by doing various coursework), if you prefer front end, or beck end, find out which IDEs there are and try to find one which suits you best (best plugins, lightweight or not, runs on your OS).

Also as the other guys said, practice, practice, practice. They dont have to be full programs which 100% work, they could just be a GUI or just a single algorithm, as long as you are practising, you should do well.",50k6ec,t3_50k6ec,SherbTheSuperb,,Comment,5,0,5
d74wcg4,2016-08-31 22:28:15-04:00,Nasdasd,,The biggest thing you're going to learn in school is how to learn,50k6ec,t3_50k6ec,SherbTheSuperb,,Comment,6,0,6
d74v1x5,2016-08-31 21:54:29-04:00,impala454,,"Find some problems to solve.  I've been developing software for 12 years, 8 of them at NASA, and to this day, the little piddly apps I write that saved somebody hundreds of hours of work automating some menial task are my favorite pieces of work.  Concentrate on the problem you need to solve and the coding skills will follow.  Never lose sight of the big picture.  As for school, find and cling to the professors that have real world experience.  I learned ten times as much from those guys as I did from the straight through PhD to Profs.",50k6ec,t3_50k6ec,SherbTheSuperb,,Comment,3,0,3
d74yu4k,2016-08-31 23:36:32-04:00,yes_thats_right,,"I've been working in IT for 12 years now, starting with a computer science degree.

The biggest piece of advice I can give you is that it is not about the languages and it isn't about the tools, it's about the concepts.

Understand how things work, and why they are teaching you specific things. Question everything, asking questions will lead you to understand the intricacies and help you understand why certain other solutions aren't practical in the real world.

Secondly, do some programming of your own. Choose an application and build it. You will come across many challenges, and then these will relate to what you learn in school. This is how you REALLY learn.",50k6ec,t3_50k6ec,SherbTheSuperb,,Comment,3,0,3
d75eosr,2016-09-01 10:27:26-04:00,HollowWiener,,Go to class.  Even when you don't feel like it.  Even when you don't think you need to.,50k6ec,t3_50k6ec,SherbTheSuperb,,Comment,2,0,2
d75hnmb,2016-09-01 11:33:17-04:00,icendoan,,Take maths classes. A good understanding of mathematics makes a lot of things in cs much easier.,50k6ec,t3_50k6ec,SherbTheSuperb,,Comment,2,0,2
d75w3qz,2016-09-01 16:44:33-04:00,somedifferentguy,,"As someone from Germany this always confuses me. Can you study CS without taking any maths courses? As in, can you just choose what courses to take? Because we are just obligated to take the maths classes and have to pass them.",50k6ec,t1_d75hnmb,icendoan,,Reply,2,0,2
d75y4i1,2016-09-01 17:32:13-04:00,icendoan,,"I definitely had to take some maths classes: however, many other students only took the bare minimum of mathematics. You benefit enormously from taking more than required.",50k6ec,t1_d75w3qz,somedifferentguy,,Reply,1,0,1
d75iqd5,2016-09-01 11:56:26-04:00,fladam123,,"Enjoy it. Don't try to cram , especially for programming modules! It feels as though there are endless things you can learn, always new languages etc , and you should be learning stuff outside the scope of your course if you want to be competitive when it comes to finding employment but don't forget to manage it with time away from study :) The most important thing of all is enjoying yourself! 

I don't know what your level of programming/compsci ability is currently but I would say don't be freaked out if you meet some people who seem to know a lot more language wise than you..

I just finished my first year and I had very limited programming experience before I began but that didn't hold me back at all, not missing any class and studying throughout the year allowed me to achieve just as well in class as those who knew much more than me about Java so don't be worrying about it.


Final tip would be to create a github account if you haven't already and follow their tutorial on your first repo/commiting, even if you don't have any code right now to upload eventually you can add some small projects there and they will be a huge help because employers like to see that sort of stuff !",50k6ec,t3_50k6ec,SherbTheSuperb,,Comment,2,0,2
d75k26r,2016-09-01 12:25:09-04:00,FenPhen,,"Be prepared for the inevitability that coding for class assignments is not going to fully prepare you for working in a professional environment. Class work often has a start and an end, often you're given a framework or harness to build into, and you may not be exposed to testing infrastructure and refactoring. Try to learn concepts and not habits from school, and try to get real internships to learn the trade. Computer science and being a commercial developer are pretty different.

Also, I'd say don't focus solely on games. The games industry is very competitive. It's like saying you want to be a pop singer. You might make it, but most don't. There's some luck and creative genius involved and bigger players will exploit you for your enthusiasm. Make games cause you enjoy them but don't assume that because you have drive that you'll have a successful career. Being a good software engineer is a solid career choice and you can make some pretty cool things that may or may not be games.",50k6ec,t3_50k6ec,SherbTheSuperb,,Comment,2,0,2
d75x3ns,2016-09-01 17:07:00-04:00,complich8,,"You should assume that your curriculum in itself is not a complete education. Do extra-curricular projects for yourself. The best programmers I know had projects outside of schoolwork. Can't be prescriptive there -- some people will say ""contribute to open source"" as an easy default answer, but honestly, just doing stuff is what's important. 

Examples from my friends' and my own life include building (and scaling) websites, building phone apps, building a funky emacs authoring plugins, modding existing code to do new novel things, and doing undergrad research opportunities where that was possible. While a lot of us older-timers had a lot of low-hanging fruit to choose from, there's never been a richer time for that sort of play than right now. Choose your adventure!

If you *only* do the course work, you'll learn the material, you'll take the tests, you'll finish the projects, and you'll probably forget a bunch of it before it ever really gels. But if you do the course work and give yourself a series of creative projects to work on, those projects will lend practicality to the theory in a way you care about, and drive your desire to learn more. ",50k6ec,t3_50k6ec,SherbTheSuperb,,Comment,1,0,1
d773y2j,2016-09-02 14:52:29-04:00,dxk3355,,"Shower before class, leave the laptop in your dorm.",50k6ec,t3_50k6ec,SherbTheSuperb,,Comment,1,0,1
50iv68,2016-08-31 14:59:29-04:00,holymolytoly,[Discrete Math] Which textbook would you recommend?,"I am majoring in ICS and I have to take discrete math in the spring next year. 

I've been told by a lot of students and the advisor of the major himself that taking discrete math at my university is a nightmare. If I wanted to do well, he suggests that I rent out a textbook at the library get a head start.

I am curious to know what textbook any of you would recommend. My library will probably have it. So far, I picked out a couple and the only one that looks promising (based on Amazon reviews) is Introduction to Discrete Mathematics by McEliece, Ash and Ash. https://www.amazon.com/Introduction-Discrete-Mathematics-Robert-McEliece/dp/0394358198/ref=sr_1_1?s=books&ie=UTF8&qid=1472669472&sr=1-1&keywords=introduction+to+discrete+mathematics+mceliece

My only concern about this book is that you only need up to high school algebra to go through this book therefore not being truly reflective of an actual discrete mathematics course. Then again, I haven't taken the class yet so maybe any of you could shed more light on this.*",,,,,Submission,2,0,2
d74gr2s,2016-08-31 16:11:39-04:00,faxekondikiller,,"I can recommend ""Discrete Mathematics and Its Applications"" by Kenneth H. Rosen. We used this in the discrete mathematics course I took, and I found the book quite good.",50iv68,t3_50iv68,holymolytoly,,Comment,4,0,4
d74mux4,2016-08-31 18:25:12-04:00,DarkFusionPresent,,Yeah I thought Rosen was pretty good as well. The lessons are clear and there are a bunch of questions too.,50iv68,t1_d74gr2s,faxekondikiller,,Reply,1,0,1
d74z9pf,2016-08-31 23:49:13-04:00,Teddy-Westside,,"I have a copy of Discrete Mathematics with Applications by Epp. If you're in Maryland, you're welcome to have it. ",50iv68,t3_50iv68,holymolytoly,,Comment,2,0,2
d74gaz9,2016-08-31 16:02:23-04:00,ppc23,,"Are there no textbooks advised by your lecturer? Just ask her or someone who did the course already. This is usually the best choice because it reflects her opinion of the important topics.

If this doesn't help I can recommend german textbooks to you xD",50iv68,t3_50iv68,holymolytoly,,Comment,1,0,1
d752ukg,2016-09-01 01:58:42-04:00,holymolytoly,,"I'd rather not read the textbook advised by my lecturer. The lecturer himself (he's the only professor that teaches discrete math my university) is notoriously bad. 

Even the adviser for the department hinted at that.",50iv68,t1_d74gaz9,ppc23,,Reply,1,0,1
50iqi7,2016-08-31 14:34:31-04:00,pas43,Math for computer scientists,"So I'm going into my 3rd year of uni and I am starting to realize how a solid base for mathematics is for designing good programs and also for any job a programmer might have in the future, especially the field I want to go into. What good books are out there? 

I know linear algebra is a must but I would like to go into finance which is obviously dealing with non-linear systems what would I need to learn? 

I also enjoy the real geeky stuff information theory, compression,  hashing functions, encryption and data structures. 

Also I would like to learn about the field of computational biology, and genetics. For this I would like to learn machine learning techniques mathematics playing with tensor flow at the moment. 

As you can tell there is way to much for 1 brain, and I get anxious when it comes to learning I want to learn it all in the shortest amount of time. Learning is very important to me and close to my heart. 

With all that I have mind what books do you know of that are good at teaching the concepts of mathematically theory's to the point I will have enough confidence to read higher level math books? 

If you read any of the ""head first"" book series are there any in this ELI5  style? 

I find Amazon review a bit hit an miss. 

Thanks.",,,,,Submission,6,0,6
d750ozc,2016-09-01 00:34:18-04:00,screamconjoiner,,[this book](https://www.amazon.com/gp/aw/d/0387955852/ref=mp_s_a_1_5?ie=UTF8&amp:qid=1472704303&amp:sr=8-5&amp:pi=SL75_QL70&amp:keywords=Discrete+mathematics.) really helped me in undergrad. Has a lot of really good concepts. It went along with a course but it does a great job on its own explaining some of the most relevant concepts to computer science.,50iqi7,t3_50iqi7,pas43,,Comment,1,0,1
d780dbr,2016-09-03 10:43:36-04:00,yes_thats_right,,"https://www.quantstart.com/articles/Self-Study-Plan-for-Becoming-a-Quantitative-Analyst

This has great recommendations.",50iqi7,t3_50iqi7,pas43,,Comment,1,0,1
50fwu5,2016-08-31 03:05:18-04:00,sssssunshine,"As a non-CS undergrad, what field should I choose for my Masters these are my interests?","I'd like to mine huge chunks of data and make something out of it in the long run. Or design softwares for specific purposes, which **learn as time goes by** and adapt accordingly.

I know I'm being too specific, what are you suggestions?",,,,,Submission,4,0,4
d749la8,2016-08-31 13:44:40-04:00,deong,,"The field you want is called ""machine learning"", or perhaps ""data mining"", or potentially even the more general ""artificial intelligence"". Machine Learning is an extremely hot discipline right now, so you'll find those programs pretty much anywhere.

For machine learning, you need to be really solid in CS fundamentals like programming and algorithms, and be equally solid in statistics and mathematics. If you're currently deficient in either of those areas, you should plan to spend up to a couple of years just catching up before any reputable program will let you start taking more advanced courses directly related to ML.",50fwu5,t3_50fwu5,sssssunshine,,Comment,3,0,3
d74cqmb,2016-08-31 14:49:32-04:00,sssssunshine,,"Thank you for the answer, I'll look up these fields.

Well, my major is electronics and communications. And my current curriculum had some basics of C. I l loved coding at school level but that might not be too relevant now.

At uni, I pretty much loved Data Structures and Algorithm, the only CSE course I got to take an elective. On a side note, I've been coding off and on, on codecademy, but never really took it seriously. I know it would be hard but I have a passion for computers so I'm willing to give it all in. Can you elaborate on the last part of of your comment about a catching up program that might help me? :)

Cheers.",50fwu5,t1_d749la8,deong,,Reply,1,0,1
d74f8yf,2016-08-31 15:40:54-04:00,deong,,"Mostly what I had in mind was the systems that most graduate programs have in place to handle students in your situation.

That is, you apply for the MS program, and if you're accepted, they'll tell you that you need to take a specific list of courses before you're allowed to earn credits toward your graduate degree. The specific courses they'd make you take will usually depend on an individual assessment of where your ""gaps"" are as compared to someone who did an undergrad degree in CS at a reputable university.",50fwu5,t1_d74cqmb,sssssunshine,,Reply,2,0,2
d74d92r,2016-08-31 14:59:57-04:00,videoj,,"Go visit /r/machinelearning, they can give you good advice on MS degrees in the field.",50fwu5,t1_d74cqmb,sssssunshine,,Reply,1,0,1
d753r1u,2016-09-01 02:40:50-04:00,sssssunshine,,Thanks bro,50fwu5,t1_d74d92r,videoj,,Reply,1,0,1
d73rca0,2016-08-31 04:28:11-04:00,visvis,,"Artificial Intelligence, though it would be very hard if you don't know the computer science basics yet.",50fwu5,t3_50fwu5,sssssunshine,,Comment,1,0,1
d7457tn,2016-08-31 12:14:00-04:00,jhartwell,,">though it would be very hard if you don't know the computer science basics yet.
  
  The school should require prerequisite classes that cover at least some of the basics.",50fwu5,t1_d73rca0,visvis,,Reply,2,0,2
d74ctju,2016-08-31 14:51:13-04:00,sssssunshine,,"Most of the courses I come across, and I'm speaking generally, have a whole bunch of MAT and CSE basic level courses, they would be compulsory for a non-cse student to take up before he proceeds to advanced topics.",50fwu5,t1_d7457tn,jhartwell,,Reply,1,0,1
d74e2j9,2016-08-31 15:16:45-04:00,jhartwell,,I got my MSCS after a non-cs degree and had to take 6 pre req courses and get a B or better in each class to be accepted into the grad program,50fwu5,t1_d74ctju,sssssunshine,,Reply,1,0,1
d753sn2,2016-09-01 02:43:00-04:00,sssssunshine,,"Ohh I see, and how long were the duration of these pre-requisites?

Do you begin them as soon as you complete your undergrad? Are these courses offered by the same university, you apply to, by the same professors who'll teach you further?",50fwu5,t1_d74e2j9,jhartwell,,Reply,1,0,1
d75bu2i,2016-09-01 09:14:23-04:00,jhartwell,,"They were undergrad classes, so it was no different than if I was an undergrad. The school is on quarters so each class was 10 weeks. 
  
  I began them when I was accepted into the program. It was a conditional acceptance and if I couldn't do those classes then I wouldn't be able to get into the grad program. The classes were taught at the same school that I applied to and some of the same professors, not all of them do graduate level classes.",50fwu5,t1_d753sn2,sssssunshine,,Reply,1,0,1
d743chj,2016-08-31 11:33:39-04:00,RuthBaderBelieveIt,,"How's your programming? If the answer isn't ""good"" you'll struggle with a CS masters ",50fwu5,t3_50fwu5,sssssunshine,,Comment,-1,0,-1
d7456az,2016-08-31 12:13:07-04:00,jhartwell,,"Depends on the program. Where I went there was a pure theory masters track. That said, any decent program would require some sort of pre req",50fwu5,t1_d743chj,RuthBaderBelieveIt,,Reply,2,0,2
d749dp5,2016-08-31 13:40:19-04:00,ponchedeburro,,I prefer theory. ,50fwu5,t1_d7456az,jhartwell,,Reply,1,0,1
d74cwjm,2016-08-31 14:52:58-04:00,sssssunshine,,"I can't really say my programming is 'good', that is pretty subjective.

Though I've been in touch with 'programming', coding on MATLAB, Verilog, Assembly Level Programming on Microcontroller throughout my undergrad.",50fwu5,t1_d749dp5,ponchedeburro,,Reply,1,0,1
50eyps,2016-08-30 22:35:24-04:00,bareily,Ajax and Python compatibility,"Hey guys. I am just testing out stuff in a web environment. I am making some test pages and running them on an apache server. I am just fooling around to try to learn as much as I can about web development. Any server-side programming is one of the most important, if not the most important aspects to this. Most people usually go with ASP or PHP but I was like nah, I want to be different. I figured out it is possible to use Python (via CGI) :D! Let me get down to the issue. I was looking for a way to update a page without reloading it. That's where Ajax comes in. Ajax is super compatible with ASP and PHP but I don't want to take that route. Now I know there is some stuff that helps Python work with Ajax friendlier like jQuery and Flask and stuff, but I am looking for the simplest demonstration of just pure Python to Ajax (if even possible). I am starting to think it is not because I looked everywhere. Anyway, here is a JavaScript of very simple Ajax in action:
""function loadDoc()
{
var xhttp = new XMLHttpRequest();

xhttp.onreadystatechange = function()
{
if (xhttp.readyState == 4 && xhttp.status == 200)
{
document.getElementById(""result"").innerHTML = xhttp.responseText;
}
};

xhttp.open(""GET"",""examp.txt"",true);
xhttp.send();
}""

Now without complicating the above script too much, is there a way to pass, say a string, to a python program? And if so, what would be the simplest way  to just return that string and thus printing out to the element identified by 'result' (if that is also possible?

It would be very helpful and appreciated if you are able to show some example code, but if you don't feel up to it then explaining is fine too.

Thank You in advance everyone!!!! :D

",,,,,Submission,1,0,1
d73s3qv,2016-08-31 05:13:07-04:00,brtt3000,,"AJAX is just way of doing HTTP requests from JavaScript, so your python program would need to run a HTTP server. Or you'd have something else do the HTTP and call python as a subprocess or CGI (maybe through apache, or use PHP/ASP subprocess calls).

You can make a python based HTTP service from parts found in the standard library, but it is not trivial to build anything substantial on it: you can get something of a hello-world app on HTTP easily (cool exercise) but then you want multiple endpoints, routing etc. The easiest way to get a HTTP server with basic plumbing is using a lightweight framework, for example Flask (there are a ton more on pypi obviously).

Then you just GET or POST (or PUT) your data from your JavaScript code to the HTTP server. That is not python specific so look at some documentation on using XMLHttpRequest (MDN is solid), and at some point you may want to use a library. Also XMLHttpRequest is being replaced with the HTML5 `fetch` API that is a bit friendlier in use (but maybe need a polyfill etc so YMMV).",50eyps,t3_50eyps,bareily,,Comment,1,0,1
50dfzi,2016-08-30 17:06:48-04:00,nigel_meech,Identifying algorithms with O(log(n)) complexity?,"I'm confused about how to recognize an algorithm of the order O(log(n)) . (I hope i'm saying that correctly). Here is an example of O(logn) from my textbook but I still don't understand how to identify it logically. 

    count = 1
    while (count < n)
    {
    count *= 2;
    /* some sequence of O(1) steps */}
",,,,,Submission,11,0,11
d73710o,2016-08-30 18:01:56-04:00,Quintic,,"Suppose the loop was executed k times. How big would count be?

Should be clear that count = 2^(k). (Edit: If it's not clear, try proof by induction on k.)

So the question is how many times would the loop execute?

The loop will execute when 2^(k) < n, that is k < lg(n).

Thus the loop will execute floor(lg(n) - 1) times, that is order O(log(n)).",50dfzi,t3_50dfzi,nigel_meech,,Comment,8,0,8
d743jzv,2016-08-31 11:38:14-04:00,DukeBerith,,"When figuring out O, your goal is to calculate how many steps the algorithm takes before halting.

If in your example, n was 16:

With the operation being count *= 2

value of *count* | at step #
:--|:--
1 | 0
2 | 1
4 | 2
8 | 3
16 | 4

Even though n is 16, the algorithm didn't iterate 16 times before halting, it only iterated 4 times. Since 4 is log2(16), we can wager that it is O log n.

To prove it we'd need to check that it wasn't just lucky for 16 to stop at 4 iterations, and check that if it halts it halts at a logarithmic number iteration, but this is just the ""at a glance"" explanation, the others went into more detail. 
",50dfzi,t3_50dfzi,nigel_meech,,Comment,2,0,2
d7361y9,2016-08-30 17:39:43-04:00,TheCriticalSkeptic,,"Yikes that was a terrible example from your text book. 

O(n) implies that the algorithm needs to traverse all the input data once (or some fixed number of times). So to find the largest number in an unordered list takes O(n) because you have to go through every element. If the list is sorted you can take the head of the list and it is O(1). 

O(log(n)) implies that you only need to traverse (at most) a fixed fraction of the data structure. In order for this to be possible it must be that the data structure has some useful structure for your purpose. 

You never get less than O(n) when you need to look at every element. So sorting, searching unstructured data, etc. cannot be less than O(n). To go less there must be a means that allows you to choose to ignore data without having to inspect it. 

For example a tree lookup is O(log(n)) because at each leaf you only need to look at half the data. You ignore the branch coming off the leaf that's irrelevant. As a result at each step your search decreases by half. This means that at most you look through log(n) of the total size of the input. ",50dfzi,t3_50dfzi,nigel_meech,,Comment,3,0,3
d737xhx,2016-08-30 18:23:28-04:00,ad_tech,,"Let me add a couple caveats to your comments, since these might come up in an algorithms class--

> If the list is sorted you can take the head of the list and it is O(1).

Only if the list is sorted in decreasing order. If it's in increasing order and you're using a singly-linked list, you'll have to traverse the list to the end in O(n) time. You can find the last element in O(1) time if the list is implemented as an array (like a Java ArrayList), or if it's a circular doubly-linked list, or if you have a pointer to the tail node.

> a tree lookup is O(log(n))

Only if the tree is balanced. An unbalanced tree could be one long linked list, with lookup time O(n).
",50dfzi,t1_d7361y9,TheCriticalSkeptic,,Reply,3,0,3
d739mid,2016-08-30 19:05:15-04:00,LasagnaAttack,,"Suppose you have a list of n items in an order (alphabetic/numerical/etc). You want to find a specific item. Instead of going through everything (O(n)), you look at the item in the middle, based on that, you decide to check the first or second half, and check the middle item of that. You go on until you find ur target. Instead of checking everything, you check O(logn) things. It's faster than O(n) obviously, and I believe it's what humans do (whenever we're faced with a new list)",50dfzi,t3_50dfzi,nigel_meech,,Comment,1,0,1
d741ycc,2016-08-31 11:03:13-04:00,randcraw,,"If a search/sort algorithm can exclude a constant fraction of items after each iteration, then it will run in log time.

For example, a binary search retains 1/2 of the data with each iteration.  So it runs in O(log2(N)) time.  If another more efficient algorithm retains only 1/10 of the data, it runs in O(log10(N)).  And if a third less efficient algorithm retains 3/4 of the data, then it runs in O(log4/3(N)) time.  In each case, the log base is the reciprocal of the fraction of the data that was retained after each iteration.  

Thus if a constant fraction of the 'work' can be excluded with each iteration, the algorithm runs in log time.",50dfzi,t3_50dfzi,nigel_meech,,Comment,1,0,1
d746lsj,2016-08-31 12:42:59-04:00,njaard,,"* `O(1)` you have to do the same work no matter the number of items
* `O(n)` you have to process each item
* `O(log n)` every item added only requires a little more work, less than adding the previous item
* `O(n log n)` every item you add an item, you have to process each item, and also a little bit more work, but less than the previous
* `O(n²)` every item requires you to process every other item
* `O(n!)` this algorithm will never complete if you have more than 10-ish items. For every item, you have to do this algorithm again for the same container minus the item you're processing.",50dfzi,t3_50dfzi,nigel_meech,,Comment,1,0,1
50asko,2016-08-30 08:13:45-04:00,mikhailarden,Quick question - How do I revert to mac apps staying?,"Hi!

I have an issue, I did a tweak to my mac (forgot what it was) and what happens is that it when I open a window/app it minimizes the previous one - it's great for productivity but I want to change that and go back to windows opening above others.",,,,,Submission,0,0,0
d72qgh7,2016-08-30 12:05:03-04:00,lewisj489,,This is not the right subreddit.,50asko,t3_50asko,mikhailarden,,Comment,2,0,2
5093yh,2016-08-29 23:34:07-04:00,Doub1eVision,I need help choosing what to specialize in for my MS in CS,"I'll be graduating soon and I need to start prepping myself for graduate school. I attend the University of Texas at Dallas. If you want to browse what I'm looking at, here's a link.

https://catalog.utdallas.edu/2015/graduate/programs/ecs/computer-science

Below is a list of what I am choosing from. I would say that Interactive Computing is the one I am most interested in immediately, but that is largely because it is the one that most closely relates to my hobbies. I was also thinking of pairing it with several courses from the Systems Track. Though, I am also interested in the other areas to a lesser extent. I wanted to hear some people's perspective on these areas of study. Thanks for any advice.

***Data Sciences Track***

CS 6313 Statistical Methods for Data Science

CS 6350 Big Data Management and Analytics

CS 6363 Design and Analysis of Computer Algorithms

CS 6375 Machine Learning



Choose one course from the following five courses:

CS 6301 Special Topics in Computer Science [when topic is Social Network Analytics]

CS 6320 Natural Language Processing

CS 6327 Video Analytics

CS 6347 Statistical Methods in AI and Machine Learning

CS 6360 Database Design


***Information Assurance Track***

CS 6324 Information Security

CS 6363 Design and Analysis of Computer Algorithms

CS 6378 Advanced Operating Systems


Choose two courses from the following four courses:

CS 6332 System Security and Malicious Code Analysis

CS 6348 Data and Application Security

CS 6349 Network Security

CS 6377 Introduction to Cryptography


***Intelligent Systems Track***

CS 6320 Natural Language Processing

CS 6363 Design and Analysis of Computer Algorithms

CS 6364 Artificial Intelligence

CS 6375 Machine Learning


Choose one course from the following two courses:

CS 6360 Database Design

CS 6378 Advanced Operating Systems


***Interactive Computing Track***

CS 6326 Human Computer Interactions

CS 6363 Design and Analysis of Computer Algorithms


Choose three of the following five courses:

CS 6323 Computer Animation and Gaming

CS 6328 Modeling and Simulation

CS 6331 Multimedia Systems

CS 6334 Virtual Reality

CS 6366 Computer Graphics


***Networks and Telecommunications Track***

CS 6352 Performance of Computer Systems and Networks

CS 6363 Design and Analysis of Computer Algorithms

CS 6378 Advanced Operating Systems

CS 6385 Algorithmic Aspects of Telecommunication Networks

CS 6390 Advanced Computer Networks


***Systems Track***

CS 6304 Computer Architecture

CS 6363 Design and Analysis of Computer Algorithms

CS 6378 Advanced Operating Systems

CS 6396 Real-Time Systems


Choose one course from the following four courses:

CS 6349 Network Security

CS 6380 Distributed Computing

CS 6397 Synthesis and Optimization of High-Performance Systems

CS 6399 Parallel Architectures and Systems


***Traditional Computer Science Track***

CS 6363 Design and Analysis of Computer Algorithms

CS 6378 Advanced Operating Systems

CS 6390 Advanced Computer Networks


Choose two courses of the following three courses:

CS 6353 Compiler Construction

CS 6360 Database Design

CS 6371 Advanced Programming Languages",,,,,Submission,2,0,2
d79jiyb,2016-09-04 17:33:19-04:00,assface,,"You're likely to get a better job with the ""Data Science"" or ""Systems"" track.",5093yh,t3_5093yh,Doub1eVision,,Comment,1,0,1
507k5r,2016-08-29 17:51:02-04:00,Ninebythreeinch,When will home computer components reach 1nm or less?,"Today most of our modern home computer components have transistor size of 16nm or less. Does Moore's law still apply, or will we get significant problems reaching 1nm in the years to come?",,,,,Submission,11,0,11
d71y89s,2016-08-29 20:14:49-04:00,kakarotsan,,"Semiconductor engineer here.

They can't get any smaller so they stack them now into 3D chips.",507k5r,t3_507k5r,Ninebythreeinch,,Comment,14,0,14
d71yuon,2016-08-29 20:29:58-04:00,Ninebythreeinch,,Well 1 nm is like a carbon atom large so I understand how that can be tricky :-D,507k5r,t1_d71y89s,kakarotsan,,Reply,-4,0,-4
d724sd3,2016-08-29 22:55:27-04:00,794613825,,"Even if we could fit the things in that space, at that scale, quantum effects start coming into play. Transistors will fail because the elections can just quantum tunnel through a closed one. That's why quantum computing is becoming such a big thing. Quantum computers get around that issue. I don't know enough about them to describe how in detail, but they use a different process to regular computers. However, even quantum computers will have a size limit eventually. ",507k5r,t1_d71yuon,Ninebythreeinch,,Reply,7,0,7
d726pkx,2016-08-29 23:46:17-04:00,thang1thang2,,"Quantum computers get around the issue because they themselves utilize quantum behavior. It's like saying sharks are waterproof: of course they are, they live in the water.",507k5r,t1_d724sd3,794613825,,Reply,9,0,9
d727jrs,2016-08-30 00:12:21-04:00,Ninebythreeinch,,"Kewl, I just wanna know when video games graphics are like real life",507k5r,t1_d726pkx,thang1thang2,,Reply,-12,0,-12
d729omi,2016-08-30 01:27:35-04:00,794613825,,Quantum computers will actually be worse at menial computations like rendering. They're much better at things like optimization. Life-like graphics are already possible with current tech. ,507k5r,t1_d727jrs,Ninebythreeinch,,Reply,3,0,3
d72et3y,2016-08-30 06:01:36-04:00,masthema,,"But life-like real-time lightning is not, sadly. ",507k5r,t1_d729omi,794613825,,Reply,1,0,1
d72svcm,2016-08-30 12:57:49-04:00,Ninebythreeinch,,I think video game graphics still look far from real life. But movies with frame by frame editing is pretty close tho.,507k5r,t1_d72et3y,masthema,,Reply,0,0,0
d72yxdk,2016-08-30 15:06:47-04:00,None,,[deleted],507k5r,t1_d72svcm,Ninebythreeinch,,Reply,1,0,1
d731xkt,2016-08-30 16:09:40-04:00,Ninebythreeinch,,"Yeah I'm still pretty impressed, I play the Battlefield 1 beta right now and it's great. I think it's still like a decade left until it's impossible to see the difference between a movie and a video game.",507k5r,t1_d72yxdk,None,,Reply,1,0,1
d737b85,2016-08-30 18:08:38-04:00,UtterlyDisposable,,"There are problems apart from computing power which make ""real life"" quality graphics somewhat unlikely. Art assets are already monumentally expensive to create at current standards, so having to do thousands of ultra-high-resolution textures and models on the scale needed for a modern AAA game would be astronomically expensive. These sorts of things don't advance in the same way that computer technology does and even if we have CPUs that are 100x more powerful than our current level of technology, the process won't really be much less expensive.

Movie CGI looks better, but you'd be surprised the kinds of shortcuts that are possible because of the constrained perspective. If you're doing heavy chroma key compositing with CGI only the parts of the scene which the viewers will see matter. If you're doing a realistic simulation of a room you need to model everything at a level of detail far greater than even a scene in a movie. ",507k5r,t1_d72svcm,Ninebythreeinch,,Reply,1,0,1
506bzj,2016-08-29 13:58:22-04:00,a_merriweather,Partition size in quickselect?,"This is a quiz question for which I got the right answer, but it seems like the answer is missing something: ""Let .5<α<1 be some constant. Suppose you are looking for the median element in an array using RANDOMIZED SELECT (as explained in lectures). What is the probability that after the first iteration the size of the subarray in which the element you are looking for lies is ≤α times the size of the original array?""
The algorithm as described picks a pivot uniformly at random.
The correct answer to the quiz corresponds to the probability that the pivot is between α and (1-α) times the size of the original array, but it seems that this misses the cases when the pivot is less than α or more than 1-α and the median lies in the smaller of the two array partitions. Shouldn't those cases factor in as well?",,,,,Submission,2,0,2
504245,2016-08-29 04:44:55-04:00,soccer_sauce,Help with Project,"Not sure if i can really post this here but i need some help in writing this project in java for a class I'm in right now. I'm not the best at computer science and don't really understand it but its a requirement in my major. If you could also post any resource i could use to help me understand this better i would really appreciate it.

Here are the instructions for it:
Assignment 1 – Classes, Two-dimensional Arrays & ArrayLists 
Battleship is a two-player guessing game in which you search for your opponent’s ships and eliminate them one by one. In our simple version of the game, the user will compete against the computer to sink the computer’s randomly placed battleship(s). Each battleship will only take up one cell. Open water is indicated by an O, incorrect guesses by an X, and battleships by an asterisk (*). 
Task 
1. Start by building your game board. This is a 5 x 5 two-dimensional char array. Initialize the board to store O’s for the open water, then print the board. Your board would look a bit like this: 

2. Create a Location class with only 2 instance variables (attributes/fields), row and column. Add getters and setters for both and a constructor that accepts the row and column values as arguments and assigns them appropriately. This class will store locations of your battleships. 
3. Create an ArrayList capable of storing Location objects. Get a random location for three battleships, create corresponding Location objects, and append these objects to the ArrayList. 
4. The user gets 6 attempts to guess where your battleships are. For each attempt: 
a. Tell them how many attempts and battleships remain 
b. Allow the user to make a guess 
i. Check their guess against the boundaries of the board, if it is outside the boundaries, display an appropriate message 
ii. Check the guess against cells already marked with an asterisk (*) or an X (indicating a miss or battleship), display a message to tell the user that they’ve already guessed that location 
iii. Check the guess against all the battleship locations in the ArrayList. If they guess correctly, remove the location from the ArrayList, mark the location on the board with an asterisk (*), and display an appropriate message. 
iv. Otherwise, mark the guessed location with an X and display a message telling the user that they’ve missed your battleship 
c. Print the board 
d. Decrement the number of turns 
5. After all guesses have been made, if there are still battleships remaining, mark all of them on the board with an asterisk (*), display a message stating that the game is over, print the board 

Test your game! Make sure correct and incorrect guesses work properly according to the above. 
",,,,,Submission,3,0,3
d710zju,2016-08-29 06:10:25-04:00,Mines_of_Moria,,"Ask specific questions, not simply post your assignment. What have you done thus far? ",504245,t3_504245,soccer_sauce,,Comment,4,0,4
d712ng7,2016-08-29 07:37:14-04:00,soccer_sauce,,I've done everything but steps 3 and 4 and I'm sorry about the post first time posting on here.,504245,t1_d710zju,Mines_of_Moria,,Reply,1,0,1
d714ku7,2016-08-29 08:51:13-04:00,urielsalis,,"Warning: Some (a lot) of professors consider asking friends or the internet or posting you code as plagarism, so if I were you I would ask my professor directly",504245,t1_d712ng7,soccer_sauce,,Reply,3,0,3
d714rsf,2016-08-29 08:57:28-04:00,soccer_sauce,,My professor doesn't mind it for this one.,504245,t1_d714ku7,urielsalis,,Reply,1,0,1
d71a266,2016-08-29 11:18:25-04:00,Mines_of_Moria,,"What problem are you having? It sounds like you simply have to scan through your array list and replace values with an * or X and print the array. 

wrap the entire thing in a loop that runs while there are still turns left. I would probably use a while loop but there are other ways to do it. 

If you're lost, try writing out the assignment in pseudo code and then convert that to Java. ",504245,t1_d712ng7,soccer_sauce,,Reply,1,0,1
502y6c,2016-08-28 23:01:52-04:00,pinecat13,Passwords and electrons,"Years ago, I had a conversation with a savvy programmer who worked with my friend. He said something that I've been trying to understand ever since. Can someone explain what he meant?


Every password is really just the position of an electron. 


Keep in mind, it's possible I'm misquoting him. Also, he could have been oversimplifying so that I could understand. I know almost nothing about CS, beyond a casual interest.",,,,,Submission,5,0,5
d70srws,2016-08-28 23:44:11-04:00,wackyvorlon,,"Basically, every password is a number. In a computer, numbers are represented using electrical signals which are composed of electrons. What he says is true, with allowance for poetic license. ",502y6c,t3_502y6c,pinecat13,,Comment,8,0,8
d70uwdr,2016-08-29 00:55:44-04:00,pinecat13,,"Thanks for answering! And forgive my ignorance, as I don't really know how to ask this follow-up. But,

if someone sophisticated wanted to guess your password by brute force (trying all the combinations), would they attempt to guess your actual password or would they attempt to guess your password's electronic signal value? . . . or am i vastly oversimplifying this beyond the point of salvage",502y6c,t1_d70srws,wackyvorlon,,Reply,1,0,1
d70vf7d,2016-08-29 01:15:41-04:00,wackyvorlon,,"You assume disparity where none exists. They have to try guessing your password using a computer, but the computer cannot represent anything without electrical signals. They're really two sides of the same coin. ",502y6c,t1_d70uwdr,pinecat13,,Reply,8,0,8
500vam,2016-08-28 15:08:47-04:00,Zulban,How do I find points in n-dimensional space that have large square distance to my set of points in that space?,"**Edit:** Oops, definitely don't want to maximize distance squared. [Here's another example of what I'd like](https://i.imgur.com/9DVFffg.png). I removed some incorrect descriptions below.

What is this problem even called? Anti-clustering?

Context: this is for a free education resources aggregator website I'm building. Each resource is classified with n features. So I'm adding a tool where you get suggestions on what kind of education resources the system most lacks.

I'm sure there are solutions/heuristics and libraries already but I'm having trouble phrasing the question concisely. [Here's a 2D visual](https://i.imgur.com/p7NKS8u.png). Given the red dots, I'd like to find the position of the blue X, which is not a position of any of the red dots. Value range is [0,1]. Currently I have a few hundred points and fifty dimensions but that may grow to tens of thousands at least, and a few hundred dimensions at least. I only need to compute this once on some schedule, like daily or weekly.

I could randomly choose some points and follow a gradient. Sounds okay... I'd rather not code it myself though (Python). And I'd rather not deal with jump distance/plains/local optima myself. Or maybe there's something better. Bonus points for Python suggestions with lean/no dependencies. I can also write algorithms/heuristics myself if I get some general ideas. Thanks :o",,,,,Submission,0,0,0
d70etc7,2016-08-28 17:26:44-04:00,Madsy9,,"Computing the [Voronoi Diagram](https://en.wikipedia.org/wiki/Voronoi_diagram) of your point set could be of help. Once you have the edges and vertices that makes up the Voronoi cells, the point furthest away from all other points in your set will either be a vertex where multiple Voronoi cells meet, or on an edge separating two Voronoi cells. That should make the search a lot faster.",500vam,t3_500vam,Zulban,,Comment,1,0,1
d70qk89,2016-08-28 22:40:51-04:00,RamsesA,,"This will not maximize the square distance to all the points, only the linear distance to the nearest points.

The question the op asked feels like a quadratic optimization problem. Not knowing if an optimal solution exists, I'd probably try something like simulated annealing, which is similar to what the op described. Maybe there's something more appropriate for this particular use case.",500vam,t1_d70etc7,Madsy9,,Reply,2,0,2
d70vf9o,2016-08-29 01:15:45-04:00,Zulban,,I've discovered in another thread that maximizing distance squared does not get me the kind of results I want actually. I've added another picture.,500vam,t1_d70qk89,RamsesA,,Reply,1,0,1
d70x9v8,2016-08-29 02:36:59-04:00,RamsesA,,"I see your second picture, I agree it is not distance squared. It doesn't look like linear distance either. You probably want to minimize something sub-linear, like log distance, sqrt distance. Each of these will reward additional distance to the closest points more than the furthest, pushing you into open spaces. Exactly which metric is appropriate probably depends on your use case.

In any event, I suspect the same solution technique (some form of stochastic gradient descent) would work for all of the above metrics. I don't know of any great libraries for this off the top of my head, but it's plausible one exists where you can punch in an evaluation metric and a parameter generator.",500vam,t1_d70vf9o,Zulban,,Reply,2,0,2
d71fzes,2016-08-29 13:28:47-04:00,Zulban,,"> You probably want to minimize something sub-linear, like log distance, sqrt distance.

Thanks for the tip. So I tried that, seems off. I think the problem is that maximizing square distance is mathematically very similar to minimizing sub-linear distance. I've started [a new thread](https://www.reddit.com/r/compsci/comments/5065u7/what_function_is_maximized_to_find_the_points_in/) that I think describes the problem much clearer.",500vam,t1_d70x9v8,RamsesA,,Reply,1,0,1
500b8r,2016-08-28 13:12:56-04:00,chromeobie,a negative floating number to binary,How to write -0.625 in binary ???,,,,,Submission,0,0,0
d70a8wl,2016-08-28 15:32:02-04:00,lneutral,,"See the [IEEE floating point page on Wikipedia](https://en.wikipedia.org/wiki/IEEE_floating_point#Formats) for a little bit of background, but the basic idea is that you use a sign bit to represent that the value is negative.

The rest of the (probably homework-related) problem is left as an exercise to the reader.",500b8r,t3_500b8r,chromeobie,,Comment,3,0,3
d709ueu,2016-08-28 15:22:11-04:00,AnsibleThing,,Depends on the notation you use,500b8r,t3_500b8r,chromeobie,,Comment,0,0,0
d70enpa,2016-08-28 17:22:33-04:00,sassinator1,,"Edit: Why all the down votes?

There are different ways of representing it. Usually it's done in Twos Complement notation but it can also be represented in Sign and Magnitude notation. 

Sign and Magnitude is the simplest, you just make the Most Significant Bit (MSB aka the bit furthest to the left) a 0 for a positive or a 1 for a negative sign. The rest of the byte is just the normal positive number. 

In Twos Complement the MSB becomes a negative number. For example, if previously the MSB was the 4 column, it will now be the -4 column. Then you fill in the rest of the number so that it totals your -0.625. Just remember that you can have columns less than the value of 1, such as a half, quarter, eighth etc. 

Afterwards you will need to normalise the number, but there are much better videos online explaining this. Essentially you just make sure that a Twos Complement number begins with 01 if positive or 10 if negative. 

This was a quick brush over a complicated topic, but just do lot of practice and it becomes second nature",500b8r,t3_500b8r,chromeobie,,Comment,0,0,0
4zyura,2016-08-28 06:15:52-04:00,mrstat88,Resume-worthy projects for beginners?,"Hi, I'm a freshman intending to major in CS with very little programming experience. I'm taking an intro class taught in Python right now, and am looking to get an internship at a respected company or interesting startup this upcoming summer. I've been searching the web and have found that a lot of companies start filling their intern spots as soon as November and December, and this has worried me because my resume basically has nothing on it as of now. What are some fairly easy to learn project subjects that I could pursue in order to get some personal experience under my belt for internship season?",,,,,Submission,11,0,11
d6zx0cq,2016-08-28 09:29:31-04:00,superdemongob,,"Every project is resume worthy as long as you can talk about what you did and what it taught you. 

I'm huge into cigars and made a cigar tracker app for personal use that I stuck on my resume and got a lot of interest. 

I built a small clock out of a raspberry pi zero. That project was trivial from a coding perspective but I talk about hardware challenges and all I learned from that. 

So yea, make sure you have good stuff to say and any project is a good project. ",4zyura,t3_4zyura,mrstat88,,Comment,9,0,9
d70ey5f,2016-08-28 17:30:12-04:00,dis_box,,"Would you say this always holds true? with some exceptions, of course. For instance, I like to write games for fun, but it serves as practice for what I've learned and keeps me from getting rusty on stuff I'm already comfortable with. Say I'm applying for a position as a developer (but not writing games). Do you think companies would still see the games as an asset even if it's not the kinda stuff I would be doing at work? Certainly, they'd want to see other examples, too. But I fear that an employer may be less likely to take me seriously. ",4zyura,t1_d6zx0cq,superdemongob,,Reply,1,0,1
d70qsfr,2016-08-28 22:47:02-04:00,magikker,,I recruit at my alma mater for my employer. The biggest concern I have with hiring people right out of school is that you can get the degree without knowing how to write code. Even worse is that you can get the degree without liking to write code. Anything that you do that shows me you know you stuff and enjoy doing it moves your to the top of the resume pile. ,4zyura,t1_d70ey5f,dis_box,,Reply,1,0,1
d6zw48h,2016-08-28 08:52:20-04:00,None,,"I would definitely look in to project tutorials online, then you'll also learn while making the project.

http://knightlab.northwestern.edu/2014/06/05/five-mini-programming-projects-for-the-python-beginner/",4zyura,t3_4zyura,mrstat88,,Comment,3,0,3
d703qfr,2016-08-28 12:49:40-04:00,Filmore,,"It's not about utility of what you made, it's about your track record of how you overcome challenges and if you are able to communicate technically with a bright person who might not be as familiar with the subject matter as you.",4zyura,t3_4zyura,mrstat88,,Comment,1,0,1
d70g04j,2016-08-28 17:57:06-04:00,cdrootrmdashrfstar,,"My general advice is to have problems or inefficiencies in your life (as everyone does), identify them, then programmatically create something that makes those daily, repetitive tasks more efficient/automated/be more pleasant to do... or find something cool/interesting that you think could benefit you in some way, and then make it! Necessity (and boredom) is the mother of invention.",4zyura,t3_4zyura,mrstat88,,Comment,1,0,1
4zxv74,2016-08-28 00:08:15-04:00,only_male_flutist,Getting into computer science,"I am a teenager that is very interested in CS, while I understand that CS is a very diverse field with many parts, I want to see what intrigues me in the field and possibly go to college and make a carrier of it, if not a hobby.

I covered the basics of several programming languages,and feel I have a basic understanding of those concepts, I want to learn more. I am taking a AP Computer Science class as my high school, but as the class is taught to the test, both the AP class/test is new, and the teacher is new to CS as well I feel, while I will probably enjoy the class, I will not get the tools and experiences I all looking for.

I was wondering if there were any online resources such as websites, books, and seminars (preferably free) that could be recommend to really get into the field.",,,,,Submission,4,0,4
d703yrt,2016-08-28 12:55:29-04:00,ISvengali,,"/u/KyleRochi 's suggestions are great.  

One thing I wish I was told while I was in high school (learned it in college) is that learning is more on you than on anyone else.  

So take each assignment, and go _just_ a bit further.  Add a feature, make part of the algorithm fast, solve a bigger problem set than is required, mix multiple assignments together.  Do this both on easy assignments, and on the hard ones.  ",4zxv74,t3_4zxv74,only_male_flutist,,Comment,3,0,3
d6zoz5i,2016-08-28 02:11:58-04:00,KyleRochi,,"Not sure you're learning style, but I would do one of two things. (Or both)

1. Make stuff. Pick something you want to build and build it, one of the best ways to learn to program is by doing, and the more you do the more you will learn. It can sometimes be hard to find stuff to work on, but look on GitHub and try to contribute to some projects. (if you know JS you can contribute to this [chrome plugin](https://github.com/KyroChi/GitHub-Profile-Fluency))

2. Textbooks. Textbooks are great resources (hence why colleges use them). Do some research into college CS course-loads and start reading. I personally like buying physical textbooks but most are freely available online.

Here is a sample list of standard CS courses and some textbooks that would help you through related material

|Course|Book|
|:--------|:-----:|
Intro to OOP/CS (Sounds like you are past this) | n/a
Data Structures and Algorithms | [Introduction to Algorithms](https://mitpress.mit.edu/books/introduction-algorithms)
Discrete Structures | Can't recommend a good one :S
Compilers | [Compilers: Principles and Techniques](https://www.amazon.com/Compilers-Principles-Techniques-Tools-2nd/dp/0321486811)
Operating Systems | [Hands on](https://www.valorebooks.com/textbooks/operating-systems-design-and-implementation-3rd-edition/9780131429383?utm_source=Froogle&utm_medium=referral&utm_campaign=Froogle&date=08/28/16), [Theory](https://www.amazon.com/Modern-Operating-Systems-Andrew-Tanenbaum/dp/013359162X)
Computer Architecture | [Great book, not a text book](https://www.amazon.com/Code-Language-Computer-Hardware-Software/dp/0735611319/ref=sr_1_1?s=books&ie=UTF8&qid=1472364407&sr=1-1&keywords=code)
Calculus | [Calculus w/ Diff. Eq.](https://www.amazon.com/Calculus-Differential-Equations-Dale-Varberg/dp/0132306336/ref=sr_1_1?ie=UTF8&qid=1472364468&sr=8-1&keywords=calculus+rigdon)

These are all resources I am familiar with, I can't recommend a good discrete structures text because I haven't yet found one I like. If anyone knows of a good one let me know.

Also consider looking into artificial intelligence, rigorous computer architecture, computer theory and other topics.

Starting with [""Introduction to Algorithms""](https://mitpress.mit.edu/books/introduction-algorithms) and [""Code""](https://www.amazon.com/Code-Language-Computer-Hardware-Software/dp/0735611319/ref=sr_1_1?ie=UTF8&qid=1472364621&sr=8-1&keywords=code) should be enough for you to figure out whether or not you would like to pursue CS in college. I cannot recommend these two books enough and I think everyone in the field should read them or an equivalent text. The fact is there is a ton of stuff to know/learn, and learning it all in the next year is un-realistic, CS takes years to master and for a specific field most people figure that out during or after college.
",4zxv74,t3_4zxv74,only_male_flutist,,Comment,2,0,2
d70dp7d,2016-08-28 16:57:52-04:00,only_male_flutist,,"Thanks, I'll definitely be checking some of these out!",4zxv74,t1_d6zoz5i,KyleRochi,,Reply,1,0,1
d758waf,2016-09-01 07:31:45-04:00,EruptingVolcanus,,"Give the CodeCademy website a look, its awesome for learning programming in different languages!",4zxv74,t3_4zxv74,only_male_flutist,,Comment,1,0,1
4zw4mz,2016-08-27 16:44:04-04:00,uisge-beatha,ELI15: question about Hashing and the 'uniqueness' of Hash values.,"so, im learning a bit about cryptography atm, and one of the things im learning about is Hashing.    
  
My understanding is as follows:  
I have a file, there is lots of data in this file, and lots of ways that data might be.  I Hash the file, and I get a short series of digits and letters (40 long? does that sound right?), which should be unique to the file.  Move a single colon, or change a digit by a value of 1, and we will get a huge variation in the Hashes.  this sound right?  
  
My question is: how can the Hash be guarenteed to be unique when there are so few variables in the Hash compared to the file.    
  
If the Hash is 40 characters long, and the file contains thousands of characters, and they have more or less the same set of character available to them - how do we get unique Hashes?    
  
(i know that, under controlled conditions, someone managed to get a 'collision' (same hash for different files) using either MD5 or SHA1, but my course suggests that SHA2 doesn't have this problem :S how? :S )",,,,,Submission,13,0,13
d6z7tg0,2016-08-27 17:11:30-04:00,None,,[deleted],4zw4mz,t3_4zw4mz,uisge-beatha,,Comment,17,0,17
d6zsmlz,2016-08-28 05:31:42-04:00,uisge-beatha,,"that's clarified it.  thanks :D  
(the course im doing is for total novices, so they simplify things a bit much)",4zw4mz,t1_d6z7tg0,None,,Reply,1,0,1
d6zbu11,2016-08-27 19:07:01-04:00,IgnorantPlatypus,,"The ""uniqueness"" of a hash is based on the [birthday paradox](https://en.wikipedia.org/wiki/Birthday_problem) -- that is, the likelihood that any two source contents hash to the same value becomes astronomically small if the hash function is designed well and produces a large enough value.

There exist infinitely many sources that hash to any given MD5, SHA1, or even SHA2 value. But for something like SHA2, we don't (yet?) know how to find any of the colliding source values.",4zw4mz,t3_4zw4mz,uisge-beatha,,Comment,3,0,3
d6zci2b,2016-08-27 19:26:53-04:00,uisge-beatha,,"so it's not the case that the hash values are nececarily unique to a file, just that the odds of collision are so low (i suppose, like unrelated people having the same fingerprints)?
",4zw4mz,t1_d6zbu11,IgnorantPlatypus,,Reply,2,0,2
d6zcosi,2016-08-27 19:32:03-04:00,IgnorantPlatypus,,"Exactly -- there do exist multiple files that hash to the same value. But with large enough hashes the chance of seeing a collision is sufficiently small to be ""never"" for all practical purposes. And with a good enough hash, there is no way to *find* another file that would hash to the same value.

The inability to force a collision is the key to why using a hash is secure, and the existence of an algorithm for forcing an MD5 collision is why that hash is no longer cryptographically secure.

Regarding your username, are you Gaelic or do you just love whiskey?",4zw4mz,t1_d6zci2b,uisge-beatha,,Reply,3,0,3
d6zsnnw,2016-08-28 05:33:38-04:00,uisge-beatha,,"tha beagan agam / i speak a bit of gaidhlig    
(im also fond of a dram, but in hindsight its not the best defining characteristic :P ) ",4zw4mz,t1_d6zcosi,IgnorantPlatypus,,Reply,1,0,1
d6zezvc,2016-08-27 20:38:45-04:00,elperroborrachotoo,,"Exactly :)   

Note that ""low chance fo a collision"" is sufficient for e.g. hash tables, but cryptographic hashes have stronger requirements. 

For a hash table, collisions should be few, but a few of them they are not a problem. 

Functions where - as you say - *""change a digit by a value of 1, and we will get a huge variation in the Hashes""* are statistically most likely to have the fewest collision, no matter what your input is.  So they make good *general purpose* hash functions. However, this ""bit mixer"" property is not required: for a dictionary it doesn't matter if the hash reveals something about the values. 

Storage systems that index data by their hash often use a cryptographic hash function simply because it guarantees a very low probability of a collision. Either such a collision is easily detected in the code or by the user, or the results of a collision are relatively benign, or - as in the case of git - it can be argued that more catastrophic problems are significantly more likely. 

For crypto, the requirements are stricter: most importantly, the hash must not reveal anything about the data (which is another way of saying it's a one-way function), and it should be computationally unfeasible to try possible inputs until you find one for a given hash. The ""bit mashup"" is essential for that. 

",4zw4mz,t1_d6zci2b,uisge-beatha,,Reply,1,0,1
d6zf79d,2016-08-27 20:44:30-04:00,thatwasntababyruth,,"Correct. Hash functions aren't meant to provide unique values, they are meant to be a) hard to reverse (for any given hash, you cannot craft an input for it without bruteforcing), and b) extremely different for similar inputs (if you change one bit in the input value, it should result in a significantly different output, such that you can't tell how many/what bits were changed).

The first property is what you usually want for file hashes. The hash is provided to guarantee to you that nobody tampered with it, because any sort of tampering will wildly change the hash. It's not possible to inject extra code into a binary and have the hash stay the same (outside of random happenstance).

The other property is the one you want with something like a hashmap structure, where you just need a good distribution of outputs so that you can send inputs into buckets.",4zw4mz,t1_d6zci2b,uisge-beatha,,Reply,1,0,1
d6zq25p,2016-08-28 03:02:56-04:00,xiongchiamiov,,"We're also helped that in many cases, a collision isn't actually useful.

For instance, say you're going to run a binary and are checking its hash against the known good file. If I can intercept the download and provide a different file, not only does it need to collide, but it needs to run and do something I'd find useful, like sending your passwords to my server.

Note that not all hash functions are suitable for cryptography. CRC32, for instance, only has six digits - so collisions are not difficult to generate. But it's very fast, and as such is a good choice for verifying that a download hasn't been accidentally corrupted in a situation where you're not concerned about an active adversary.",4zw4mz,t1_d6zci2b,uisge-beatha,,Reply,1,0,1
d6zspi0,2016-08-28 05:36:59-04:00,uisge-beatha,,"just wanted to say thanks to this sub.  sometimes i ask an ELI type question and no one is interested.  you guys are good to the laity    
remain awesome ^.^",4zw4mz,t3_4zw4mz,uisge-beatha,,Comment,3,0,3
4zvfzx,2016-08-27 14:13:54-04:00,PhysoTronic,What's the difference between a Parallel Computer and a Supercomputer?,"Can someone explain to me what is difference between these two? Parallel computer is computer who use many cpu's and other hardware but when I see architecture for latest supercomputers, they also have like 100-200 CPU's in them. Both can have many dozens of processors, so what's the technical difference?
",,,,,Submission,8,0,8
d6zqd13,2016-08-28 03:18:25-04:00,panderingPenguin,,"A parallel computer is simply a computer than can run multiple threads of execution simultaneously.  In english, it can do multiple things at the same time.  You almost certainly wrote your post using a parallel computer as even modern smartphones generally have at least 2 CPU cores, and I seen phone core counts as high as 8.  Laptops generally have 2-8, and desktops and servers can have far far more.  Essentially any modern computer (or at least what you would think of as a computer, I'm not talking about embedded devices) is a parallel computer.

A super computer is just a massively parallel computing system.  Modern super computers are actually combinations of many computers.  They're generally arranged in an architecture such at a super computer has many clusters, each cluster has many nodes, and each node has many cores.  These are all hooked up with very high speed interconnects to allow fast communication between them.  In your post you cite a 100-200 CPU count, but that's actually off by at least three orders of magnitude for modern super computers.  That might be a single node in a modern super computer.",4zvfzx,t3_4zvfzx,PhysoTronic,,Comment,2,0,2
d6zrw7f,2016-08-28 04:45:33-04:00,PhysoTronic,,"Thank you for you answer :D 
I have few more questions.If I had 2 same CPU in my PC would that count like parallel computer? or does by definition every PC who has more than 1 core in cpu is considered parallel computer? Also when you are talking about clusters, does that mean that its similar to parallel computers but you dont need to have same hardware x2, for example, you and me dont have same PC (probably) and if we connected them together that would count like cluster pc right?",4zvfzx,t1_d6zqd13,panderingPenguin,,Reply,1,0,1
d703j3e,2016-08-28 12:44:19-04:00,panderingPenguin,,">or does by definition every PC who has more than 1 core in cpu is considered parallel computer? 

Yes, this is the case

>Also when you are talking about clusters, does that mean that its similar to parallel computers but you dont need to have same hardware x2, for example, you and me dont have same PC (probably) and if we connected them together that would count like cluster pc right?

Generally, a cluster will be made up of many pieces of the same hardware connected together, but that's really not a hard and fast rule.  I don't think just connecting your PC to mine really makes a cluster, as clusters generally have very special setups that make the connections between machines very fast.  Also, you need software written with a compute cluster in mind to actually take advantage of the hardware. Just hooking my PC up to yours won't make any of your software run any faster ",4zvfzx,t1_d6zrw7f,PhysoTronic,,Reply,1,0,1
d6z2609,2016-08-27 14:37:39-04:00,ISvengali,,"100 to 200?  The current top end supercomputer has 10,649,600 cores!  

Typically, the communication channels between the cores is *extremely* fast.  

The code written for them is custom for their processor type and extremely optimized assembly.  There little to no OS running on them, at least nothing youd recognize as one.  ",4zvfzx,t3_4zvfzx,PhysoTronic,,Comment,2,0,2
d6z4a4e,2016-08-27 15:34:22-04:00,BonzoESC,,"[Nine of the top ten run Linux,](https://www.top500.org/lists/2016/06/) but yeah, I bet there are higher level systems in play that provide other abstractions useful at scale like job scheduling, IPC, and such. ",4zvfzx,t1_d6z2609,ISvengali,,Reply,3,0,3
d6z4jde,2016-08-27 15:41:19-04:00,ISvengali,,"Well, 'run linux'.  Im sure there are nodes in the system that run linux, and the compute nodes may even run something stripped down.  

That used to be how it is when there was lower number of nodes.  Ide have to look, but I bet its that way still.

The PS3 technically ran FreeBSD, but the SPUs just had a little tiny little bit of stub code to run jobs on them.  

",4zvfzx,t1_d6z4a4e,BonzoESC,,Reply,3,0,3
d6zbxx0,2016-08-27 19:10:15-04:00,bacondev,,Linux isn’t an operating system. It’s just a kernel.,4zvfzx,t1_d6z4a4e,BonzoESC,,Reply,-4,0,-4
d6ziv06,2016-08-27 22:31:02-04:00,alien_screw,,When someone tells you a machine runs Linux they mean it's running a Linux based operating system. It's a general term. ,4zvfzx,t1_d6zbxx0,bacondev,,Reply,0,0,0
d6zkcuc,2016-08-27 23:18:48-04:00,bacondev,,My point is that it can still have Linux without having an operating system.,4zvfzx,t1_d6ziv06,alien_screw,,Reply,0,0,0
d6zl6m5,2016-08-27 23:46:16-04:00,alien_screw,,What tools come with the linux kernel when run alone? Can you access a getty or any terminal?,4zvfzx,t1_d6zkcuc,bacondev,,Reply,1,0,1
d6zq5o0,2016-08-28 03:07:54-04:00,panderingPenguin,,"Most super computers run Linux, and the software for them is rarely written in assembly.  C, C++, and FORTRAN are far more common.",4zvfzx,t1_d6z2609,ISvengali,,Reply,2,0,2
d6z1t9p,2016-08-27 14:28:27-04:00,tavianator,,"Supercomputers are, like, *super* parallel.",4zvfzx,t3_4zvfzx,PhysoTronic,,Comment,1,0,1
4zqd1y,2016-08-26 15:22:55-04:00,BasedKami-sama,How do I reverse a singly linked list?,"This is in C. Below, I created a linked list and printed the numbers 1-10 using it. How do i create a reverse() function to reverse the linked list so i can print 10-1? 

#include <stdio.h>
#include <stdlib.h>

typedef struct node node;

struct node
{
    int data;
    node *next;
};

void Output(node *h)
{
    node *p = h;
    
    while(p!=NULL)
    {
        printf(""%d\n"", p->data);
        p = p->next;
    }
}

void reverse(node *head)
{
    
}

int main(int argc, const char * argv[]) {
    
    node *first = NULL;
    node *current = NULL;
    node *previous = NULL;
    
    for(int x=1;x<=10;x++)
    {
        current = malloc(sizeof(node));
        current->data = x;
        if(first == NULL)
        {
            first = current;
            previous = current;
        }
        
        else
        {
            previous->next = current;
            previous = current;
            
        }
        
        
    }
    
    Output(first);
    
    reverse(first);
    
    Output(first);
    
    
    return 0;
}",,,,,Submission,0,0,0
d6y52dx,2016-08-26 18:41:57-04:00,gyroda,,"Without actually reading your code:

Simply go through the list and make the first node point to nothin/null, then get the second and make it point to the first, then get the third and make it point to the second and so on. 

At the end make the last item the head of the list. ",4zqd1y,t3_4zqd1y,BasedKami-sama,,Comment,5,0,5
d6xx13u,2016-08-26 15:32:44-04:00,crookedkr,,try /r/programminghelp,4zqd1y,t3_4zqd1y,BasedKami-sama,,Comment,4,0,4
d6yk1v2,2016-08-27 02:31:13-04:00,knightdiver,,"Hey, that's an interview question I've been using 10+ years ago. Given time and a computer with a compiler, that should be doable. If you have code that attempts to reverse the list but doesn't work, I'll gladly point you in the right direction, but the completely empty reverse() function makes me think you're not actually trying but just getting somebody to do your homework.",4zqd1y,t3_4zqd1y,BasedKami-sama,,Comment,3,0,3
d704ad7,2016-08-28 13:03:47-04:00,PearlyDewdropsDrops,,"I'm sure this problem has *never* been solved and posted to google before. You're like, the only one who has ever pondered how to do this.",4zqd1y,t3_4zqd1y,BasedKami-sama,,Comment,1,0,1
d6xyvvy,2016-08-26 16:13:48-04:00,None,,[deleted],4zqd1y,t3_4zqd1y,BasedKami-sama,,Comment,1,0,1
d6y4722,2016-08-26 18:19:45-04:00,FluffyOgreJet,,There are solutions for singly linked lists that take O(1) space.,4zqd1y,t1_d6xyvvy,None,,Reply,2,0,2
d6y4g72,2016-08-26 18:26:06-04:00,None,,[deleted],4zqd1y,t1_d6y4722,FluffyOgreJet,,Reply,0,0,0
d6ylw07,2016-08-27 04:11:28-04:00,bacondev,,"Yeah, but that requires extra instructions to rebuild each node. Why bother when you can just update a pointer in each node?",4zqd1y,t1_d6y4g72,None,,Reply,1,0,1
d6y7vgh,2016-08-26 19:56:15-04:00,radjic,,"You can do it in O(1) space by doing it in place through a single traversal. This means that space is constant cuz u aren't making any new data structure for the solution. This is also O(n) time because you traverse once through n items in the linked list.

//python code. Take head of list

    def reverse(head)

//create a var to reference the previous node when traversing. The head has no previous node, therefore is set to 'None'

    prev = None

//reference to the head of the list

    curr = head

//traverse entire list 

    while curr is not None:
//create a reference to the next node in list

    next_node = curr.next
//point current node's 'next' to prev node

    curr.next = prev
//keep reference of current node before traversing
    prev = curr
//traverse to next node

    curr = curr.next

//This will be the last node that is not None also will be the new head of the list

    return prev",4zqd1y,t1_d6xyvvy,None,,Reply,1,0,1
4zo3tm,2016-08-26 07:26:54-04:00,torbmol,"What's the name of this type of hash table, and is it any good?","A year ago I wrote a custom hash table (in java) without really knowing the theory. In pseudocode:

    class HashTable<K,V>:
        buckets: (int,int)[];
        content: (K,V)[];

Where `buckets` stores the index of the first entry with that hash, and the size of the bucket.  
There is no load factor; The arrays are grown when it cannot easily make an adjacent free spot to insert in.  

On [the Wikipeda page](https://en.wikipedia.org/wiki/Hash_table#Collision_resolution)  there is no exact match, but ""separate chaining with dynamic arrays"" is the most similar approach.

My goal was to be memory-efficient and avoid the indirection of `Entry` classes.

EDIT: The actual code: [shared code between set and map](https://bitbucket.org/tormol/leanhash/src/74b8e192874156da9abb95cb9300664ea6b73168/src/leanhash/LeanHash.java?at=master&fileviewer=file-view-default),
[LeanHashMap](https://bitbucket.org/tormol/leanhash/src/74b8e192874156da9abb95cb9300664ea6b73168/src/leanhash/LeanHash.java?at=master&fileviewer=file-view-default),
[tests](https://bitbucket.org/tormol/leanhash/src/74b8e192874156da9abb95cb9300664ea6b73168/tests/leanhash/LeanHashTests.java?at=master&fileviewer=file-view-default)  
I haven't programmed in Java in a while, so the tests might not pass newer versions of guava.",,,,,Submission,14,0,14
d6xf8jw,2016-08-26 08:54:20-04:00,DrDougPhD,,Could you provide a concrete example for it's usage?,4zo3tm,t3_4zo3tm,torbmol,,Comment,2,0,2
d6xhrdn,2016-08-26 10:02:34-04:00,torbmol,,"I have never actually used it,  tested it with the guava test suite.

I intended to use it in an intepreter (which I never finished) to hold the variables declared in a scope.
Since there would be few variables per scope, another goal was low constant size.  

The code: [shared code between set and map](https://bitbucket.org/tormol/leanhash/src/74b8e192874156da9abb95cb9300664ea6b73168/src/leanhash/LeanHash.java?at=master&fileviewer=file-view-default),
[LeanHashMap](https://bitbucket.org/tormol/leanhash/src/74b8e192874156da9abb95cb9300664ea6b73168/src/leanhash/LeanHash.java?at=master&fileviewer=file-view-default),
[tests](https://bitbucket.org/tormol/leanhash/src/74b8e192874156da9abb95cb9300664ea6b73168/tests/leanhash/LeanHashTests.java?at=master&fileviewer=file-view-default)

I haven't programmed in Java in a while, so the tests might not pass newer versions of guava.",4zo3tm,t1_d6xf8jw,DrDougPhD,,Reply,1,0,1
d6xgmxv,2016-08-26 09:34:20-04:00,crookedkr,,"I don't think you have included enough details to understand what scheme you are using. There are many many ways to deal with collisions, but if you never have to resize the number of buckets you are using some sort of chaining.",4zo3tm,t3_4zo3tm,torbmol,,Comment,1,0,1
d6xiq2j,2016-08-26 10:25:15-04:00,torbmol,,"Does this explain it?

    content: [a,_,b,c,d,_]
             |_| |___|_|-----.
               \   *-------.  '---.
    buckets: [(0,1),(0,0),(2,2),(4,1)]
EDIT: the buckets array is resized when `content` is, But it's not required.",4zo3tm,t1_d6xgmxv,crookedkr,,Reply,1,0,1
d6xrx2z,2016-08-26 13:42:22-04:00,crookedkr,,"That's better but let me make some assumptions and you can tell me what i have wrong compared to your method. We have some keys, integers for simplicity, and we map them to single letters again just for exposition. Lets limit keys to the domain 1 to 100. Now we create a partitions, say 5 which can hold pairs. We get the first pair, (28,f) and hash 28 in whatever way and see that that goes in partition 3, so we put it there {[],[],[],[(28,f)],[]} here {} is the whole structure, [] is a partition, and () is our pair. Now we add another pair say (89,x) and it collides (maybe this one doesn't collide but by the pigeon hole principal we will have a collision on the 6th insert if not before), that is, it also belongs in the 3rd partition. We think this is ok because the partition can expand so out structure looks like {[],[],[],[(28,f):(89,x)],[]}. And other inserts continue. 

To do a lookup we hash the key, 89, and then sequentially search the third partition, we either find it or the key is not in the table.

Is that about right?",4zo3tm,t1_d6xiq2j,torbmol,,Reply,1,0,1
d6y3fg9,2016-08-26 18:00:15-04:00,torbmol,,"Yes,

Some more details: If neither the slot before or after are empty (`null`), before growing the array `insert()` will try to move the entry there if it's bucket has size 1, and then try to move the bucket of the new entry.

This means the partitions might not be ordered: the elements in bucket 3 might be stored before the elements in bucket 2.

This means it needs to hash the element before or after to find its bucket, which might not be optimal. ",4zo3tm,t1_d6xrx2z,crookedkr,,Reply,1,0,1
d6y7aij,2016-08-26 19:40:34-04:00,crookedkr,,hash tables aren't generally ordered anyway so that's not a problem. This really just sounds like a hash table that uses linear probing at max 1 step and then chaining. There is nothing inherently wrong with this approach. The are many approaches to dealing with collisions: which is best is frequently a matter of workload and data set.,4zo3tm,t1_d6y3fg9,torbmol,,Reply,1,0,1
d6y8a8z,2016-08-26 20:07:27-04:00,torbmol,,"The entries in a bucket aren't sorted in either case, so ordered buckets won't make the table sorted.

But if the (non-empty) buckets point to increasing indexes I could just search the bucket array for the closest non-empty bucket, and not need to hash the ""blocking"" entry.",4zo3tm,t1_d6y7aij,crookedkr,,Reply,1,0,1
4zlr0b,2016-08-25 20:13:10-04:00,ddaarrbb,Please understand: I know very little about Computer Science. I've always had this one question about file compression however...,"I know very little about file compression, but I've always wondered since I first learned about it when I was 12:  Since, ultimately, computer files are just collections of 1s and 0s, are there any compression techniques that are just a short, small formula that adds up (or multiplies, etc.) to the binary number that the uncompressed file is comprised of?  Like, for an extremely simple example, say you have a file that is

    1010101011110001011010010101

couldn't you just have a 'compressed' version of the file be an equation that results in that binary number, thus producing the original file?

My assumption is that the equation required to produce the file's binary would be larger than the original file itself, but I was just curious.",,,,,Submission,24,0,24
d6wydkj,2016-08-25 21:53:43-04:00,ldpreload,,"For a sufficiently loose definition of ""equation,"" that's how compression works.

In information theory, we have the concept of ""[Kolmogorov complexity](https://en.wikipedia.org/wiki/Kolmogorov_complexity)"", which is the length of the shortest possible computer program that will produce some output. For complicated-enough files, finding and proving the actual shortest program, but it's a good theoretical concept. (And the fact that it's theoretical lets you ignore the difference between different programming languages.)

Let's say I have a file that contains

    abcdeabcdeabcdeabcdeabcdeabcdeabcdeabcde

I can type the following into a Python interpreter to generate it:

    ""abcde"" * 8

So I can store 11 bytes, plus the knowledge to use Python, instead of the original 40. Or if I have these 40 bytes:

    340282366920938463463374607431768211456

I could use the following 6 bytes of Python:

    2**130

But the hard part is _finding_ that program. I'm cheating because I know how these were generated, and it's very hard to write a computer program that comes up with an efficient computer program to generate some data. (In fact, if you scroll a bit on that Wikipedia article, you'll see that it's impossible for a program to reliably generate _the most efficient_ program for every possible data file you might give it.) But if I generate 40 random digits:

    9431248719977428535517507493588228520926

I'm not sure how to express that as an equation. Wolfram Alpha tells me I can prime-factor it as

    2 * 29 * 1579 * 23801 * 4326770602087117130626036977593

which is in fact longer than the original. In general, the Kolmogorov complexity of a random string is going to be approximately the length of the string (since, if it were reliably less than the length of the string, it wouldn't be completely random!).

So only certain files compress at all, and many files are more difficult to compress than others because the pattern is less obvious. Usually compressors are not going to come up with entire computer programs, but they do come up with _inputs_ for a computer program, which can consist of pretty complex instructions: repeat this structure, but inside it repeat this other structure a varying number of times, or modify it in this way when you repeat it, or re-use some data that was previously output, or whatever.

But most files have structure of some sort. This entire comment only uses letters, numbers, and a very small number of punctuation characters. That's way less than the 256 possible bits per byte, so you can compress it into fewer bits per character. English text has structure: most of the time the letter ""q"" is followed by ""u"", instead of any other letter. Most of the time ""Th"" is followed by ""e"" or ""a"", not ""w"". So if you _know that structure_, you can compress English text efficiently. And even if you don't, you can often convey that structure at the top of your compressed data (you can certainly imagine doing that for a program), and compress large documents efficiently. Same with other structured documents like HTML files with tags, programs with repeated sections of code, etc.

Music / audio compression algorithms are one place which make heavy use of equations. A musical note consists of a sine wave at some particular fundamental frequency, plus quieter overtones at various multiples of that frequency. Different instruments will have different patterns of overtones, but ultimately a note can be expressed by a combination of these frequencies (and that's how synthesizers work). So for a recording of actual music, you can take the [Fourier transform](https://en.wikipedia.org/wiki/Fourier_transform) of the sound to figure out what frequencies are being heard, which will have some error because the recording process (and physical instruments) aren't perfect&mdash:and then ignore the error terms. If you're hearing a 440-Hz note on a piano, your ear won't consciously hear that the 2,200-Hz overtone on this particular piano is occasionally wavering to 2,199 Hz, so the compressor can just round it off and it'll sound fine. (But if you do this too much, it will sound mechanical and flat!) Another lossy compression technique that does the same sor of thing is JPEG, which is why [low-quality JPEGs of solid colors](https://upload.wikimedia.org/wikipedia/commons/a/a4/Comparison_of_JPEG_and_PNG.png) have characteristic patterns.

By the way, if you're curious about audio compression tricks, I like [this video from the Ogg guy](https://xiph.org/video/vid2.shtml).",4zlr0b,t3_4zlr0b,ddaarrbb,,Comment,30,0,30
d6wxrsh,2016-08-25 21:38:27-04:00,suffixaufnahme,,"File compression comes in two flavors: lossy and lossless. Lossy compression actually involves the loss of information, which means that what you get after compressing a file and then decompressing it is not exactly the same as what you started with. That works okay for pictures, video, and audio, where a minor drop in quality is usually not a big deal, but it would be very bad if you were compressing a text file or Word document. Lossless compression, on the other hand, guarantees that the decompressed file is bit-for-bit identical to the original file. This seems to be what you are describing.

There is a mathematical caveat when it comes to lossless compression. Any lossless compression algorithm that makes a certain file smaller will inevitably make some other file bigger. To use a small example, consider a compression algorithm that only runs on files that are two bytes long. Because there are 2^8 = 256 possible bytes, there are 256^2 = 65536 pairs of bytes and therefore 65536 distinct files that our mini compression algorithm can take as input. But if our goal is to make these 2-byte files smaller, they each need to be compressed down to a single byte, of which there are only 256 possibilities, and because our compression must be lossless, duplicates are not allowed. Obviously, because 65536>256, not all the possible inputs are going to be able to compressed to a smaller file.

Because of this theoretical limitation, real-world lossless compression algorithms are designed to perform well on typical data. Their true strength is that they can exploit redundancy in the input (such as the letter ""e"" occurring more frequently in a text file than ""z"") to produce a smaller file for real-word input than they would for just a random collection of bits. This is where your proposed method seems to have a flaw: if you treat the entire file as a large binary number and try to formulate a mathematical expression that evaluates to that number, then you are throwing away the patterns that make typical files compressible. The very predictable and repetitive input ""to be or not to be"" or ""a rose is a rose is a rose"" would be on level ground with the very unlikely input ""yr3TYwBuSBPHLEHkkM3mLAUj"". And many of the files that someone would want to compress would end up growing larger when compressed.",4zlr0b,t3_4zlr0b,ddaarrbb,,Comment,5,0,5
d6wyxf1,2016-08-25 22:07:41-04:00,CoopNine,,"Numbers don't compress very well.  While you could have an equation that equals that number, it will likely take up as much or more space.

There's two general types of compression.  Lossless and Lossy.  Lossless results in the uncompressed data being identical to the original.  Lossy compression results in the data being pretty much like the original.  A zip file is lossless.  An MP3 or JPEG image is lossy.

At a very high level compression is about finding patterns and simplifying them.  So in an image you might have 9 red pixels next to each other.  Instead of saying RRRRRRRRR you can say 9R, and save quite a bit of space.  Lossy compression might take colors that are very similar and say, eh, these can all be the same.  

So data with discernible patterns compresses very nicely.  To find those patterns, the more data you have, the better... So in general a longer set of data will compress better than a smaller set.  

Finally, it's not really the 1's and 0's that you're trying to compress.  It's the actual data that you're working with... you then store it as 1's and 0's. ",4zlr0b,t3_4zlr0b,ddaarrbb,,Comment,4,0,4
d6xc2u5,2016-08-26 06:49:21-04:00,dragonnyxx,,"> Numbers don't compress very well.

Every file is a number. Some of them compress very well.",4zlr0b,t1_d6wyxf1,CoopNine,,Reply,2,0,2
d6xfmok,2016-08-26 09:06:22-04:00,CoopNine,,"I assume you understand what I mean by that.  Discrete numbers don't compress very well.  If you were to have a bunch of numbers which represent data, they can often be compressed very well, but that is usually due to the ability to find patterns or repetition within the data.  In general using equations to represent discrete numbers is going to take more space, and more processing time to decipher.  We generally find it easier and more efficient to compress data by starting with the data itself at a higher level than binary.  Which is one of the reasons why we have so many different ways of compressing different types of data.",4zlr0b,t1_d6xc2u5,dragonnyxx,,Reply,1,0,1
d6wur99,2016-08-25 20:24:00-04:00,None,,[deleted],4zlr0b,t3_4zlr0b,ddaarrbb,,Comment,5,0,5
d6wuwwq,2016-08-25 20:28:00-04:00,ddaarrbb,,"So, maybe, once computers inevitably more powerful, this may be a possibility!  That's interesting.",4zlr0b,t1_d6wur99,None,,Reply,1,0,1
d6x9uc2,2016-08-26 04:41:06-04:00,teraflop,,"It's not a matter of how powerful your computer is. The real issue is that on average, it takes just as much space to store the square root and remainder as it does to store the original number.",4zlr0b,t1_d6wuwwq,ddaarrbb,,Reply,2,0,2
d6xdwgw,2016-08-26 08:09:51-04:00,thechao,,"You're describing a class of algorithm called ""arithmetic coding"" that was first popularized in the '80s. There are a family of post AC algorithms called FSE that represent the state of the art.",4zlr0b,t1_d6wur99,None,,Reply,1,0,1
d6xkcsi,2016-08-26 11:02:07-04:00,HeKis4,,"It could work, but not with the way computers store numbers, especially decimal ones. I dare you to give me a precise, unambiguous *and finite* answer to the square root of two. That's the first issue. 

You can't store a number with infinite digits without infinite memory. Sure, you can round the result, but how do we make a difference between 2 and 2+1^-25 ? And the problem gets even worse for high numbers. I don't even know if a computer can make a difference between, say, the square root of 1x10^30 and 1x10^30 +1, and these are small numbers when we're talking about megabyte-sized files.

EDIT : i'm sure this can be proved using math, using the fact that any number has one and only one square root and the other way around.",4zlr0b,t1_d6wur99,None,,Reply,1,0,1
d6wwt70,2016-08-25 21:15:30-04:00,xcombelle,,A compressed file act more or less like a formula to produce the original file. More precisely it contain all the information useful to reconstruct the original file. What make compression efficient is that it remove redundant information. And if your original file don't contain redundant information you can't reduce it's size  by compression techniques.,4zlr0b,t3_4zlr0b,ddaarrbb,,Comment,2,0,2
d704gbu,2016-08-28 13:08:04-04:00,PearlyDewdropsDrops,,you might find [this](http://www.patrickcraig.co.uk/other/compression.php) interesting.,4zlr0b,t3_4zlr0b,ddaarrbb,,Comment,1,0,1
4zit5k,2016-08-25 10:39:58-04:00,kmjohnson02,I don't understand how this is the answer to this quiz question from school.,"I’m very confused over this and would appreciate some opinion and understanding. One question in my most recent quiz for school is as follows:

The read, write, and execute permissions of a file are an example of a:
A. discretionary access control
B. firewall control
C. mandatory access control 
D. system kernel control

The quiz result says that the answer is DAC, but I don’t think there is enough detail in the question to arrive at this answer. I think Firewall control is clearly not the answer, and kernel is probably not correct either. However, DAC and MAC seem to be equally correct. I understand that usually you have to choose the most correct of the correct answers, but there just isn’t enough detail in the question to differentiate between MAC or DAC in this case. If the question included a statement as to HOW the permissions were granted, this probably wouldn’t be in question, but it doesn’t.

Correct me if I’m wrong, but the “read, write, and execute” part of the question are essentially useless to the answer in this case. It’s just fluff. Said in another way, the questions could have just said, “permissions of a file are an example of a ____”. When the question is broken down like this, how does DAC take priority over MAC as a correct answer???? Thank you for any insight. I want to learn!!!",,,,,Submission,6,0,6
d6w5d5u,2016-08-25 11:10:32-04:00,FluffyOgreJet,,"The read, write, execute permissions of a file likely refers to Unix permissions, which are discretionary. The owner of a file in Unix gets to set the RWX permissions.",4zit5k,t3_4zit5k,kmjohnson02,,Comment,8,0,8
4zhi6o,2016-08-25 04:29:24-04:00,chromeobie,Compression of a text that contains only the 5 first letters of the alphabet??,"So the question is: 
""Imagine the compression of a text written only with the first 5 characters of the alphabet (a, b, c, d, e). Knowing that their apparition's frequency is 1)e, 2)a, 3)c, 4)d, 5)b, code the letters on the minimum bits possible."" 

I don't see how to start resolving the problem and couldn't find a similar question on the coursebook nor the internet.",,,,,Submission,12,0,12
d6vuf6q,2016-08-25 04:37:20-04:00,icendoan,,"Okay, so you have to do two things:

1. Convert each character to some binary representation.

2. Make that binary representation use the fewest bits possible.

If you have just 5 characters, how many bits will you need to encode any of them in any combination? Is there a useful way of giving out these encodings to each character, given you know their relative frequencies? (Hint: some encodings might be longer than others!)

This will lead you to Huffman Encoding, but I encourage you to try and work out the details yourself before looking it up.
",4zhi6o,t3_4zhi6o,chromeobie,,Comment,7,0,7
d6vwlue,2016-08-25 06:44:49-04:00,chromeobie,,"We have 5 characters, so 3bits will be enough for the characters (2^3 =8). 
We can put: e=001, a=010, c=100, d=011, b=110 

But I don't see how to use their frequency (we don't have the value of it, they just gave a ""scale"" of their frequency) ",4zhi6o,t1_d6vuf6q,icendoan,,Reply,3,0,3
d6vwsos,2016-08-25 06:55:14-04:00,icendoan,,"Well, consider making `e` (the most frequent character) have a smaller bit representation, even if this requires that you increase the number of bits in other representations.

This works because `e` is more frequent than the others: if saving 1 or 2 bits on the `e` encoding forces you to spend another bit on the `b` encoding, that's fine.

If we make `e` just have the encoding `1`, do you see how to encode the others? (Hint: consider that the first bit of any other character has to be `0`, so as to mark it as ""not `e`"": then look at what code you might give to the next most frequent character).",4zhi6o,t1_d6vwlue,chromeobie,,Reply,4,0,4
d6vx304,2016-08-25 07:10:04-04:00,chromeobie,,"e:1
a:01
c:011
d:001
b:010   
what about this ??",4zhi6o,t1_d6vwsos,icendoan,,Reply,3,0,3
d6vx8s6,2016-08-25 07:18:02-04:00,icendoan,,"We're getting there! This particular encoding has a problem, though: how do you decode the string: `011`? It could be decoded as either the single character `c` or the two character string `ae`.

Once you resolve the ambiguity (it might require adding a fourth bit to some encodings), you should be done. Then, consider how you'd prove that this encoding yields the fewest bits (Hint: consider a smaller encoding, and show that they're either the same, or introduces some ambiguity).",4zhi6o,t1_d6vx304,chromeobie,,Reply,5,0,5
d6vx8z3,2016-08-25 07:18:19-04:00,chaos10101,,"Getting better, but depending on the question there might be an issue:
If my binary string is 011, I cannot distinguish between 01 1 and 011 (ae or c). You'll have to find an encoding that does not have this problem.",4zhi6o,t1_d6vx304,chromeobie,,Reply,3,0,3
d6vxfro,2016-08-25 07:27:03-04:00,chromeobie,,"e=1
a:01
c:001
d:0001
e:00001
This does it without ambiguity but how do I check if it yields the fewest bits ??",4zhi6o,t1_d6vx8z3,chaos10101,,Reply,1,0,1
d6vxhss,2016-08-25 07:29:34-04:00,chaos10101,,"It's hard to check in general, and even impossible without knowing the exact distribution. Since you are given only the order of frequency, this is the best you can do as far as I know. Maybe someone else can add something to this?",4zhi6o,t1_d6vxfro,chromeobie,,Reply,3,0,3
d6vxov6,2016-08-25 07:38:16-04:00,icendoan,,"Consider two codes, your code, and the optimal code. Then examine where they are different, and show that either the optimal code must be the same as your code, or it must be ambiguous (which would be a contradiction).

Also, notice that the encoding could be shortened: `b: 0000` does not introduce any ambiguity.",4zhi6o,t1_d6vxfro,chromeobie,,Reply,2,0,2
d6vxxdf,2016-08-25 07:48:23-04:00,chromeobie,,e:1 a:01 c:001 d:0001 b:0000 and e:0 a:10 c:110 d:1110 b:1111 are the two possible optimal codes !!! ,4zhi6o,t1_d6vxov6,icendoan,,Reply,1,0,1
d6vy6p6,2016-08-25 07:58:54-04:00,icendoan,,You should consider how you'd prove this.,4zhi6o,t1_d6vxxdf,chromeobie,,Reply,2,0,2
d6vyfxs,2016-08-25 08:08:41-04:00,chromeobie,,"I used the Huffman tree, isn't that enough ??
",4zhi6o,t1_d6vy6p6,icendoan,,Reply,1,0,1
d6vxlo1,2016-08-25 07:34:26-04:00,chromeobie,,u/icendoan can you confirm please ??,4zhi6o,t1_d6vxfro,chromeobie,,Reply,1,0,1
d6vxiwc,2016-08-25 07:30:54-04:00,ad_tech,,[Huffman coding](https://en.wikipedia.org/wiki/Huffman_coding),4zhi6o,t3_4zhi6o,chromeobie,,Comment,1,0,1
d6wr0qt,2016-08-25 18:50:15-04:00,Bottled_Void,,"The simple way is to write out all the different representations

**1-bit** : Can't use both of these as there wouldn't be anything to distinguish them between everything else.

 - 0

 - 1

**2-bit** : Doesn't quite have enough states to represent a through to e. But let's use the first 3.

 - 00

 - 01

 - 10

 - 11 - special code 11 means it's a 3-digit code.

**3-bit**

 - 000 - don't need any of these

 - 001

 - 010

 - 011

 - 100

 - 101

 - 110 - except these bottom two

 - 111 - this one too

Use smaller representations for most frequent:

    | a | 2 | 110 |
    | b | 5 | 00  |
    | c | 3 | 10  |
    | d | 4 | 01  |
    | e | 1 | 111 |

Some trees, the further from the root, the more bits you need. Just showing the frequency here instead of the letter itself.


                   |
                  / \
                 /   \
                /     \
               /        \
              |          |     
             / \        / \
            /   \      /    \
           5     4    |      3    
                     / \   
                    /   \  
                   |     | 
                   2     1 


5x2 + 4x2 + 3x2 + 2x3 + 1x3 = 33


                   |
                  / \
                 /   \
                /     \
               /        \
              5          |     
                        / \
                       /    \
                      4     |    
                           / \   
                          /   \  
                         |     | 
                         3     | 
                              / \   
                             /   \  
                            2     1


5x1 + 4x2 + 3x3 + 2x4 + 1x4 = 34
",4zhi6o,t3_4zhi6o,chromeobie,,Comment,1,0,1
4ze59u,2016-08-24 14:54:49-04:00,xFury86,[Degree Help] Cyber Security - Digital Forensics or Cyber Security - Network Security?,"Hello, I'm trying to figure out what would  be the correct degree to take for Cyber Security. Tried to read the FAQ's, other sub, and Google, but couldn't find what I was looking for and wants to know what employer/recruiter look for.

Been wanting to do Cyber Security since my first computer, I'm about 22 and haven't started college yet, but I want to get start soon instead of waiting. Mr. Robot has definitely motivated me (The works, how he hacks' dissect, etc. I know work wont be like that but its a field I can see myself having a career in). Only thing is, I need to re-work and re-learn all my math to catch up to High School math.


Now my college offers two Cyber Security AAS. Network Security and Digital Forensics. I think I should be taking Network Security but wanna make sure and get advice from those who are in he field and more experienced than I am.

End goal is being a security engineer/pen-tester (Not sure of the name since every company/person call it differently.) or a consultant.

Thanks!

Edit: this is what my school offer, not sure if it's decent.

https://www.csn.edu/sites/default/files/degreecert/aas_cit_cyber_security_network_security_16-17.pdf

Then transfer to a university here that offers BS for Computer Science. ",,,,,Submission,1,0,1
4zcfim,2016-08-24 08:57:16-04:00,Compscifingerprint,I processed fingerprints. How do I compare the results ?,"Hi,

*Full disclosure: I also asked this on Stackoverflow. Does it make me greedy ?*

I'm a trying to compare fingerprints. Here's what I got so far.

1. Get the raw image from a digitalPersona sensor. [image](http://imgur.com/tWhAXEp)

2. Binarize it. [image](http://imgur.com/1mEU1w0)

3. Skeletonize it. I used Hall's algorithm because that is the only one I got to work more or less properly. You can still see some flaws. [image](http://imgur.com/SxzDd1q)

4. Strip the convex hull (inverted Jarvis algorithm) and get all the ridge endings as an array of (i, j) coordinates (points with one neighbour only). I also have a script to get the bifurcations (3 neighbours). [representation of ridge endings only](http://imgur.com/Jt5TVab)

**What I need :** knowing that I can get these so-called minutiae (ridge endings and also bifurcations, which are not represented on the image) and also a direction for each point (e.g. direction of ridge for the ridge ending), how do I match two sets of minutiae and give a similarity score ?

**Tools and languages used:** digitalParsona U.are.U 4500 scanner, fprint library on linux, C for image aquisition, Python3 with PIL for image processing.

**My thoughts so far :**

* I can neglect  the finger rotation for now, but I probably need to normalize the sets to compensate the (x, y) shifts between two images of the same finger.
* Maybe I could transform the sets so that they have the same barycenter, but I don't know how well that will work.
* I could create a matrix of intensities for each set (e.g 5 on a point, 4 around the point, etc) so as to get something that could be represented like [this but with more spikes (best image I could find right now)](http://i.stack.imgur.com/BAiSL.png) and if A minus B gives me a pretty flat surface (mathematically, that could be a low sum of squares of values in the resulting matrix) I will know that the images look alike.

**How do you think I could compare to such sets ?**

I tried to show you that I really did put some effort in this and am not trying to get you to do the work for me ;)

If you have any questions I'll be happy to answer them. Thanks for your attention!",,,,,Submission,26,0,26
d6un7tq,2016-08-24 09:57:33-04:00,yes_thats_right,,"It's been a long time since I studied computer vision, but if I were handed this problem, I would start by stepping away from code and looking into [what characteristics of finger prints makes them identifiable](http://documents.routledge-interactive.s3.amazonaws.com/9781455731381/student/Resources/Classification_of_Ridge_Line_Details.pdf).

i would then look into how I can isolate these features (you have partially done this) and store this information.

Finally, I would match fingerprints based on the count of these features, the size of the features and their relative position from one another.

That's the theory, and it is how I have done facial recognition in the past. Hopefully it inspires an idea or two in you.",4zcfim,t3_4zcfim,Compscifingerprint,,Comment,2,0,2
d6uv18l,2016-08-24 12:53:27-04:00,BarqsDew,,"This is probably the paper you're looking for, to align the sets of points to each other: https://igl.ethz.ch/projects/ARAP/svd_rot.pdf

(I got it from [this](http://stackoverflow.com/a/18091472) stackoverflow post)

You'd want to add an additional step after transforming point cloud A so it (roughly) overlaps point cloud B - adding the orientation differences to the fitness score.",4zcfim,t3_4zcfim,Compscifingerprint,,Comment,1,0,1
d6uyzm1,2016-08-24 14:24:11-04:00,Compscifingerprint,,YES! I was thinking of the least squares method used in linear regression. This seems like a really good article,4zcfim,t1_d6uv18l,BarqsDew,,Reply,1,0,1
d6v008w,2016-08-24 14:45:54-04:00,Compscifingerprint,,"Humm, actually this is not exactly it. Because I don't know in advance which points are supposed to be close to each other. Plus, both sets can have a different amount of points...",4zcfim,t1_d6uyzm1,Compscifingerprint,,Reply,1,0,1
d6v5fya,2016-08-24 16:35:26-04:00,BarqsDew,,"Hmm...

Since the points have a ""rotation"" type attribute, it may be easier to ignore the positions entirely to start... Get a list for each point cloud, of all the rotation values. For each item in list A, find the closest point in the list B, add difference^2 to an accumulator and remove it from B. When A or B are empty, the sum is the cumulative difference for the initial orientation. Offset the rotations, and iterate to find the best rotation. Once you have the best rotation, comparing the rotations of each point should show you which points should be close to a match, allowing for a finer-grained approach.

No idea if I'm barking up the wrong tree here! :) This is similar to what 3d scanners do - but they can get away with it because they use normal maps to get a spherical representation.",4zcfim,t1_d6v008w,Compscifingerprint,,Reply,1,0,1
d6y2t3a,2016-08-26 17:45:01-04:00,grey_net,,"I actually work with fingerprint recognition in a VB.NET project I have created 6-7 years ago, but there it was all included in an SDK that was free for use with the Digital Persona readers. Even though this was some years ago, I still use that version of the SDK as it was completely free, pretty fast and very easy to use.

Anyway, the use of a specific selected algorithm comes down to how many templates you need to compare . If its for a smartphone f.ex, it doesn't need to be a super fast algorithm but preferably with a higher precision, as there only are 1 or 2 templates to check against.

In a 1 to many solution, you need a fast optimized algorithm, as you could end up checking against thousands of templates, like I do in my project.

But one thing you might need, is eliminating some ""noise"" before using the dataset, and then do the comparison with a correlation algorithm, as that doesn't need to make changes to the angle of the entire image, or try to replace the coordinates found in the 2D image which can be placed several points in any direction on each new template. A further consideration is whether each minutiae should be considered equal, or rather compared individually.
I found this study:
https://www.researchgate.net/publication/228644313_A_minutiae-based_matching_algorithms_in_fingerprint_recognition_systems
which have some more recent sources than the other study that there is a link to (it has only one reference and that's from 1954), and this research discuss a few various algorithms they have tested.

So as I see it, the problem is how to calculate your template into a dataset, and what algorithm should be used for comparison (also considering the two important factors: accuracy vs. speed). There should also be tested against what gives the best result when the number of minutiae is different, and whether to insert a fitting value in a shorter template, or remove some points in the other template when comparing.

A couple of years ago I wrote my bachelor in medical drug forensics where large datasets of chemical biproducts in many many samples of a confiscated street drug, seized in different countries, was compared against each other to try and find a pattern distinctive for each country, hopefully making it possible to determine which country a random given sample would come closest to. Here it was around 10.000+ samples compared to each other (10.000 compared against 10.000) where each sample was a dataset of 16 decimal numbers where each value was the amount of the bi-product present. Here many calculations was tried, and the best way that I concluded gave the best result, was using a cosine distance correlation algorithm, and taking some steps on the datasets when one or more of the values in a dataset couldn't be measured or was very much off.


With the fingerprint, one way of creating a template could be with the use of a vector for each point to another point, choosing between nearest neighbouring point or most far away point, and then having a large set of vector lengths which then possibly could be sorted into smallest number to largest (to accommodate for any missing points in one of the templates). But that is something to try out and see what is best or easiest. However this would give a dataset for a template, which could be tested with a distance calculation. I think this will give a fast way of comparing templates. There could then be other methods more precise, and probably some that is both faster and more accurate. (I don't think is a very usable method at all, but just something that popped up in my head as an example to maybe give some ideas).


But it looks interesting what you are doing, so will be checking in to see how its going, if you would be kind enough to update you findings as they progress!",4zcfim,t3_4zcfim,Compscifingerprint,,Comment,1,0,1
4zbqng,2016-08-24 05:37:00-04:00,desnier,How do you not get bored?,I love programming and I hope some day to make a career out of it but I always seem to get bored after trying to teach myself some new languages or delve deeper into a language. How do you guys keep yourself interested?,,,,,Submission,10,0,10
d6ugotm,2016-08-24 05:47:35-04:00,poopMachinist,,"Work on a project that YOU want to work on. Grab an idea from your idea sketchbook and do it in a language you want to learn. That will keep you motivated, because you'll be building a project that means something to you.

Doing tutorials and exercises becomes tedious real fast while personal projects are always fun and interesting. ",4zbqng,t3_4zbqng,desnier,,Comment,12,0,12
d6undzh,2016-08-24 10:01:56-04:00,IgnorantPlatypus,,"Languages are a bit boring, the way a hammer or a screwdriver is boring.

*Problems* are interesting, and I get paid to solve them.",4zbqng,t3_4zbqng,desnier,,Comment,8,0,8
d6ugn3a,2016-08-24 05:44:49-04:00,jamuza,,That's a hard one... I guess find an application for that language/tech you want to learn/use that interests you and try to build it. For me I find it blends other unrelated subjects I enjoy with cs/technology and keeps me interested ,4zbqng,t3_4zbqng,desnier,,Comment,2,0,2
d6uv2wb,2016-08-24 12:54:26-04:00,albatrek,,"As others have said, find a project. I can't work on something on codeacademy for very long without boring myself to tears. Find a project (/r/dailyprogrammer !) or get ideas from friends. 

I find I learn the most when I've broken something and have to learn how to fix it. Get yourself a virtual machine or an old Android phone and start trying things - just mess around or look for ideas online.",4zbqng,t3_4zbqng,desnier,,Comment,2,0,2
d6v6uku,2016-08-24 17:03:58-04:00,ilikestring,,How have I never seen that sub before...,4zbqng,t1_d6uv2wb,albatrek,,Reply,1,0,1
4zaq5k,2016-08-24 00:13:45-04:00,wharrel,IB Computer Science project help!!! Grade 10,"hi guys I really need help with my computer science project, I was planning to create some sort of logging app with a visual interface which allows me to save statistics for a tennis match.
I was doing some research and found this video on youtube
https://www.youtube.com/watch?v=odR_d6ro_pk
that looks exactly like what I want my project to look like.
I'm only grade 10 and have limited coding knowledge on php.
What sort of program should i use for this kind of app I want to create?",,,,,Submission,2,0,2
d6ucs5r,2016-08-24 02:15:47-04:00,zorkmids,,"What platform?  iOS, Android, or other?  Good choice, that looks like a challenging but achievable project.  

",4zaq5k,t3_4zaq5k,wharrel,,Comment,1,0,1
d70vhcc,2016-08-29 01:18:02-04:00,wharrel,,preferably PC,4zaq5k,t1_d6ucs5r,zorkmids,,Reply,1,0,1
4z6aa2,2016-08-23 08:49:24-04:00,andybill64,"While attempting to mod a game, my whole whole computer crashed because a kernel mode proccess attempted to access memory which it didn't have access to, but im confused on why what i did would cause that to happen.","so after unpacking a .DAT file from this game, and changing some stuff around, i attempted to launch the game with the extracted folders from the original .DAT file in the root directory of the game, as this is what you were supposed to do. the game crashed on startup, presumably because it couldn't load GAME.DAT, which no longer exists. So just to see if it would work, i threw all the extracted files into a new file called GAME.DAT (not an actual .dat file, normal file with that name) not only did it not work, but it blue-screened my entire computer with the error message of:
IRQL_NOT_LESS_OR_EQUAL

why would doing this cause a memory access violation on the kernel level?

(Don't want helpp with the mod or anything, just wondering the science behind it)",,,,,Submission,7,0,7
d6t9ygh,2016-08-23 10:43:54-04:00,earslap,,"Did you try it again to see if it'll BSOD again? There is a very high probability that it was just a coincidence.

If not, the way the game was programmed (and the exception it generates due to the circumstances you forced upon the game's loader) probably triggers a bug in the video drivers (or sound drivers, or any other kernel level drivers). It is impossible to know for sure but a simplified speculative example: The game somehow thinks it has loaded the resources correctly (in reality it didn't) and passes the textures to your graphics driver. The driver, seeing that the data is invalid, should handle this problem gracefully but for some reason it makes a blunder that triggers a kernel panic.",4z6aa2,t3_4z6aa2,andybill64,,Comment,8,0,8
d6tc1dq,2016-08-23 11:29:49-04:00,andybill64,,"Hm, that's an interesting explanation, thanks!",4z6aa2,t1_d6t9ygh,earslap,,Reply,2,0,2
d6u8nrj,2016-08-23 23:48:39-04:00,darthandroid,,"> It is impossible to know for sure

All you need to do is:

1. Turn on minidumps or memory dumps if neither is already turned on
2. Reproduce crash
3. Now you have full debug information, showing what driver caused the crash, stack trace of what the driver was doing, etc.",4z6aa2,t1_d6t9ygh,earslap,,Reply,1,0,1
d6wygue,2016-08-25 21:56:00-04:00,bchociej,,The game `.dat` file probably normally contains code (instructions) which interact with driver or kernel level code. Your new `game.dat` probably inadvertently contained instructions that resulted in accessing memory not assigned to the game process.,4z6aa2,t3_4z6aa2,andybill64,,Comment,1,0,1
4z0sdz,2016-08-22 10:31:17-04:00,Danyaalk123,Help on computing homework,I got homework where we got given a database on microsoft access 2010 the database has 5 fields for 5 different concerts each one being a boolean we have to create a query that tells us if a person has been to atleast 3 concert. I dont know how to do a query like that,,,,,Submission,6,0,6
d6s15b8,2016-08-22 12:41:29-04:00,yes_thats_right,,"I'm not familiar with Ms access, but a quick search shows that True = -1 and false = 0.

Therefore you just need to write a query using format:

*Select name where col1 + col2 + ... + col5 <= -3*",4z0sdz,t3_4z0sdz,Danyaalk123,,Comment,5,0,5
d6skcwh,2016-08-22 19:50:38-04:00,dajoli,,That sounds like a seriously horrible database design.,4z0sdz,t3_4z0sdz,Danyaalk123,,Comment,3,0,3
d6sl0zy,2016-08-22 20:07:47-04:00,nsaisspying,,"A select query is used to create subsets of data that you can use to answer specific questions.

You can create this query either in Design view or by using a wizard. In addition, if you are familiar with writing SQL statements, you can create a query while working in SQL view by writing a simple SELECT statement.

I know this is kind of a non-answer, I am not against people seeking direct answers here. But ,

More here :https://support.office.com/en-us/article/Create-a-simple-select-query-de8b1c8d-14e9-4b25-8e22-70888d54de59,
for a more fundamental understanding of SELECT.

u/yes_thats_right and u/Coolisbetter hit the mark really, but follow that link for a one time read to a better more wholesome knowledge of a topic.",4z0sdz,t3_4z0sdz,Danyaalk123,,Comment,2,0,2
d6sz63b,2016-08-23 03:39:59-04:00,Danyaalk123,,Sorry i should have said this at the start but we are not using sql we normaly just use the design view to make a query. The reason i am having so much trouble is because normaly we do query that rely on data from only one field.,4z0sdz,t3_4z0sdz,Danyaalk123,,Comment,1,0,1
d6ryvyc,2016-08-22 11:51:42-04:00,Coolisbetter,,"    SELECT Name
    From test.concerts
    Where ((case when C1 = 1 then 1 else 0 end) + (case when C2 = 1 then 1 else 0 end) + (case when C3 = 1 then 1 else 0 end) + (case when C4 = 1 then 1 else 0 end) + (case when C5 = 1 then 1 else 0 end)) > 2

This works, but is not very elegant and probably not proper. 

Here's the table:

ID | Name | C1 | C2 | C3 | C4 | C5
---|---|----|----|----|----|----
1 | Aaa | 1 | 1 | 0 | 0 | 0
2 | Bbb | 1 | 1 | 0 | 1 | 0
3 | Ccc | 1 | 1 | 1 | 1 | 1

Running the query gives Bbb and Ccc

EDIT: I did this in MySQL, but the logic is similar 

EDIT2: Seeing as how the columns in my table are actually ints since MySQL doesn't have bools, you could just do:

    SELECT Name
    From test.concerts
    Where C1 + C2 + C3 + C4 + C5 > 2:",4z0sdz,t3_4z0sdz,Danyaalk123,,Comment,1,0,1
4yyppx,2016-08-21 23:38:28-04:00,questionsforyou11,Get a degree in computer engineering/science or teach myself?,"I have a B.A. in English and have not been able to find a job. I am not very interested in being a writer anymore either. A friend of mine became interested in programming a few years ago, and after learning more about it and attending conferences, I think I would really enjoy programming, systems administration, network security, or something like that.

I am trying to decide if I should go back to college or if I should teach myself, get certifications, and build up a good portfolio instead, which would save a lot of money. I've talked to people who work various technology jobs, and most tell me not to go back to get a degree. Which path would be best: teach myself or get a degree?
",,,,,Submission,3,0,3
d6rgghf,2016-08-21 23:57:59-04:00,None,,"If you do end up deciding to go back to college, many universities will accept non STEM majors into a masters in CS provisionally.  You'll just need to take a set of prereqs to be fully admitted into the program.  I went that route to switch from a BA in political science.",4yyppx,t3_4yyppx,questionsforyou11,,Comment,2,0,2
d6struj,2016-08-22 23:58:03-04:00,questionsforyou11,,Thanks for your response: I'll look into this!,4yyppx,t1_d6rgghf,None,,Reply,1,0,1
d71lj3d,2016-08-29 15:26:57-04:00,questionsforyou11,,"Which prereqs did you have to take? I've been trying to figure out what classes I'd need and have emailed different colleges in which I'm interested and asked, but nobody has gotten back to me.",4yyppx,t1_d6rgghf,None,,Reply,1,0,1
d71n01i,2016-08-29 15:57:13-04:00,None,,"It all depends on the university.  Sometimes the list is published (http://www.ecs.csun.edu/csgrad/prospective.html) while others isn't (http://cs.iupui.edu/graduate/admissions/requirements/cis/masters).  At least with IUPUI, the list ended up being the core CS classes (intros, data structures, discrete math, computer architecture).

If where you're applying to doesn't publish a list, I'd call the CS office and see if you could talk to the graduate coordinator (if they have one)",4yyppx,t1_d71lj3d,questionsforyou11,,Reply,1,0,1
d71qsdr,2016-08-29 17:17:32-04:00,questionsforyou11,,"Thanks, which ones did you have to take exactly, and how long did it take you to complete them?",4yyppx,t1_d71n01i,None,,Reply,1,0,1
d71s3vu,2016-08-29 17:47:07-04:00,None,,"I took a majority at IUPUI, so the ones I listed.  Ended up transferring to CSUN for reasons, and had to additionally take Software Engineering/Intro to Algorithms/Operating Systems/Programming Languages/Automata before I was unconditionally admitted",4yyppx,t1_d71qsdr,questionsforyou11,,Reply,1,0,1
d72z5a9,2016-08-30 15:11:21-04:00,questionsforyou11,,"Oh, okay. How long did it take you to complete the prereqs and the master's degree in total?",4yyppx,t1_d71s3vu,None,,Reply,1,0,1
d6rkfr4,2016-08-22 02:28:03-04:00,bekroogle,,Might ask /r/cscareerquestions,4yyppx,t3_4yyppx,questionsforyou11,,Comment,2,0,2
d6rg3dc,2016-08-21 23:46:48-04:00,Kaidelong,,"One of my good friends has a BA in computer science and mathematics (double major). He got in touch with me recently saying that, after a temporary contract for low paying contract work expired, he was finally able to find a new job working part time in construction. You might want to see what the local job market is like first, if you live in a place where there are no programming jobs, a degree or portfolio will not help you.",4yyppx,t3_4yyppx,questionsforyou11,,Comment,1,0,1
d6rg4sv,2016-08-21 23:48:00-04:00,questionsforyou11,,"A B.A., not a B.S.?",4yyppx,t1_d6rg3dc,Kaidelong,,Reply,1,0,1
d6rgjbh,2016-08-22 00:00:25-04:00,None,,BA in computer science is a thing.  I've seen some programs offer both: the major distinction usually is in the amount of math/science/engineering required.,4yyppx,t1_d6rg4sv,questionsforyou11,,Reply,2,0,2
d6rgdl6,2016-08-21 23:55:33-04:00,Kaidelong,,"I don't actually know, I'm not sure all schools make that kind of distinction, and at the one I am at it's not actually important (you trade some, but not all of your lab science for some additional foreign language, which is arguably more marketable, not less).",4yyppx,t1_d6rg4sv,questionsforyou11,,Reply,1,0,1
d6rgul7,2016-08-22 00:10:16-04:00,cdrootrmdashrfstar,,"Before committing to a degree, take a look at a guide for programming or a book on programming that includes projects, and see if you have a knack for it/if it holds your interest. I would suggest beginning with a programming language called ""Python"". Look for books published by ""O'Reilly"" on Amazon about Python, and look for a guide called ""Automate the Boring Stuff"" for an online, free guide for learning python.

https://automatetheboringstuff.com/chapter0/

Check out some popular subreddits for new programmers:

/r/learnprogramming 

/r/learnpython 

/r/dailyprogrammer

https://howtoprogramwithjava.com/programming-101-the-5-basic-concepts-of-any-programming-language/

http://www.tutorialspoint.com/computer_programming/computer_programming_basics.htm -- Good reference to explain specific topics better, while the above link is a very nice overview of the beginner topics you should be aware of when starting out.

Also, if you're using windows, take a look into learning Linux with versions like [Ubuntu](http://www.ubuntu.com/desktop) or [Linux Mint](https://www.linuxmint.com/).

New to Linux? [What is Linux?](https://www.youtube.com/watch?v=tFFNiMV27VY) and [What is Linux?](https://www.youtube.com/watch?v=zA3vmx0GaO8)

Okay, that's enough information for now. 

Please, let me know if you have ANY questions and feel free to PM me ALL of them. If you would like my contact for Google Chat, Skype, or Slack, PM as well and I'll hit you up (and we can do a quick run through of the basics/you can ask me for help with anything you need).

",4yyppx,t3_4yyppx,questionsforyou11,,Comment,1,0,1
d6stys2,2016-08-23 00:04:09-04:00,questionsforyou11,,"I have already been reading Automate the Boring Stuff, lol, and have been learning about Linux and installed it on my computer. Thanks so much for your help and the links. I will definitely PM you if I have more questions, and I probably will. ",4yyppx,t1_d6rgul7,cdrootrmdashrfstar,,Reply,2,0,2
d6suhel,2016-08-23 00:20:31-04:00,cdrootrmdashrfstar,,"Okay awesome, I'm very glad you're already made that jump into Linux -- many people are scared by going into that unfamiliar of an environment when they're starting out, so kudos to you!

But yeah, if you have any idea for a project (and if you have any question about it), or you're having some syntax issue, or you want resources for some new project/idea, etc., just hit me up with a PM or other social media and I'll be sure to help out quick.",4yyppx,t1_d6stys2,questionsforyou11,,Reply,1,0,1
d6rqo1r,2016-08-22 08:01:23-04:00,albatrek,,"(I'm currently in school for a CS degree, so remember that that's my bias as you read this :) )
In my experience, people who are self taught run right alongside, or even ahead of, the people who got degrees - until they hit theory. Self taught people generally have a great understanding of several programming languages,  how to make things work, specific systems, and other practical skills. They usually have a much weaker understanding of theory - things like computational complexity (a way of analyzing how the time your program takes will scale with more data), optimization, how the computer works at a low level, and the like.
Note that I don't mean that self taught people don't understand theory, just that they have to make a strong effort to go out and learn it. It's not the kind of thing you ""pick up along the way"" or that most online courses will cover. You can learn a decent amount of theory yourself, but, in my experience, most self taught people don't.
There are tons of good jobs for someone who only really knows programming - jobs where theory might help a bit here and there, but certainly isn't a necessity. However, programming and CS are different, and you'll need to choose a direction before you decide about school.",4yyppx,t3_4yyppx,questionsforyou11,,Comment,1,0,1
d6su6xp,2016-08-23 00:11:16-04:00,questionsforyou11,,"Thanks for the insight! I have heard some employers complaining that students only know math and theory and don't have enough practical skills. At the last conference I attended, one speaker said applicants with a master's or higher would go to the bottom of the resume pile for this reason. He wasn't enthusiastic about a bachelor's either. =/ If I go back to school, I really hope I would learn a good balance of theory and concrete skills. What year are you currently?",4yyppx,t1_d6rqo1r,albatrek,,Reply,1,0,1
d6v2wn7,2016-08-24 15:44:16-04:00,albatrek,,"lol yeah I could see some employers saying that

I'm just starting my sophomore year, so we'll see what I think about theory/practical balance in a few years :)

I tried to get more practical experience by doing an internship this summer - my school is sometimes known for being too theory heavy, so I really wanted to make sure I got extra practical time.

I've been realizing recently that I really like theory, maybe more than practical, so some of my ""all the theory for everyone all the time"" is probably my own preference talking.",4yyppx,t1_d6su6xp,questionsforyou11,,Reply,2,0,2
d6s7a0q,2016-08-22 14:52:29-04:00,codeforkjeff,,"Fellow English major here, been working in tech for 10+ years.

If you want/need a job as quickly as possible, just teach yourself. Skip the certifications. Build up some code samples. Apply to entry-level jobs. This route takes a lot of self-discipline and strong intellectual curiosity. If you half-ass it, you won't get anywhere.

If you are young, are very certain that you want to learn CS theory (which is not the same thing as software development skills, though they overlap), including the math, and would enjoy/can spare the time back in school, then think about a 2nd bachelor's. This is also a good choice if you need externally imposed discipline to learn.
",4yyppx,t3_4yyppx,questionsforyou11,,Comment,1,0,1
d6sucqa,2016-08-23 00:16:23-04:00,questionsforyou11,,Thanks for the response! How did you get started exactly? How long did it take for you to get a job in tech after you taught yourself? ,4yyppx,t1_d6s7a0q,codeforkjeff,,Reply,1,0,1
d6t6tb4,2016-08-23 09:28:35-04:00,codeforkjeff,,"I was ""lucky"" to get started as a programmer during the dot-com boom, when it was easy to find work in San Francisco. I took one programming class in college, but had been a hobbyist since I was a kid, which helped a lot. Since then, I've been in and out of tech work. Demand for tech workers is obviously not as strong as back then, but it's still very high, and people are a bit less crazy. So this is a very good time to get into it.

It's hard to say how long it will take. There's not necessarily a ""I'm ready now!"" point that you reach. There's nothing to stop you from applying right now for entry-level jobs and tell employers about where you are with your skills. Even if you know you won't get it, an HR person or someone else might be willing to email a bit of feedback about where your skills/resume need to be, which is super helpful. Just keep applying as things come up. Meanwhile, keep learning.

Be targeted. You mentioned three things: programming, sysadmin, network security. Each of these is very different. If you're learning to code, pick a popular language, preferably one that you see a lot in job postings in your area. Read a few books on it, do exercises, create a small hobby project and put it online somewhere. Blog about it. If you're doing sysadmin, set up a few Linux or Windows virtual machines (whatever you're interested in working with), and experiment with doing common administration tasks. Blog about that too to demonstrate you understand what you did. Ditto with network security.

If you're not employed right now, and can approach self-learning as a full time job, a few months can get you very far if you're diligent. The reason a lot of people don't make it is because they lose motivation or aren't disciplined--they discover that tech isn't actually interesting to them (fair enough!). But it's totally doable.
",4yyppx,t1_d6sucqa,questionsforyou11,,Reply,1,0,1
d6s8aeb,2016-08-22 15:14:15-04:00,KronktheKronk,,"If you want to go into software engineering you can find an entry level job without a degree if you can create something relevant in the area you want to work and have a portfolio to show off.  It is really hard and requires a lot of sticking to it and follow through.  You can't just crap out some code into something that runs and meet the minimum requirements, it has to be well-formed and show mastery.  If you can get there, you can start applying for entry level jobs at start-ups.  Most big corporations won't give you the time of day without the degree, that's just life.  From an education perspective, getting a BS in computer science can be pretty expensive. I think it generally does matter if you get your degree from a community college or CS cert shop than a legit university.

Getting an IT/administration job is a little different.  You can try to get an entry level job without any experience if you're ""good with computers"" doing helpdesk support stuff.  It usually requires knowing someone or some place that offers that kind of role.  Otherwise most IT shops aren't going to give you the time of day without some certs/education showing you have any idea what you're talking about.  From an education perspective, this option is much cheaper to reach the minimum bar, but you'll be doing grunt work for several years before you become an engineer.",4yyppx,t3_4yyppx,questionsforyou11,,Comment,1,0,1
d6sb54t,2016-08-22 16:14:45-04:00,jimmy0x52,,"Sorry to be that guy, but that's like asking: 

> Do I go to culinary school, or just teach myself how to cook?

Both are valid ways of learning the material and have very different expectations of outcomes at the end. 

My two cents:

If you want to be an engineer and get a decent salary relatively quickly get the degree. If you're ok making not great money doing something you love and growing into it, try self-taught.

There's a bunch of stuff you'll learn in the degree program you can't really self-teach, but then again there is a bunch of garbage you'll learn in the degree program you'll never need again.

Unfortunately both paths could lead to the same $ a ways down the line since one may pay more upfront but cost more upfront and the other may pay less upfront with less cost. It might be worthwhile checking your breakeven point on both and seeing which is better.",4yyppx,t3_4yyppx,questionsforyou11,,Comment,1,0,1
d6suh2d,2016-08-23 00:20:11-04:00,questionsforyou11,,Thanks for your response! What kind of things could I not self-teach?,4yyppx,t1_d6sb54t,jimmy0x52,,Reply,1,0,1
d6t5lhb,2016-08-23 08:53:58-04:00,jimmy0x52,,"I think design patterns, data structures and algorithms would all be very difficult to self-teach. Most self taught people I've met and worked with are good at knowing how to do something but not why solution A might be worse than B. They tend to have a narrow view of the tools and techniques available.

Why is this? I think it's partly because that stuff can be boring to slog through. In school you're forced to learn it and it pays dividends later - but if you're self teaching I'm not sure there's a huge incentive to learn this stuff because here might not be an immediate reason to.

Now, this isn't always the case. Eventually with enough experience you do learn how to do these things - and certainly you don't learn the best way for everything just by getting the degree - but I think the degree certainly gives you a leg up to start out.",4yyppx,t1_d6suh2d,questionsforyou11,,Reply,1,0,1
4yyp07,2016-08-21 23:32:39-04:00,popejoshua,Is it worth working my way (and taking) 'Discrete Mathematics'?,"Hey all,

I'm about to start my first semester of computer science courses this semester and had a quick question. 

'Discrete Mathematics' is not a required course for my university's Software Engineering major, however I've read around that the class is beneficial to take if you're in Computer Science/Software Engineering field. In order to take the 'Discrete Mathematics' course, you have to take 'Calculus 1' as a prerequisite. I have already completed another math course to satisfy my LAC requirements (since adding Software Engineering as a double major was a more recent decision), but have not taken 'Calculus 1' at the university.

I'm stressing over whether or not to take 'Calculus 1' this semester and 'Discrete Mathematics' next semester. Should I follow through with working my way towards 'Discrete Mathematics' or can I live without the class?",,,,,Submission,11,0,11
d6rhfco,2016-08-22 00:28:53-04:00,high_side,,Absolutely take discrete mathematics.,4yyp07,t3_4yyp07,popejoshua,,Comment,24,0,24
d6rn2eb,2016-08-22 04:47:15-04:00,falafel_eater,,Discrete Math is super important. Do not skip it.,4yyp07,t3_4yyp07,popejoshua,,Comment,4,0,4
d6rgxhn,2016-08-22 00:12:56-04:00,lordvadr,,"Like it or not, computer science is essentially a glorified math degree. You cannot be a decent engineer (software or otherwise) without a solid understanding of calculus.  You can be a programmer, but not an engineer.

With that said, discrete math is HUGE in computer science.  Depending on what you end up doing, you'll likely not use it all that much, but it is worth understanding.",4yyp07,t3_4yyp07,popejoshua,,Comment,11,0,11
d6ro08n,2016-08-22 05:42:36-04:00,danltn,,"It's probably one of the purest CS-esque modules available, I would highly recommend it: it underpins a *lot*.",4yyp07,t3_4yyp07,popejoshua,,Comment,5,0,5
d6rq4yr,2016-08-22 07:38:03-04:00,albatrek,,Discrete math is valuable for the way it forces you to think as well as the specific topics you learn. I'd say it's essential if you want to be able to understand any CS theory.,4yyp07,t3_4yyp07,popejoshua,,Comment,3,0,3
d6st8l3,2016-08-22 23:41:52-04:00,screamconjoiner,,Discrete math is probably the most relevant mathematics course to CS. The concepts you learn are hugely more applicable to CS concepts (as well as real world job applications) than their continuous cousins.,4yyp07,t3_4yyp07,popejoshua,,Comment,2,0,2
d6sd2ba,2016-08-22 16:55:55-04:00,KyleRochi,,"Yes... Take it, the concepts pop up all the time in programming",4yyp07,t3_4yyp07,popejoshua,,Comment,1,0,1
d6snhbx,2016-08-22 21:10:04-04:00,SirSourdough,,"I agree with everyone else that you should try to take discrete math. It accounts for a whole huge area of math that you get very little exposure to in school, since schooling for most people focuses almost exclusively on continuous mathematics. A lot of discrete is as or more applicable to CS than most continuous math is as well.

The calc pre-req is annoying if you've already taken Calc, but pretty worthwhile if you haven't taken it in the past. There's a lot of good stuff in there too, though not as applicable for most people.

",4yyp07,t3_4yyp07,popejoshua,,Comment,1,0,1
4yvsm9,2016-08-21 12:54:08-04:00,thecookiesayshi,I'm taking my first Data Structures course this semester. Might anyone be able to spare some D.S. wisdom?,,,,,,Submission,12,0,12
d6qxs99,2016-08-21 15:52:00-04:00,technobull,,"As a professor with about 5 semesters of teaching an Intro to Data Structures and Algorithms course, I suggest the following:  

1. make sure you get the fundamentals down at the start of term as many concepts build off of one another. Do not wait to see a tutor or visit office hours. 

2.  Use the data structures in the preferred language of your course / program. Don't keep these concepts as theory. Write the code. Test the code using good testing and debugging principles. Learn from your errors.  Save your code in a repository such as github. 

This helps you better understand the theory through the application. It helps you mature as a developer. It also gives you sample code you can reflect back on later. 

3.  Observe timing of inserts, deletes, retrieval and compare to big-o estimates. This helps you better conceptualize performance of each algorithm (so long as you vary your test data set size and it is sufficiently large). 

4.  Look online for multiple references as there are so many resources for this topic as the coursework evolves very slowly. ",4yvsm9,t3_4yvsm9,thecookiesayshi,,Comment,6,0,6
d6qy0rh,2016-08-21 15:57:45-04:00,thecookiesayshi,,"Awesome. 

I know we're going with C++ specifically for the course, which we've been introduced to last semester, so I'll try writing along with the concepts.

I would imagine there are resources with sample sets of data to be used for comparisons of the timing of different structures, yes?

Thank you so much for your time and help!",4yvsm9,t1_d6qxs99,technobull,,Reply,2,0,2
d6qyuwx,2016-08-21 16:18:04-04:00,technobull,,"If you are working with C/C++ learn pointers and focus on their use with link focused data structures such as linked lists and trees. 

I find this is a hard subject for students but easier to learn in C++ than Java because both work with references to objects and data, but the pointer concept is much more overt so it's easier to learn it right the first time.

As a student, I struggled in my object oriented programming class when working with variables storing values vs variables storing references. It was not until my class on Information Retrieval where I did C++ did many things click with both languages. 

Think of data structures as tools for your developer toolbox. Know how to use each for the right task. Think about the data volume of the product you are developing. Think of how the data will be entering and leaving. Think about if it will be ordered or unordered going in or coming out, and make sure your data structure meets your use case. ",4yvsm9,t1_d6qy0rh,thecookiesayshi,,Reply,4,0,4
d6rifhr,2016-08-22 01:03:49-04:00,thecookiesayshi,,"Yes, we've worked with pointers in our aforementioned course last semester, actually! Definitely pretty tricky.

I've done a little bit of reading in preparation for the first assignments and lectures... I believe hash tables use pointers to linked lists to avoid collision that may result from the used hash function, correct? I would imagine that pointers serve similarly useful purposes with other data structures.

Yes, understood. Thanks again for sharing. :)",4yvsm9,t1_d6qyuwx,technobull,,Reply,1,0,1
d6qyxcb,2016-08-21 16:19:50-04:00,StephenSwat,,"It's been a while since I took datastructures myself but I've TA'd the course a few times and it's easily one of the most important couses you'll take for many aspects of computer science. The most important tip I can give you for datastructures itself is that you should not drill the course matter and that you should instead aim to develop a fundamental understanding of why everything works. That starts with understanding two building blocks that make up almost all datastructures you can imagine.

The first building block is that you must learn to see all variables and values as little boxes of memory in a field. Sometimes these boxes of memory are next to each other and sometimes they are not. Sometimes a single elongated box can store more than single value. Values do not just magically appear in the computer at a convenient (and inconsequential) location in the machine.

Then you must understand pointers. Conceptually simple but they can be a nightmare for many students. Essentially pointers are nothing more than a piece of paper telling you where another box you need can be found. Make sure you understand both how they work conceptually and practically.

Now move on to a few basic types of datastructures. I would say the most important ones are linked lists, trees and hashtables. More abstractly, stacks, queues and sets are also important. From these you can construct the vast majority of data structures you will use.

Then, when you are still working with relatively simple datastructures, acquaint yourself with big-O notation. Do *not* drill the complexities of different operations. Always make sure you understand, fundamentally, why an operation takes as long as it does. Figure out why something has a certain complexity by running the operation in your head, visualising the datastructure.

When more complex datastructures come around such as red-black trees, again do not spend all of your time drilling into your head the exact methods for inserting values. Instead, learn what the properties of a red-black tree should be for it to be valid. This will allow you to actually understand why insertion works as it does and even if you eventually forget the exact algorithm your knowledge will not completely collapse.",4yvsm9,t3_4yvsm9,thecookiesayshi,,Comment,5,0,5
d6rijm9,2016-08-22 01:08:13-04:00,thecookiesayshi,,"I absolutely love your statement about visualizing storage, and the accompanying analogy that a pointer is basically a note saying to look in a specific location. I think that'll be useful to keep in mind, moving forward.

Yeah, Big-O didn't sit too well with me during Discrete Math, although I know I didn't give most of my courses that semester the attention they warranted. I'll be sure to try and understand each structure (and their complexities!) from the ground up.

Thank you so much!",4yvsm9,t1_d6qyxcb,StephenSwat,,Reply,1,0,1
d6qt4i5,2016-08-21 13:55:35-04:00,visvis,,This question is very vague. What are you looking for?,4yvsm9,t3_4yvsm9,thecookiesayshi,,Comment,2,0,2
d6qu4qj,2016-08-21 14:20:59-04:00,thecookiesayshi,,"Hi! Sorry, I wasn't really looking for anything specific. 

Maybe something along the lines of ""Yeah! I took Data Structures last year. The most complicated part of the course seemed to be x, y, and z, so pay extra attention to those areas, as they were the foundation for a lot of other material in the course. Also, p-section of the course has come up in interviews a lot specifically, so maybe think of it in that context when you're working through it.""

But like I said, I'm really not looking for anything specific, and I'd be happy to hear any knowledge, tips, recommendations that anyone has. :)",4yvsm9,t1_d6qt4i5,visvis,,Reply,5,0,5
d6qxhqo,2016-08-21 15:44:53-04:00,Bonaz,,Pretty much everything in your data structures course is important if you want to be a developer. You use data structures everyday.  ,4yvsm9,t1_d6qu4qj,thecookiesayshi,,Reply,1,0,1
d6qxo2c,2016-08-21 15:49:09-04:00,thecookiesayshi,,"Sure, I'm not intending on neglecting any part of the course. I could just imagine that some parts of the course may require more care, and thus, am curious as to know what they are so I can look out for them.",4yvsm9,t1_d6qxhqo,Bonaz,,Reply,1,0,1
d6qxseh,2016-08-21 15:52:06-04:00,Bonaz,,"Make sure you learn as much as you can about hash maps, trees, and graphs for sure. Not undermining any other structure, but in terms of interviews and problem solving, those are good to focus on. ",4yvsm9,t1_d6qxo2c,thecookiesayshi,,Reply,1,0,1
d6qxww5,2016-08-21 15:55:13-04:00,thecookiesayshi,,"You're saying to focus on those structures (without neglecting the others, of course). Is it that those structures in particular cover a good number of the common situations a developer may find themselves in?",4yvsm9,t1_d6qxseh,Bonaz,,Reply,2,0,2
d6qy60k,2016-08-21 16:01:05-04:00,Bonaz,,"They're harder to understand over say, a simple list or stack. Theyre typically quite efficient structures that are used in a variety of problems. ",4yvsm9,t1_d6qxww5,thecookiesayshi,,Reply,2,0,2
d6rikf8,2016-08-22 01:09:05-04:00,thecookiesayshi,,Sure. In my initial readings I came across the general hash table. Pretty neat stuff!,4yvsm9,t1_d6qy60k,Bonaz,,Reply,1,0,1
d6r7hkd,2016-08-21 20:01:26-04:00,Chandon,,"The initial CS courses on data structures end up being a bit weird. You're generally learning the concepts of data structure design and a selected history of easy to explain but diverse data structures, rather than the most practical data structures for programming.

As you work through the course, I recommend taking a little bit of time to self study the following data structures:

 * Variable length arrays (e.g. vector, ArrayList)
 * Hash tables
 * B-Trees
 * Hash tries
 * Persistent vectors and persistent maps.

Frequently in programming you're given a built in sequence type and a built in map type. Understanding which ones you got and how they behave is extremely useful, because it lets you build efficient algorithms with those standard types.",4yvsm9,t3_4yvsm9,thecookiesayshi,,Comment,2,0,2
d6riq6b,2016-08-22 01:15:20-04:00,thecookiesayshi,,"I'm definitely planning on compiling a document containing my notes for all the structures I come across. I'll be sure to make sure those are on the list.

Gotcha. Thanks!",4yvsm9,t1_d6r7hkd,Chandon,,Reply,1,0,1
d6r8227,2016-08-21 20:16:40-04:00,metaobject,,"If you're using Intro to Algorithms by Cormen, et al, I found it helpful to view the Intro to Algorithms videos on the MIT Open Courseware site for topics I needed more insight/practice on.

http://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-006-introduction-to-algorithms-fall-2011/",4yvsm9,t3_4yvsm9,thecookiesayshi,,Comment,2,0,2
d6ris5a,2016-08-22 01:17:25-04:00,thecookiesayshi,,"Noted. Online resources should prove useful when it comes time to review. I'll take a gander at a couple of these when the time comes, thanks!",4yvsm9,t1_d6r8227,metaobject,,Reply,1,0,1
d6r37pn,2016-08-21 18:09:33-04:00,theroyalham,,"Last year I took Data Structures and Algorithms course (Transferring Universities, so i'm taking a very similar class.. again. Yay!)

I'd say the hardest part were graphs and trees. I'd try and learn those as much as you can.

**Graphs**: Learning transversals: Breath-first, depth first. Dijkstra's Algorithm (for finding shortest path). 

**Trees**: There's a lot more than just simple Binary trees. There's BST trees, 2-3 trees, Red/Black trees. All are pretty cool, but they can get confusing really fast because they all perform similarly but are different. My professor would give us a list of numbers, and we had to show how each number would be entered into the tree, and show how it readjusts itself every time a number is added. ONE MISTEP and I'd get the entire 15 points wrong and automatically an 85 on my exam... and we had multiple of these. 

Good luck though. There are *TONS* of videos on every data structure, so just spend a few hours on YouTube for each data structure and practice. There's also online applets that simulate how data structures work. For example just google ""*2-3 Tree Applet*"". This helps with learning how data is entered and stored into different structures. But dont get lazy and use this as your only source of learning, because you still have to learn how the code for it works. So it's also best to practice on your own data structures after you hand-built them yourself. 

As for finding the Big-O complexity of algorithms, I always fucked that up because i'd miss a variable or forget that *i*/2 is ln(*i*) rather than log(*i*) (unless log based 2). Calculating Big-O for loops is a pain in the ass if the *i* variable increments at weird paces. ",4yvsm9,t3_4yvsm9,thecookiesayshi,,Comment,1,0,1
d6rintn,2016-08-22 01:12:42-04:00,thecookiesayshi,,"I'll definitely look out for those graphs and trees, in particular. Those exams you mentioned sound pretty rough. 

Yeah, online resources are pretty sweet. Any channels or websites you'd recommend in particular? I don't mind shopping around for each one, anyways. :)

I've just written down a note to go ahead and review ln and log while I'm getting comfortable with Big-O.

Thanks for sharing!",4yvsm9,t1_d6r37pn,theroyalham,,Reply,1,0,1
d6rlg9j,2016-08-22 03:18:12-04:00,theroyalham,,"I'm not really certain of any specific websites/channels. I mostly just YouTube'd it and Google'd it and chose the first few. You can get a feel of which ones are good by the ratings/views.

If you see a video with a lot of dislikes, or someone in the comments correcting the video, it's probably made by some college student who made an error. That's all i'd look out for. But the rest are pretty much good for learning.",4yvsm9,t1_d6rintn,thecookiesayshi,,Reply,1,0,1
d6rryd1,2016-08-22 08:49:45-04:00,thecookiesayshi,,"Yup, sounds like a plan. Thanks!",4yvsm9,t1_d6rlg9j,theroyalham,,Reply,1,0,1
4yvmya,2016-08-21 12:23:01-04:00,zSilverFox,How do compilers optimize programs?,,,,,,Submission,14,0,14
d6qs41n,2016-08-21 13:29:51-04:00,panderingPenguin,,"That's an extremely complicated and broad question, and you can take entire courses on the subject. Hell, people do PhD dissertations on the topic. You're going to have to be more specific if you want truly meaningful answers, but the basic idea is to modify the program (reordering things, removing unnecessary code, combining things, etc) without actually affecting any observable invariants. One such example is loop unrolling. Basically, if you have a loop from 0 to 800, with one statement in it, maybe it's more efficient due to fewer jumps the instruction pointer has to make, to put 8 nearly identical statements in the loop, and only iterate 100 times.",4yvmya,t3_4yvmya,zSilverFox,,Comment,11,0,11
d6qsw9x,2016-08-21 13:49:52-04:00,zSilverFox,,"I should have thought a bit more before asking. Your response covers it enough for me, thank you.",4yvmya,t1_d6qs41n,panderingPenguin,,Reply,3,0,3
d6qx2s5,2016-08-21 15:34:25-04:00,StephenSwat,,"There are indeed a huge number of compiler optimisations that can make code faster. These can range from extremely simple to horrifyingly complex and can take place at many stages during the compilation process as some can happen before any low level code is written and peephole optimisations happen all the way at the back of the compilation. And then there's optimisations that might work well on one architecture but might completely ruin performance on another!

You can find a number of optimisations with a short description used in *gcc* [here](https://gcc.gnu.org/onlinedocs/gcc/Optimize-Options.html).

In the eight-week compiler construction couse I followed I implemented the following basic optimisations:

* Constant folding, such that an expression `5 + 5` becomes a constant `10`. `!true` can become `false`. While not common, expressions multiplying by zero can be removed entirely if they are not side-effecting.
* Some removal of dead code.
* Peephole optimisation for some instructions.",4yvmya,t3_4yvmya,zSilverFox,,Comment,6,0,6
d6r94vj,2016-08-21 20:44:29-04:00,Madsy9,,"Other replies already mentioned that this is a huge topic and that your question could be a bit more specific. But here are some examples (not exhaustive:)

 * Constant propagation  - Constants are merged together to avoid doing operations at runtime
 * Dead-code removal - What it says on the tin. Remove code which the compiler can prove is never called/evaluated
 * Reordering of operations, replacing multiple operations with fewer and faster computer instructions - Silly example: x^a * x^b = x^(a+b). Compilers have a huge list of such common identities, where it can replace expressions with faster ones, or rearrange them so that specialized instructions can be used. SIMD instructions and conditional-move are good examples. (though auto-vectorization still sucks)
 * Tail call optimization - If a function is tail recursive (it calls itself on the last line), then the recursion can be changed into iteration instead, which can be a lot faster and also save a lot of memory.
 * Function inlining - Instead of calling a short function, the compiler chooses to insert the code of the function inplace of the callee. A function like ""swap"" does so little work that the overhead of a function call could be a large contribution to the running time: especially as the number of invocations increases. You trade storage space (larger executable) for a small increase in performance.
 * Register allocation - Registers are tiny storage areas on the CPU die itself, often 8, 16, 32, 64 or 128 bits long. CPUs either prefers executing instructions with registers as inputs and outputs, or in the worst case the use of registers is mandatory. But since a CPU has a limited amount of registers, you get the challenge of figuring out which values from memory should be stored in a register at a specific point in time, and which should be ""spilled"" to memory. Taking out a value from a register and storing it to memory is called register spilling, and the concept of computing which values should be placed in registers is called register allocation. Usually you want ""hot"" data in registers and ""cold"" data spilled, but living time also matters. Register allocation is isomorphic to the [graph coloring problem](https://en.wikipedia.org/wiki/Graph_coloring).",4yvmya,t3_4yvmya,zSilverFox,,Comment,4,0,4
d6t89hy,2016-08-23 10:04:59-04:00,jephthai,,"Others answered your question from a technical perspective. I'll offer a slightly more abstract point of view. Consider that for any given program there are likely many ways to write it. Some would be provably worse because they might be less efficient. 

The compiler transforms your program in ways that it can prove are equivalent, moving towards more efficiency. 

I have disassembled code I've compiled and often found it interesting to see what the compiler did. E.g., a loop that went from 0 to 1023 might become a loop that counts backwards because the compiler can save an instruction by testing for zero instead of comparing to a constant. Sometimes you could write a more efficient version yourself if you knew how it was going to be assembled. Other optimizations might do things at the machine code level that can't be expressed in the original high level language.",4yvmya,t3_4yvmya,zSilverFox,,Comment,1,0,1
4yulcr,2016-08-21 08:19:09-04:00,thecrowontheroof,"Can online communication tools help to achieve trust between geographically distributed developers when creating new mobile applications? - - SURVEY for disseration, please help! / 23 female computer science student",,,,,,Submission,0,0,0
4yn3v9,2016-08-19 23:49:13-04:00,blingblingscrackpipe,Thinking about doing a masters program but not sure how much it will actually help me to reach my goals. Looking for some input.,"I have a BSIT, but I have never worked in IT. I originally went to school industrial automation. In my senior year (I was going into the second semester of what would have been a 3 semester senior year) I ran out of money and chose to take the ""industry"" route of my program since it allowed for an entire semester internship. This meant getting paid and only having to pay for 1 class (messed up I had to pay to work...). My internship was 5 hours away from school and long story short they offered me a job as a controls engineer and a very competitive salary to stay. In my head everything would work out because out of the 6 classes I had left 3 were gen eds I had put off, 1 was continuing my internship and making a senior project out of it, one was a 300 level robotics lecture but it had previously been available at a satellite campus near me, and the other a 200 level that I found (or at least thought I found) a transfer equivalent at a nearby college. 

Long story less long, it all fell apart. The satellite campus closed a few months before this and to take that class as a transfer equivalent would put me under the required 300/400 level classes needed to be awarded my degree. So I ended up taking my 3 gen ed classes while pleading with the school to work with me with no luck. I thought about just going back, but giving up the money I was making to give more money to a school that wouldn't help me out (I know they didn't owe anything) when I needed it just didn't seem like a good idea. The degree was too specialized and I couldn't find a direct transfer, plus I realized over a year ago I really just wanted to be a programmer, so I started looking into starting up in CS. I ran into another snag where the college I was transferring from let me substitute classes like cultural diversity for diversity in education (an EDU class) and macro econ for psychology, all the colleges in my area weren't so flexible. I was looking at a full year (not including the semester I just took finishing humanities, communications, and sociology) of gen eds plus 2 years of CS core. It just didn't seem reasonable. I ended finding a school that took my credits by level and department rather than individual classes, but they didn't offer CS only IT. I figured it was good enough and I'd just get this damn bachelor's out of the way. 

So I knocked that out and got my piece of paper. I did well, graduated with a 3.8, but I'm 25 now and have been making over $80k (10-15K in OT) for a little over 2 years. There's not way I'm going to start an entry level IT job and take a 50% pay cut. However, I'm also over having my phone ring 24 hours a day to help someone navigate an HMI or robot teach pendant. 

So I'm back where I was two years ago. I want to learn CS, or more precisely I want to develop software. I'm considering starting up a Master's of Software and Information Systems program. I've had classes in Java, C, C++, Python, and a database management class where I went the extra mile and actually coded the database for our project even though it was just a theoretical project. I'm by no means particularly good at any of these languages, they felt more like intro to programming classes (with the exception of Java, that class was legit). I looked through the curriculum for the Software Engineering program and it looks like it is mostly theory. What I mean by this is I'm not confident I'm going to come out of this program with the technical skills gained from actually doing. 

I've been particularly drawn to the .NET framework but I'll admit I have not looked into a whole lot else. I considered The Odin Project but just like it's intended it seems from my point of view to be strictly web development. I liked that MVC could do web and app development, and that the Visual Studio and C# in it sets you up nicely for mobile app development with Xamarin. I don't really want to limit myslef to .NET, but as something to tackle that can cover desktop apps, web develpment, and mobile development it just seems like a good focus. 

I apologize for being so long winded and I'll get to the actual question.... 


I want to get the technical skills and knowledge to be able to code anything from desktop apps to web to mobile. Are any of these in particular the best way to go:

1) Pursue a Masters in Software Engineering without a CS foundation.
2) Get a second Bachelor's in CS now that I don't feel any pressure for not having my degree when I was so close.
3) Self teach/learn .NET or another framework. If option 3 any recommendations on a path to take. 

TL;DR
I have a BSIT with no IT experience. I have a beginners level knowledge of programming. Which of the following options above is recommended while taking into consideration my overall goal is to be able to do app and web development.",,,,,Submission,8,0,8
d6pc8al,2016-08-20 09:22:19-04:00,Schermanification,,I would recommend you to repost this question in /r/cscareerquestions. Good luck!,4yn3v9,t3_4yn3v9,blingblingscrackpipe,,Comment,4,0,4
d6pu0db,2016-08-20 17:35:10-04:00,ctec_astronomy,,"Yes definitely recommend.  A degree is for work, A Masters is Personal and a Phd is for research. I would advise the Masters route but only for you to have time to refine your thinking on your goals. Good luck on your path. ",4yn3v9,t1_d6pc8al,Schermanification,,Reply,2,0,2
d6pru5e,2016-08-20 16:35:38-04:00,c0cktail,,"I am really not sure if I can help you out. I live and work in the Netherlands. I am not sure if I fully understand all the different levels of your education. But here goes.

You sound eager and you sound willing to do a lot of work. You also seem to have reached a certain level of reasoning and thinking that is required to do a CS job.

That said, CS is not just programming, or choosing .NET or something else. It is about algortihms  and problem solving techniques and critical and logical thinking and interfaces etc etc.

I lead a small team of developers. I do not have the means at work to accommodate someone like you at the moment. But that doesn't mean other companies cannot do that.

So maybe option 4, find somewhere that you can apply your current skill set and learn more on higher level? Yes it could mean a cut back on salary. But if that means your education is paid for you, then it might be worth it.

Good luck!",4yn3v9,t3_4yn3v9,blingblingscrackpipe,,Comment,1,0,1
4ykfm9,2016-08-19 14:24:47-04:00,smorrow,The gap buffer - what's it used for?,"Text editors is the obvious one and the only one you can *find*
anything on, but I can't believe a good data structure only has one
use. Scorewriters, MIDI editors? I can see the gap buffer working,
although I'm not sure how it fits in when you have multiple
tracks/staves.

What about games? Seems like it would make sense for a
side-scroller with destructibles and/or in which things can be
moved around (gap buffer is populated beforehand with the
environment of the level).

Any non-interactive uses?
",,,,,Submission,7,0,7
4yjajl,2016-08-19 10:42:18-04:00,finessseee,Will I need a laptop for Programming 1 ?,"My old Mac is shot (the 07 white one. Doesn't even turn on anymore).. I can use my moms at home and the lab computers at school. I'm waiting the new Mac to release this fall.. Until then, can I get away without a laptop for Programming 1? ",,,,,Submission,0,0,0
d6o5189,2016-08-19 11:20:08-04:00,Coolisbetter,,I would ask your professor. Depends on the curriculum. ,4yjajl,t3_4yjajl,finessseee,,Comment,2,0,2
d6o84x6,2016-08-19 12:24:05-04:00,futureFastRunner,,"I've had friends that have done it, I kind of did it (my laptop was SLOW, so I'd usually use the on campus computers). 

One thing though that was a huge help that allowed me to do that was our IT bldg and its computer lab being open 'til 3 am. Otherwise, it may be a bit of a struggle at times. ",4yjajl,t3_4yjajl,finessseee,,Comment,2,0,2
d6ob6k1,2016-08-19 13:33:40-04:00,artillery129,,"One way you could do it is if you had a virtual machine on a cloud provider that you used to write your code, something like Amazon or Digital Ocean, then you can always SSH into that from just about any device and do your work, really though, depends on the scope of work and tools required",4yjajl,t3_4yjajl,finessseee,,Comment,1,0,1
d6qtgtm,2016-08-21 14:04:11-04:00,x_Zoyle_Love_Life_x,,Depends on how the class gets laid out,4yjajl,t3_4yjajl,finessseee,,Comment,1,0,1
4yhxwa,2016-08-19 05:23:27-04:00,Amar11372,Any suggestions on CS electives to take?,"Hello, I am a 3rd year Computer Science Student. My school requires all CS students to take 5 additional elective classes. I know CS is a big field (some sector growing faster than other) and I still haven't decided on my career path yet. Can you guys suggest from the following electives, which ones I should take? I want to have a strong resume and go into a sector that is still growing by the time I graduate. Thank You. 

|                         |                     |                             |                                      |                              |
|:-----------------------:|:-------------------:|:---------------------------:|:------------------------------------:|:----------------------------:|
|        Compilers        | Data Communications |       Machine Learning      | Information Organization & Retrieval | Operating System Programming |
|  Computational Finance  | Distributed Systems | Natural Language Processing |      Internet & Web Technologies     |                              |
|    Computer Graphics    |   Graphical Models  |      Numerical Methods      |       Object-Oriented Databases      |                              |
| Artificial Intelligence | Cryptography        | Image Processing            | Data Mining & Warehousing            |                              |",,,,,Submission,8,0,8
d6nzvir,2016-08-19 09:17:08-04:00,icendoan,,"Machine Learning and Data Mining are really hot topics right now, and have some very interesting career options (and are fun subjects in their own right!): you should fairly strongly consider at least one of those, but being able to handle and process lots of data is a general requirement for effective machine learning, so they will go hand in hand.

Compilers is always a good course to choose. You go through a very large swathe of computer science, from automata theory (in parsing) to type theory, all the way down to computer architecture and assembly languages. Even if you don't end up writing many compilers, a good grasp of parsing theory is invaluable (since people like giving text to computers!), and understanding the sorts of transformations an optimising compiler does with your code gives you many insights into how things will work as you're programming.",4yhxwa,t3_4yhxwa,Amar11372,,Comment,6,0,6
d6o3rpc,2016-08-19 10:53:14-04:00,jalagl,,"My choices would be:

**Machine Learning**: It is a tranding topic right now, and will be useful now and in the future

**Data Mining & Warehousing**: Same as ML, right now we are creating more and more data, so knowing what to do with that data would be a good skill to have.

**Information Organization & Retrieval**: My field of study :) ... right now we are burdened with information overload, the idea behind Information Retrieval is how to better manage it and help you find what you are looking for. Lots of fun things to do with ML as well.

**Compilers**: Because it is a cool subject and can help you better understand programming and design.

**Cryptography**: It is interesting and fun

Other interesting ones, to me, are **Distributed Systems**, **Natural Language Processing** and **Operating System Programming**
",4yhxwa,t3_4yhxwa,Amar11372,,Comment,5,0,5
d6ocqo8,2016-08-19 14:22:21-04:00,Amar11372,,"Thanks for the suggestion, Machine Learning is definitely a hot topic. That course is usually packed in my college and hard to get into.",4yhxwa,t3_4yhxwa,Amar11372,,Comment,2,0,2
d6odilf,2016-08-19 14:40:44-04:00,Bottled_Void,,"Favourite: I found Artificial Intelligence really fun and it helps you think of new ways of tackling problems. I strongly suggest anyone do this.


Regret: Computer Graphics was a bit of a let down for me. It's kind of interesting but it didn't really teach me anything useful. It mostly covered ray-tracing, bump and texture mapping. Some bits about polygons. But I never felt I could do anything with the knowledge that I learned.


Other suggestion: Image processing is a little more interesting than the graphics because it'll probably cover things like edge detection and compression. Things which are more easily implemented by one person.


Like to know more about: Cryptography is something I feel would be interesting, and occasionally an article comes up about it. But I didn't pick it as an option, so I can't really say.


Those are the ones I have the strongest feelings about.",4yhxwa,t3_4yhxwa,Amar11372,,Comment,1,0,1
d6oxfwk,2016-08-19 22:34:19-04:00,dmazzoni,,"Besides the other advice, choose the courses taught by the best professors. You will learn more and enjoy the class more. The right course taught by a prof who doesn't know the material as well or just doesn't put as much effort into teaching can be a colossal waste of time.",4yhxwa,t3_4yhxwa,Amar11372,,Comment,1,0,1
4ycvk3,2016-08-18 11:27:54-04:00,finessseee,Did you have to take calc based physics for CS?,Or can I just take regular Physics ,,,,,Submission,0,0,0
d6mvq5g,2016-08-18 13:45:21-04:00,andybmcc,,I highly suggest taking calculus-based physics if you have the choice.  I tend to see non-calc physics as more rote memorization rather than an actual understanding of the underlying concepts.,4ycvk3,t3_4ycvk3,finessseee,,Comment,10,0,10
d6mqozn,2016-08-18 12:05:33-04:00,Bottled_Void,,At what level?  Highschool? Part of your course? What kind of Physics?,4ycvk3,t3_4ycvk3,finessseee,,Comment,3,0,3
d6myn98,2016-08-18 14:43:01-04:00,BonzoESC,,"I believe calc-based physics was a requirement when I got my degree, and I'd highly recommend it. Without calculus, physics doesn't make sense, and without physics, calculus doesn't make sense.",4ycvk3,t3_4ycvk3,finessseee,,Comment,2,0,2
d6n5d9y,2016-08-18 16:58:00-04:00,theseldomreply,,Why does calculus not make sense without physics?,4ycvk3,t1_d6myn98,BonzoESC,,Reply,4,0,4
d6qr90i,2016-08-21 13:08:00-04:00,zSilverFox,,I'm guessing that because physics is one of the first times you'll be applying calculus outside of the textbook examples. Sometimes seeing a concrete example makes the abstract easier to understand.,4ycvk3,t1_d6n5d9y,theseldomreply,,Reply,1,0,1
d6t8pp8,2016-08-23 10:15:39-04:00,jephthai,,"Since calculus was invented for physics, it's like observing calculus in its natural habitat.",4ycvk3,t1_d6n5d9y,theseldomreply,,Reply,1,0,1
d6n3uyx,2016-08-18 16:27:17-04:00,Rangsk,,This is highly dependent on the CS program. I'd recommend looking up graduation requirements for the Universities you're interested in.,4ycvk3,t3_4ycvk3,finessseee,,Comment,1,0,1
d6na1xp,2016-08-18 18:42:26-04:00,Kaidelong,,"Take it with calculus. For physics, and for economics, too, calculus tends to make the mathematics easier, rather than harder, once you get it down.",4ycvk3,t3_4ycvk3,finessseee,,Comment,1,0,1
d6mtnr5,2016-08-18 13:04:28-04:00,bigrigs420,,Neither? Depends on the program but I've never heard of a lab science requirement outside of CS for a CS major.,4ycvk3,t3_4ycvk3,finessseee,,Comment,1,0,1
d6mvk96,2016-08-18 13:42:10-04:00,andybmcc,,I had to take three lab-based physics classes and 1 of either chem or bio for my CS degree.,4ycvk3,t1_d6mtnr5,bigrigs420,,Reply,2,0,2
d6n98yo,2016-08-18 18:23:30-04:00,ACoderGirl,,"I had some natural science requirements (you had some flexibility between picking physics, bio, or chem). But it's mostly intended such that your first year is very diverse and as sort of a skill test, I guess.

I don't really agree with their usage. It's akin to how I had to take an English class, social sciences like economics (in fact, they practically forced you to pick econ as the social science, otherwise you had to take some kind of economics-like class as an ""elective"").",4ycvk3,t1_d6mtnr5,bigrigs420,,Reply,1,0,1
d6nq1sh,2016-08-19 01:46:03-04:00,KyleRochi,,"Where did you go to school? O_o We need to take Calc I, II, and III, Linear Algebra, Diff Eqs. (O&P) and 3 Calc based Physics (General, Magnetism, and Electricity I think). They want you to take Gen Chem I and II but its not required.",4ycvk3,t1_d6mtnr5,bigrigs420,,Reply,1,0,1
d6muvcc,2016-08-18 13:28:27-04:00,finessseee,,"You need a lab science for any bachelors degree.. Bio, Chem or Phys..",4ycvk3,t1_d6mtnr5,bigrigs420,,Reply,1,0,1
d6n96if,2016-08-18 18:21:58-04:00,ACoderGirl,,"I didn't have to, no. My school gave some variance in natural science classes. Basically, it was ""pick 3 classes out of biology, physics, and chemistry subjects, but no more than 2 per subject"". So you could do one of each subject or 2 physics and 1 biology (which was what I did).

So you could totally take a calc based physics class if you could meet the prereqs. But the two first year physics classes didn't involve calc and there's no reason to take any further physics classes except as electives.

I did have to take a calculus-based stats class (probability theory), but only the honours degree required that. Non-honours allowed for a much easier stats class (which was a general purpose intro to stats). I do think that class was quite useful. And a great look at applying calculus to something practical (mind you, my calc 2 classes did have some useful problems, but mostly it was concerned with volume or surface area of shapes defined by curves -- less useful).",4ycvk3,t3_4ycvk3,finessseee,,Comment,1,0,1
4y7cr8,2016-08-17 14:44:22-04:00,Lifelong_Throwaway,Creating Full Mirrors of Websites,"Hi all, the recent shutdown of the torrenting site [kickass.to](http://kickass.to/) and the effort to mirror the site in response (see [here](https://proxyportal.org/kickass)) has led me to ask the question: how were all of these mirrors created, and so quickly?

As far as I understand, the server side parts of Kickass (the actual torrent database, search functionality, etc.) were never made public. I'm aware of cloning techniques for static websites which involve simply downloading all of the static files used, but I'm not sure how that would apply for situations like this.

Does anyone know if the server-side stuff for Kickass was just made available somewhere? Did each mirror re-implement existing functionality? Even if they did, where did they get their database from? Note, the same thing happened for thepiratebay when it got shut down, and I'm pretty sure even more recently for Torrentz.

I'm at a loss here for how it was accomplished. Thanks in advance :)",,,,,Submission,4,0,4
d6lup37,2016-08-17 18:58:00-04:00,jadamita,,I think some indexing or crawling software was used. They probably used something like Httrack,4y7cr8,t3_4y7cr8,Lifelong_Throwaway,,Comment,1,0,1
d6mjejj,2016-08-18 09:17:21-04:00,xNeshty,,"I didn't keep myself up-to-date about kickass.to, but those servers always have backups. If the site is down, the backup will be uploaded to a new server. Since the links are proxies, I could imagine the proxies are simply targeting the new server, rather than the old one, making you feel like kickass was mirrored.

Mirroring such a website takes an impresive amount of time and can't be held up-to-date always. You have to consinstently crawl through the website. If the website offers feeds (kickass does, kindof) you can simply keep your crawler aware of the feed.",4y7cr8,t3_4y7cr8,Lifelong_Throwaway,,Comment,1,0,1
4y6z4e,2016-08-17 13:38:41-04:00,Shyrtle,Starting Computer Science ~ Any Tips? ~,"So after a few semesters of Electrical and Computer Engineering I've decided to change majors to Computer Science. 

***

###I'm looking for some tips on starting out:
* Are there any good materials, that I should keep a reference to?
* Do you know of any good books I should read, outside of the assigned ones. 
* Is there anything you wish you knew going in, that you know now?
* Are they any cool projects I should look into doing for my later years in the field?
*  Any great ""Aha!"" moments that you remember?
* Anything to look out for in a teacher? Good or bad.
* Any good hardware, or software to invest in?
* Do you have anything that you couldn't live without?
* Are there any good communities to stay active in, post questions, learn from, etc. (including subreddits ^.^)
* Any good study tips?
* Any good partner majors, that compliment computer science, worth looking into?
* Anything else you can think of!
 
That's just a general list, if you think of anything that you think might be helpful, please post it I'd really appreciate it :) 

***

I'm really looking for anything I can get my hands on as of now. Class just started and I'm pretty pumped to get going. 

Thanks for anything you provide! I'm looking forward to reading it.

***

***

Edit: I decided that it might make it easier to respond if I take all of the questions, and put them in a easily copy-pasted format. Use this if you want. :) I made this format using the response below from /u/v11che. After seeing his response, I figured I could have saved him some time, and might some other people time by making this.


    **Are there any good materials, that I should keep a reference to?**

    <Response Here>

    **Do you know of any good books I should read, outside of the assigned ones?**

    <Response Here>

    **Is there anything you wish you knew going in, that you know now?**

    <Response Here>

    **Are they any cool projects I should look into doing for my later years in the field?**

    <Response Here>

    **Any great ""Aha!"" moments that you remember?**

    <Response Here>

    **Anything to look out for in a teacher? Good or bad.**

    <Response Here>

    **Any good hardware, or software to invest in?**

    <Response Here>

    **Do you have anything that you couldn't live without?**

    <Response Here>

    **Are there any good communities to stay active in, post questions, learn from, etc. (including subreddits ^.^)**

    <Response Here>

    **Any good study tips?**

    <Response Here>

    **Any good partner majors, that compliment computer science, worth looking into?**

    <Response Here>

    **Anything else you can think of!**

    <Response Here>
",,,,,Submission,1,0,1
d6lg7rs,2016-08-17 14:02:22-04:00,Shyrtle,,"I found this on Quora, and edited it a little bit, to make it work with Reddit formatting. 

But here is the link to the original: https://www.quora.com/What-subreddits-should-a-software-engineer-follow

All credit goes to the original author of that answer, Yad Faeq. 

***

Wooh, I hope I'm not too late to answer this :)!

Here goes my favorites:
***
programming • /r/programming

is a reddit for discussion and news about computer programming
***
coding • /r/coding

Interested in programming? Like to read about programming without seeing  a constant flow of technology and political news into your proggit?   That's what /r/coding is for.
***
Computer Science: Theory and Application • /r/compsci

Welcome Computer Science researchers, students, and enthusiasts. The  aim of this subreddit is to share interesting papers, blog posts, and  questions about topics such as algorithms, formal languages, automata,  information theory, cryptography, machine learning, computational  complexity, programming language theory, etc... 
***
For learning, refreshing, or just for fun! • /r/dailyprogrammer

Challenges
***
webdev: reddit for web developers • /r/webdev

Want to know what's new for Web Developers? You're in the right place!
***
Reverse Engineering • /r/ReverseEngineering

If you have a question about how to use a reverse engineering tool,  or what types of tools might be applicable to your project, or your  question is specific to some particular target, ask it on the Reverse Engineering StackExchange site.   If you have a broader, more general question about reverse engineering  as a practice or as an occupation, please post it on our bi-weekly questions thread.
***
***
Go here and find out what people want to build:

Startups • /r/startups

Welcome to r/startups, a community for all backgrounds, levels of  expertise, and business experience. We are a forum of entrepreneurs  working towards unbiased and anonymous feedback, advice, ideas, and  discussion.
***
***
Go here to see what people are worried about in CS Career:

CS Career Questions • /r/cscareerquestions

This subreddit is responsible for answering questions about careers in  Computer Science, Computer Engineering, Software Engineering, and other  related fields. Respect the reddiquette, and reserve this space for  questions concerning careers and degrees only. Please keep the conversation professional.
***

####Languages:
* python - /r/Python
* javascript - /r/Javascript
* ruby - /r/ruby
* cpp - /r/cpp
* haskell - /r/haskell
* php - /r/php
* lisp - /r/lisp
* perl - /r/perl
* erlang - /r/erlang
* java - /r/java
* c_programming - /r/c_programming
* scheme - /r/scheme
* asm - /r/asm
* c_language - /r/c_language
* scala - /r/scala
* cplusplus - /r/cplusplus
* ocaml - /r/ocaml

***

####Frameworks:
* Rails hackers • /r/rails
* Django • /r/django
* Databases /r/databases

***
Open Source on Reddit • /r/opensource

A subreddit for everything open source related.

Looking to contribute? Try OpenHatch.

***

hacking • /r/hacking

What we are about: constructive collaboration and learning about  exploits, industry standards, black and white hat hacking, new hardware  and software hacking technology, sharing ideas and suggestions for small  business and personal security.
***
Machine Learning • /r/MachineLearning

News, Research Papers, Videos, Lectures, Softwares and Discussions on:

Machine Learning

Data Mining

Information Retrieval

Predictive Statistics

Learning Theory

Search Engines

Pattern Recognition

Analytics

***

This was just related to relevant sub reddits, but I still felt like it would be useful.",4y6z4e,t3_4y6z4e,Shyrtle,,Comment,4,0,4
d6lgxgz,2016-08-17 14:16:14-04:00,v11che,,"**Are there any good materials, that I should keep a reference to?**

Wikipedia is very good. As are youtube videos. Like [this course from MIT](https://www.youtube.com/watch?v=k6U-i4gXkLM) 

**Do you know of any good books I should read, outside of the assigned ones.**

[Learn C in 21 days](https://www.amazon.co.uk/Sams-Teach-Yourself-21-Days/dp/0672324482) is good to get your head in the programming game, but I only read that after graduating. For me books didnt help, so I can't really advise on that.

**Is there anything you wish you knew going in, that you know now?**

You learn more after you graduate than when you're in uni. 

**Are they any cool projects I should look into doing for my later years in the field?**

I ended up going into software engineering so my projects were a lot of games type programming, AI and search trees.

**Any great ""Aha!"" moments that you remember?**

None whatsoever. it was 4 years ago... :)

**Anything to look out for in a teacher? Good or bad.**

You're gonna get some who literally just read from the slides and not actually teach anyone. If that's the case, I found the lectures to be a waste of time and got their lecture notes online and found myself improving on my own. A bad teacher can ruin a module for you, but a good teacher can make a module. You've done this school lark before, you know what you like in a teacher. There will be some lecturers who are there just because they have to do some teaching hours to go towards their research funding or PhD.

**Any good hardware, or software to invest in?**

A small laptop, portable too. Don't use it for anything other than work. I used to work on a 10.1 netbook and it was perfect for the few things I needed it for. You can take it with you every day and make sure you back up everything. I don't need to teach you about dropbox.

**Do you have anything that you couldn't live without?**

It sounds lame, but a  good group of friends. When your stuck you can bounce ideas off them. Don't plagiarise or ask them to do anything for you, but work on the high level concepts of problems. People brains work differently, I guess. 

**Are there any good communities to stay active in, post questions, learn from, etc. (including subreddits .)**

Stack overflow will become your bible.

**Any good study tips?**

Don't ask me, I used to do all-nighters before an exam. then sleep in the afternoon...

**Any good partner majors, that compliment computer science, worth looking into?**

Did pure compsci sorry, I did focus a lot on software engineering and algorithms though.

**Anything else you can think of!**

Your end of year projects are very important. That's what you'll show your first employer to get that first job. Pick them wisely and don't be lazy.


Sorry I can't be more of a help. I was a slacker. Still got a First Class Masters somehow. Must have done something right. \^_^",4y6z4e,t3_4y6z4e,Shyrtle,,Comment,2,0,2
d6lih9a,2016-08-17 14:45:35-04:00,Shyrtle,,"Thank you so much, this is helpful! I really appreciate you taking the time to do this. 

I will go check out stack overflow. 

And I have a Microsoft Surface Pro. Would that be able to handle most of the stuff, or should I pick up an actual laptop? I'm working full time while doing the CS degree, so I should be able to afford a laptop if I have to.",4y6z4e,t1_d6lgxgz,v11che,,Reply,2,0,2
d6lkjb1,2016-08-17 15:24:34-04:00,v11che,,No problem. Yeah a surface will be just fine :) Good luck :),4y6z4e,t1_d6lih9a,Shyrtle,,Reply,1,0,1
d6lu7vk,2016-08-17 18:46:50-04:00,dbsndust,,"On the topic of what to look for in a teacher, I would give you this advice. Just some questions to think about, it doesn't really matter what the answers are but it can help you understand what the professor can offer you as a student, depending on your personal goals.

1. Has your professor ever worked outside of academia?

2. Do other faculty members seem to respect this professor?

3. Has the professor ever been part of a renowned group (e.g. the w3c, IEEE)?

4. Does your professor publish research or give talks at conferences? On what topics?

5. Do they talk about their own accomplishments in class? (Preferably not in a boasting manner, more like a way of fostering discussion. I had a professor who built our school's original student database and likes to use it as an example in his database classes)

6. Does your professor like to use newer technologies or more tried-and-true technologies? (You will notice this more in elective courses)

Make sure you talk to them and learn the answers to these questions! Don't be afraid. If they're a good teacher they should be happy to talk to you. They could probably also give you some great answers to the rest of the questions you posted up there.",4y6z4e,t3_4y6z4e,Shyrtle,,Comment,1,0,1
d6mk6bv,2016-08-18 09:38:39-04:00,yes_thats_right,,"I graduated from a comp sci degree over a decade ago but I imagine it is still mostly the same.

One thing I wish I had done was to start coding my own side projects whilst studying. It's great to read about challenges and solutions in a book, but when you come across them in real world applications and find solutions to them it helps you understand so much better.

Also, don't focus too much on learning a language, what's more important at this stage is to learn programming in general.",4y6z4e,t3_4y6z4e,Shyrtle,,Comment,1,0,1
4y2uab,2016-08-16 20:38:46-04:00,dbsndust,In JavaScript you can have arrays of undefineds. Does an array of 1000 undefineds take up more memory than an array of 10?,,,,,,Submission,9,0,9
d6kn3nh,2016-08-16 22:56:20-04:00,ICanRememberUsername,,"I am not overly familiar with the inner workings of JavaScript, but in most programming languages, yes it would. If it's undefined, each index is probably a pointer to a null location in memory, but it's still a pointer that takes up 32 (or 64) bits.",4y2uab,t3_4y2uab,dbsndust,,Comment,8,0,8
d6m3cly,2016-08-17 22:33:03-04:00,ClassyCamel,,"I'd like to preface this with I don't normally get into questions like this, but it nerd-sniped me. And I'm also not a JavaScript expert, but I do have a fair amount of experience with JS (and things like this is how to become an expert).

OK, so for this I think there are 2 full questions to answer. Keep in mind that the average computer scientist or programmer will answer ""yes"" based on their experience in how other languages work, the thing with javascript is it's *dynamic* and really likes to tout that fact. So when I make an assumption, I'll assume on the side of the answer to this question being ""yes"". Anyway, here are the two questions:


1) how do undefined variables work in JavaScript?
2) what the hell is an array in JavaScript?


1) Ok, so first we need to understand that JS has 5 primitive types:


* number
* string
* object
* boolean
* undefined


What's interesting about the undefined type is that any variable of this type can only assume one value: undefined (or, the result of calling the ""void"" function).

Quick aside: ""void"" will evaluate an expression then **always** return the value undefined, independent of what the global variable ""undefined"" has been assigned to.

So now we have 3 ""undefined""s:

* the primitive type
* the value, and
* the (global) variable

(and some wonder why people don't like JavaScript)

My primary question was this:

Given:

    x = void(0):
    y = void(0):

do x and y point to the same memory address?

The standard [ECMA-262](http://www.ecma-international.org/publications/files/ECMA-ST/Ecma-262.pdf) document says ""no"", these are two different variables containing two separate (but equal) values, but I suppose the JS engine can optimize that if it wanted to.

For the purpose of being implementation agnostic, let's assume ""no"".

So now we have answered question 1 according to the ECMA standard, cool!

Now on to question 2:

2) Let's get a quick & easy answer on what an array is:

    >  x = new Array()
    <- []
    >  typeof Array
    <- ""function""
    >  typeof x
    <- ""object""

You read that right: classes are functions and arrays are just objects.

The Array function has 3 different constructor invocations:

    new Array()
    new Array(length)
    new Array(item1, item2, ...)

Since an array is an object, the indices are actually object keys. If you have some array x, then:

    x[0] === x['0'] === x.0


Except for the fact that object keys can't begin with a number, so that last one throws a syntax error. The reason x[0] works is because JS coerces the accessor key to a string behind the scenes ([source](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Array)).

Ok, so let's set up our scenario now that we have answered both questions:

    x = new Array(5)
    y = new Array(1000)

    >  x.length
    <- 5
    >  y.length
    <- 1000
    > x[0]
    <- undefined
    >  y[0]
    <- undefined

ok, let's try something more complicated:

    >  x[5]    // an index past the length of the array
    <- undefined

just like any object, accessing a key that doesn't have a corresponding value or a key that was never set on the object returns undefined.

Another things that's nice about objects is that we can look up that specific object's properties (without going higher on the prototype chain) with the hasOwnProperty method.

    >  x[0] = 1
    <- 1
    >  x.hasOwnProperty(0)
    <- true
    >  x[1]
    <- undefined
    >  x.hasOwnProperty(1)
    <- false

...what? Oh! Duh, it might not be coercing it to a string. Let's try:

    >  x.hasOwnProperty('1')
    <- false

woah.

    >  x.length
    <- 5

Interesting. So it looks like:

    new Array(n)

has the same size in memory as:

    new Object()

save for an extra reference pointer on the prototype chain, pointing to the Array prototype, which then subsequently points to the Object prototype.

This is the key here: you create a new array and it sets the array's ""length"" property and nothing else. It's just used internally (it looks like) for keeping track over how many keys you should iterate over.

Thus, while x and y have different lengths, the size in memory (at least on Chrome, where I'm testing this) is the same.",4y2uab,t3_4y2uab,dbsndust,,Comment,4,0,4
d6mqu60,2016-08-18 12:08:34-04:00,dbsndust,,"Excellent answer, I was suspicious of this from the start but I am certainly not a JavaScript expert and wasn't sure how I would test it out. Goes to show you that Reddit likes to upvote things that ""sound"" right. Thanks!",4y2uab,t1_d6m3cly,ClassyCamel,,Reply,2,0,2
d6ms4m6,2016-08-18 12:34:21-04:00,ClassyCamel,,"Glad you liked it! Keep in mind that while this is in Chrome (which uses the v8 engine), all arrays are objects, so this likely applies to all JS engines.",4y2uab,t1_d6mqu60,dbsndust,,Reply,1,0,1
d6mt4c0,2016-08-18 12:53:44-04:00,dbsndust,,Which means it would also apply to backend frameworks like Node,4y2uab,t1_d6ms4m6,ClassyCamel,,Reply,1,0,1
d6knja8,2016-08-16 23:08:12-04:00,bacondev,,"Yes. Each slot in the array has a copy of the `undefined` value. The interpreter must represent `undefined` in some form or another in memory. In other words, a slot in the array occupies memory just like a C variable that holds `NULL`. Theoretically, the array could be optimized to hold just the count and the value, but what happens if you want to set the item in the middle to a defined value? For this reason, it’s highly unlikely that an interpreter would bother with such an optimization.",4y2uab,t3_4y2uab,dbsndust,,Comment,3,0,3
d6loqdt,2016-08-17 16:46:52-04:00,omrog,,"Do js engines do much optimization? 

I can imagine given all the places it could run you'd want it to. And it's been a while since I've done much js stuff by hand but I can't remember much emphasis on keeping things light. ",4y2uab,t1_d6knja8,bacondev,,Reply,1,0,1
d6lckj6,2016-08-17 12:52:03-04:00,ClassyCamel,,"The answer is no! I'm writing up why right now, only commented so I could find my place later to explain my answer",4y2uab,t3_4y2uab,dbsndust,,Comment,1,0,1
d6ltu57,2016-08-17 18:37:46-04:00,dbsndust,,Waiting haha,4y2uab,t1_d6lckj6,ClassyCamel,,Reply,1,0,1
d6krvr5,2016-08-17 01:21:10-04:00,jpflathead,,"Since everyone here says yes, I'm going to go with no.

A bit of a contrarian strategy, but sometimes it pays off wonderfully.",4y2uab,t3_4y2uab,dbsndust,,Comment,-6,0,-6
d6ligrv,2016-08-17 14:45:20-04:00,Am0s,,"Unless you're contributing an explanation to go with being contrarian, that's just dumb.",4y2uab,t1_d6krvr5,jpflathead,,Reply,2,0,2
d6lji1c,2016-08-17 15:04:52-04:00,jpflathead,,"I would say that's the joke and point to a meme, but this thread seems filled with the humorless so why bother?",4y2uab,t1_d6ligrv,Am0s,,Reply,-1,0,-1
d6ljyv5,2016-08-17 15:13:46-04:00,Am0s,,"Somebody asks a reasonable, technical and serious question. They get reasonable, serious, and technical responses. 

You give an unreasonable, joking, and nonsensical response. You then don't seem to understand why this is unwanted.",4y2uab,t1_d6lji1c,jpflathead,,Reply,1,0,1
d6lkh9p,2016-08-17 15:23:27-04:00,jpflathead,,">  They get reasonable, serious, and technical responses.

Exactly! How many ""Yes""s do they need? Time for some silliness.

",4y2uab,t1_d6ljyv5,Am0s,,Reply,-2,0,-2
d6lklew,2016-08-17 15:25:41-04:00,Am0s,,"In your view. Not in the view of people who don't consider that a way of contributing to the conversation. 

Many people don't want intentionally bad answers here, just like like how most people wouldn't want nonsensical shitpost-style answers on StackOverflow.

Reddit has a means for indicating that you don't believe a comment contributes to the conversation.",4y2uab,t1_d6lkh9p,jpflathead,,Reply,1,0,1
d6lms1f,2016-08-17 16:08:08-04:00,jpflathead,,"> Many people don't want intentionally bad answers here, just like like how most people wouldn't want nonsensical shitpost-style answers on StackOverflow.

Just a week ago, someone in learnprogramming or somewhere wanted to know how reddit became as much a bunch of cunts as stackoverflow. I would have pointed to you had I known.

So downvote me and get on with your life. You have a lot of learning to do since you somehow thought there was any manner whatsoever in which I could provide an explanation to how arrays of 1000 undefineds take up less space than an array of 10 in JavaScript.
",4y2uab,t1_d6lklew,Am0s,,Reply,-1,0,-1
d6ltvfj,2016-08-17 18:38:38-04:00,dbsndust,,Ok buddy time to calm down,4y2uab,t1_d6lms1f,jpflathead,,Reply,0,0,0
d6ltybe,2016-08-17 18:40:30-04:00,jpflathead,,"Hey, so just how many explanations did you need before realizing the answer was ""yes"" and understanding why?  And did my answer confuse you in any manner?  (I suspect not)",4y2uab,t1_d6ltvfj,dbsndust,,Reply,-1,0,-1
d6khtwt,2016-08-16 20:43:00-04:00,CARGLE,,"As a know nothing student, my intuition says yes because it would have to alot that memory but don't quote me on that please. ",4y2uab,t3_4y2uab,dbsndust,,Comment,-3,0,-3
d6khu3d,2016-08-16 20:43:07-04:00,QuoteMe-Bot,,"> As a know nothing student, my intuition says yes because it would have to alot that memory but don't quote me on that please. 

~ */u/CARGLE*",4y2uab,t1_d6khtwt,CARGLE,,Reply,9,0,9
d6kidvt,2016-08-16 20:56:47-04:00,dbsndust,,"I think the sheer fact that there are more accessible indices would increase the amount, but I'm more concerned with the actual storing of the data. Like having 5 indices vs 10 means you have 5 less places to care about, but some languages (like JavaScript) map multiple references to the same object using pointers to save memory",4y2uab,t1_d6khtwt,CARGLE,,Reply,1,0,1
4xuzoz,2016-08-15 14:20:58-04:00,PINEAPPLE_lN_MY_ANUS,"If there is no single antivirus software that can detect all malware by itself and you can only have one installed at a time, how can you ever be sure that your computer isn't compromised?",I know there is virustotal.com but it can only scan one file at a time and not a whole system afaik.,,,,,Submission,0,0,0
d6isikp,2016-08-15 16:43:09-04:00,BonzoESC,,"Most people I know (including myself) in computer security don't bother with antivirus software. AV works on either having a complete list of bad software or trying to run bad software in a safe environment to see if it's bad. This isn't doable. 

If you're curious about what is done normally, read through https://www.theguardian.com/technology/2015/jul/27/security-experts-keep-safe-online-password-manager-seven-things",4xuzoz,t3_4xuzoz,PINEAPPLE_lN_MY_ANUS,,Comment,3,0,3
d6ioafg,2016-08-15 15:13:55-04:00,wackyvorlon,,Don't use Internet Explorer. ,4xuzoz,t3_4xuzoz,PINEAPPLE_lN_MY_ANUS,,Comment,2,0,2
d6in5jm,2016-08-15 14:48:00-04:00,rewardiflost,,"This is why some systems seriously limit all access,  and others require code authentication before execution. ",4xuzoz,t3_4xuzoz,PINEAPPLE_lN_MY_ANUS,,Comment,1,0,1
d6lizup,2016-08-17 14:55:21-04:00,Lifelong_Throwaway,,"In addition to what other people said, I thought I would chime in with a direct answer to your question. There really isn't a way to know with absolute certainty that your system isn't compromised unless the only software you install is 100% trusted and doesn't have any exploitable bugs. This is a situation that usually doesn't happen on most people's computers.

The good news though is that most viruses that you usually find yourself with make it very obvious that they're present, and can usually be dealt with in the worst case scenario by reinstalling Windows.",4xuzoz,t3_4xuzoz,PINEAPPLE_lN_MY_ANUS,,Comment,1,0,1
d6ixtyi,2016-08-15 18:44:19-04:00,EntreriPrime,,"Obviously not, there cannot be and there never will be. Even an AI...another AI will create something undetectable to #1 at least for a short duration. 

An internet security suite is still good though, it may help prevent you from going into nefarious sites...drive by downloads. 

There are specific tools to use alongside main AV, such as malwarbytes, hitman pro and other standalone scanners. 

Certainly a PC going on the internet, can never be sure. 

iOS device that only has pre-installed Apple apps, bought from Apple Store, no malware, even browsing internet. More likely to win a lottery than get malware.  

",4xuzoz,t3_4xuzoz,PINEAPPLE_lN_MY_ANUS,,Comment,-2,0,-2
4xuwlb,2016-08-15 14:04:13-04:00,PINEAPPLE_lN_MY_ANUS,Why does some software suddenly start to break/crash after months of use when you haven't changed one thing in your OS/settings/hardware?,,,,,,Submission,7,0,7
d6ilnep,2016-08-15 14:16:31-04:00,earslap,,"Depends on the software. Suppose you have a todo list application that you enter your todo lists and the ones you do are archived in history. If the parts in the software that are dedicated to writing to this database has bugs, it might corrupt its own database. Then any time you are reading from and writing to this database, a myriad of unexpected bugs might surface (because the code is not running in the ""happy path"" anymore). This is just one example, but in general, if your assumption of nothing changing in the OS / settings hold (might not be the case, the OS does many things on its own that you are not aware of), then the problem must be with the data files the program in question creates / writes to / reads from.",4xuwlb,t3_4xuwlb,PINEAPPLE_lN_MY_ANUS,,Comment,15,0,15
d6im7jd,2016-08-15 14:28:19-04:00,PINEAPPLE_lN_MY_ANUS,,"That makes perfect sense, I guess that explains why a clean reinstall of the OS and everything else usually solve most problems.",4xuwlb,t1_d6ilnep,earslap,,Reply,1,0,1
d6imwb0,2016-08-15 14:42:42-04:00,earslap,,"An example just happened to me last week in fact: the Firefox browser I keep around for development purposes would just open and hang. Everything in my computer works, but Firefox would show its window and just sit there unresponsive. Firefox has a data folder it keeps its settings, history, addons and whatnot. Deleting the addons didn't help so I had to delete the entire data folder. When I launched the browser again, Firefox recreated those folders and files and is now working flawlessly.

Even more rarely, these can be caused by subtle hardware problems. A few years ago, I ran into a situation where my bittorrent client stopped doing its job properly. It would download the file to 99%, then download the last part, then instead of finishing, it would start re-downloading a random part of the file without ever finishing the whole download. It would be stuck at 99%. All files, at all times. I reinstalled it and it didn't help.

Turns out I had a bit flipped in my RAM. So the stick was basically dead. When the client finished downloading all files, it ran a checksum to make sure every bit of the file was downloaded correctly. Since this is a RAM intensive process, even a single wrong bit would cause checksum to fail (which is checksum's whole purpose). It would finish, check, fail (due to the flipped bit in the RAM), redownload the failed part, check it, fail and so on. A RAM check diagnosed the issue. Changed the stick and it started working as intended.",4xuwlb,t1_d6im7jd,PINEAPPLE_lN_MY_ANUS,,Reply,4,0,4
d6iom51,2016-08-15 15:21:23-04:00,PINEAPPLE_lN_MY_ANUS,,"That last part would I have driven me insane after I would have reinstalled the OS and that still wouldn't have fixed it. It would have taken me forever to even consider diagnosing my RAM, which is kinda stupid now that I think about it.",4xuwlb,t1_d6imwb0,earslap,,Reply,1,0,1
d6iptxi,2016-08-15 15:46:50-04:00,earslap,,">It would have taken me forever to even consider diagnosing my RAM, which is kinda stupid now that I think about it.

It's not stupid at all. Hardware failure is rare so it is not one of the first many things that come to mind. It took me a while to figure it out!",4xuwlb,t1_d6iom51,PINEAPPLE_lN_MY_ANUS,,Reply,3,0,3
d6iq8wo,2016-08-15 15:55:28-04:00,brendanrivers,,"> Months of use

> Haven't changed anything

This is an extremely unlikely circumstance from a software point of view.  *Something* is changing.",4xuwlb,t3_4xuwlb,PINEAPPLE_lN_MY_ANUS,,Comment,9,0,9
d6jdaoz,2016-08-16 01:50:17-04:00,bacondev,,"In addition to the current answers, there’re also external factors. For example, consider a “bug-free” piece of software. Computer are susceptible to electromagnetic radiation and there’s really not much that you can do about that. Sometimes, this radiation can affect the memory such that it becomes corrupt. And sometimes it’s not even radiation. It could be a tiny piece of a computer part acting up. So even if everything on the computer is “bug-free”, shit can still go wrong. This can mitigated with stuff such as [ECC memory](https://en.wikipedia.org/wiki/ECC_memory), but that’s never 100% effective.

You’d be surprised how common your computer is affected by this type of stuff. Google published a [study](http://research.google.com/pubs/pub35162.html) that suggests that it’s actually quite common. A few weeks ago, I did the math and (if I remember correctly) found that a typical modern computer could be affected at least on a weekly basis. This could be something trivial like the affected data are unimportant. Or it could bring your system to a grinding halt. Was it a bug? Or was it hardware-related? Sometimes, you’ll never know.",4xuwlb,t3_4xuwlb,PINEAPPLE_lN_MY_ANUS,,Comment,1,0,1
d6ipc7b,2016-08-15 15:36:43-04:00,dxk3355,,"Even if you change nothing, the clock is always changing.  Probably some timed condition like DST, license expires, or maintenance thing in whatever program you're using.",4xuwlb,t3_4xuwlb,PINEAPPLE_lN_MY_ANUS,,Comment,0,0,0
d6jcbgs,2016-08-16 01:12:52-04:00,deftware,,malware,4xuwlb,t3_4xuwlb,PINEAPPLE_lN_MY_ANUS,,Comment,0,0,0
4xt1s6,2016-08-15 07:02:11-04:00,4k33m,Why don't 32 bit systems use bank switching?,"A 32 bit computer can handle up to 4 GB of RAM, as the address bus is the limiting factor. If there was a bank switching system, the address bus could be switched to different areas of memory. Why isn't this done?",,,,,Submission,12,0,12
d6i78xm,2016-08-15 08:14:50-04:00,tavianator,,x86 has PAE which seems similar: https://en.wikipedia.org/wiki/Physical_Address_Extension,4xt1s6,t3_4xt1s6,4k33m,,Comment,12,0,12
d6isr6b,2016-08-15 16:48:05-04:00,INCOMPLETE_USERNAM,,The correct answer. Bank switching has been supplanted by paging.,4xt1s6,t1_d6i78xm,tavianator,,Reply,1,0,1
d6i6smy,2016-08-15 07:54:58-04:00,Weltschmerz93,,"I'm not an expert on system architecture, but isn't switching between two banks of memory basically adding a single bit to the address to indicate in which bank the address is?

If that's so, it's basically using 33 bits instead of 32.",4xt1s6,t3_4xt1s6,4k33m,,Comment,6,0,6
d6i7ouf,2016-08-15 08:32:32-04:00,4k33m,,a byte could be assigned as bank number to allow up to ~4000(?) banks to be accessed ,4xt1s6,t1_d6i6smy,Weltschmerz93,,Reply,-3,0,-3
d6i7zea,2016-08-15 08:43:33-04:00,LogicallyFuzzy,,How do you get to 4000? A single byte can only make 256 distinct addresses.,4xt1s6,t1_d6i7ouf,4k33m,,Reply,9,0,9
d6icejj,2016-08-15 10:53:14-04:00,splenetic,,"It is done where appropriate. Pretty much all modern 32-bit processors have at least some capability for bank switching (with the exception of micro-controllers and other designs aimed at embedded systems).

In general, though, bank switching is sub-optimal compared to just having a bigger contiguous address space. If you're switching from one entire 4GB bank to another, where will the OS live? Do you really want to have to switch from one bank to another, with all the overhead that entails for memory caching etc, for every OS call? Probably not. So bank switching is something of a stop-gap measure where what you really want is a larger address bus.
",4xt1s6,t3_4xt1s6,4k33m,,Comment,2,0,2
d6ii4cb,2016-08-15 13:01:59-04:00,BonzoESC,,"A lot of what a process's address space is used for isn't just their private RAM. A common usage is memory-mapped files, to allow a region of a file to be accessed like a big data structure instead of having to use file I/O to seek and read around in it. [MongoDB has used memory-mapped files for at least seven years](http://blog.mongodb.org/post/137788967/32-bit-limitations) and being able to memory map files bigger than 4GB or more than one of those at once is extremely valuable.",4xt1s6,t3_4xt1s6,4k33m,,Comment,2,0,2
d6ivprx,2016-08-15 17:53:42-04:00,UtterlyDisposable,,Memory-mapping is an extremely ancient technique that has its origin in the very first virtual memory systems of the 1960's.  ,4xt1s6,t1_d6ii4cb,BonzoESC,,Reply,1,0,1
d6i9uku,2016-08-15 09:44:52-04:00,Cland,,Nobody needs more than 640kB of RAM.,4xt1s6,t3_4xt1s6,4k33m,,Comment,-3,0,-3
4xro55,2016-08-14 23:18:03-04:00,casprus,Why did GNOME decided to repackage data types? It seems just more hard for developer and nonintuitive.,,,,,,Submission,6,0,6
d6i3nid,2016-08-15 05:02:44-04:00,epakai,,"https://developer.gnome.org/glib/stable/glib-Basic-Types.html#glib-Basic-Types.description

There are lots of similar questions if you google ""why glib types"" or ""why glib""",4xro55,t3_4xro55,casprus,,Comment,2,0,2
d6i418h,2016-08-15 05:25:42-04:00,Sqeaky,,"That is informative but is a ""what"" or ""how"" and not a ""why"".

I cannot speak for Gnome specifically, but likely they did it for the same reason other C and C++ projects to account for future changes at low cost. Meaning this isn't a compsci question and more a pragmatic question.

Setting up typedefs like that costs almost nothing early in a project. It can potentially save man-weeks of time when porting to a new platform or compiler, like when you learn that ""char"" is different from a ""signed char"" or ""unsigned char"" in different places.

It also allows decisions about what guarantees are important across the whole project, for example preferring speed over size. Like 32 bit bools on some platforms and 8 bit bools on others.

It reduces bugs with sizeof when changing types a function accepts. If a dev changes the guint32 typedef then a sizeof(guint32) will always produce the correct answer even when it is backed by a 64 bit integer for performance reasons. Devs directly changing the function signature could miss things like this deep inside functions because there is often no explicit type dependency.",4xro55,t1_d6i3nid,epakai,,Reply,4,0,4
d6j2pby,2016-08-15 20:46:18-04:00,casprus,,for portabilitys sake?,4xro55,t1_d6i418h,Sqeaky,,Reply,2,0,2
d6mvxef,2016-08-18 13:49:16-04:00,Sqeaky,,"I think I covered that when talking about *port*ing to a new platform, unless I misunderstood your question.",4xro55,t1_d6j2pby,casprus,,Reply,1,0,1
d7rg8bx,2016-09-18 01:06:01-04:00,casprus,,"sorry, this is late, but i thought about it: can't they just change compiler options?",4xro55,t1_d6mvxef,Sqeaky,,Reply,2,0,2
d7rgnyf,2016-09-18 01:22:13-04:00,Sqeaky,,"Better late than never. :) Forgive my grammar, it is late and I am tired.

In principle one could make a compiler that accepted the size of integers and floats and all that, but in practice they don't.

Most compilers are built with knowledge of the target system built in. A compiler building for x64 Linux for example would know that 8, 16, 32 and 64 bit integers are well supported and each operation on those generally can be done in a single instruction. So providing a 24 bit int wouldn't make much sense since it would just be a 32 bit int that ignored 8 bits.

With this information it still might not be obvious why these typedefs are useful. Think about the perspective of the application developer. He doesn't know if his software will be built on x86, x64, ARM, SPARC or whatever. Perhaps the developer just knows that he needs a signed integer type of at least 32bits that is fast. If the Gnome authors, or any other library authors, promise to provide a type called IntFast32 that does this, they have options for providing that. They might define a fixed 32bit by doing something like ""typedef int32_t IntFast32"" on most platforms. But imagine if on 64 bit Sparc the 32 bit int was super slow then doing something like ""typedef int64_t IntFast32"" might make sense. This couldn't be a build option because the dev wouldn't know but the library authors promised to know.

There are other reasons to do this: allow types with extra padding, handle alignment restrictions, guarantee types aren't larger (sometimes size matters more than speed), or on systems without unsigned types sometimes the next larger int is used as the smaller unsigned type (or so I have heard).",4xro55,t1_d7rg8bx,casprus,,Reply,1,0,1
d7s6fkb,2016-09-18 15:57:07-04:00,casprus,,what about the new fast/small int types,4xro55,t1_d7rgnyf,Sqeaky,,Reply,1,0,1
d7s9vej,2016-09-18 17:08:48-04:00,Sqeaky,,"What do you mean?

Do you mean how does the application dev access them?

The more layers of abstraction between you and the hardware to older and more standardized the features you will have access to in general. If you want access to every hardware feature do everything in assembly. If you want to guarantee your code will work on every system write all you code in a VM (like the JVM) or in an interpreted language.

If want a ton of control but want a large set of practical deployment options pick something in the middle like using Gnome and waiting for them to wrap the primitive types you need. You could also fix Gnome yourself and submit a pull request.

If I misunderstood, let me know.",4xro55,t1_d7s6fkb,casprus,,Reply,1,0,1
d7sfh7u,2016-09-18 19:19:58-04:00,casprus,,http://en.cppreference.com/w/cpp/types/integer,4xro55,t1_d7s9vej,Sqeaky,,Reply,1,0,1
d7snokh,2016-09-18 22:46:25-04:00,Sqeaky,,"Not every project has the benifit of being able to use C++11. 

But yeah in C++11 there is less of a reason for this with those.",4xro55,t1_d7sfh7u,casprus,,Reply,1,0,1
4xkmu0,2016-08-13 15:40:20-04:00,lijas,"Why is it easy to make emulators for some systems (for example gameboy), but more difficult to make emulators for other systems (for example ps4, ps3)?",,,,,,Submission,25,0,25
d6gb0s0,2016-08-13 17:17:57-04:00,bio4554,,"It mainly comes down to documentation and computational power. The gameboy uses a Z80 CPU, which is well documented and easy to emulate. The PS3 uses an obscure architecture that is hard to emulate and not well documented. PS4 is just a matter of processing power I believe.",4xkmu0,t3_4xkmu0,lijas,,Comment,13,0,13
d6gdkzh,2016-08-13 18:25:58-04:00,TheCommador,,Somewhere I read that we will likely have the an emulator for the ps4 long before ps3 simply due to the reasons you have stated.,4xkmu0,t1_d6gb0s0,bio4554,,Reply,8,0,8
d6gflrf,2016-08-13 19:21:58-04:00,bio4554,,"Probably, since PS4 is just an x86 processor running a custom OS.",4xkmu0,t1_d6gdkzh,TheCommador,,Reply,7,0,7
d6giaez,2016-08-13 20:37:14-04:00,UtterlyDisposable,,"Not just that, but it's graphics hardware that is more or less identical to stuff found in the PC world. It seems more likely to me that what will be made will end up being less of an emulator in the traditional sense, but something more like a set of custom libraries and a hacked version of the dashboard application that allows you to run games more or less on bare metal.

It'd be a hell of a project though, and Sony is next to homicidal about anything it views as copyright infringement so its creators would likely be sued into oblivion if they were ever identified.",4xkmu0,t1_d6gflrf,bio4554,,Reply,8,0,8
d6giu95,2016-08-13 20:52:52-04:00,bio4554,,If I remember correctly it's just an AMD APU in the XBone and PS4,4xkmu0,t1_d6giaez,UtterlyDisposable,,Reply,1,0,1
d6gopmi,2016-08-13 23:44:35-04:00,sandwichsaregood,,"The OS is also not really fully custom either, it's a fork of FreeBSD. They have a lot of custom code running on top of it though.",4xkmu0,t1_d6gflrf,bio4554,,Reply,4,0,4
d6gdjos,2016-08-13 18:25:00-04:00,urielsalis,,Older consoles are easier to bruteforce too,4xkmu0,t1_d6gb0s0,bio4554,,Reply,2,0,2
d6gxf52,2016-08-14 06:32:49-04:00,EpicSolo,,What do you mean by that?,4xkmu0,t1_d6gdjos,urielsalis,,Reply,1,0,1
d6gy9vs,2016-08-14 07:29:03-04:00,gyroda,,"Presumably fewer instructions to figure out, smaller APIs and OS tools as well. 

x86 is a monstrous instruction set and you need to have something that emulates what the OS does. On old consoles you were much closer to the actual hardware (which tends to be standard components with easy to find documentation).",4xkmu0,t1_d6gxf52,EpicSolo,,Reply,3,0,3
d6h7xgz,2016-08-14 12:57:17-04:00,urielsalis,,Also they ran at lower speeds meaning you didnt have to be as clever as you could just ran all instructions and have time to spare,4xkmu0,t1_d6gy9vs,gyroda,,Reply,2,0,2
4xf0g7,2016-08-12 14:25:46-04:00,hemenex,Is getting into Android development still perspective?,I am looking for a side job while studying and Android development seems attractive to me as I already have some experience with it. Could it possibly be a waste of time? Is Android on decline or is there some new technology on horizon that could make it obsolete?,,,,,Submission,4,0,4
d6fwpps,2016-08-13 10:50:02-04:00,self_raising,,"Android isn't going anywhere. It's definitely an attractive skill from a recruitment perspective. I've dabbled in it over the last few years but, as a Java middleware engineer, I find android studio quite fiddly. But there might be a better IDE these days so don't let me put you off.",4xf0g7,t3_4xf0g7,hemenex,,Comment,2,0,2
d6g0dzl,2016-08-13 12:34:16-04:00,hemenex,,Better IDE than current Android Studio from IntelliJ? The code would have to write itself (which it already kinda does).,4xf0g7,t1_d6fwpps,self_raising,,Reply,1,0,1
d6g0pkn,2016-08-13 12:42:51-04:00,self_raising,,I meant the Google one (essentially an eclipse skin). IntelliJ is awesome for Java - I didn't realise there was an android developer flavour.,4xf0g7,t1_d6g0dzl,hemenex,,Reply,1,0,1
d6g19hb,2016-08-13 12:58:06-04:00,hemenex,,I think Google cut itself completely from Eclipse years ago.,4xf0g7,t1_d6g0pkn,self_raising,,Reply,1,0,1
d6g1rib,2016-08-13 13:11:25-04:00,okmkz,,Android Studio is Google's fork of IntelliJ,4xf0g7,t1_d6g0pkn,self_raising,,Reply,1,0,1
d6g3pzv,2016-08-13 14:03:34-04:00,BonzoESC,,"Don't worry about technologies obsoleting ones you work with. With few exceptions, much of what you learn will transfer over quite nicely and old tech will still exist (and potentially be more stable or lucrative). A smart employer is going to first look for people that are flexible and willing to learn new things vs. somebody who is totally dedicated to a single technology (let me tell you about my friend that was really into Adobe Air.)

Android isn't going away, Java isn't going away, and future platforms will almost certainly have principles and concepts that make them familiar and appealing to people familiar with either.",4xf0g7,t3_4xf0g7,hemenex,,Comment,2,0,2
d6f4hxw,2016-08-12 17:35:32-04:00,nacholicious,,"Android developer here. Smartphones have been on the rise year after year, even though it might slow down a bit everyone still has a smartphone. Like 85% of the market share of smartphones is Android, which is crazy high.

I would say that it's definitely not a waste of time to learn Android, it's there to stay for a good while. The only problem is the opportunity cost, that the time you spend learning Android you could have instead spent on Java, iOS or whatever. But that's entirely up to you",4xf0g7,t3_4xf0g7,hemenex,,Comment,3,0,3
d6f7cw0,2016-08-12 18:43:33-04:00,hemenex,,You made me look up some [statistics](http://gs.statcounter.com/#mobile_os-ww-monthly-200812-201608). I am not sure what to think about that polarization between iOS and Android. I hope it's just sign of maturity.,4xf0g7,t1_d6f4hxw,nacholicious,,Reply,1,0,1
d6g1hll,2016-08-13 13:04:05-04:00,UncleMeat,,"Android has way better market share in places outside of the richest countries, simply because its possible to buy a cheapo Android phone and that's not possible with iOS.",4xf0g7,t1_d6f7cw0,hemenex,,Reply,1,0,1
4xdbcf,2016-08-12 08:41:15-04:00,Biermoese,"What is the meaning of ""single-precision"" in combination with a data format (integer, float etc.)?","Hello friends,

Microsoft (https://msdn.microsoft.com/en-us/library/ayazw934(v=vs.100).aspx) defines the floating point data type to be a ""four-byte, single-precision, floating-point number"" and [Wikipedia also talks about single-precision](https://en.wikipedia.org/wiki/Single-precision_floating-point_format) in combination with floating point numbers.

German wikipedia says that it means that the data type occupies one storage unit, while a *double-precision* data type occupies two storage units. Therefore, they say, the actual size depends on the computer architecture.

I find this confusing. To me, those two statements appear contradictory.

How can the first definition say that single-precision floating point has size 4Bytes, while the second definition says that it depends on the computer architecture? Are the definitions I gave above correct? If not, what is actually meant by single-precision?

Thanks!",,,,,Submission,9,0,9
d6ehdnt,2016-08-12 09:14:31-04:00,NoNotTheDuo,,"I think they were being specific and accurate, although maybe not relevant.  The single precision Wikipedia article has the following sentence: ""Before the widespread adoption of IEEE 754-1985, the representation and properties of the double float data type depended on the computer manufacturer and computer model.""

So, maybe before the adoption of the standards by all major computer manufacturers, the actual size may have been different - but in today's world, that's not true.  ",4xdbcf,t3_4xdbcf,Biermoese,,Comment,3,0,3
d6ehht2,2016-08-12 09:17:51-04:00,Biermoese,,"Ok but I still do not quite understand what *single-precision* means as a term. As far as I now understood, it simply means: ""size = 4Bytes"". Is that correct?",4xdbcf,t1_d6ehdnt,NoNotTheDuo,,Reply,2,0,2
d6ek6mw,2016-08-12 10:26:53-04:00,NoNotTheDuo,,Yep.  Single precision = 4 bytes.  Double precision = 8 bytes.  ,4xdbcf,t1_d6ehht2,Biermoese,,Reply,3,0,3
d6etukw,2016-08-12 13:52:04-04:00,Naethure,,"Single precision is just the common name for a 32 bit floating point. The table [here](https://en.wikipedia.org/wiki/IEEE_floating_point#Basic_and_interchange_formats) shows half-precision through octuple precision. [IEEE 754-1985](https://en.wikipedia.org/wiki/IEEE_754-1985) defines four levels of precision, but the ones we normally use are single and double.",4xdbcf,t1_d6ehht2,Biermoese,,Reply,4,0,4
d6ekqcy,2016-08-12 10:39:43-04:00,ad_tech,,"Also, in C/C++ and Java, ""float"" means a single-precision float and ""double"" means a double-precision float.",4xdbcf,t1_d6ehht2,Biermoese,,Reply,2,0,2
d6eu12v,2016-08-12 13:55:42-04:00,Naethure,,"This is the same in many languages, including the JScript language reference OP linked.",4xdbcf,t1_d6ekqcy,ad_tech,,Reply,1,0,1
d6ezn8z,2016-08-12 15:50:15-04:00,BluntnHonest,,"To address your entire question, Microsoft is talking in terms of exactly what others are talking about: single-precision is 32 bits and double-precision is 64 bits. 

What the German Wikipedia is talking about when saying storage unit is a ""word"" unit of memory (usually the most common size the hardware's instruction set operates in).

Since IEEE defined a standard for floating point numbers, pretty much everyone follows this and so ""single precision is 32 bits"" explanation is practically good enough. The ""word"" definition is less concrete but more conceptual-computer-sciency.",4xdbcf,t1_d6ehht2,Biermoese,,Reply,1,0,1
d6fcfte,2016-08-12 20:55:04-04:00,adamfowl,,Single precision refers to the word size of the architecture i.e. 32 bit arch = 4 byte word 64 bit arch = 8 byte word.,4xdbcf,t1_d6ehht2,Biermoese,,Reply,0,0,0
d6etyoi,2016-08-12 13:54:19-04:00,Naethure,,"I'm not sure why you think specifying that the float is single-precision isn't relevant. It's extremely important to know the precision of your floating point types -- that's why both a [float (single)](https://msdn.microsoft.com/en-us/library/ayazw934\(v=vs.100\).aspx) and [double](https://msdn.microsoft.com/en-us/library/ak0e23t9\(v=vs.100\).aspx) exist. If you look at [this table](https://en.wikipedia.org/wiki/IEEE_floating_point#Basic_and_interchange_formats), you can see the definitions for half, single, double, quadruple, and octuple precisions. [IEEE 754-1985](https://en.wikipedia.org/wiki/IEEE_754-1985) defines four levels of precision, but the ones we normally use are single and double.",4xdbcf,t1_d6ehdnt,NoNotTheDuo,,Reply,1,0,1
d6eut6f,2016-08-12 14:11:40-04:00,NoNotTheDuo,,"I wasn't intending to say that, only that before the adoption of the standard, the definition of double depended on the manufacturer.  Now that the standard has been adopted, it doesn't matter who manufactures the hardware - as long as they conform to the IEEE 754 standard - a single is 4 bytes and double is 8 bytes.  

The reason we're all able to answer that a single = 4 bytes and double = 8 bytes is because of the standard.",4xdbcf,t1_d6etyoi,Naethure,,Reply,1,0,1
d6etcxi,2016-08-12 13:42:11-04:00,lneutral,,"I think I can add one more detail here:

Registers are a kind of memory used to hold individual values the processor can operate on. The machine-code instructions all relate to different registers in different ways, whether it's ALU operations like adding, or operations that store register values in memory, or jumping execution depending on values held in flag registers. Processors have had different register sizes throughout the history of computing, depending on a number of engineering (and budget!) decisions.

A Z80, for example, has a set of 8-bit registers that can be paired to work as 16-bit registers. In one sense, you could say that the set of instructions that operate on 8-bit registers are single, whereas the more limited set of instructions that pair the registers could be considered double. This, of course, is an abuse of the terminology. Instead, documentation for processors like the Z80 and Motorola 68000 tend to refer to byte-length, word-length, or long operations.

In the absence of a standards body like IEEE, terms like word-length were extremely limited, because they referred to a property of the processor: namely, the typical length of an address, encoded operation, or register operation (again, it's kind of loosey-goosey). 

So you do have these two competing ideas - IEEE floating point types are specified exactly, but the idea of single- or double-precision is rooted in processor design as related to the size of registers.",4xdbcf,t3_4xdbcf,Biermoese,,Comment,2,0,2
d6g386a,2016-08-13 13:50:47-04:00,Biermoese,,Thank you!,4xdbcf,t1_d6etcxi,lneutral,,Reply,1,0,1
4xcghb,2016-08-12 03:55:10-04:00,tobyisthebest,Javascript Array that is Empty but not Actually Empty?,"After pushing multiple elements into an array and logging it, Chrome DevTools is telling me that it's empty. However, the console still its displays elements.

Here's a picture that describes what I'm saying: http://imgur.com/a/XHgEw.

The first displayed array is the atypical array and the second array is just a normal array that I logged for comparison. 

I'm at a complete loss as to what is happening here. How can I have an empty array with elements still inside of it? And why does the console say that the array has a length of 7 when it's empty. 

If it makes any difference I'm using Cordova and the DevTools is on a page with Ripple. ",,,,,Submission,4,0,4
d6earg0,2016-08-12 04:08:53-04:00,Branks,,How do you know the array is empty? ,4xcghb,t3_4xcghb,tobyisthebest,,Comment,1,0,1
d6ers86,2016-08-12 13:10:24-04:00,tobyisthebest,,Because array.length is 0,4xcghb,t1_d6earg0,Branks,,Reply,1,0,1
d6erz32,2016-08-12 13:14:17-04:00,Branks,,"No its not, I can see it says it's 7 and 4",4xcghb,t1_d6ers86,tobyisthebest,,Reply,1,0,1
d6f02bx,2016-08-12 15:59:02-04:00,tobyisthebest,,"The second array is a normal array with a length of 7: for the first array, if I log array.length it says 0. You can also see that there's nothing between the brackets whereas there are elements between the brackets for the second array.",4xcghb,t1_d6erz32,Branks,,Reply,1,0,1
d6eb47d,2016-08-12 04:28:01-04:00,TilBlue,,"Here's my guess:

The Chrome DevTools enumerate through the properties of an object when displaying it in the console. Since an array is an object, it will enumerate its properties (in this case, the array indices). If a property isn't enumerable, it wouldn't display it.",4xcghb,t3_4xcghb,tobyisthebest,,Comment,1,0,1
d6ecsyz,2016-08-12 06:04:15-04:00,kor321,,"To add to this, the small i in the blue box on the right side of the array means that the values were computed when you clicked the arrow on the left side of the array. So, when you first printed the array it was empty. When you clicked on the arrow, chrome checked if things were added later on and shows you the state of the array at the time you clicked the drop down arrow. ",4xcghb,t1_d6eb47d,TilBlue,,Reply,4,0,4
4x92uq,2016-08-11 13:08:55-04:00,PutridPleasure,Is it bad practice to work with datatypes that consist of less bits than the cpu's register size? Expressiveness of the code put aside.,"So im working with serialization alot where i compress loads of longs and ints down to their percentage of maxvalue and store them as a byte to the HD.

Now I was wondering if I could have less of an RAM impact if I used more bytes and shorts in my running code.
To my knowledge this would have a negative impact on my OP/s as the CPU converts all values smaller than its register size up to match it.

Is the same true for the ram? I.E slot sizes are fixed size and a byte would take up just as much space as an int? Or is there something like labeled slots that signify the datatype, so that you could fill a slot with 2 bytes, one on the first 8 bits and another on the following 8, this storing 2 values in one slot?",,,,,Submission,5,0,5
d6diznx,2016-08-11 14:01:47-04:00,drobilla,,"Memory on the stack, or in structs, is packed such that this is not the case.  If you are individually mallocing bytes, then you will waste a ton of memory, but this has little do do with hardware.  In short this answer can't really be answered generally, it depends on where the integer resides in memory (i.e. what is surrounding it).  Assuming you are using an OS, there is an entire VM layer in between the memory your program allocates and what actually happens in RAM.

You should be thinking more about packing, fragmentation, and so on: it's extremely unlikely that this kind of very low level hardware detail is relevant.  In other words, if your memory is so fragmented that you had to care if storing one byte in RAM took up more space, your code is going to be extremely slow anyway (but yes, RAM is byte addressable, so you can have one byte take up one byte).",4x92uq,t3_4x92uq,PutridPleasure,,Comment,4,0,4
d6dkzmx,2016-08-11 14:42:51-04:00,PutridPleasure,,"Thanks I believe that cleared it up!

To make sure:

 class foo
        {
            byte a:

            byte b:

            byte c:

            byte d:

        }

takes up the same space in ram as:

        class bar {
            Int32 a:
        }

however class foo would fill 4 cpu registers but bar only 1?",4x92uq,t1_d6diznx,drobilla,,Reply,2,0,2
d6dw06y,2016-08-11 20:26:00-04:00,cogman10,,"The answer is truly ""it depends"".

The usage of foo will greatly determine what eventually gets stored off in the registers.

For example, doing something like `foo.a + foo.b + foo.c + foo.d` will likely only use one (maybe two depending on architecture) register.  Further, it may only be 2->3 instructions if the compiler decides to use some sort of SIMD instructions.

For memory usage.  Yes, those two take up the same space.  However, you'll need to read up the packing rules to absolutely determine size.  Field order matters when it comes to packing (so same fields organized differently can take more/less memory).",4x92uq,t1_d6dkzmx,PutridPleasure,,Reply,3,0,3
d6drp8v,2016-08-11 17:02:22-04:00,Chappit,,It will be faster to pack the information into 32 or 64 bit words because you can't access anything less than a byte on modern PCs. So if you have a number of one bit things to send across you might as well stuff them into the bits of an int. ,4x92uq,t3_4x92uq,PutridPleasure,,Comment,2,0,2
d6eb33a,2016-08-12 04:26:17-04:00,visvis,,"The Intel x86 architecture has no problem with working on 1, 2, 4, and 8-byte values and it even has some instructions for specific operations on 1-bit and 16-byte values. There is very little reason to avoid them on x86, though it makes sense to arrange them in such a way that you don't need a lot of padding (because the compiler tries to align fields on their natural boundaries). If you put small fields next to other fields of the same size, you usually save space.

On other architectures, however, it is quite possible that working on values smaller than the machine word size requires extra operations. For example, writing a byte might require reading a machine word, masking in the byte and writing back the entire machine word. This not only requires extra instructions but also affects atomicity if you're operating on the other values stored in that machine word from multiple threads.
",4x92uq,t3_4x92uq,PutridPleasure,,Comment,2,0,2
d6ecw6d,2016-08-12 06:09:19-04:00,PutridPleasure,,"So 

Byte a:
Byte b:

Int c:
Could be better than:

Byte a:
Int c:
Byte b:
?

If you ignore that a modern compiler may make the adjustion from example 2 to 1 by itself?",4x92uq,t1_d6eb33a,visvis,,Reply,1,0,1
d6ed0cn,2016-08-12 06:15:55-04:00,visvis,,"Yes, the former is definitely better than the latter. Compiler version shouldn't matter: changing #2 to #1 would break the ABI so it's very unlikely any compiler would do that.",4x92uq,t1_d6ecw6d,PutridPleasure,,Reply,2,0,2
d6ed5q8,2016-08-12 06:24:09-04:00,PutridPleasure,,"Even in object oriented languages like c#?
I am having a hard time thinking of a case where the order in which i write my variables affects the logic of the code.

The term ABI( application binary interface) is new to me, so I don't really have a clue if my question here makes sense.",4x92uq,t1_d6ed0cn,visvis,,Reply,1,0,1
d6edg6n,2016-08-12 06:39:47-04:00,visvis,,"I'm talking about C/C++ here. I don't know how structs are organized in C# as this isn't really exposed to the programmer and it's not easy to get to the assembly code. Presumably C# could define its own ABI that might include reordering fields, but do keep in mind that C# still needs to interface with the C++-based runtime and the C-based Windows API. As such, it would seem most practical to follow the C/C++ ABI for the platform it is running on.",4x92uq,t1_d6ed5q8,PutridPleasure,,Reply,2,0,2
d6koj14,2016-08-16 23:35:59-04:00,Merad,,"C# by default automatically decides how structs should be laid out. If you want to use your structs with unmanaged code (say a C or C++ function) you're required to explicitly specify the layout, either by saying that it's sequential (like C/C++) or by using attributes to specify the byte offset of each field. ",4x92uq,t1_d6ed5q8,PutridPleasure,,Reply,2,0,2
d6koxfe,2016-08-16 23:47:05-04:00,PutridPleasure,,Thanks for the info!,4x92uq,t1_d6koj14,Merad,,Reply,1,0,1
4x774k,2016-08-11 05:29:44-04:00,I_AM_SCUBASTEVE,Looking to Build CS Skills,"Hey guys! So I know a lot of people often ask questions about what CS school to attend, how to transition into the CS field, but I have (what I believe) to be a bit of an unusual situation that I'm hoping to get some guidance on. So I've got my B.Sc's in Mechanical Engineering and Aerospace Engineering and received my MBA a few years back. I've been playing the corporate game for some time now and have been doing well... But I've always wanted to start my own business. I can obviously stay in my lane, but an opportunity has come up that I think I'd like to take advantage of.

&nbsp;


A friend of mine is a Software Engineer who is also getting burnt out by the corporate world and has a little side business working on various small projects each year - mostly creating software tailored to a particular client's needs (an example was a local construction contractor hired him to develop an automated system which would schedule and contact personnel based on the specific type of work that was required). After some discussion, we are considering expanding this business and if we get a fair number of new clients, may switch to this operation full time. We would obviously maintain our current job roles until we had more certainty that this new venture would be something that could support us and grow.

&nbsp;


Through the various jobs and research roles I've had throughout my career thus far I've had a fair amount of what I will consider ""patchwork programming"". I've had to learn a little Python, for example, but only to reach an end. I've worked quite a bit with microcontrollers and the associated programming languages with them. However, I can't help but feel that I lack a true foundation in CS. If I am to join a new venture with someone, even a friend, I insist that I be playing with a full deck; I won't be our primary developer, but I want to be able to feel comfortable with what we are working on. So, my question to you fine folks... What would you recommend as a program/path forward to get a firm foundation in CS? I would love to go back to school for four years for the whole experience, but that really isn't an option right now. I read quite a bit about OSU's online program which I'd be eligible for given my degrees but I think it might be unnecessary given my goals here (I don't really need the degree, just the skill set). Is there anything out there that offers as comprehensive a program? I know services like EDX have a class here and there but I don't think they offer anything as encompassing as what I am looking for. I want to stress that I am not necessarily looking to become an expert in one particular language of programming, but trying to build that larger knowledge base (perhaps specialize later?). If anyone can provide some advice here I'd really appreciate it. Thanks so much!",,,,,Submission,7,0,7
d6dl40c,2016-08-11 14:45:24-04:00,crookedkr,,"I don't think you want a foundation, it sounds like you want to just understand the technical details of what you are creating. 

Programming is a lot like writing. If you want to be an author it's good to have a background in great literature, know the rules and when and why to break them. If you just want to be well read you just need to be able to distinguish good writing from crap and know a few interesting plot devices. The same sounds true here, you don't want to know what a CFG is, the difference between branch prediction and cache hits,  or the complexity class of some piece of your code, you want to be able to look at your product and be able to intelligently sell it to your client: know it well enough to describe what the code accomplishes not how it works line by line. 

To do this you want to read much more code than you write. Pick some things that your partner has written and read them. Try to see where he may have made a design choice and try to think of other ways that it could have been implemented.",4x774k,t3_4x774k,I_AM_SCUBASTEVE,,Comment,3,0,3
d6d98iq,2016-08-11 10:32:32-04:00,theHiddenNigga,,"I might not be the best person to answer your question since I haven't finished school yet, but I can tell you my experiences. ""A solid foundation"" can not be built by going through a course or reading a book, but all the work you do will contribute to your knowledge. While there are things that don't depend much on the language you are using, such as most algorithms, you might not end up needing to know any algorithms if say, you decide you want to do web development. That being said what has been working for me is learning as much as I can on the way. For example: when I got to work with MCU's I tried to understand the way they work in order to not only know how to program them, but how to do so more efficiantly(to be frank the arduino platform removes all the low level fun)
TLDR: try to know/learn more than you need to do the job",4x774k,t3_4x774k,I_AM_SCUBASTEVE,,Comment,2,0,2
4x28rz,2016-08-10 09:58:14-04:00,rasharahman,I'm starting at a community college but my classes are limited,"I am trying to transfer to UC Berkely, but they suggest I take a few CS classes that I honestly cannot get anywhere around my area other than data structure class in Java.

Should I learn the basics of Java before I take the class? ",,,,,Submission,9,0,9
d6c6lft,2016-08-10 15:00:04-04:00,rfinger1337,,"Yes.

Learn as much Java as you can before class starts.  It can never hurt, and it minimizes your chances of getting lost or confused.  If you need clarification, you will know which questions to ask the teacher and you should do well on the tests as there will be a review component to each lecture.

From now until you retire, you will be learning about your chosen career, may as well get started sooner rather than later.",4x28rz,t3_4x28rz,rasharahman,,Comment,9,0,9
d6cjskd,2016-08-10 19:50:55-04:00,I-Suck-At-Games,,"Hey man, not sure if you have visited the sub before, but /r/learnprogramming would probably be a great help for you! They can point you into the right direction and help you when you have problems.",4x28rz,t3_4x28rz,rasharahman,,Comment,4,0,4
4wzw91,2016-08-09 22:08:34-04:00,indigomaster,Python best practices helper functions question,"Which one is better? more professional? 


    def add(x,y):
      return (x+y)

    class foo(bar):
      pass
     
OR

    class functions():
      def add(x,y):
        return(x+y)

    class foo(bar):
      pass

",,,,,Submission,3,0,3
d6bay2b,2016-08-09 22:32:56-04:00,james41235,,"If your helper functions don't have anything to do with data or state, then start saline functions is more appropriate.  Just put them in the appropriate module.

I have no idea what class foo is there for though...

Also, your formatting is broken.",4wzw91,t3_4wzw91,indigomaster,,Comment,2,0,2
d6bdam4,2016-08-09 23:37:21-04:00,indigomaster,,"The class foo is there to ask if I should be consistent and keeping it all inside a classes, or is it okay to have methods outside of functions.. Thanks for your response!",4wzw91,t1_d6bay2b,james41235,,Reply,1,0,1
4wyek2,2016-08-09 16:43:55-04:00,groz_v,Help to be a cool guy!,"Hello! I'm 19 and I try to study CS.  So I surround by really cool people who know all interesting things in programming and so on. So 
1) What I have to read to be good in CS: twitter, blogs and something
2) I know C++ and python. What kind of thing (app, site or other) I can do to publish it in my resume. I  don't know what people do and what young programmers can do
3) Do somebody know some really good books for Unix/Linux
Thank you!",,,,,Submission,0,0,0
d6axybt,2016-08-09 17:21:30-04:00,pballer2oo7,,"think of a problem you always have every morning you get out of bed. a problem that's just big enough to be a little annoying but you've never done anything about. fix it. 

expand your solution to a web app so you can fix it from anywhere. use TLS because this isn't amateur hour. 

as you write and improve your solution, use hosted source control and issue tracker like a professional.",4wyek2,t3_4wyek2,groz_v,,Comment,2,0,2
d6brkjb,2016-08-10 09:34:13-04:00,groz_v,,"It's really big and annoying problem for me. I'm sorry but I think your pieces of advice are generally. It's like: Just do it!
But what I should do? That all so big and I don't know where to start.",4wyek2,t1_d6axybt,pballer2oo7,,Reply,1,0,1
4wy6cn,2016-08-09 16:00:56-04:00,BasedKami-sama,what are some things i can do to bolster my application to colleges?,"I am aiming to get DA into UW's (university of Washington) CS program. I am currently a Junior, and am taking AP CS (includes competing in Microsoft's Hunt the Wumpus). I have also self studied C. What are some things you guys did to help your applications?",,,,,Submission,6,0,6
4wuhqw,2016-08-09 01:06:17-04:00,PINEAPPLE_lN_MY_ANUS,"If it was theoretically possible to RAID an infinite number of SSDs, would they eventually be faster than RAM modules?",,,,,,Submission,17,0,17
d6a2qgz,2016-08-09 02:13:37-04:00,teraflop,,"Not really. You can increase your throughput by adding additional drives and striping the data across them (e.g. using RAID0), but you can't really decrease the latency, and that's where the problem comes in.

When a CPU is executing code, it frequently needs to read a value from memory. If that address happens to be cached, great. Otherwise, it has to wait for the result before it can decide what to do next. With typical RAM, that might take a few tens of clock cycles: the CPU will make an effort to keep doing other stuff while it waits (by reordering other instructions if possible), but eventually it will have nothing left to do except sit idle until the memory request finishes. This typically accounts for 40-60% of your total CPU time, even when ""CPU usage"" is at 100%.

If you replace your RAM with an SSD, the same thing happens -- except that the read latency of an SSD is maybe 100 times worse (microseconds, instead of tens of nanoseconds). So now your CPU is spending 99.5% of its time waiting, and your overall performance goes through the floor.",4wuhqw,t3_4wuhqw,PINEAPPLE_lN_MY_ANUS,,Comment,17,0,17
d6afmkh,2016-08-09 10:59:54-04:00,lordvadr,,"This. Bus speed, bus latency, medium latency. Nothing flash based on the PCIe bus is going to compare to DRAM on the memory bus.

Plus:

> would it be possible to build a create a OS that tricks the computer into using the drives as RAM

This technology already exists (called swapping, disk paging, virtual memory, various other names), although due to the performance impact--which is severe--it's used when it's needed and not by default. Basically when a computer runs out of physical memory, it will find the least-used (this is a very complicated algorithm) chunk of physical memory, write it out to disk, and then free up that physical memory.

So there is a significant benefit to putting your swap space on a VERY fast drive, although it's not as much of a benefit as adding my physical memory.

While I've never gone out of my way to try to make a modern operating system swap, I don't see why you couldn't make it do it at least after boot up.

",4wuhqw,t1_d6a2qgz,teraflop,,Reply,1,0,1
d6aikqc,2016-08-09 12:03:23-04:00,dhobsd,,"It's also useful to point out that latency and throughput are correlated through [Little's Law](https://en.wikipedia.org/wiki/Little%27s_law). 

That the problem is related to latency kind of reveals an interesting presupposition in OP's question. Namely, with infinite anything, distance increases towards infinity, and latency increases. Given that the information density of memory on a SSD is higher than most RAM chips (at least at this time), an infinite amount of SSD would actually perform better than an infinite amount of RAM as you tended towards infinity.

I'm not sure what problem one would possibly want to solve with that, though.",4wuhqw,t1_d6a2qgz,teraflop,,Reply,1,0,1
d6a1t1l,2016-08-09 01:35:27-04:00,RoboNerdOK,,Hopefully I understand your idea correctly. The data bus to that storage is the primary bottleneck. At some point you have to move the data to and from the processor / GPU / etc. The maximum speed of transfer on that bus limits any performance gains on either side of it. ,4wuhqw,t3_4wuhqw,PINEAPPLE_lN_MY_ANUS,,Comment,5,0,5
d6a24fs,2016-08-09 01:47:57-04:00,PINEAPPLE_lN_MY_ANUS,,"Allright, but if you had the funding is it possible with todays technology to custom build all the components needed to solve the technical issues?",4wuhqw,t1_d6a1t1l,RoboNerdOK,,Reply,1,0,1
d6a5gly,2016-08-09 04:30:46-04:00,Davehig,,Sure. I'll PM you my Paypal and you can send the funding over.,4wuhqw,t1_d6a24fs,PINEAPPLE_lN_MY_ANUS,,Reply,6,0,6
d6a8cjh,2016-08-09 07:14:57-04:00,None,,That's research worth doing. ,4wuhqw,t1_d6a5gly,Davehig,,Reply,1,0,1
d6aekpd,2016-08-09 10:35:40-04:00,epakai,,"This isn't a technical issue. It's a physics issue.

This is why every computer has many levels of subsequently slower storage. Register, cache, memory, bulk storage.",4wuhqw,t1_d6a24fs,PINEAPPLE_lN_MY_ANUS,,Reply,3,0,3
d6ae3kx,2016-08-09 10:24:10-04:00,RoboNerdOK,,"Well, you could just use a RAM drive since it solves the problem of bottlenecks (to the closest extent possible) for fast storage. Of course the major drawback is that it's volatile. If you replicated the contents to nonvolatile (NV) storage by an asynchronous process it would mitigate this problem to a degree.

As for me, I would just use M.2 for an SSD drive because it's the best going solution right now for NV storage.",4wuhqw,t1_d6a24fs,PINEAPPLE_lN_MY_ANUS,,Reply,2,0,2
d6avy1g,2016-08-09 16:39:10-04:00,bit_banger,,The technology to connect SSD's directly to the CPU is coming soon. It doesn't get much better than that. Oh did I mention they will be a 1000 faster/more reliable than nand flash? http://www.computerworld.com/article/3104675/data-storage/micron-reveals-3d-xpoint-memory-quantx.html,4wuhqw,t1_d6a24fs,PINEAPPLE_lN_MY_ANUS,,Reply,2,0,2
d6axot8,2016-08-09 17:15:55-04:00,Chandon,,"It's a bit more complicated than people are saying.

With existing hardware and software, no, that won't work. Modern CPUs require RAM to boot, and existing SSDs don't pretend to be RAM well enough to fool them.

But making a processor that can boot of PCIe RAM and making  SSDs like we have that act as PCIe RAM would be pretty easy. You'd probably just need a firmware rewrite. The main problem then would be latency, as people mention. Getting a cache line (64B) from RAM takes about 50ns. Over PCIe that would be more like 250ns. Existing processors already do sequential prefetching over multiple channels. If you write your software to work with prefetching, then the SSDs-as-RAM will be faster once you have enough channels of it.

But the only reason existing SSDs can give you data that fast is because they have RAM caches. Effectively what you'd be doing is replacing your system RAM with the RAM in the SSDs, which is probably much more boring than what you're thinking. You could leave the flash memory out of your PCIe RAM devices and get the same benefits.

With the kind of flash memory in SSDs, you need some RAM somewhere to make this work, simply because the flash is designed to read much larger blocks (4-16k) than cache lines (64B), and the latency for reading a block and throwing all of it away would be too much to make up given the small size of processor caches. Probably even the L4 cache size on one of the Intel Crystal Well parts (128M) would be enough to make this work if you tweaked it to read full pages from the SSD and you had a predictable pattern for the prefetcher. On the other hand, some of the new SSDs have a gig of RAM cache in them - so in general you probably want gigs of RAM in the machine.

Another alternative is to switch from the NAND flash used in SSD to NOR flash. This lets you read individual cache lines from flash at the latency of PCIe, but you'd probably still need RAM for write caching.",4wuhqw,t3_4wuhqw,PINEAPPLE_lN_MY_ANUS,,Comment,2,0,2
d6a2xk5,2016-08-09 02:22:18-04:00,zefyear,,"There is no way, even with an infinitely fast procedure for routing IO requests.

The significant advantage of memory is not the *amount* transferred, but rather the speed at which the transfer can be completed after it has been requested (about 25ns). How much can be transferred out of memory is fairly small: everyday operations a computer uses are typically 32 or 64 bits (called the ""word size"") although some general purpose processors (Intel's flagship x86 & x86_64 among them) have facilities for transferring larger amounts up to 256 bits or from multiple regions with a single instruction.

An SSD on the other hand can transfer at about, at max, 50 microseconds, or about **2000x slower**. If you had 2000 / 4096 / 64 more hard drives you could transfer the same *amount* of data but the *speed* at which the first data was received after your computer requested it would be identical. 

Computer architecture is such that fetch and modify cycles are occurring many millions (on desktops, billions) of times a second - the latency would quickly ""overwhelm"" the advantage in bandwidth, except in special cases where the output is known ""in advance"" (which you might notice is what SSDs are used for now!)


Now, I will say that there are possibly clever workarounds that I've overlooked: Contemporary CPU design is sophisticated enough to ""predict"" which memory will be needed in advance of actually using it, even in complex cases. Also, at a machine language level, many computer programs spend a lot of time doing ""repeat this instruction until this piece of cache memory is something different"", typically needed for copying, zeroing or otherwise modifying large blocks of memory. Truly revolutionary thinking about the very structure of computer architecture (and a whole lot of user and developer buy-in) could someday permit for a completely different mechanism, perhaps making this possible.
",4wuhqw,t3_4wuhqw,PINEAPPLE_lN_MY_ANUS,,Comment,1,0,1
d6a4rb6,2016-08-09 03:51:44-04:00,TheoryOfOne,,"Would something like this technology provide the necessary bump to overcome the latency hurdle?

http://www.levyx.com/content/helium-demo",4wuhqw,t1_d6a2xk5,zefyear,,Reply,1,0,1
d6c0nbi,2016-08-10 12:59:02-04:00,rektide,,Feels to me like you are just throwing Amdahl's law and parallelization totally out the window. The interrupt steering work in NVMe should allow for very effective asynchronous completion of an absolutely colossal number of outstanding IO (2^32 outstanding per drive).,4wuhqw,t1_d6a2xk5,zefyear,,Reply,1,0,1
d6dpdkj,2016-08-11 16:13:48-04:00,zefyear,,Considering the ratio of parallel to serial processing could *only* make the system here slower.,4wuhqw,t1_d6c0nbi,rektide,,Reply,1,0,1
d6enbzc,2016-08-12 11:37:40-04:00,rektide,,"Depends on the workload, doesn't it?",4wuhqw,t1_d6dpdkj,zefyear,,Reply,1,0,1
d6aqxey,2016-08-09 14:54:55-04:00,rektide,,"The latency argument is hogwash, unless that's the fast you care about. New tech like NVMe is meant to make sure applications get their data efficiently when available. As opposed to how hard drives have always done it- a single queue, there are now up to 2^16 queues for read/write/&c requests on the drive, with the idea that interrupt steering lets each program have it's own set of queues it's waiting on, such that I/O routes easily/efficiently. Queues are also deeper (2^16 at max, vs 32).

You ought be able to keep adding SSDs.

But on most systems RAM will still be faster (throughput wise). Simply because the I/O subsystem on the processor- PCIe- is on almost all cores is not as fast as RAM. An off the cuff 50GBps of main memory would need 50 lanes of fully saturated PCIe 3.0 talking to SSDs. Intel cores top out at 40 lanes/core, I think.

You could get slow old RAM and have more PCIe throughput than ram. It may in some cases even be usable- ex, copying data between drives: DDIO &c lets PCIe devices do DMA without going through main memory, transferring into/out of processor cache directly.

Anyways, getting past this PCIe platform limit is of super high interest to many folk. NVDIMMs and Crosspoint both include attempts to fit SSD like devices where main memory normally would go. This allows for faster latency, and makes the PCIe bus no longer a limit. It's also easy to imagine custom cores that simply have more PCIe or have other I/O interfaces that make them higher throughput than main memory, if one wanted to design a new processor around existing SSDs.",4wuhqw,t3_4wuhqw,PINEAPPLE_lN_MY_ANUS,,Comment,1,0,1
4wsjri,2016-08-08 17:15:43-04:00,rgbimbochamp,Ideas around RAFT or other consensus based protocols .,"Hi, I'm a senior at college studying computer science . One of my professors got me interested into file synchronization protocols . Raft and Paxos to be precise . I found them very interesting . Our aim is to implement a project based on it . Anything from fixing a bug in its implementations to developing a data store implementing RAFT or etc .
Currently , as a takeaway problem my professor has asked me to think of a problem statement around it . Any ideas ? Need help here . My current knowledge level regarding these protocols is not enough to be able to nitpick things that function badly in this . Any ideas regarding these will do ? Thanks !",,,,,Submission,6,0,6
d6a0uhr,2016-08-09 01:00:01-04:00,Filmore,,How about a suite of doors connected over wireless where only one is allowed to be open at a time?,4wsjri,t3_4wsjri,rgbimbochamp,,Comment,1,0,1
d6ad4fy,2016-08-09 09:59:24-04:00,rgbimbochamp,,can you elaborate further ?,4wsjri,t1_d6a0uhr,Filmore,,Reply,1,0,1
d6a4ns8,2016-08-09 03:46:26-04:00,inetic,,"You could help me with [libclub](https://github.com/inetic/libclub) :) . If interested, I'm happy to explain what's happening under the hood.",4wsjri,t3_4wsjri,rgbimbochamp,,Comment,1,0,1
4wp1ki,2016-08-08 03:51:38-04:00,exo762,Treap vs AVL,What are the actual benefits or scenario when Treap (or other random tree) may be better than AVL (and other self-balancing tree)? I'm having a hard time finding such.,,,,,Submission,1,0,1
d690moo,2016-08-08 10:18:56-04:00,exo762,,It should have better average complexity for some workflows. E.g. doing series of inserts of increasing values.,4wp1ki,t3_4wp1ki,exo762,,Comment,1,0,1
4woteh,2016-08-08 02:30:35-04:00,rocknroe_v_wade,Why do phones generally tend to not get viruses?,,,,,,Submission,22,0,22
d68rj6b,2016-08-08 03:26:09-04:00,Dijit,,"Most phones operate under a sandbox model and additionally have a chain of trust to install programs (or indeed, run them).

If you were able to modify one of the programs running on, say, an iPhone: then that iPhone would no longer allow the modified binary to run, since it would no longer match the signature supplied by it's trusted source (the app store).

Virus' do exist, though usually it's malicious software pushed through the play/app stores.

The answer is really: due to the nature of being a newer system which is not completely general purpose developers were able to add provisions to the security model of mobile devices before launch.

Make it hard enough and people will target the easier platforms instead.",4woteh,t3_4woteh,rocknroe_v_wade,,Comment,24,0,24
d68zcx8,2016-08-08 09:42:54-04:00,yes_thats_right,,"I think you missed another, huge reason, and that is app stores.

Just about every application that a regular user will install on their phone will be downloaded through an App Store, meaning it has already been tested, reviewed and confirmed probably safe.",4woteh,t1_d68rj6b,Dijit,,Reply,7,0,7
d69kume,2016-08-08 17:46:20-04:00,chromaticgliss,,"I assume that's part of what they meant when they said ""chain of trust to install programs.""",4woteh,t1_d68zcx8,yes_thats_right,,Reply,2,0,2
d69mgn1,2016-08-08 18:23:27-04:00,yes_thats_right,,"I'll read it again, but I think that is a reference to applications requiring to be signed by a trusted publisher.",4woteh,t1_d69kume,chromaticgliss,,Reply,2,0,2
d694red,2016-08-08 11:58:48-04:00,1nf,,"Phones do get malware especially Android due to its more open app eco-system, the ability to side-load apps and rooting. One recent example of malware for Android is HummingBad.

However malware for phones will usually need an action from the user e.g. install an app or at least confirm the install. So-called drive-by malware is rarer.

iOS is relatively safer due to the tightly controlled market and Apple's stringent review of apps.",4woteh,t3_4woteh,rocknroe_v_wade,,Comment,2,0,2
4wiuf4,2016-08-06 21:37:46-04:00,paperweight233,"Looking for some clarification in my understanding of""bus sniffing""","[Bus sniffing](https://en.wikipedia.org/wiki/Bus_sniffing) is used to maintain cache coherency. As I understand it, each CPU cache has its own controller which monitors reads and writes which are sent over the data bus from and to main memory, respectively.

There are situations where a single address may be stored individually in separate caches; as such, there is the possibility that one cache may receive an update from a corresponding write, and bus sniffing is a method which is used to ensure that all other caches which contain the address in question will also be updated according to the new value which is written.

Say, for example, we have the address 01, which contains the value x. Caches A and B have both loaded actual cache lines previously which contained the address 01, and therefore are aware that 01's address does indeed have x written to its ""cell"". Because of this, both copies for address 01 have a flag known as ""shared"" set to 1.

Let's also say that a current write is about to be performed to address 01, in which the new value to be written to that location will by y. If a cache line from cache A is currently loaded, that contains address 01, the CPU will write to the address using that cache, and set A's ""dirty"" flag for address 01 to 1. Consequently, B's controller will be aware of this given that it monitors data which is sent across from the bus; it's aware that a write was about to take place, and since no actual data has been sent across the bus at all (else it would have gone to main memory), it changes B's flag for address 01 from ""valid"" flag from 1 to 0. Since A's value was set to dirty, its valid flag remains 1, however its *shared* flag becomes 0, given that its dirty flag was recently set. Likewise, since B's valid flag was set to 0, its shared flag also becomes 0 given that B's controller is aware that its copy of address 01 should no longer contain the value x. 

Let's say that a certain instruction is executed later on in some other subroutine which hopes to read a value from address 01. This results in a legitimate cache miss, since neither A's line or B's line contain the address 01. While the read is forced to go to main memory, and therefore is still slower than it would be when reading from the cache, both cache controllers are aware that there is a miss; in this case, the controller for cache A sets its dirty flag to 0, but maintains its ""valid"" state since there hasn't been any write to it that's changed it from its current value y; since cache B initially had an invalid entry, it remains in an invalid state, but will update its copy of address 01's value when its loaded cache line actually contains address 01.

Is my understanding of this correct, given the example I've illustrated? Any help would be appreciated :D",,,,,Submission,6,0,6
d67wa2y,2016-08-07 12:13:56-04:00,NasenSpray,,"1. unlearn the ""implementation"" in the bus sniffing article
2. read: [MESI protocol](https://en.wikipedia.org/wiki/MESI_protocol)
3. profit?",4wiuf4,t3_4wiuf4,paperweight233,,Comment,2,0,2
4whugz,2016-08-06 17:24:44-04:00,LyeInYourEye,Can we just compress all files?,"I'm noticing that a lot of software can read the contents of lossless compressed files. That is to say, VLC can play rar archives that have video in them. I assume the reason that we bothered to not compress files in the past is computers weren't fast enough to unzip them and play them without lag so everyone unzipped things. Can we just compress everything now and save space? Is there a reason to uncompress things?",,,,,Submission,6,0,6
d673lnx,2016-08-06 17:44:33-04:00,dangil,,"Zfs can use lz4 compression on disk. It is very light on the cpu

Video rar files are loseless compressed for file splitting and checksuming. Video is already compressed with visually adaptive codecs that take in consideration that we can't see all the raw video information

Today, most things are compressed in one way or the other. xlsx and docx files are zip archives of xml files as well

Every common video file is compressed

Every game uses compressed textures in different formats

You can compress executables that decompress on the fly

The linux kernel is compressed and decompressed into memory on boot

Every music you hear is also compressed

Most webservers and browsers can send and receive compressed data to each other that is compressed and decompressed on the fly

Some OSs even compress rarely accessed RAM as needed 

So most things are already compressed in one way or the other",4whugz,t3_4whugz,LyeInYourEye,,Comment,13,0,13
d67aunl,2016-08-06 21:21:20-04:00,LyeInYourEye,,This is super interesting. Thanks.,4whugz,t1_d673lnx,dangil,,Reply,2,0,2
d673vmy,2016-08-06 17:52:32-04:00,thiagobbt,,"Just to add on the ZFS thing: BTRFS, NTFS (Windows) and HFS+ (macOS) also support transparent compression.",4whugz,t1_d673lnx,dangil,,Reply,1,0,1
d674fh9,2016-08-06 18:08:21-04:00,dangil,,"As I understand, NTFS compresses file by file. Isn't that right? ",4whugz,t1_d673vmy,thiagobbt,,Reply,1,0,1
d674tpb,2016-08-06 18:19:38-04:00,thiagobbt,,I think so,4whugz,t1_d674fh9,dangil,,Reply,1,0,1
d673a56,2016-08-06 17:35:18-04:00,None,,[deleted],4whugz,t3_4whugz,LyeInYourEye,,Comment,2,0,2
d677g13,2016-08-06 19:37:11-04:00,IgnorantPlatypus,,"> We usually don't use compression file systems anymore because disk space is really cheap

But note CPUs and DRAM is faster than spinning disk and networks: depending on how a filesystem is used, and the compression ratio, compressing files may give a performance boost in addition to using less space.",4whugz,t1_d673a56,None,,Reply,3,0,3
d677eg7,2016-08-06 19:35:52-04:00,IgnorantPlatypus,,"Yes, some filesystems do attempt to compress everything, though note some compression algorithms leave the ""compressed"" version larger than the original: in this case it is usually wise to use the uncompressed version.",4whugz,t3_4whugz,LyeInYourEye,,Comment,1,0,1
d67qc0j,2016-08-07 08:52:06-04:00,gyroda,,"When you compress something  there's going to be some overhead. 

Look at this sequence:

>    AAAAAAAAAA

You could compress this by saying ""4 `A`s"".

But now look at this:

>     AABCDEFGHI

Now you can ""compress"" this by saying ""Two `A`s, then `BCDEFGHI`""  but that's *longer* than saying  `AABCDEFGHI`. 

When you compress losslessly you're looking for patterns (like the aforementioned repeated `A`s) that can be described in less space than the patterns themself or that are repeated so that you can describe it once and just refer back to it (e.g `ABCJABCDHYABCHDRTKABCJNTEABCPABC` has `ABC` often enough that you could just call it ""pattern 1""). This requires extra space though, you need to store extra information to describe the compression in order to be able to expand it again. 

Additionally, as you've said, it takes more computing power and usually that's a more valuable resource than storage space. In storage limited devices (like phones) the computing power is similarly limited and often uses another resource (battery). ",4whugz,t3_4whugz,LyeInYourEye,,Comment,1,0,1
d6a834w,2016-08-09 07:01:57-04:00,zSilverFox,,"Everyone else did a great job of explaining how many things are compressed. If you want to get theoretical not every file can be compressed.

Let S be an arbitrary string and assume you have an algorithm A that compresses any string of bits. So A(S) yields S', whose length is shorter than S. Plug in A(S'), you get another even shorter string, S''. If you repeat this, eventually your string S^n' is compressed to nothing. You can't decompress nothing and there's really no information in it. Files are ultimately strings of bits, so not all files can be compressed.",4whugz,t3_4whugz,LyeInYourEye,,Comment,1,0,1
4wblv0,2016-08-05 13:40:19-04:00,jnzq,"Weak WiFi, wonky GPS calibration in Pokemon GO","Just wanted to ask a technical question about GPS calibration in Pokemon GO. I noticed that when I got a poor connection to a wifi hotspot, my GPS calibration went berserk and my character started running all over the local vicinity even though I was stationary. It's definitely due to the poor connection, but does anyone have a network-wise explanation why this happens?",,,,,Submission,0,0,0
d65pgcu,2016-08-05 14:37:38-04:00,PastyPilgrim,,"I've never played it, but my best guess is that Pokemon Go has a lot going on under the hood between you and their servers. GPS is pretty simple: a bunch of satellites are broadcasting the time. Your GPS receiver takes those times and compares them with its own internal clock and the difference between the times is used to calculate a distance between you and the satellites. Using a couple of signals, you're able to triangulate your position in this way. So GPS itself shouldn't be a bottleneck whatsoever.

I don't know how Pokemon Go works, but I would bet that there's a constant data stream where you're feeding your location and other data to their servers. Then the servers are validating your data and responding with relevant data to play the game (like Pokemon near you, etc.). If you have a poor connection to the server, or the servers are overloaded with traffic, then suddenly the servers are receiving out dated GPS information from you as well as providing bad game data in return.

The proper way to implement this is to have the game just sending you game data in a wide area around you (or have a lot of the data computed internally based on game rules). That way, you can continue to play even if you have a poor signal because it isn't dependent on split second transmissions on a routine basis. Your phone would just update the servers if it is going to leave the area it currently has data from.

But the Pokemon Go team probably wants constant data on your location and actions so that it can include that in its game models, statistics, and logistics. As well as prevent any kind of cheating. Their way is easier to implement and has some benefits, but it's very inefficient.",4wblv0,t3_4wblv0,jnzq,,Comment,6,0,6
d65yt97,2016-08-05 18:08:42-04:00,jnzq,,"Great explanation, really in depth! Thanks!!",4wblv0,t1_d65pgcu,PastyPilgrim,,Reply,2,0,2
d663en9,2016-08-05 20:08:54-04:00,dangil,,"Most wifi base starions get tagged by your cellphone to a specific location using A-GPS in your phone. That means that both real satellite GPS and cell tower location are used to determine your position

After this association is made, the wifi tagged route is much faster and preferred over A-GPS

If you disable your wifi connection your cellphone has no choice but to fall back to A-GPS

It's even worse if you have low cell signal or few towers nearby

Besides that, Pokemon Go has to deal with the variable precision of your position. How it handles it is not known to me, but some kind of ajustment must be made as your location precision varies",4wblv0,t3_4wblv0,jnzq,,Comment,3,0,3
4w91pd,2016-08-05 02:16:15-04:00,theroyalham,Advice Needed,"*Before you read, this is a repost from /r/compsci . Just trying to get as many opinions/advice as possible.* 

Hello /r/AskComputerScience, 

I'm currently starting Junior year of my Computer Science major (actually just completed my associates today). But i feel as if I haven't learned as much as i should have so far.


**Here's what I know so far**: Programming in Java, calculations of Algorithms and Designing algorithms (Dijkstra's Algorithm, Knuth-Morris Pratt algoritm, algorithm complexity etc.), manipulating data structures, computer organization (Binary numbers, ALU's, Logic Gates, Assemblers, Compilers, but weak in this area), and a bit of knowledge in several paradigms (such as parallelism, heuristics, dynamic programming, greedy algorithms).


And thats about it. The information is very useful, and I love learning new things about anything related to the Science of computers. I also see the importance of everything i've learned.


**The problem**: I have absolutely no clue how i'd implement any of the things i've learned in the Real World. Every time i look at a real-world implementation of ANYTHING i've learned, it looks extremely difficult and out of my league. Am i speaking too soon? Will I ""mature"" and eventually and gradually learn everything i'll need to learn?


I've been told that most of the learning will come when you get a job and are knee deep in work, and that the college level of CompSci is mostly introductory and theory. This doesn't sit well with me though, I want to push forward, and get a head start. What do I need to do? The logical thing is to pick the reigns up and start teaching myself and get my own experience. I just dont know where to start.


**TL;DR** I feel incompetent with what i've learned so far in CS. Should I take action and learn more?


Looking for any tips from experienced individuals both in Computer Science and other related fields.",,,,,Submission,2,0,2
d66a00j,2016-08-05 23:15:35-04:00,impala454,,"Looks like you're doing a pretty standard CS degree (which is awesome IMHO).  Don't worry so much about the ""real world"".  I've had my CS degree for 12 years and had three different jobs.  You will learn 80% of what you need to know *on the job*.  The best thing you can do is be a good problem solver and quick to learn new things.  
edit:  Definitely get the full bachelors degree.  Most places won't give you the time of day until you get that paper in hand.",4w91pd,t3_4w91pd,theroyalham,,Comment,2,0,2
d66afj4,2016-08-05 23:28:27-04:00,theroyalham,,"> Definitely get the full bachelors degree. Most places won't give you the time of day until you get that paper in hand.

Oh absolutely! I'm currently transferring to University and will complete my Bachelors. I'm loving CS so far! I've made it halfway through.

And thank you thank you! I appreciate the advice. May I ask, are jobs hard to find when having a BS in CS? My professors tell me you'll find a job in no time with a CS degree. However, I read threads ALL THE TIME about how the industry is awful and the ""Piece of paper"" isnt good enough and no one is going to hire you and blah blah blah. I even have friends who are already working, and they found a job fairly quickly... Most of the ""complaints"" about not finding a job are on the internet, and I never notice that problem when talking to people at my school. It's always a  ""The industry is thriving"" type of conversation. ",4w91pd,t1_d66a00j,impala454,,Reply,1,0,1
d66ay2j,2016-08-05 23:44:11-04:00,impala454,,"You shouldn't have a problem finding a job.  Masters degrees help some majors, CS isn't really one of them.  Just don't expect to bank right out of school.  You'll bank once you get some experience under your belt and prove yourself to your company.  Best of luck to ya!",4w91pd,t1_d66afj4,theroyalham,,Reply,2,0,2
d69h82e,2016-08-08 16:26:24-04:00,andybmcc,,The availability of jobs really depends on the area and your specialty / experience.  There are quite a few oases of high demand and reasonable cost of living.,4w91pd,t1_d66afj4,theroyalham,,Reply,2,0,2
d69mph5,2016-08-08 18:29:00-04:00,theroyalham,,"That's reassuring. I'm willing to move to anywhere in the country after I graduate. 

Thanks!",4w91pd,t1_d69h82e,andybmcc,,Reply,1,0,1
d65i59e,2016-08-05 12:04:35-04:00,Bottled_Void,,"Computer Science will give you a toolbox full of tools to tackle problems. It doesn't so much concern itself with what those problems will be.


I suggest learning one of the Java GUIs. It brings things together much better when you can click a button and have it do what you want.",4w91pd,t3_4w91pd,theroyalham,,Comment,1,0,1
d66amys,2016-08-05 23:34:47-04:00,theroyalham,,"I did a *really* basic GUI for my Java class using the Javax.swing package which had a lot of data simulation for our class lab assignment. I agree 100%, it was actually helpful in a lot of ways.

Thank you again for your input. I appreciate it, /u/Bottled_Void",4w91pd,t1_d65i59e,Bottled_Void,,Reply,1,0,1
d652bao,2016-08-05 02:48:51-04:00,None,,[deleted],4w91pd,t3_4w91pd,theroyalham,,Comment,1,0,1
d652drs,2016-08-05 02:51:49-04:00,theroyalham,,"Thank you! And I agree... but it's not my fault. Those are all subjects I learned from class curriculum. If i took your advice i'd fail unfortunately. But I 100% agree, and thank you again!",4w91pd,t1_d652bao,None,,Reply,1,0,1
d6bs0lz,2016-08-10 09:46:17-04:00,groz_v,,"Hello! Did you do any project when you were at University? I'm third-year student now and I have same problems like /u/theroyalham.
I don't know what to do and what will be useful at job in future.",4w91pd,t1_d652bao,None,,Reply,1,0,1
4w7030,2016-08-04 17:46:00-04:00,Space_StarOrdering,Specification Document,"Hi, I am working on a minesweeper game using Java. The game is basically complete. However, I now need to make a Specification Document. 

I was wondering what this would constitute. Do I talk about my code? If so what do I need to write about?

I tried googling it, but the answers did not make sense. Was too high level, things that were like 20 pages long for big projects. 

Is Specification document the same as Requirement Specification? Where does Design documentation come in?

Any answers would be great. 

",,,,,Submission,2,0,2
d64jl0h,2016-08-04 18:04:22-04:00,IgnorantPlatypus,,Whoever is telling you you need a Specification Document is probably the best person to answer the question.,4w7030,t3_4w7030,Space_StarOrdering,,Comment,10,0,10
4w47km,2016-08-04 08:14:34-04:00,moeseth,Can anyone answer MySQL analytics problem?,,,,,,Submission,0,0,0
4w3dpd,2016-08-04 03:31:18-04:00,iddy93,looking for feedback on my masters project,"
Hello, i am a postgraduate student currently doing my Masters in IT. My research topic is to develop an algorithm that can predict the future value of multiple network speed, and automatically connect to the best network available. So, should I only design an experimnt to record network speed only (uplink and downlink speed) without considering other factors, or if i need to consider other factors, what factors do i need to consider. Grateful for any feedbacks or comments from the community.",,,,,Submission,4,0,4
d64b4dg,2016-08-04 15:04:32-04:00,visvis,,"This depends a great deal on the context:

* If you're talking about a server, you'll probably want to do load balancing, using all the networks simultaneously. I'm pretty sure plenty of people have already worked on this.
* On desktop computers, you usually only have one network available so there is little to do.
* On laptops, you almost always want to use wired if available and wifi otherwise. Moreover, they often connect to the same internet connection which is probably the bottleneck, in which case it doesn't even matter.
* On phones it's probably most interesting because there are usually different connections (wifi, 4G, ...) with different performance characteristics but also differences in factors such as cost. However, changes in performance are probably because the user is moving around, which seems too hard to predict. Moreover, connecting to a new wifi is often not seamless. because of authentication in the browser",4w3dpd,t3_4w3dpd,iddy93,,Comment,1,0,1
4w1l13,2016-08-03 19:33:18-04:00,fondaschielkgu,Specific Attacks against BX - BX Protocol Forum,,,,,,Submission,1,0,1
4vyfye,2016-08-03 09:20:51-04:00,rakup_master,Final Year Project,"I am a Computer Science Student, currently in my Fourth Year - and I want to have a *kickass* project in my resume before being a graduate from a reputed College. I have 6 months to a year to complete my project, and I want that to be interesting, challenging plus something of high value which includes some hot topics [high valued as in ranked among best of bests projects if successfully interpreted]

I am short of ideas for my projects, kinda in dilemma on which topics could make it sound really interesting.  Any suggestions??",,,,,Submission,11,0,11
d62heh3,2016-08-03 10:21:20-04:00,AlexDGr8r,,"My school does this, but you as an individual can do this too. You could reach out to a company of interest and see if they might have some sort of project for you. As a college student doing his capstone project, you wouldn't have to be paid so that's a huge incentive to a company. This could also help nail a job after this year as well.",4vyfye,t3_4vyfye,rakup_master,,Comment,3,0,3
d630rgc,2016-08-03 17:05:31-04:00,yes_thats_right,,"The biggest barrier is intellectual property and any beuracracy surrounding your relationship/employment status with the company (even if not being paid, the company may have some level of liability).",4vyfye,t1_d62heh3,AlexDGr8r,,Reply,2,0,2
d62ntmh,2016-08-03 12:41:03-04:00,v11che,,"Is there a particular field you want to look at for your career? Robotics? Games? Etc? There's a lot you can do with a comp sci degree, so being specific for your career goals will help nail that job down. Especially when they ask for sample code.",4vyfye,t3_4vyfye,rakup_master,,Comment,3,0,3
d62ugg1,2016-08-03 14:56:51-04:00,rakup_master,,Maybe some dynamics under IOT,4vyfye,t1_d62ntmh,v11che,,Reply,1,0,1
d62vvrd,2016-08-03 15:25:51-04:00,dorkus,,"You know, a big problem with IoT is that many of the devices out there are a complete MESS when it comes to security. There are lots of examples of lightbulbs providing back doors into your network.

http://arstechnica.com/security/2014/07/crypto-weakness-in-smart-led-lightbulbs-exposes-wi-fi-passwords/

How could we detect such devices on our network and secure them transparently? Would be great to have an appliance based solution it so that our parents could use them easily and keep our home networks protected.",4vyfye,t1_d62ugg1,rakup_master,,Reply,2,0,2
d62ybx8,2016-08-03 16:15:35-04:00,v11che,,"Do you have an arduano or pi? Maybe do something with that? 

There's a good list of projects here http://nevonprojects.com/iot-projects/",4vyfye,t1_d62ugg1,rakup_master,,Reply,1,0,1
d62sq4r,2016-08-03 14:21:18-04:00,jamesk93,,I'm in the same situation. I'm going into my 3rd and final year and struggling for ideas. I'd like to do something on big data / data mining but I'm struggling on what to do it on,4vyfye,t3_4vyfye,rakup_master,,Comment,3,0,3
d62y5d3,2016-08-03 16:11:52-04:00,PM_ME_UR_RACOON,,"If you want to do something with a data set, look at the UCI Machine Learning Repository.  There's a crapton of different data sets, and the ones that aren't brand new have one or more published papers on them, which means if you don't have any ideas, you can do a replication/expansion of an existing project.",4vyfye,t1_d62sq4r,jamesk93,,Reply,2,0,2
d62uf5q,2016-08-03 14:56:08-04:00,rakup_master,,"Absolutely. After part of my project is over, I had planned to go through big data and data mining. These are some crazy areas of interest for my PG. ",4vyfye,t1_d62sq4r,jamesk93,,Reply,1,0,1
d630p2h,2016-08-03 17:04:06-04:00,yes_thats_right,,"Do something with data mining and machine learning in the fields of finance or health.

This gives a lot of scope for variable complexity, will always be hot topics and are very marketable.",4vyfye,t3_4vyfye,rakup_master,,Comment,2,0,2
d62zaak,2016-08-03 16:34:59-04:00,unoriginal42,,"I did some stuff on sentiment analysis of tweets. 

Not sure you'd be able to do it now as it's harder to get a big corpus of tweets - but any sort of system for looking at large amount of user generated content is interesting. ",4vyfye,t3_4vyfye,rakup_master,,Comment,1,0,1
4vy3u9,2016-08-03 07:55:09-04:00,pickituputitdown,Audio books to for comp sci?,"Is there any audio books you would recommend for a computer science student? I know its probably too much to ask for a 'Learn C++ with your ears' but anything that could be useful or interesting would be greatly appreciated (anything on Object Orientated System design would be amazing:)


Cheers ",,,,,Submission,10,0,10
d6319ce,2016-08-03 17:16:12-04:00,JudgeGroovyman,,"Good question!  I've been looking for this for a while now and while I can't exactly say ""yes"" I can say I have found a great workaround in YouTube video lectures.  Rationale: They go into great detail and there are lots of cutting edge and interesting topics (probably even more cutting edge stuff than you would typically find in an audiobook)  and the visuals are almost never critical to get the picture.  If they do you can go back later and watch the video.  Listentoyoutube.com is a clumsy way to download the audio (you'll probably find a better way) 

Cool YouTube audio Trick on the iPhone:  when you are watching YouTube vids and then you lock your screen the audio goes off!?!  I found a Workaround: swipe up from the bottom of the screen and hit play there.  The audio will continue to play and you can lock your screen.

Here are a few good ones I've watched recently (yeah lots of uncle bob!)



The future of programming (from two different perspectives both of which talk a lot about the HISTORY)
https://m.youtube.com/watch?v=8pTEmbeENF4
https://m.youtube.com/watch?v=ecIWPzGEbFc

Object oriented stuff:
https://m.youtube.com/watch?v=o_TH-Y78tt4
https://m.youtube.com/watch?v=t86v3N4OshQ

Really fun talk about why programming is terrible (tons of great insights in this!)
https://m.youtube.com/watch?v=csyL9EC0S0c",4vy3u9,t3_4vy3u9,pickituputitdown,,Comment,4,0,4
d63fc9r,2016-08-03 22:57:00-04:00,AmaDaden,,"> Is there any audio books you would recommend for a computer science student?

Lots, but hardly any that help you with new CS concepts. I mostly use audio books to go through fun books when I'm walking around so when I do sit down to read I can focus only on the hard core CS books that simply can't be digested in an audio format. This is a list of some books I recommend. Most of these will not be any help now but you will likely thank me later. 

I'll start with CS history and biographies. They can be a fun jumping off point so that you can get a broad intro in to new topics.

* Masters of Doom. The story of the guys who made the first FPS
* Just for Fun: The Story of an Accidental Revolutionary. The story of, Linux Torvalds,  the guy who made Linux.
* Digital Gold: Bitcoin and the Inside Story of the Misfits and Millionaires Trying to Reinvent Money
* iWoz: Computer Geek to Cult Icon: How I Invented the Personal Computer, Co-Founded Apple, and Had Fun Doing It 
* I'm Feeling Lucky: The Confessions of Google Employee Number 59 
* The Master Switch: The Rise and Fall of Information Empires
*  Steve Jobs by Walter Isaacson
* The Signal and the Noise: Why So Many Predictions Fail - But Some Don't
* The Phoenix Project: A Novel About IT, DevOps, and Helping Your Business Win
* The Lean Startup: How Today's Entrepreneurs Use Continuous Innovation to Create Radically Successful Businesses

There are lots of interesting books that I consider ""Philosophy of the Internet age"" kind of things.

* No Place to Hide: Edward Snowden, the NSA, and the U.S. Surveillance State
* Wikinomics: How Mass Collaboration Changes Everything
* The Wisdom of Crowds
* Free: The Future of a Radical Price
* The Long Tail: Why the Future of Business is Selling Less of More

I would also look into books popular in CS pop culture. Like Cryptonomicon, The Hobbit, or Hitchhiker's Guide to the Galaxy. Your fellow developers are likely to talk about them. 

Finally, I would recommend you try to check out some Psychology. Development tends to involve more people problems then code problems. This is why ""The Phoenix Project"" is such a great book. Here are some of my faves. 

* Thinking, Fast and Slow. This is the bible of human irrationality. 
* Search Inside Yourself: The Unexpected Path to Achieving Success, Happiness (And World Peace). Might be strange to have a book on meditation here, but Google seems to consider valuable and I loved it so I've added it
* Willpower: Rediscovering the Greatest Human Strength. If you're as lazy as I am learning about willpower is essential
* Predictably Irrational: The Hidden Forces That Shape Our Decisions. Along with the rest of Dan Ariely's books. ""Thinking, Fast and Slow"" is more organized and has more depth but Ariely's books are much more fun and memorable. 
* Outliers: The Story of Success. This was his best, but I recommend all of  Malcolm Gladwell's books. They get lots of hate for lacking scientific rigor but I think they can still show you some valuable new points of view if you simply take them with a grain of salt.



",4vy3u9,t3_4vy3u9,pickituputitdown,,Comment,2,0,2
d64u2bg,2016-08-04 22:26:54-04:00,Teddy-Westside,,"+1 for Free: The future of a radical price. It's actually free on Audible, so you can listen to it there podcast style. ",4vy3u9,t1_d63fc9r,AmaDaden,,Reply,2,0,2
4vuxgm,2016-08-02 17:23:58-04:00,k_g_Bull,"Have an exam on Fundamentals of Networking in a few hours, came across this question that I can't answer. Help!","Exam is in a few hours, and i can't answer this question:

> Devices on LANs, MANs, and WANs use different techniques to access theNetwork. Explain why they have to use the different techniques and yet using the same techniques would simplify the protocols. [10 marks]

The main problem is that I don't know *how* to approach the question. I mean, what exactly does it *mean* to ""access the Network""? 

&nbsp;

Any guidance is highly appreciated! Thanks in advance!",,,,,Submission,2,0,2
d61lifp,2016-08-02 17:32:29-04:00,wackyvorlon,,"Well, if I get what they're driving at, mainly these networks span different physical distances. While a homogeneous network is generally preferred, technologies which are best suited for long distance connections (FDDI, etc) are usually not very feasible for smaller networks. ",4vuxgm,t3_4vuxgm,k_g_Bull,,Comment,2,0,2
d635maa,2016-08-03 18:54:55-04:00,k_g_Bull,,Thanks! this was very helpful!,4vuxgm,t1_d61lifp,wackyvorlon,,Reply,1,0,1
d66c6wo,2016-08-06 00:23:21-04:00,jephthai,,"That's a very badly written question.  Hope you have an agreeable prof :-).  I think this is what happens when they want to ask a specific question, but where being specific gives away too much of the answer.  E.g., in this case, maybe they originally wrote this, but decided it was too much of a gimme:

> Devices on LANs, MANs, and WANs use different *physical layers* to access the Network. Explain why they have to use the different *technologies* and yet using the same *physical layers* would simplify the protocols. [10 marks]

I'd guess they wanted the reader to have to make the leap from ""techniques to access the network"" to the physical connection at layer 1.  ",4vuxgm,t3_4vuxgm,k_g_Bull,,Comment,1,0,1
4vr4an,2016-08-02 02:26:45-04:00,The_Firefly,What resources can I look at to begin learning and programming AI for Crowd Simulations?,"I posted this over on /r/artificial but it didn't get any traction unfortunately, so maybe someone here can either point me to the correct subreddit or provide other helpful advice!

I'm currently a student studying visual effects and would like to extend my interest of visual effects and programming into the development of AI for crowd simulations (see [MASSIVE](https://en.wikipedia.org/wiki/MASSIVE_(software))).
The only problem I'm having is that there seems to be very little online help/readings/resources on the topic of this field as it relates to visual effects. 

If anyone has any suggestions/tips on where I could start learning how to develop my own AI to drive crowd simulations that would be amazing! Thanks!",,,,,Submission,2,0,2
d7wtms2,2016-09-21 20:31:55-04:00,knut77,,"There is very little online material specific to the fuzzy logic ai in Massive.  There is a very small community of users who mostly share brain bits through the Massive forum, which is closed except to paying customers.  I would suggest the FXPHD courses as a place to start.  This is a daunting subject area.  I have been at it for a decade and in no way do I feel like an expert.",4vr4an,t3_4vr4an,The_Firefly,,Comment,2,0,2
d7wuwd3,2016-09-21 21:03:44-04:00,The_Firefly,,"Thanks for the info! It does seem very daunting, especially with the sheer lack of information that seems to be available as you mention...",4vr4an,t1_d7wtms2,knut77,,Reply,1,0,1
d60ulco,2016-08-02 06:42:54-04:00,aldrin12,,"try asking here??

[/r/MachineLearning/](https://www.reddit.com/r/MachineLearning/)

or maybe here??

[/r/MLQuestions](https://www.reddit.com/r/MLQuestions)",4vr4an,t3_4vr4an,The_Firefly,,Comment,1,0,1
d61202p,2016-08-02 10:44:32-04:00,The_Firefly,,"I will try... Thank you! 
",4vr4an,t1_d60ulco,aldrin12,,Reply,1,0,1
4vpeyx,2016-08-01 19:17:58-04:00,NotThatICareBut,Looking for introductory reading material and some general advice,"I am going to be a sophomore in college and am currently an economics major. I am thinking about picking up computer science as a second major. I have always been interested in computers but never got around to learning how to code or dive deep into the workings of computers. I want to get a upperclassman or graduate's opinion on the feasibility of this. 
Also, I was wondering if there are any books on either the history of computers or just basic reading materials that I should read before I make the commitment on this double major. ",,,,,Submission,3,0,3
d628sdn,2016-08-03 04:40:03-04:00,Legumez,,"From someone who did econ and is making the transition towards CS, I'm not sure how helpful something on the history of computing would be. Nonetheless, [Code](https://www.amazon.com/Code-Language-Computer-Hardware-Software/dp/0735611319/ref=sr_1_1?ie=UTF8&qid=1470211852&sr=8-1&keywords=code) seems to be well regarded, if a little bit dated now. I enjoyed [this](https://www.amazon.com/Turing-Information-B-Jack-Copeland/dp/0199639795/ref=asap_bc?ie=UTF8) biography of Turing, and my algo prof recommended that I take a look at [GEB](https://www.amazon.com/G%C3%B6del-Escher-Bach-Eternal-Golden/dp/0465026567/ref=sr_1_26?ie=UTF8&qid=1470211974&sr=8-26&keywords=alan+turing) which may be a bit esoteric, depending on your preferences.

I think the real question is do you enjoy the work and thought process? Depending on your angle of approach it can also be a ton of math if that's what you're into. Overall, I think CS meshes well with econ, even if you don't decide to major in it. There are a couple obvious areas and a couple more subtle ones. You're gonna have to take some metrics, which requires programming (probably in R) and you might also need to numerically solve some optimization problems, which requires a computer (probably Matlab/octave, or python). Depending on the subfield, CS can use a lot of probability and stats, which are key for econ as well. Graph theory is a great way to model networks and comes up in econ as well. Also, ML techniques could prove to be useful in economic research.

If you have some degree of interest, I would suggest trying out your school's intro CS or intro programming and discrete math. If you're a fan of one/both, take algo.",4vpeyx,t3_4vpeyx,NotThatICareBut,,Comment,1,0,1
4vo6iw,2016-08-01 15:05:19-04:00,sprashoo,What's the term for preventing something from being triggered again if it has been triggered recently?,"I'm wondering if there's a term for this: I want to implement a check in a script i'm writing to see when it was last run - if that was less than X hours ago, it will not run again.

Is there a **name** for this concept? Was trying to come up with a name for the parameter, and I can't think of one.

lockout_period maybe?",,,,,Submission,7,0,7
d600352,2016-08-01 15:31:09-04:00,faxekondikiller,,Cooldown perhaps?,4vo6iw,t3_4vo6iw,sprashoo,,Comment,20,0,20
d5zzc57,2016-08-01 15:15:45-04:00,IgnorantPlatypus,,Throttling or rate-limiting come to mind.,4vo6iw,t3_4vo6iw,sprashoo,,Comment,12,0,12
d6003m8,2016-08-01 15:31:26-04:00,daV1980,,Rate limiting would be my preference. ,4vo6iw,t1_d5zzc57,IgnorantPlatypus,,Reply,5,0,5
d601jrd,2016-08-01 16:00:36-04:00,5225225,,"Rate limiting implies that you're allowed to do it more than once, but no more than X times with in Y amount of time.

I would personally go for cooldown, since that has more of the implication that you can only do it *once* before it's triggered.",4vo6iw,t1_d6003m8,daV1980,,Reply,6,0,6
d604w77,2016-08-01 17:09:35-04:00,daV1980,,"Cool down seems reasonable to me as well (I certainly wouldn't be upset to see it in code), but rate limiting doesn't imply more than once to me: once every three days is a rate, for example. (As is 5 times every 3 seconds, of course).",4vo6iw,t1_d601jrd,5225225,,Reply,3,0,3
d60628e,2016-08-01 17:35:02-04:00,AmaDaden,,"We use ""throttle"" for this idea. It's always clearly understood by the new guys",4vo6iw,t1_d5zzc57,IgnorantPlatypus,,Reply,1,0,1
d602bia,2016-08-01 16:15:59-04:00,bossier330,,"This could be debouncing, although in my experience, that applies to milliseconds rather than hours. ",4vo6iw,t3_4vo6iw,sprashoo,,Comment,6,0,6
d63vnmz,2016-08-04 09:35:51-04:00,targumon,,"I also thought about debouncing! But then I read OP's question again...

With debouncing you actually *delay* the triggering up to X time units after the *last* event (the common example is: don't initiate a search operation while users are still typing the characters of their keywords).

OP wants to *prevent* more triggering for X time units after the *first* event.",4vo6iw,t1_d602bia,bossier330,,Reply,1,0,1
d63wt9b,2016-08-04 10:04:54-04:00,bossier330,,"A lot of denounce implementations have an immediate flag though, effectively making the denounce work on the leading edge of the triggers. ",4vo6iw,t1_d63vnmz,targumon,,Reply,1,0,1
d5zzocf,2016-08-01 15:22:45-04:00,BonzoESC,,"""Lockout"" brings to mind password timeouts, which might be appropriate.",4vo6iw,t3_4vo6iw,sprashoo,,Comment,3,0,3
d60lyhj,2016-08-02 00:09:20-04:00,enigma_x,,"As others have mentioned, rate limiting. But there is a specific technique called back off. Coolest and my favorite is exponential back off with jitter. ",4vo6iw,t3_4vo6iw,sprashoo,,Comment,2,0,2
d60nwgy,2016-08-02 01:11:23-04:00,Filmore,,Backoff?,4vo6iw,t3_4vo6iw,sprashoo,,Comment,2,0,2
d60dkny,2016-08-01 20:35:38-04:00,None,,Guard ,4vo6iw,t3_4vo6iw,sprashoo,,Comment,1,0,1
d60i2k3,2016-08-01 22:24:41-04:00,IAmNotMyName,,I would say you are looking to blacklist your script with a TTL.,4vo6iw,t3_4vo6iw,sprashoo,,Comment,0,0,0
4vn6i1,2016-08-01 11:55:23-04:00,Tukky12,"As an incoming undergraduate to study CS, some personal thoughts and ideas on the common question of - how should I spend my summer?",,,,,,Submission,0,0,0
d5zsrd5,2016-08-01 12:58:39-04:00,esmith327,,"Honestly... Just enjoy your summer. Depending on your options/choices, it may be the last summer you spend at home with your old friends, and may be the last care-free time you have for a while. The only preparation you likely need is to get serious when school starts. ",4vn6i1,t3_4vn6i1,Tukky12,,Comment,3,0,3
d5zycsj,2016-08-01 14:55:24-04:00,None,,"I always recommend learning on your own outside of school. There's no reason to postpone learning if you're passionate about the subject. I would always get the textbooks for my programming courses early & work through them at my own pace. By the time school starts, you already have a good understanding. This especially helps ease the pressure when the end of the semester rolls around.

Or if you are already versed in a language, join a few open source projects. Eavesdrop on issue tracker discussions & wade through the code. Learn git. A history of contributing to open source projects will help you immensely when you start sending out resumes.",4vn6i1,t3_4vn6i1,Tukky12,,Comment,2,0,2
4vjhg6,2016-07-31 18:50:12-04:00,4k33m,Do modern games soffer from 'old' issues?,"Gemes like Donkey Kong suffered from [Kill Screens](http://errors.wikia.com/wiki/Kill_screen), In these games it happens because the integers that they used for various purposes reached the limit (255 on 8 bit systems) and reset to 0, causing errors. Do modern 'endless runners' e.g temple run, encounter glitches if you run too far?",,,,,Submission,3,0,3
d5z4zeu,2016-07-31 23:17:01-04:00,mgrieger,,"This doesn't have to do with terrain generation or anything, but integer overflows have messed with the economies in some multiplayer games.

For example, Diablo III had an exploit a few years ago that allowed players to make tons of in-game gold: http://minimaxir.com/2013/05/stones-of-jordan/",4vjhg6,t3_4vjhg6,4k33m,,Comment,3,0,3
d5zdjzo,2016-08-01 04:48:11-04:00,794613825,,"Exactly the same with Gandhi in Civilization. His aggressiveness was 1 by default, and befriending him lowers that by 2. Because they used an unsigned int, it looped back around and became the absolute maximum aggressive he could possibly be, leading to the trigger happy Gandhi we all know and love. ",4vjhg6,t1_d5z4zeu,mgrieger,,Reply,8,0,8
d5yxly8,2016-07-31 19:55:28-04:00,Mordrag_,,"Here's an example:
http://minecraft.gamepedia.com/Far_Lands",4vjhg6,t3_4vjhg6,4k33m,,Comment,2,0,2
d5z9gj4,2016-08-01 01:37:35-04:00,urielsalis,,It was fixed long ago,4vjhg6,t1_d5yxly8,Mordrag_,,Reply,-3,0,-3
d5zaz4r,2016-08-01 02:39:24-04:00,MOnsDaR,,Still a good example,4vjhg6,t1_d5z9gj4,urielsalis,,Reply,3,0,3
d5zd50p,2016-08-01 04:25:12-04:00,Davehig,,"Are you asking if modern games (and all other software) have bugs in them? And if developers today still make elementary mistakes? Because yes, they do.",4vjhg6,t3_4vjhg6,4k33m,,Comment,2,0,2
d5ze9uk,2016-08-01 05:27:48-04:00,4k33m,,"Did any 'endless runner' games (falling Fred, temple run, etc) behave abnormally or crash if the player reached the limit of the 'distance travelled' value (max 65,535?)",4vjhg6,t3_4vjhg6,4k33m,,Comment,1,0,1
d6899hj,2016-08-07 17:46:20-04:00,UsuallyQuiteQuiet,,"Fundamentally those issues are still present, yes. Integers can 'roll over' and reset to 0 when they get too high, among other things.

Of course games today may be using different techniques for their gameplay to alleviate potential issues. (Although these days integer values are so enormous it's rarely a problem).",4vjhg6,t3_4vjhg6,4k33m,,Comment,1,0,1
d6kn6sm,2016-08-16 22:58:46-04:00,mortalspirit,,"I think at some level, although quite insignificant today, there will always be an integer problem if the integer is capped but it depends how game breaking you are talking about. In Runescape for example, there is a max cash stack because the java system they have cannot process another number on the item. In all games there will always be limit on how many numbers can be displayed until they float off screen, but I'm sure there has to be an integer limit on most modern systems that can actually be processed. The only thing is that, that number is incredibly high and probably is very difficult to reach.",4vjhg6,t3_4vjhg6,4k33m,,Comment,1,0,1
d6ll7wf,2016-08-17 15:37:38-04:00,4k33m,,"I'm just wondering what happens when that (unreasonably high) int limit is reached. Say if you ran far enough is Temple Run.. what would happen? Crash? Counter stops but game carries on normal? Counter reset? BTW, **all** computers have a limit to how large an integer can be.",4vjhg6,t1_d6kn6sm,mortalspirit,,Reply,1,0,1
d6ppndi,2016-08-20 15:36:31-04:00,mortalspirit,,"Any of the above. I don't think there's ever a concrete answer, i think it would always dependent upon the system assuming there's no test for the maximum integer. Integer overflow would react differently in different environments. Although counter resets would probably be unlikely assuming we are talking about undefined overflow",4vjhg6,t1_d6ll7wf,4k33m,,Reply,1,0,1
4viqmk,2016-07-31 16:02:10-04:00,moeseth,How do multiprocesses and multithreading work?,"Let's say my computer is dual core, and I have a binary named ""scoobydoo"" which pulls jobs from a queue and do something with the job and then remove jobs from a queue.

What would be the differences between the two? Who would be faster to clean the jobs from the queue?

./scoobydoo &
./scoobydoo &

Will it make the job cleaned up two times faster?

What if I thread in my code to run two threads? Will it get the same effects as running ./scoobydoo & twice?

Thanks",,,,,Submission,5,0,5
d5yrx0s,2016-07-31 17:24:32-04:00,BonzoESC,,"The big difference between multiple OS processes and multiple OS threads is what's shared. Threads don't share registers (or a call stack, which is just a particular semantic for register/memory usage), but they live in the same process, and nothing stops them from interfering with each others' memory, file handles, network sockets, etc.

Processes, on the other hand, have much more strict rules about sharing. They only share memory if they opt-in, they can share file handles and sockets that they've inherited from a common parent, which makes them a bit more predictable than threads. 

The benefit to threads is that their shared memory makes coordination between them more complicated but possibly faster, and reduces duplication. The benefit to processes is that coordination can be really naïve but still correct, which will be faster for initial development, and might be less frustrating and buggy in the long run. 

Implement both and see which is faster. Depending in the task, more threads/processes than your core count may be optimal. ",4viqmk,t3_4viqmk,moeseth,,Comment,6,0,6
d5zk2st,2016-08-01 09:35:29-04:00,dxk3355,,"You know there's whole courses on this subject, a single comment on Reddit isn't going to cover the topic.",4viqmk,t3_4viqmk,moeseth,,Comment,-1,0,-1
4vhr1x,2016-07-31 12:23:23-04:00,jaimeburbano,[Help] Website to Mobile Friendly,"Hi everyone,

I have multiple clients asking for my advertising services, but their website is not mobile friendly and they need one in this time. Moreover, They do not want to rebuild everything manually.

Is there any open source code/technology/resource/startup which can help people to convert their website to mobile friendly much easier?

Thank you in advance",,,,,Submission,2,0,2
d5ywjca,2016-07-31 19:25:50-04:00,justlikestoargue,,Bootstrap is free and immensely usful.  But you do have to sift through a lot of documentation and manually implement the changes.,4vhr1x,t3_4vhr1x,jaimeburbano,,Comment,1,0,1
d60gvu5,2016-08-01 21:54:57-04:00,aaronr93,,"Any solution which may solve part of the problem will only become a bigger and more expensive problem for them in the future. It's best to build a universal (desktop + mobile) website from the ground up than to squeeze a desktop site into a ""mobile"" site.

That being said, Wordpress is probably the fastest and easiest.",4vhr1x,t3_4vhr1x,jaimeburbano,,Comment,1,0,1
4vdchr,2016-07-30 14:38:54-04:00,Junaga,Is there Firmware(Bios/uefi alternatives) that can read partitions from storage devices?,Edit: Firmware (like Bios/Uefi) that,,,,,Submission,3,0,3
d5xi59b,2016-07-30 15:58:16-04:00,ldpreload,,"You specifically don't want BIOS or UEFI? BIOS almost doesn't care about partitions at all, but UEFI can definitely read GPT partition tables and FAT filesystems.

Coreboot supports a GRUB payload, and GRUB can read a lot of partition formats and filesystems.",4vdchr,t3_4vdchr,Junaga,,Comment,2,0,2
d5xig10,2016-07-30 16:06:25-04:00,Junaga,,Wow. Even **Files** in Partitions on its own? The Firmware? Dude.. Thats crazy. Can it also read extended ones? or ntfs? May I ask. What is a Payload?,4vdchr,t1_d5xi59b,ldpreload,,Reply,1,0,1
d5xihj7,2016-07-30 16:07:35-04:00,Junaga,,And also. Thanks for the answer. :),4vdchr,t1_d5xi59b,ldpreload,,Reply,1,0,1
4vdbg1,2016-07-30 14:32:04-04:00,PM_ME_UR_SOURCECODE_,Is there a subreddit for building any kind of automated bot?,"Just curious as I would love to sub there, didn't know where else to ask.",,,,,Submission,14,0,14
d5xf1sv,2016-07-30 14:34:40-04:00,PM_ME_UR_SOURCECODE_,,Just gonna answer my own question... Just found /r/botting,4vdbg1,t3_4vdbg1,PM_ME_UR_SOURCECODE_,,Comment,10,0,10
d6bdvud,2016-08-09 23:54:57-04:00,achNichtSoWichtig,,**Offtopic but:** cool Username OP ( ͡° ͜ʖ ͡°) .,4vdbg1,t3_4vdbg1,PM_ME_UR_SOURCECODE_,,Comment,2,0,2
4vcjus,2016-07-30 11:39:55-04:00,Yourfavooreo,Question related to decision making,"Hey guys, I am revising for an exam and on reviewing past papers there this question that I am unsure of and was hoping if someone could approve of my idea on how to answer the question

Assume  we  have  two  decision  problems A and B and  someone proves  that A can be reduced to B, A <=B.  Consider the following four statements in isolation, one after the other, and say for each statement what consequence follows for the other problem.(1)  Problem A is not computable.(2)  Problem B is not computable.(3)  Problem A is computable.(4) Problem B is computable.

The idea I had was, since A <= B, in the situation where A is not computable and since B is a greater problem wouldnt that make B not computable and if B is not computable then there is a chance that A is computable but if A == B then A would have no chance of being computable and the concept of relation to each would go on for the next 3 and 4? 

Sorry, if its that simple of a question, I am just really paranoid .",,,,,Submission,1,0,1
4va6q8,2016-07-29 22:35:29-04:00,jnzq,Why Linux and Command Line?,"I've noticed a common trend among programmers that they tend to prefer using more arcane tools like Linux and things like Bash/Shell, etc. Can anyone explain? I understand these are more geared towards low-level computing, closer to the machine's assembly, but can anyone give examples of why people tend to use stuff like this?",,,,,Submission,6,0,6
d5wsih6,2016-07-29 23:34:14-04:00,TurboLazerForce,,"I used to wonder the same thing once.  The reason the command line ( and especially Linux)  is preferred is because it is easy to automate and write tools for.  Using the command line is very similar to coding, where everything is text, logic, and data.  Every tool has it's place.  IDE's are great for syntax highlighting and debugging.  However, suppose you need to test a program that takes large and varied input.  This would be relatively simple to do with some command line tools glued together with a shell script.  Or suppose you want to write a tool that goes through code and searches for a pattern.  Rather than build a complex GUI on top of this simple tool, it is much easier to run a program from the command line.  

If all you have is a hammer, everything looks like a nail. ",4va6q8,t3_4va6q8,jnzq,,Comment,20,0,20
d5wsdt2,2016-07-29 23:30:11-04:00,abathologist,,"It is very difficult, when not practically impossible, to automate complex routines via GUI (Graphical User Interface). CLIs (Command Line Interfaces) allow one to enter commands directly, and usually accompany software which is quick and easy to automate via ""scripts"" which sequence these commands.

There is no essential connection between Linux and scriptable CLIs (of which the various shells, Bash, Sh, Csh, Zsh, etc. are but one prominent example), but there is a historical connection, since the concept of a shell and the strategy of scriptable sequences to dispatch batches of commands was developed on the ancestors of UNIX, from which Linux is descended. There is also no essential sense in which shells are ""closer to the machine's assembly"" than a GUI, except that an implementation of a text based UI probably uses less intermediary code then an implementation of a graphical based UI. Otherwise, there is no reason why typing the name of an executable and pressing ""Enter"" is farther away from the machine's core operations than double clicking an icon which a mouse pointer. 

Peole tend to *nix systems over Windows because the former is elegantly designed, modular, tends to be very friendly to OSS, and is for these reasons is easy to design and use programs on.

To reiterate, the appeal of a CLI shell is that is makes automating tasks very easy, and it can be much quicker to execute commands once you know your system well enough. Instead of navigating through sequences of window metaphors and menu metaphors and toggle boxes and switch interfaces, you just type the command you want to run and hit enter.",4va6q8,t3_4va6q8,jnzq,,Comment,12,0,12
d5x29gk,2016-07-30 07:25:35-04:00,eraserman,,"Once you know it, the command line is super fast and efficient. All that pointing and clicking in a GUI feels painfully slow in comparison.",4va6q8,t3_4va6q8,jnzq,,Comment,5,0,5
d5x3phx,2016-07-30 08:41:54-04:00,TeamHelloWorld,,">prefer using more arcane tools like Linux and things like Bash/Shell, etc. Can anyone explain?

These tools aren't arcane: they aren't magical black boxes and are well documented.

>but can anyone give examples of why people tend to use stuff like this?

I don't need a gui for grep or ack commands.  ",4va6q8,t3_4va6q8,jnzq,,Comment,3,0,3
d5x8hng,2016-07-30 11:33:09-04:00,EquationTAKEN,,"I'll use Git as an example.

One thing I do very often, is 

`git checkout master`

`git fetch`

`git pull`

`git checkout -`

`git merge master`

Or something to that effect.

Now, this can also be done using the GUI tool, but what I can't do with the GUI tool, is to type `do_stuff`, and have it do all of those things in order.

What I did for this, is I created a file called `git_master`, in which I listed those commands. And I moved this file to PATH.

Now, whenever I type `git_master`, it does all of the above commands, in order.

Similarly, you can consolidate a lot of multi-command tasks. Using cronjobs you can also call them automatically at certain points of the day.

Now when we start getting into Docker, Vagrant and more elaborate development environments, there is no way in hell anyone is going to point-and-click all of this stuff, without having hours per day of overhead, and ripping their hair out.",4va6q8,t3_4va6q8,jnzq,,Comment,1,0,1
d5x8u2n,2016-07-30 11:43:19-04:00,Yourfavooreo,,"Terminal (Linux and Mac Command Line) has a multitude of commands that allow for automation of a variety of tasks, for example replacing a specific string pattern throughout a whole document. Also eventually GUI feels genuinely slow for some tasks ",4va6q8,t3_4va6q8,jnzq,,Comment,1,0,1
d5xagxn,2016-07-30 12:30:27-04:00,None,,"Because these old tools are still powerful, efficient, and useful. Old != bad. In fact, if it is super old and still super commonplace, that means they are usually really good. They stood the test of time, and are constantly being improved. 

It's like asking why do people still use ancient tools such as the hammer or pulley? We now have helicopters capable of lifting tons of weight, and huge industrial presses that could flatten a truck. Why still use the ancient tools? Because, quite simply, they are good at what they do.",4va6q8,t3_4va6q8,jnzq,,Comment,1,0,1
d5xuzim,2016-07-30 22:13:06-04:00,AmaDaden,,"* Speed. Clicking around is great when you are doing something for the first time but once you do it for the hundredth time every click counts
* Programmability. If you DO need to do something hundreds of times you can make a short script to turn what would have been 5 minutes of work in a GUI to typing a short word in the command line
* Feature rich tools. Any slightly complex thing you want to do has likely already been done so much by other people that they created tools to do it effectively. [Master a tool like Vi](http://www.tecmint.com/how-to-use-vi-and-vim-editor-in-linux/) and notepad will feel like your pushing a car uphill
* Easy and predictable feature access. Every Unix machine you get on should have some flavor of Vi installed
* Arcane = more time using a tool: more features you master. It took me FOREVER to get good enough with Vi to be as quick as I was in notepad but as I've gotten even better I quickly made that time back and then some
* Text Auto-complete. Tab auto complete on directory names means that I can see the contents of a directory far more quickly then if I navigated there
* You need to for most modern tech. Many dev tools are command line only. OS X, Android, and Chrome books are all based on the Unix design
* Didn't have to move my hand from the mouse to the keyboard and vice-versa

I'll be honest and include a few of the dumb reasons

* Showing off. ""Wow you're using Vi?!""
* Peer pressure. ""Wow you're NOT using Vi?!""

 ",4va6q8,t3_4va6q8,jnzq,,Comment,1,0,1
4v9gem,2016-07-29 19:26:25-04:00,_should_be_studying,What would you do if your child has zero interest in technology?,This may not be programming related but I recently have been thinking about this for some time and I am actually terrified of this. My dad and I have been taking a data science & R programming course together which is not anything out of the ordinary for us to do for bonding considering I am a CS major and he has been in the industry since before the personal computer. I would have never known I had any interest in pursuing a career in technology if it were not for having a software engineer as a father. I believe this mutual interest is the main reason why my dad has a closer relationship with me more so than my brothers who have clearly expressed little to no interest in the area and I cannot imagine what my dad and I would even talk about if it were not for this. This all got me thinking how would I deal if I were to have children and none of them wanted any part in what is such a large part of my life. So I was wondering if anyone else has a similar experience as mine or has faced this fear of mine?,,,,,Submission,0,0,0
d5wlt02,2016-07-29 20:09:39-04:00,Aganomnom,,"tl/dr : kids are people with interests. Embrace them for what they are and have to offer you. Don't try to make a clone of yourself!

Without meaning to sound harsh, if the only thing you have in your life that's worth talking about is tech, you need more things in your life! And I'm sure that's not the case. And any kids you might have will similarly have a range of interests, likes and dislikes. You need to find whatever areas overlap with yours.

If there's nothing at all? Try something new. Take an interest in their hobbies. Don't expect or pressure them to be interested in yours. I would look on it as an opportunity to grow yourself and learn from these new people in your life.

Basically: Treat them like a normal person. Enjoy them for who they are, not one topic they may or may not enjoy.",4v9gem,t3_4v9gem,_should_be_studying,,Comment,11,0,11
d5wnccn,2016-07-29 20:55:22-04:00,huck_cussler,,"First, I would look into effective written communication.  People like paragraph breaks for separation of ideas.

Secondly, I would not freak the fuck out about it.  Either your kids dig what you do or they don't.  I believe that whether they do or don't depends primarily on whether you instill in them, from an early age, a sense of curiosity about the world around them.",4v9gem,t3_4v9gem,_should_be_studying,,Comment,3,0,3
d5wl74i,2016-07-29 19:52:53-04:00,ciaran036,,"> if I were to have children 

I'm baffled as to why you would be concerned about such a hypothetical scenario. But you can't force children to be interested in anything, you can only encourage it or expose your children to technologies and hope that your children become interested in it. Being pushy about it could put off kids I imagine. ",4v9gem,t3_4v9gem,_should_be_studying,,Comment,2,0,2
d5wnl7w,2016-07-29 21:02:39-04:00,acromulent,,Return them and get a new one.,4v9gem,t3_4v9gem,_should_be_studying,,Comment,0,0,0
4v6xco,2016-07-29 10:47:35-04:00,I_punish_myself,help with understanding register windows,"here is the question I'm trying to understand: 
""A RISC processor has 152 total registers, with 12 designated as global registers. The 10 register windows each have 6 input registers and 6 output registers. How many local registers are in each register window set?""

so, my thinking is that 10 register windows or sets have 6 in, 6 out, and x local, then: 

10(6+6+x)=152 total registers, but when I do this I end up with 3.2 local registers. this doesn't seem right, what am I missing? 

or 12(global) + 6(input) + 6(output) + x(local) = 32 

12+6+6+x=32, x=8

would 8 local registers be right?",,,,,Submission,6,0,6
d5vytnj,2016-07-29 11:18:07-04:00,thewataru,,"Maybe 152=12+10\*(6+6+x)

Global regiaters are counted once,    the rest is splitted between 10 windows. It leaves x=2.",4v6xco,t3_4v6xco,I_punish_myself,,Comment,2,0,2
d5whpy5,2016-07-29 18:19:24-04:00,tgwozdz,,"I think you're double counting some registers. The output registers of one window will overlap the input registers of the next window. 

So you have 140 (152-12) registers to work with for your windows. The first window has 6 inputs, x local and 6 output registers. The next shares the output registers of the previous window as its Input registers and adds x more local and 6 more output registers. 

So from that we get
140=6+9(x+6)
Which solves to x=8.8888....

But that equation forgets the part where the first 6 input registers should be shared with the output registers of the last window. In which case we get
140=10(X+6)
Which gives us 8.

That is to say, each register window adds an additional X+6 registers, and shares 6 more with a previous window (which we already counted). 

Does that make sense?  Please correct me if I'm wrong. It's been a while since I've done this :). ",4v6xco,t1_d5vytnj,thewataru,,Reply,2,0,2
d5xrd0z,2016-07-30 20:22:45-04:00,I_punish_myself,,"this is helpful, thank you",4v6xco,t1_d5whpy5,tgwozdz,,Reply,1,0,1
d5xrdbj,2016-07-30 20:23:02-04:00,I_punish_myself,,thanks for the help,4v6xco,t1_d5vytnj,thewataru,,Reply,1,0,1
4v6r46,2016-07-29 10:11:50-04:00,Yourfavooreo,IEEE question,"EXTRA UNECESSARY INFO YOU CAN SKIP TO NEXT PARAGRAPH FOR QUESTION: Hello gents, You will probably see me here quite often for the next couple of days as I revise for my exam and dont understand a couple of things, regardless. My question

We consider an operation int that turns ia rational number into an integer by throwing away the non-integer part. For example, int(7.2) = 7. Consider the problem of building hardware that takes as input the 32-bits IEEE 754 representation of a rational number x and returns a binary representation of the integer int(x). Which of the four representations (1) signed magnitude , (2) one's complement, (3) two's complement and (4) excess is best suited for this problem? Give reasons for your answer
",,,,,Submission,0,0,0
d5w0lhd,2016-07-29 11:55:52-04:00,nathanpaulyoung,,">give reasons for your answer

No. This sub isn't here to help you with your homework.",4v6r46,t3_4v6r46,Yourfavooreo,,Comment,4,0,4
d5w0t6e,2016-07-29 12:00:25-04:00,lordvadr,,"So we don't really take a good view of, ""guys, here's a homework/review question, do it for me,"" type questions.  A better way to ask this question would be to throw out the question, and then throw out some of your ideas.  Tell us what you know about floating-point and what you don't know, or don't quite understand.

Tell us what you know about the various integer formats, or, more specifically, what each tries to accomplish and why.

Give us your reasoning.

This is actually a fairly simple answer.",4v6r46,t3_4v6r46,Yourfavooreo,,Comment,3,0,3
d5w1nxt,2016-07-29 12:18:45-04:00,Yourfavooreo,,"Ah sorry, I hadn't known I couldn't ask without giving ideas... people threw shade and I had no idea. I've been trying to solve it for a while and I gave up being unable to prove to myself that any of my answers were correct.

First idea was that it would be excess notation but then we need the ""dot skips"" in order to be able to calculate which decimal digits become the mantissa so that wouldnt work

Second idea and the only one I thought would be the answer is signed magnitude because it wouldnt require any extra calculations since the mantissa is stored in basically signed magnitude.",4v6r46,t1_d5w0t6e,lordvadr,,Reply,1,0,1
d5w82xp,2016-07-29 14:35:23-04:00,lordvadr,,"I would agree with signed magnitude.  The difference between the mentioned integer representations deals primarily with sign.  Specifically mechanisms to represent sign that allow for efficient manipulation, or, more specifically, simplifying ALU design.

I feel like the answer to the question is signed magnitude simply because the amount of work to do the conversion is the same either way, and lastly you have to deal with sign, which you can do by simply pulling the sign bit from the FPU register.

Although, disclaimer, admittedly, I haven't done anything academically in CS in 13 years.",4v6r46,t1_d5w1nxt,Yourfavooreo,,Reply,1,0,1
d5wz877,2016-07-30 04:14:08-04:00,Yourfavooreo,,"Thank you for your input , you've been a huge help :D <3",4v6r46,t1_d5w82xp,lordvadr,,Reply,1,0,1
d5wdyhu,2016-07-29 16:46:34-04:00,ACoderGirl,,"As the others have said, this isn't really a good place for homework questions that haven't had *any* thinking put in. But what I wanted to point out is the title. That's not a good title. IEEE is an organization that makes many standards. Your title is akin to saying you have a ""Microsoft question"" when you actually have a question about where Windows 10 stores something.

IEEE 754 is the specific standard that details the format of floating point numbers that virtually every language uses. Although this question would be better described as something like ""best way to represent an integer that was converted from an IEEE 754 float?"" (etc).",4v6r46,t3_4v6r46,Yourfavooreo,,Comment,1,0,1
d5wz7sb,2016-07-30 04:13:26-04:00,Yourfavooreo,,"Thank you so much for that bit of info, I am really having a hard time with this specific module for some reason. Thank you so much
",4v6r46,t1_d5wdyhu,ACoderGirl,,Reply,1,0,1
4v3xvc,2016-07-28 20:43:55-04:00,nShiv,How do microcontrollers run without an operating system?,"When I write for my Arduino it's basically Java. Are there specialized libraries that take the place of an OS? Could I buy a little microprocessor and be able to write for it in C or Java, or would it have to be assembly?

EDIT:
http://www.jameco.com/webapp/wcs/stores/servlet/ProductDisplay?storeId=10001&productId=35561&catalogId=10001&langId=-1&CID=GOOG&gclid=COL-xIe5l84CFRJZhgodWoANrQ something like this for example. Let's say if I make something cool with my Arduino and want to make 50 of them without spending 50 bucks each on new fully blown microcontrollers.",,,,,Submission,14,0,14
d5v9zco,2016-07-28 21:11:55-04:00,tuankiet65,,"Nope, an AVR or most microcontrollers don't have an OS because of resources constrains. They might be able to run FreeRTOS though (it's a realtime OS but it's just probably a library that you can use to add some OS functionalities to your program like multitasking and such (maybe someone could correct me here because I'm not quite knowledgeable on this subject))

Most microcontrollers only run machine code, so when you code in Java, C or assembly there are programs called compiler that translate your code to machine code. As long as there are compiler from language A to machine code you can code in A.

Arduino is pretty expensive because besides the core microcontroller there are other things that add up to the cost, so Arduino should only be used to ease prototyping. For mass manufacture, you would custom design a board that can handle your needs, while being as cheap as possible, for example:

* choose the cheapest microcontroller that can handle your program (ATmega328P could be too overwhelm for some applications)

* reduce the components to a minimal (like the Arduino allows you to connect your USB cable to upload your program, but you likely don't need it in your final products cause you only have to upload once)

* mass production and whole buying could greatly reduce the cost",4v3xvc,t3_4v3xvc,nShiv,,Comment,10,0,10
d5vbpb1,2016-07-28 21:54:49-04:00,nShiv,,thanks so much for the detailed response! Answered all my questions.,4v3xvc,t1_d5v9zco,tuankiet65,,Reply,3,0,3
d5vn5cj,2016-07-29 04:01:42-04:00,mehum,,"C or C++ are the most popular choices for ATMega328, using Atmel Studio.

Actually that's not true, the Arduino IDE is probably the most popular choice, which is a set of extensions and preprocessors for C (written in C++ I believe) designed to make hardware interfacing easier.  Once cost of using it is a big performance hit.

Because the hardware is open source, there are cheaper alternatives that use the Arduino IDE.  Have a look at the Arduino Nano or Pro Mini clones which can be had for a couple of bucks on eBay / AliExpress.  For a (very) limited production run you could make these socketable into your main board.",4v3xvc,t1_d5vbpb1,nShiv,,Reply,3,0,3
d5vo530,2016-07-29 04:55:01-04:00,tuankiet65,,"This is also a viable option (I'm using a $3 Arduino Nano clone and haven't had any problems with it), however remember that the quality could be a bit flaky (my friend bought some Nano and most of them had its diode burned out after a short time, he said that he applied too much voltage on the Vin pin though)",4v3xvc,t1_d5vn5cj,mehum,,Reply,3,0,3
d5vpeyh,2016-07-29 06:08:28-04:00,mehum,,"Yes I've had a few random hardware glitches as well, they're definitely made to a price not a specification. ",4v3xvc,t1_d5vo530,tuankiet65,,Reply,2,0,2
d5vvto1,2016-07-29 10:08:08-04:00,BonzoESC,,"An operating system isn't magic. At their core, they're a program that runs before other ones, may provide APIs that abstract hardware, and can load and schedule other programs. They cost storage space, memory, and clock cycles.

If you don't plan on running programs that need to be compatible with different kinds of computer, and really don't want to spend overhead on an OS, not using one just means you just have to write your program to talk right to the hardware (statically compiling in a library counts), not relying on the OS to clean up after you, etc.",4v3xvc,t3_4v3xvc,nShiv,,Comment,3,0,3
d5w12re,2016-07-29 12:06:03-04:00,nShiv,,"Could you explain more about how the OS ""cleans up"" after you? I'm very new to this.",4v3xvc,t1_d5vvto1,BonzoESC,,Reply,1,0,1
d5w2rcl,2016-07-29 12:42:08-04:00,BonzoESC,,"I mostly write programs for Linux and other UNIX systems (in before pedants). If I write a program that allocates a lot of memory, opens file handles, opens a bunch of listening network sockets, and then crashes, the OS will see that my process has crashed, mark that memory space as free for other processes, close (and maybe flush?) the file handles, and close the network sockets.

On an embedded system, if my program crashes, I (or a watchdog timer) have to power-cycle or otherwise reset it because there's no OS to do that for me.",4v3xvc,t1_d5w12re,nShiv,,Reply,3,0,3
d5w4apw,2016-07-29 13:14:27-04:00,nShiv,,oh okay. In an embedded system would you say that's an involved process or something that you can write a format for once and then just copy and paste (for the most part) for different projects?,4v3xvc,t1_d5w2rcl,BonzoESC,,Reply,1,0,1
d5wl5h1,2016-07-29 19:51:33-04:00,BonzoESC,,"What, not crashing? Sarcastically, just don't write the part of code that crashes.

More seriously, most of the embedded stuff I've done has had very limited resources: only one temperature sensor, only one relay controlling the heating element, only one serial port going to the Bluetooth serial module, and so forth. ",4v3xvc,t1_d5w4apw,nShiv,,Reply,1,0,1
d5vw7af,2016-07-29 10:17:41-04:00,veritasserum,,"Well ... they don't have a *desktop operating system*, but they commonly have some kind of basic executive for realtime/interrupt management.  This can vary all the way from simple, homegrown interrupt handler for an embedded device all the way up to a full realtime executive like RTOS.",4v3xvc,t3_4v3xvc,nShiv,,Comment,2,0,2
d5wp6zs,2016-07-29 21:50:24-04:00,None,,"You often don't need an operating system in a microcontroller. Many devices don't have a files, networking, multiple processes or a GUI. Also in many cases you don't want an operating system, due to resource constraints. Nowadays, more powerful microcontrollers are available, but you still might care about power consumption, especially with battery powered devices.

You can use whatever language you want, regardless of the operating system. Certain language features won't work of course. You couldn't work with files in C because they don't exist there, but you certainly can use C.

There are resource constraints though. C isn't much less efficient than assembly, but some other languages need more resources. Very limited microcontrollers are best programmed in assembler or a low level language like C. On a microcontroller with more resources, you would probably still want to use smaller versions of language interpreters, like MicroPython instead of ordinary Python.

A Z80 was used in home computers, and can run simple operating systems. You also probably wouldn't want to use it as a microcontroller now, because there are much better newer chips. They're faster and lower power, and need far less other components.

Arduinos just provide a convenient development environment for microcontrollers, most commonly Atmel ATmega chips. You can simply buy the microcontroller chip and add whatever other components you need in your application.",4v3xvc,t3_4v3xvc,nShiv,,Comment,2,0,2
d5wtun2,2016-07-30 00:18:53-04:00,nShiv,,Do you need an operating system to do networking? Also what's the simplest way to connect one of those chips to the computer? Do you know of anybody who's made an arduino style environment (physical) that could take any kind of chip? What's one resource you would recommend that'll get my hands dirty in a straight forward progressive way with this sort of development?,4v3xvc,t1_d5wp6zs,None,,Reply,2,0,2
d5xnlkv,2016-07-30 18:30:11-04:00,None,,"Networking is usually tightly integrated with an OS, but there is some stand-alone networking software for embedded systems which can be used instead. Also, [ESP8266](/r/esp8266) and similar modules provide an easy interface to WiFi. They interface via a serial port and you use simple commands to set up the connection and send and receive data.

The simplest way to interface with a computer is via a serial port. Arduino provides this. If you use a standalone microcontroller, you would probably need a USB to serial adapter, because most computers these days don't have serial ports.

/r/arduino has some useful links in the sidebar. Arduinos are good to start with because they have a lot of tutorials, libraries you can use, others' projects which are documented online, and a large community online which can help.
",4v3xvc,t1_d5wtun2,nShiv,,Reply,1,0,1
d5va8p1,2016-07-28 21:18:36-04:00,UtterlyDisposable,,"You could buy a microprocessor and program it in C, you just can't use any libraries that depend on an underlying operating system.

You would be better off buying bare equivalent Atmega chips and programming them yourself. A Z80 CPU isn't a microcontroller, and would require you to design an entire computer from the ground up, complete with RAM, ROM, etc.",4v3xvc,t3_4v3xvc,nShiv,,Comment,1,0,1
d5vspdg,2016-07-29 08:38:38-04:00,Bottled_Void,,"C really suits microcontrollers. It works the same as most computers.


You have the program flashed into memory. The processor is powered on and at 0x00000000 there is a GOTO <program_address> for the program counter.


It just carries on from there. There are plenty of libraries out there that will do your cinit and such, so otherwise it's just like writing C. Except you usually have to pay a bit more attention to setting registers.",4v3xvc,t3_4v3xvc,nShiv,,Comment,1,0,1
d65zq15,2016-08-05 18:31:57-04:00,theHiddenNigga,,"Arduinos and such have helper programs called bootloaders witch add some functionality(functions like millis) and hide some of the underlying dirtywork, also the arduino IDE sets up your atmels fuse bits(like internal settings). These things are nice for prototyping and not that nice for production
Also each ic has its own instruction set and therefore you have to read the data sheet for your ic to know how to program it, but fortunately compilers written by smart peole can let you write for example C code and then compile it to your specific boards machine code before upload. This is what Arduino IDE does when you hit the button upload.
By the way youre most likely NOT writing/running java on your 8bit microcontroller any time soon<its not worth it>",4v3xvc,t3_4v3xvc,nShiv,,Comment,1,0,1
4v28i9,2016-07-28 14:33:50-04:00,stylenoobie,How do I start?,"I'm 14 and joining highschool and everyone keeps telling me now is a great time to start programming. Last summer I did the beginner code academy course on HTML, then did JavaScript. Then this summer, I did like 5% of one course I forget the name. I haven't gotten much time to review everything and start again. 

This year I want to program a lot and learn a lot but I don't know where to pickup. I want to see other websites other than codeacademy, preferably better free ones. Also, if you know what languages I should pick up first.  I want to know other people's experiences and how they learned to code. Thanks",,,,,Submission,3,0,3
d5uvqcn,2016-07-28 15:41:10-04:00,None,,"Pick a goal and works towards creating something. Accumulating knowledge doesn't work unless you use it. Maybe make a simple website that you can use to display your work. It can stay on your computer until you learn how to host it. You can put in a drop down menu if you want or animations. It doesn't need a back end because its static. Or try gaming with unity, or android apps with android studio and java.  ",4v28i9,t3_4v28i9,stylenoobie,,Comment,4,0,4
d5uuh9m,2016-07-28 15:15:23-04:00,samjcs,,"I started the same way. What I would recommend doing would be to go to coursera and take a free college class for computer science. It will help you learn the fundamentals. As for the language pick one you like and stick with it, dont jump around. Python is pretty easy to learn and really useful in multiple areas.",4v28i9,t3_4v28i9,stylenoobie,,Comment,1,0,1
d5uvsw6,2016-07-28 15:42:33-04:00,stylenoobie,,Thank you for your advice but is there free college computer science classes for 14 year olds?,4v28i9,t1_d5uuh9m,samjcs,,Reply,1,0,1
d5uvhzi,2016-07-28 15:36:24-04:00,Coolisbetter,,"I got into programming by playing Second Life (a game) when I was younger. 
The game lets you build things from blocks, with great detail, and write scripts to program them to do different things. 
You can build and program a car, or a teleporter, or a door. 

Of course I didn't know how to write any of these scripts at first, so I would take other people's things they made which were offered for free, opened them up and studied their code, modified it, etc. 

It was very entertaining because I could actually see the results of my coding (virtually in the game). 

I personally would recommend trying out Second Life, personally tinkering with other people's code helps me learn best and there's lots to tinker with in there. ",4v28i9,t3_4v28i9,stylenoobie,,Comment,1,0,1
d5xs8me,2016-07-30 20:49:27-04:00,TheLastKantian,,"Forget websites. if you want to learn Computer Science and programming at the same time then read this book, https://www.amazon.com/Python-Programming-Introduction-Computer-Science/dp/1887902996 . Remember, READ CAREFULLY AND DO THE EXERCISES! I cannot stress that enough. Also, don't neglect building up mathematical maturity, most people end up screwing up in CS because their math skills are really weak.",4v28i9,t3_4v28i9,stylenoobie,,Comment,1,0,1
4uxzzu,2016-07-27 20:44:56-04:00,earthceltic,How would you automate fixing of typical wordpress issues?,"Imagine you owned a server and allowed all of your friends and fellow redditors to make wordpress sites on it at will (like i do). You want to make the server absolutely dummy proof and can code a backend program to that effect. Basically, you just want to ensure that all the websites on the server stay up and are efficient on the server resources at all times, regardless of what the user actually does on the site's back end. Anything from white space to garbled CSS to high page memory usage needs to be fixed without any intervention from you. 

What kinds of technologies would you be using? What kinds of ideas would you have on how to approach this goal? ",,,,,Submission,0,0,0
d5uowcg,2016-07-28 13:22:56-04:00,pballer2oo7,,"1. tell them about wordpress.com

there is no step 2.",4uxzzu,t3_4uxzzu,earthceltic,,Comment,2,0,2
d5ttlmu,2016-07-27 21:09:21-04:00,Murikk,,"Update everything, have a tough firewall, start with a custom Wordpress install for each site that has all the security issues worked out for your use case, and figure out some way that you could reboot frequently (multiple servers or just accept the downtime). Also, switch to nginx. It's easy and will save you tons of resources.

Edit: the above will not stop the sites themselves from being malicious. You'll need additional technology for that. Maybe docker?",4uxzzu,t3_4uxzzu,earthceltic,,Comment,1,0,1
d5u74pg,2016-07-28 04:28:48-04:00,singham,,"You will need machine learning and especially deep learning. 
Without machine learning, it is totally impossible.",4uxzzu,t3_4uxzzu,earthceltic,,Comment,-1,0,-1
4uxa40,2016-07-27 18:10:12-04:00,pr1nny,Going from BA biology to computer science grad school?,"So I graduated from a university with BA biology, math minor, chemistry minor. For the past year I've been tutoring college levels physics, chemistry, and calculus, and I've found I prefer physics and math to biology. I've also always loved computers, but my school didn't offer CS as a major so I never got to try it out. In school I took a lot of math, loved working with computers in my chemistry classes. I also took a semester in quantum mechanics. 

So that is my experience in math and physics. I was wondering if it's possible to switch to computer science? Before I make a big switch like that though I want to know the best way of ""testing the waters"" so I know if it's the right way for me to go. I don't know if 1) I have the capacity to do CS and 2) if I'd even want to do this for the rest of my life? Should I take classes at a community college or read books? 

Also, I took a practice gre test in quantitative reasoning at this local testing company and I scored a 162 without any formal training or studying. Does anyone have any advice for me? Sorry, I'm sure these kinds of posts come flooding in everyday. ",,,,,Submission,3,0,3
d5tmr9y,2016-07-27 18:34:32-04:00,None,,[deleted],4uxa40,t3_4uxa40,pr1nny,,Comment,2,0,2
d5tos7j,2016-07-27 19:16:48-04:00,Jerome_Eugene_Morrow,,Was also going to say this. This is the path that I took to refocus my career from biology to computer science. Bioinformatics will provide exposure to some canonical programming problems and get you used to thinking algorithmically about data. Just make sure to pick a program that will allow you to build your experience with programming.,4uxa40,t1_d5tmr9y,None,,Reply,2,0,2
d5tom8x,2016-07-27 19:13:11-04:00,pr1nny,,"Thank you for your response. 

I love biology, I'm just kind of lost right now because I have a lot of choices with what I want to further my studies in. My advisor in college (who I am still in contact with) keeps telling me I should go into bioinformatics, but I didn't know that involved CS. What I'm trying to do is find a balance between doing what I'm interested in and selecting something I can study that has a broad job market. I've been thinking about leaving biology, or at least focusing on Something that has a broader job market. I've been thinking about CS specifically because so many disciplines. My main concern is if I end up going into bioinformatics, got my masters or Ph.D., went into a job in this disciple and I ended up hating it, would it be possible to switch to something else involving CS, without going back to school? I want to keep my options broad. ",4uxa40,t1_d5tmr9y,None,,Reply,1,0,1
d5tp1gp,2016-07-27 19:22:26-04:00,Jerome_Eugene_Morrow,,"Computer science is a field where people like to see proof of ability. If you can work on creating a good GitHub portfolio or get involved in a programming project as part of a bioinformatics MS, it will translate fairly well to programming. Bioinformatics will provide a lot of statistical knowledge that many programmers are lacking, and you'll get exposure to a lot of machine learning. 

The benefit to doing something specific in CS would be if you knew what you wanted to do in the field already - for example, to work with databases or IT or web apps etc. A masters is generally not a place to explore a field in the same way undergraduate is. It's a good place if you have a good idea of what you want to get out of it when you already have the basics down. Bioinformatics would help you there by capitalizing on your existing biology knowledge while changing your trajectory toward being more involved in problems that have solutions in programming and math.",4uxa40,t1_d5tom8x,pr1nny,,Reply,1,0,1
d5tsj3s,2016-07-27 20:43:20-04:00,pr1nny,,"After talking to you two, I'm really considering bioinformatics. I was considering biophysics since that involved math and physics, but I didn't know that bioinformatics involved math and programming, which is where I'd rather focus. I love biology, but I'm better at math so I'd like to integrate it. Thanks for your input! ",4uxa40,t1_d5tp1gp,Jerome_Eugene_Morrow,,Reply,1,0,1
d5tq2p0,2016-07-27 19:45:35-04:00,None,,[deleted],4uxa40,t1_d5tom8x,pr1nny,,Reply,1,0,1
d5ts65m,2016-07-27 20:35:01-04:00,pr1nny,,"Hmmm... I didn't consider that, I'd rather not go back to school for another 4 years for just a bachelors,   Would some classes at a community college suffice? ",4uxa40,t1_d5tq2p0,None,,Reply,1,0,1
d5u2xz6,2016-07-28 01:21:14-04:00,None,,[deleted],4uxa40,t1_d5ts65m,pr1nny,,Reply,1,0,1
d5v5o1j,2016-07-28 19:23:16-04:00,pr1nny,,"Oh that's right I forgot about that part with GE... In that case it's definitely possible, and maybe worth it... Thanks! ",4uxa40,t1_d5u2xz6,None,,Reply,2,0,2
d5tvihd,2016-07-27 21:56:08-04:00,None,,[deleted],4uxa40,t1_d5tom8x,pr1nny,,Reply,1,0,1
d5u29s0,2016-07-28 00:57:56-04:00,pr1nny,,"Awesome, thank you for the advice! ",4uxa40,t1_d5tvihd,None,,Reply,1,0,1
d5uqfvx,2016-07-28 13:53:46-04:00,yendrdd,,"There's Computer Science, *the study of computation*, and Software Engineering, *""programming.""*

I recommend taking [Discrete Structures / Discrete Mathematics](https://people.eecs.berkeley.edu/~daw/teaching/cs70-s05/) at a community college, and any other undergraduate subjects your ""deficient"" in. Discrete is the foundation of what you'll see in your theoretical courses.",4uxa40,t3_4uxa40,pr1nny,,Comment,1,0,1
d5v5pyt,2016-07-28 19:24:33-04:00,pr1nny,,I was thinking about taking more math at a community college. Thanks for the suggestion! ,4uxa40,t1_d5uqfvx,yendrdd,,Reply,1,0,1
4upybs,2016-07-26 13:54:29-04:00,Dmob436,Is Linux an operating system or is it a kernel?,,,,,,Submission,8,0,8
d5rs69g,2016-07-26 14:06:44-04:00,njaard,,"Linux is a kernel. GNU/Linux is an operating system. GNU/Linux is often called simply ""Linux"". Debian, Ubuntu, Fedora are distributions of GNU/Linux, so they could be called operating systems, too.",4upybs,t3_4upybs,Dmob436,,Comment,14,0,14
d5s5e6n,2016-07-26 18:33:12-04:00,artillery129,,"Linux is a Kernel, BSD is an OS, I'm ready for the down votes.",4upybs,t3_4upybs,Dmob436,,Comment,3,0,3
d5s6ri2,2016-07-26 19:03:16-04:00,luisbg,,BSD userspace tools are part of the BSD project. Why downvote when you are right?,4upybs,t1_d5s5e6n,artillery129,,Reply,3,0,3
d5sr72d,2016-07-27 05:33:14-04:00,artillery129,,Because reddit is full of linux zealots,4upybs,t1_d5s6ri2,luisbg,,Reply,2,0,2
d5sw7v1,2016-07-27 09:12:17-04:00,Jafit,,">I'm ready for the down votes.

Linux is a hobby, OSX is an OS.",4upybs,t1_d5s5e6n,artillery129,,Reply,-1,0,-1
d5t0m19,2016-07-27 11:00:12-04:00,andybmcc,,"TIL over 97% of supercomputers are a ""hobby"".",4upybs,t1_d5sw7v1,Jafit,,Reply,8,0,8
d5t4pmo,2016-07-27 12:24:49-04:00,luisbg,,I think he was being sarcastic. Or at least I hope he was.,4upybs,t1_d5t0m19,andybmcc,,Reply,1,0,1
d5xfc40,2016-07-30 14:42:16-04:00,Junaga,,"every time i see a such a downvoted comment it feels like the author of it was killed in a hopeless battle, and lies there as a defeated loser.",4upybs,t1_d5sw7v1,Jafit,,Reply,1,0,1
d5xffq1,2016-07-30 14:44:54-04:00,Jafit,,What is comment karma for if not for using as a buffer tank to say things that will get you downvoted by the reddit hivemind?,4upybs,t1_d5xfc40,Junaga,,Reply,1,0,1
d5rsddw,2016-07-26 14:10:46-04:00,chromaticgliss,,"Short answer: It's both.

Long answer: Linux is technically the name for the kernel, while OSes sitting on top of the kernel have been called a number of things. The closest thing to a general term for these OSes I know of is ""GNU/Linux"", since most of these OSes use a body of GNU software in addition to the Linux kernel (or as they argue, the GNU ecosystem was designed to be a complete set of necessary software in an OS, and Linux filled the kernel gap). But there is some contention about whether that is a proper name. The biggest problem is not knowing where to draw the line in terms of calling something an operating system... so more accurately you have a bunch of operating systems (Ubuntu, Red Hat, SUSE) that are based on the Linux kernel, and we refer to all of those OSes in general as ""Linux"" or ""GNU/Liinux"" by synecdoche.",4upybs,t3_4upybs,Dmob436,,Comment,5,0,5
d5t1yg3,2016-07-27 11:28:48-04:00,pope4president,,"If you'd like further reading on this topic, I enjoyed reading [How Linux Works: What Every Superuser Should Know](https://www.amazon.com/How-Linux-Works-Superuser-Should/dp/1593275676).  I didn't really understand what a kernel is until I read that.",4upybs,t3_4upybs,Dmob436,,Comment,2,0,2
d5t2wq1,2016-07-27 11:48:24-04:00,trucekill,,Yes.,4upybs,t3_4upybs,Dmob436,,Comment,2,0,2
4upuqo,2016-07-26 13:35:30-04:00,roboborbobwillrobyou,How much does GPA factor in to admissions for grad school?,I have a ton of experience with startups and a full time job. I want to get my masters in CS. Any insight into the process would be appreciated. ,,,,,Submission,18,0,18
d5rtd6g,2016-07-26 14:30:36-04:00,theGentlemanInWhite,,"So you're not going in right out of college then? If so, gpa shouldn't be as important. ",4upuqo,t3_4upuqo,roboborbobwillrobyou,,Comment,7,0,7
d5rz36r,2016-07-26 16:24:24-04:00,roboborbobwillrobyou,,"It's less than a year since I've been out of college, but I plan to apply later around the 5 year mark in my career. ",4upuqo,t1_d5rtd6g,theGentlemanInWhite,,Reply,4,0,4
d5sefm8,2016-07-26 21:56:02-04:00,zz1991,,wait you're asking us to give you advice 4 years in advance?,4upuqo,t1_d5rz36r,roboborbobwillrobyou,,Reply,12,0,12
d5so5zo,2016-07-27 02:51:00-04:00,knightry,,"Hey guys, I want to buy a house in 2020. Where should I start looking today to ensure I get the best return on my investment?",4upuqo,t1_d5sefm8,zz1991,,Reply,4,0,4
d5tbz08,2016-07-27 14:48:29-04:00,None,,"North Korea, i have a feeling their market's turning around.  ",4upuqo,t1_d5so5zo,knightry,,Reply,2,0,2
d5t1med,2016-07-27 11:21:50-04:00,roboborbobwillrobyou,,Well I made that decision now in light of the pros and cons of this discussion. Initially I was planning to do it either this upcoming school year ,4upuqo,t1_d5sefm8,zz1991,,Reply,1,0,1
d5t3mgw,2016-07-27 12:02:47-04:00,dxk3355,,"Either do it now or not, people that wait never do it.",4upuqo,t1_d5t1med,roboborbobwillrobyou,,Reply,1,0,1
d5rxo9v,2016-07-26 15:56:31-04:00,anamorphism,,"as /u/theGentlemanInWhite implied, the longer you are from graduation the less it matters, but it still matters.

but obviously it depends quite a bit on the school you're applying to.

think of it as any other interview. some companies are going to throw away your resume if you don't have something they're specifically looking for. others might be more lenient and actually look at your cover letter or portfolio.

if you had anything lower than the mid 3's, it's going to throw up flags for the admissions board. some schools will simply just throw you away based purely on your gpa ... regardless of other circumstances.

you should make sure all of your written submissions are immaculate. you need to ace the gre or whatever other standardized entrance exams you take.

basically, you need to provide as much evidence as possible that you can offer something that your gpa doesn't indicate, assuming it's bad.",4upuqo,t3_4upuqo,roboborbobwillrobyou,,Comment,4,0,4
d5rz5sc,2016-07-26 16:25:50-04:00,roboborbobwillrobyou,,That makes sense. It sucks though that GPA is a factor even though I may have stellar work experience and open source projects.,4upuqo,t1_d5rxo9v,anamorphism,,Reply,1,0,1
d5stn7o,2016-07-27 07:41:43-04:00,assface,,"For MS, it matters more. For PhD, it matters less.

We will look at your transcripts and check to see whether you did well in the advanced CS classes. We also look at trends (e.g., did you do bad as a freshman and then switch majors to CS and then you did better). The trend should be that your GPA gets  better near the end of your undergrad not worse. 

We also consider where you went to undergrad. CS programs at the more well-known schools count for (slightly) more. Obviously someone with good grades from MIT/Stanford/CMU/Berkeley/Ivies count for more than somebody with the same grades from other places. This only matters if you are applying *straight* from undergrad. If you went out in the real world after graduation and worked in a CS field, then it matters less.",4upuqo,t3_4upuqo,roboborbobwillrobyou,,Comment,3,0,3
d5rtuo0,2016-07-26 14:40:11-04:00,roflberry_pwncakes,,"Why do you want to go back for your masters? 

Just curious. ",4upuqo,t3_4upuqo,roboborbobwillrobyou,,Comment,1,0,1
d5rv9sp,2016-07-26 15:08:28-04:00,KronktheKronk,,Are you a white dude?  You're a shoe in.,4upuqo,t3_4upuqo,roboborbobwillrobyou,,Comment,-13,0,-13
4uoidx,2016-07-26 09:05:50-04:00,jawnicakes,"Can't decide between new iMac or HP ENVY curved. Will be using for video editing, graphic design, and music production. My late-2013 iMac isn't doing it anymore, and I'm getting tired of OS X. Any suggestions?",,,,,,Submission,0,0,0
d5renac,2016-07-26 09:10:17-04:00,None,,[deleted],4uoidx,t3_4uoidx,jawnicakes,,Comment,2,0,2
d5rf371,2016-07-26 09:22:47-04:00,jawnicakes,,danke,4uoidx,t1_d5renac,None,,Reply,0,0,0
4ulszw,2016-07-25 20:15:39-04:00,Plazmatic,Applying to Graduate Schools for Computer Science?,"I'm an American CS major starting the process of applying to graduate school **for a masters degree**.  I'm the first in my family to even consider grad school (or even really given the option) , so I've not been able to get much help from them. 

My specific field interests in order of interest to me are 

GPGPU Algorithmic development (yes I know vague and broad) 
Computer Vision
AI
Machine Learning.

I want to do GPU research primarily and cost of school is not an issue. 

I've been told that I should go look up papers in the area I want to work with within CS and see which professors who research in the area go to which schools and what funding those professors actually get for their research. 

I'm completely overwhelmed by even trying to figure this out, what professors are good, removing non US schools from a list, or even finding papers that are relevant to my interests.   I'm not sure about this process and I was hoping people here would be able to help me out to the process of making my decision on what schools I should apply to. ",,,,,Submission,6,0,6
d5qzycv,2016-07-25 23:21:30-04:00,Chandon,,"For a masters, professors and their research matter quite a bit less than for a PhD. Find a school with a good reputation for CS that offers graduate classes in your area of interest and where the professor who teaches them has a research lab with a web site.

A masters program is mostly just an excuse to take two extra years of electives, with the opportunity of meeting professors and maybe doing a masters thesis with them (which is basically just a directed study with extra writing).",4ulszw,t3_4ulszw,Plazmatic,,Comment,6,0,6
d5qutz2,2016-07-25 21:34:46-04:00,worst,,Grad school for a master's or PhD?,4ulszw,t3_4ulszw,Plazmatic,,Comment,3,0,3
d5qvswa,2016-07-25 21:55:13-04:00,Plazmatic,,"Masters
",4ulszw,t1_d5qutz2,worst,,Reply,2,0,2
d5r69pi,2016-07-26 02:41:36-04:00,worst,,"As /u/chandon said, for a masters, the school is overall more important than particular professors.

A masters is, generally speaking, not about research: it's more about classes and such.

That said, I would like to caution you that there isn't really such a thing as ""GPU research."" There are lots of areas where GPUs are used (e.g., a colleague of mine has a paper on using GPUs for packet classification), but there is no ""ACM conference on GPUs.""

Regardless, if you are looking at a masters, you should probably look more at the school than individual professors. A PhD is a different story: maybe something you will consider after you finish your masters :)",4ulszw,t1_d5qvswa,Plazmatic,,Reply,2,0,2
d5rdshj,2016-07-26 08:43:36-04:00,Plazmatic,,">but there is no ""ACM conference on GPUs.""

isn't ACM SIGGRAPH close? http://www.siggraph.org/ 

",4ulszw,t1_d5r69pi,worst,,Reply,1,0,1
d5re8hd,2016-07-26 08:57:48-04:00,east_lisp_junk,,"Not sure I'd expect to see much GPGPU stuff at graphics conference. I think you'll have better luck with PLDI, PPoPP, ASPLOS.",4ulszw,t1_d5rdshj,Plazmatic,,Reply,3,0,3
d5s8qyb,2016-07-26 19:48:23-04:00,Plazmatic,,PPoPP seems nearly exactly what I'm looking for interms of a parallel conference. ,4ulszw,t1_d5re8hd,east_lisp_junk,,Reply,1,0,1
d5rnyht,2016-07-26 12:41:47-04:00,worst,,"SIGGRAPH is graphics, not GPUs :)

I'm not in that community, but while I'm sure they use GPUs, the focus of the venue is definitely not ""GPU research"".

I'd almost wager that the ML guys are publishing more papers that exploit GPUs in an interesting way lately than the SIGGRAPH community.",4ulszw,t1_d5rdshj,Plazmatic,,Reply,1,0,1
d5s8pcm,2016-07-26 19:47:21-04:00,Plazmatic,,"I simply didn't know of anything that directly correlated to GPUs, AMD and NVidia are constantly posturing at SIGGRAPH and all algorithms I've seen have been convolutions, shader pipeline or otherwise GPU algorithms to perform what they want.  PPoPP seems to be more what I'm looking for though. ",4ulszw,t1_d5rnyht,worst,,Reply,1,0,1
4ul1yv,2016-07-25 17:32:29-04:00,BasedKami-sama,Multidimensional Arrays,"Language: C     
    
Say that we have a Two-dimensional array with all its values already set. If i want to get one of its values, normally, I would say (for an array named ""Array"": *x = Array[3][4]*. However, i've seen a value of the array like: *x = Array[1]*. I don't understand what this means. Thanks in advance for the help.",,,,,Submission,2,0,2
d5qkehc,2016-07-25 17:45:18-04:00,themeaningofhaste,,"The best way to think about it is as an array of arrays. That is, you could have:

x = [[a0,a1,...,a9],[b0,b1,...,b9],...[f0,f1,...,f9]]

Then, x[3][4] could be re-written as (x[3])[4] = ([d0,d1,...,d9])[4] = d4. You're taking the fourth element (starting your count with the zeroth) of the third element of x, where the third element of x is the array is the array [d0,d1,...,d9].",4ul1yv,t3_4ul1yv,BasedKami-sama,,Comment,2,0,2
d5qntj7,2016-07-25 19:01:01-04:00,SmoothB1983,,"In C you really should be using both indices to retrieve or set from a multidimensional array. Doing this is VERY bad form since now you are making it harder to read the program and thus catch bugs.

In assembly code you'll see multidimensional arrays being flattened, and it is a common pattern there. Otherwise you really shouldn't be seeing this (in well written code) except for some obscure edge cases where it might make sense.",4ul1yv,t1_d5qkehc,themeaningofhaste,,Reply,0,0,0
d5rdzii,2016-07-26 08:49:54-04:00,yes_thats_right,,If you want to retrieve or set the entire inner array then you wouldn't use both indices. This is quite common.,4ul1yv,t1_d5qntj7,SmoothB1983,,Reply,2,0,2
d5r6twu,2016-07-26 03:06:43-04:00,xiongchiamiov,,In what language? Can you show an example?,4ul1yv,t3_4ul1yv,BasedKami-sama,,Comment,1,0,1
d5rkjuy,2016-07-26 11:30:08-04:00,BasedKami-sama,,"Uh, I did both already",4ul1yv,t1_d5r6twu,xiongchiamiov,,Reply,3,0,3
d5re5fa,2016-07-26 08:55:10-04:00,yes_thats_right,,"Imagine an Array as a bucket of apples.

An Array of Arrays is then like a big bucket with small buckets of apples inside it.

If you want to find or place an apple, you need to access them by looking inside your big bucket and finding a small buck and then looking inside your small bucket for the apple. This is Array [x][y].

However sometimes you dont want to find an apple but instead you want to find a whole small bucket of apples. This is Array [x]",4ul1yv,t3_4ul1yv,BasedKami-sama,,Comment,1,0,1
4uj0ji,2016-07-25 10:59:36-04:00,ginabot,Difference between Ford Fulkerson and Edmonds Karp?,"hi, can someone please explain me the difference between Ford Fulkerson and Edmonds Karp algorithm? both calculate the max flow of a graph, and EK is an implementation of FF, also EK is using a BFS to find the max flow.. but doesnt FF  use it too? ",,,,,Submission,2,0,2
d5q3r5a,2016-07-25 11:57:07-04:00,SuprDewd,,"The Ford-Fulkerson algorithm doesn't specify how an augmenting path should be found. As is stated on Wikipedia^[[1]](https://en.wikipedia.org/wiki/Ford%E2%80%93Fulkerson_algorithm)

> The path in step 2 can be found with for example a breadth-first search or a depth-first search in {\displaystyle G_{f}(V,E_{f})} G_{f}(V,E_{f}). If you use the former, the algorithm is called Edmonds–Karp.

There are cases where using depth-first search results in exponential time complexity, but using breadth-first search guarantees that the running time is polynomial.",4uj0ji,t3_4uj0ji,ginabot,,Comment,7,0,7
d5qcmex,2016-07-25 15:03:23-04:00,ginabot,,"got it, thanks! ",4uj0ji,t1_d5q3r5a,SuprDewd,,Reply,2,0,2
d5qauzf,2016-07-25 14:22:23-04:00,Shinroo,,"Hey, as the others in the thread have said, EK uses BFS whereas you can use either DFS or BFS for FF. 

(Bist du vielleicht Student der TU-Berlin?)",4uj0ji,t3_4uj0ji,ginabot,,Comment,2,0,2
d5qclgo,2016-07-25 15:02:47-04:00,ginabot,,"hey, ja :) bereite mich für die AlgoDat Klausur morgen :)",4uj0ji,t1_d5qauzf,Shinroo,,Reply,2,0,2
d5qeg9z,2016-07-25 15:41:30-04:00,Shinroo,,"Hab so gedacht, mache das gleiche :D viel Erfolg morgen!",4uj0ji,t1_d5qclgo,ginabot,,Reply,1,0,1
d5qeqze,2016-07-25 15:47:38-04:00,ginabot,,"hehe, danke! dir auch! :) ich finde es aber ziemlich kool, das wir die Open exam version machen :) so konnte ich mir viele  nützliche Notizen machen, die ich dann morgen mitnehme :) auf jeden Fall, viel Erfolg! :)",4uj0ji,t1_d5qeg9z,Shinroo,,Reply,1,0,1
4uafca,2016-07-23 18:27:47-04:00,TromboneEngineer,What software should I be using to go between webpages with large data sets?,"I am working on a project that relies on taking text from a document on my computer, searching a webpage with that text, clicking on one link on the resulting webpage to download a file, uploading that file to another website, clicking a button on the webpage, and downloading the resulting document on the final webpage. This continues for several hundred iterations and takes a long time to do by hand. What software should I consult to operate links and downloads on webpages?

Sorry if this is a googlable question, but I wasn't sure what keywords would lead me to software that would be useful for this and did not find any help on google.",,,,,Submission,3,0,3
d5o6jnl,2016-07-23 20:03:40-04:00,None,,[deleted],4uafca,t3_4uafca,TromboneEngineer,,Comment,2,0,2
d5o6r9r,2016-07-23 20:10:05-04:00,TromboneEngineer,,"I'm actually working on a bioinformatics project that uses several online tools and databases. I know some python, but what keywords should I search within python tutorials to take strings on webpage search bars and select search, upload, and download buttons?",4uafca,t1_d5o6jnl,None,,Reply,1,0,1
d5p2e7n,2016-07-24 15:52:53-04:00,ReinH,,"Specifically, what you are asking for is known as ""scraping"" or ""web crawling"". A popular python library for this is [scrapy](http://scrapy.org/).",4uafca,t1_d5o6r9r,TromboneEngineer,,Reply,3,0,3
d5o7dtg,2016-07-23 20:28:50-04:00,None,,"I am not sure whether or not I understand entirely what you need, but you can use the Python [requests](http://docs.python-requests.org/en/master/) and [BeautifulSoup](https://www.crummy.com/software/BeautifulSoup/bs4/doc/) libraries to request and send data to websites and parse HTML. ",4uafca,t3_4uafca,TromboneEngineer,,Comment,2,0,2
4u9rfl,2016-07-23 15:49:11-04:00,fullouterjoin,[ASK] What is a good language for prototyping parallel and concurrent algorithms?,"I'd like to play around implementing different multithreaded, parallel and concurrent algorithms in a high level expressive way. Java 8 seems like a great choice since it supports so many different primitives, from low level locking to [fork/join](http://homes.cs.washington.edu/~djg/teachingMaterials/grossmanSPAC_forkJoinFramework.html) and [java.util.concurrent.atomic](https://docs.oracle.com/javase/8/docs/api/java/util/concurrent/atomic/package-summary.html).  The goal is to explore different techniques, not necessarily have the lowest wall-clock time, so I am not considering C/C++. The only downside of Java and in favor of Rust is the difficultly in accessing [SIMD intrinsics](http://huonw.github.io/blog/2015/08/simd-in-rust/) in Java.

* Java 8
* Clojure
* Rust https://www.rust-lang.org/en-US/
* Manticore http://manticore.cs.uchicago.edu/
* ??? your favorite language
",,,,,Submission,5,0,5
d5nzbuq,2016-07-23 16:34:40-04:00,None,,[deleted],4u9rfl,t3_4u9rfl,fullouterjoin,,Comment,4,0,4
d5o1d8t,2016-07-23 17:32:55-04:00,fullouterjoin,,"Thanks, checking it out now.",4u9rfl,t1_d5nzbuq,None,,Reply,1,0,1
d5oo0y3,2016-07-24 08:30:06-04:00,BonzoESC,,"A friend of mine is researching a language for building distributed data structures and algorithms, and building it in Erlang: http://lasp-lang.org",4u9rfl,t3_4u9rfl,fullouterjoin,,Comment,3,0,3
d5wdrf8,2016-07-29 16:41:56-04:00,Probably_Napping,,I just did this in VBscript with a C++ API we built a few months ago. One of our (my companies) projects was written entirely in VBS and it needed concurrent and sequential features. I do not recommend this. Do not do it this way. I still hate myself for the pain i willingly went through. ,4u9rfl,t3_4u9rfl,fullouterjoin,,Comment,2,0,2
d5o55sx,2016-07-23 19:23:16-04:00,etagawesome,,"[deleted]  
 ^^^^^^^^^^^^^^^^0.0902 
 > [What is this?](https://pastebin.com/64GuVi2F/11247)",4u9rfl,t3_4u9rfl,fullouterjoin,,Comment,1,0,1
4u3wdh,2016-07-22 13:25:07-04:00,AbsoluteZenTrash,[HELP] WSTRING new line \r\n problem,"Hi all, 
I'm working on a [basic print out function](http://imgur.com/a/eBSuI) that I will eventually write to a .csv file. My problem at the moment is that even after I add the ""\r\n"" characters, [the printout](http://imgur.com/a/1Cdpk) merely prints out the characters '\r\n' rather than a new line like I want it to?!? If anyone could help me that'd be great! Thanks, coding.",,,,,Submission,0,0,0
d5mozwp,2016-07-22 14:45:35-04:00,kboy101222,,"You need to use \\n and \\r instead of /n or /r 

Example (in python): print (""Hello \\nWorld"")

Output:

Hello 

World 

--- 
Example 2: print (""Hello /nWorld"")

Output: Hello /nWorld",4u3wdh,t3_4u3wdh,AbsoluteZenTrash,,Comment,6,0,6
d5mneeg,2016-07-22 14:12:30-04:00,PM_ME_UR_RACOON,,I think your slashes are the wrong ones.,4u3wdh,t3_4u3wdh,AbsoluteZenTrash,,Comment,3,0,3
d5n7xep,2016-07-22 22:48:12-04:00,darthandroid,,"There's a difference between `\r\n` and `/r/n`. You **MUST** use the former, the latter will do nothing. You have the latter in your screenshots.",4u3wdh,t3_4u3wdh,AbsoluteZenTrash,,Comment,2,0,2
d5n9koe,2016-07-22 23:41:44-04:00,revvupthosefryers,,Slashes are backwards. ,4u3wdh,t3_4u3wdh,AbsoluteZenTrash,,Comment,2,0,2
4tmg5d,2016-07-19 13:47:58-04:00,robjtede,What makes downloading SSL certificates over plain HTTP safe?,"Using Little Snitch on OS X I noticed the trust daemon requesting SSL/TLS certificates over HTTP?

It is because of the public/private key nature?

What's stopping a MITM with the certificate?",,,,,Submission,9,0,9
d5ihvxr,2016-07-19 14:46:32-04:00,X7123M3-256,,"In order to for an attacker to carry out an man in the middle attack they need to send the client their public key instead of that belonging to the server, so they can read the request before passing it on to the original destination.

However, the certificate that the browser requests from the server is itself signed by a certificate authority. The certificate authorities are trusted to verify that the person requesting a certificate actually controls the domain they want it for. The browser has the public keys belonging to these trusted CAs built in, so it can verify that the certificate requested from the website was actually signed by the organization it claims to be.

If the certificate recieved isn't signed by one of these trusted keys, then the browser will show a warning to the user saying that it cannot verify that the certificate is valid and the connection might be being intercepted. The user can choose to ignore the warning, but then the security of the connection can't be guaranteed.
",4tmg5d,t3_4tmg5d,robjtede,,Comment,13,0,13
d5ikese,2016-07-19 15:38:53-04:00,maq0r,,"Great explanation with a small caveat:

This does not stop rogue CAs from issuing certificates that can be used for MiTM attacks. For example, Symantec has been getting a lot of flak because they had issue a google certificate that wasn't requested by Google. 

Other companies require these sort of 'mitm' certificates i.e Bluecoat. ",4tmg5d,t1_d5ihvxr,X7123M3-256,,Reply,7,0,7
d5ihdoc,2016-07-19 14:36:07-04:00,DoItForMom,,Once a certificate has been retrieved it must be verified. This is done by a) checking that the certificate itself is valid  b) building a certificate chain that has some parent certificate that has been issued by an issuer you trust. ,4tmg5d,t3_4tmg5d,robjtede,,Comment,6,0,6
d5r9npj,2016-07-26 05:36:34-04:00,JasonParm,,"SSL certificate works on authentication and the public and private keys are exchanged between the receiver and the sender. The keys used in this exchange needs to be verified that assures both parties that the person to whom they are communicating are actual as they anticipated. SSL certificate encrypts the travelling message with 256-bit strong encryption.
 
The browser itself has root certificates/chain of trust to recognize the CA certificate so there is no question of browser warning of the untrusted certificate. Browsers can easily verify that the request of a certificate from the website is signed with reputed CA.

In that case, the man-in-middle attack would not be possible as such, attack intercept the travelling communication and plays an intermediary between the sender and the receiver of the information and SSL makes it difficult to run a man-in-middle attack.
",4tmg5d,t3_4tmg5d,robjtede,,Comment,1,0,1
4tlhq7,2016-07-19 10:48:35-04:00,Amj161,"I'm going into my freshman year for Computer Science, is OSX really better than windows?","Hi all,

I've read throughout Reddit that a lot of computer science majors prefer Macs due to the UNIX based operating system for programming. I'm looking into buying a laptop for college (budget really isn't an issue) and I was wondering if the people here thought that having a Mac was really easier/better than having a Windows laptop or what (if I got a Mac I'd probably just dual boot Windows and OSX anyways tho)",,,,,Submission,4,0,4
d5i7cbd,2016-07-19 11:01:06-04:00,thatguywhorows,,"Developing on *nix systems is far easier than any Microsoft system I've dealt with.  

But if youre thinking about dual booting windows anyway, save a little coin and go for a less expensive Windows laptop and dual boot linux(or use a vm). ",4tlhq7,t3_4tlhq7,Amj161,,Comment,14,0,14
d5i91qu,2016-07-19 11:39:44-04:00,Amj161,,So really it would make more sense to buy a Windows laptop and dual boot Linux instead of buying a Mac and dual booting Windows?,4tlhq7,t1_d5i7cbd,thatguywhorows,,Reply,3,0,3
d5i97cf,2016-07-19 11:43:12-04:00,thatguywhorows,,"It can definitely cost less. That being said, Mac's are unbeatable quality, but you get what you pay for. ",4tlhq7,t1_d5i91qu,Amj161,,Reply,5,0,5
d5ibul9,2016-07-19 12:40:00-04:00,Amj161,,"Cost isn't really an issue, my parents said they'd buy my laptop for college and they both are biased towards Apple and want me to get a Mac. I want a laptop that will last all four years of college and knowing the quality of Macs versus the quality of the average windows laptop I'm kinda thinking towards Mac myself. 

However is the Mac OS easier to use and develop for than Linux? I'm running Ubuntu and Arch on my crappy laptop but I'm not really that experienced with Linux. ",4tlhq7,t1_d5i97cf,thatguywhorows,,Reply,2,0,2
d5idke4,2016-07-19 13:16:17-04:00,thatguywhorows,,"Bravo for attempting Arch as a greenhorn. 

OSX, in my opinion, is still harder to develop on than linux (I lean towards ubuntu wrapped in GNOME), but that depends on what you're developing. For native applications, you develop on the platform that the app will run on. For web dev, who cares. All platforms have that ability.

With your parents buying the machine, the only real reason to go Windows is for gaming. I would recommend the Razer Blade for gaming laptops since they're the only computer I've seen that comes close to matching Apple's build quality (I also hate gaming laptops, so take that as you will). From there you can dual boot Linux on it and have at it. 

In the end, it doesn't matter. You should be learning every platform if you plan on going into computer science, as I've had to. ",4tlhq7,t1_d5ibul9,Amj161,,Reply,3,0,3
d5ievru,2016-07-19 13:43:31-04:00,Amj161,,"Yeah attempting is a better word than running really when it comes to arch... 

I was thinking myself really at that price I could get a nice gaming laptop. I saw the Razer Blade, but also saw some Asus ROG laptops that had better specs, however I'm not sure of the build quality (although personally I've only had good experience with Asus). And I would bring my desktop but I'm pretty sure I can't fit that in my room",4tlhq7,t1_d5idke4,thatguywhorows,,Reply,1,0,1
d5j0zft,2016-07-19 22:05:59-04:00,NeoKabuto,,"I have one of the new G752 laptops and it's pretty good. The only issue I ever had was with one of the pieces of bloatware that came with it.

However, it's really a ""portable"", not really a laptop. It's good for games if you need to move around a lot, but it's not worth lugging it (or the power cinderblock) to every class. I got a second laptop from my uni for that. Something small with a touchscreen and good battery life is perfect for taking notes in class.",4tlhq7,t1_d5ievru,Amj161,,Reply,1,0,1
d5j1blk,2016-07-19 22:14:26-04:00,Amj161,,"Okay, from doing more research it sounds like I might actually be able to bring a desktop based on the size of the dorms, so with that in mind it might be better to get a light portable laptop.

I was thinking more along the lines of the Asus ROG STRIX GL502VY DS74, a lighter powerful laptop (that's only half the weight of the one you mentioned). ",4tlhq7,t1_d5j0zft,NeoKabuto,,Reply,2,0,2
d5ihamv,2016-07-19 14:34:19-04:00,KronktheKronk,,"The built in shell and shell utilities are very helpful, but there's nothing you could do in a mac that you can't also do with a PC with minimal effort.  Putty does your ssh-ing to dev boxes and other servers, you can run a VM of whatever you need to in your machine.

I'd go with whatever you're more comfortable with, cause it don't matter.",4tlhq7,t3_4tlhq7,Amj161,,Comment,6,0,6
d5iccrl,2016-07-19 12:50:30-04:00,superdemongob,,"You gotta understand that whatever is ""easier"" to develop on really depends on what you're doing. 

Knowing Linux is a good skill to have in general so definitely keep that in mind. But beyond that, depending on your concentration,  you may require Windows anyway. For things like .Net and such. ",4tlhq7,t3_4tlhq7,Amj161,,Comment,3,0,3
d5j04po,2016-07-19 21:44:34-04:00,Britches,,"I really do not understand what you mean by better. Possibly more convent, but each OS has its owns strengths and weaknesses.

Another observation: 

If your school is anything like mine(and most other CS majors I've spoken to) you will have an account at your school's CS lab which you can SSH into. The lab computers are usually linux boxes and fairly beefy. I prefer ssh-ing into the lab computers rather than working on projects locally on my PC, but that's just me",4tlhq7,t3_4tlhq7,Amj161,,Comment,2,0,2
d5j19dl,2016-07-19 22:12:54-04:00,Amj161,,"My school has a very very small CS department (literally just two teachers) so I doubt they have that (although I wish they did!). And I meant more convenient really, but more just looking for suggestions!",4tlhq7,t1_d5j04po,Britches,,Reply,1,0,1
d5inej4,2016-07-19 16:43:03-04:00,andybmcc,,"I'd steer clear of a Mac and just run your favorite flavor of Linux or Windows and set up a virtual machine for the other.  Mac hardware is expensive, out of date, and not easily self-serviceable.  Ditto Mac-specific peripherals.  It looks pretty, I guess, if you're into that.  You'll probably at least want some form of Windows for .NET stuff because Mono still sucks.  You'll want some sort of POSIX-ish action.",4tlhq7,t3_4tlhq7,Amj161,,Comment,3,0,3
d5i76ya,2016-07-19 10:57:34-04:00,artillery129,,"It is most definitely better, you could also go Linux / Windows dual boot though. If you plan on dual booting anyway (because of games presumably) you might as well get the cheaper laptop and go with Linux. Or, in my highly unpopular opinion BSD :)",4tlhq7,t3_4tlhq7,Amj161,,Comment,2,0,2
d5i92iu,2016-07-19 11:40:14-04:00,Amj161,,"Okay, sounds like I might just go for a Windows laptop instead!",4tlhq7,t1_d5i76ya,artillery129,,Reply,2,0,2
d5idgx6,2016-07-19 13:14:17-04:00,artillery129,,"If you do go the dual boot Linux route, Google your laptop and make sure it's hardware is supported by the distro of your choice ",4tlhq7,t1_d5i92iu,Amj161,,Reply,2,0,2
d5ieru5,2016-07-19 13:41:20-04:00,Amj161,,"Okay, thanks for all the help!",4tlhq7,t1_d5idgx6,artillery129,,Reply,2,0,2
d5it0xy,2016-07-19 18:51:07-04:00,crookedkr,,Just go all in and go straight GNU/Linux.,4tlhq7,t3_4tlhq7,Amj161,,Comment,1,0,1
d5om0k7,2016-07-24 06:30:44-04:00,bartturner,,You want Linux over Windows or OS X.   One way is to just buy a Chromebook and use Crouton.,4tlhq7,t3_4tlhq7,Amj161,,Comment,1,0,1
4tk67l,2016-07-19 04:54:04-04:00,realmunk,Document Conversion from DocX and Other FileFormats to a Specific XSD,"We are trying to convert a .docx – and later other potential fileformats – into a kind of standard XML. This XML is going to be mapped through an XSLT to the XML of our choice (xsd).

For the conversion to be successful, we need to keep as many of the information elements within the document as possible. The most important ones are the structure, the content, tables, lists, and figures (images etc) within the document.

We have realised that getting a document that this job is complex, and that there are serious restrictions to what kind of documents we can support.

As there are different standards, implementing a converter for each of them would be time demanding.

Does anyone have some tips on dealing with Document Conversion to XML? Any tips on how to proceed?",,,,,Submission,0,0,0
4tk1xj,2016-07-19 04:14:53-04:00,MysticFurniture,LinkedIn finds my new stranger flatmates in its suggested connections algorithm. How?,"Dear all,

Could someone explain this to me:

Last year I moved in to a rented apartment with totally random people who also found the flat on a housing ads website - we were total strangers. After a few weeks I managed to pop up on their suggested LinkedIn connections window. Neither of us searched for each other - it just happened (seemingly?) randomly.  

Today this just happened to me again - about 3 weeks ago I moved in to a new flat occupied by a total stranger and today she cropped up on my suggested LinkedIn connections window - I recognized her from a profile picture. Once again, I told her this and she is 100% certain she did not search me on LinkedIn; she cannot even spell my foreign name. I did not search for her either.

Is this just a very rare coincidence, or am I missing something in my online behaviour that enabled LinkedIn to indicate flatmates as suggested connections?

Please explain! Thanks!",,,,,Submission,21,0,21
d5hxa0p,2016-07-19 04:35:53-04:00,trolle,,"LinkedIn could be doing a check on which IP address you are coming from and assumr that you know each other if you are the only two people using that address on a regular basis.

*edit: changed ""they"" to ""linkedin""",4tk1xj,t3_4tk1xj,MysticFurniture,,Comment,20,0,20
d5i2g0s,2016-07-19 08:49:11-04:00,987f,,"It's definitely IP based. LinkedIn can assume that if you connect from the same IP, that you know each other. Nothing creepy going on.",4tk1xj,t1_d5hxa0p,trolle,,Reply,7,0,7
d5it3dh,2016-07-19 18:52:41-04:00,hughwrang,,"Facebook Instagram and what's app all the same company now, but this linked in one sounds like IP, yes. ",4tk1xj,t1_d5i2g0s,987f,,Reply,1,0,1
d5iutzm,2016-07-19 19:33:50-04:00,yes_thats_right,,More likely just grabbing them from his phone contacts,4tk1xj,t1_d5hxa0p,trolle,,Reply,3,0,3
d5jcvwh,2016-07-20 05:28:28-04:00,trolle,,"That requires that one of them has the app installed. I don't have it so I don't assume others do. 

And you also assume that he has the roommates email or phone number which also needs to be registred to LinkedIn. If they are stragers and each rent a room from a thrid person thetøy might not have shared contact information.",4tk1xj,t1_d5iutzm,yes_thats_right,,Reply,1,0,1
d5jfxkj,2016-07-20 08:09:18-04:00,yes_thats_right,,It's not that unlikely that they would have the app installed or that they would have eachother phone numbers,4tk1xj,t1_d5jcvwh,trolle,,Reply,1,0,1
d5ivxtn,2016-07-19 20:01:13-04:00,ldpreload,,This is even totally reasonable (I think) for people accessing LinkedIn from work: you want co-workers to show up.,4tk1xj,t1_d5hxa0p,trolle,,Reply,1,0,1
d5iictm,2016-07-19 14:56:01-04:00,brtt3000,,"What if you have different providers? Might as well be geo-ip, or just street address based, or whatever they can get their hands on (like location services from OS etc).",4tk1xj,t1_d5hxa0p,trolle,,Reply,0,0,0
d5i0d9m,2016-07-19 07:27:28-04:00,Randolpho,,"Did you exchange email addresses or other contact info on your phones where you have the linked in app installed? 

Or did you ever give linked in your email password like it asks for all the time?",4tk1xj,t3_4tk1xj,MysticFurniture,,Comment,4,0,4
d5i23mw,2016-07-19 08:37:29-04:00,MysticFurniture,,"No, we do not have our email addresses and we do not have each others phone numbers (I know it does not sound like a friendly flatshare...). 

I also don't have the linked in app. ",4tk1xj,t1_d5i0d9m,Randolpho,,Reply,1,0,1
d5iifd9,2016-07-19 14:57:28-04:00,brtt3000,,"Do you have your street address visible online, or do you share internet connection?",4tk1xj,t1_d5i23mw,MysticFurniture,,Reply,1,0,1
d5in548,2016-07-19 16:37:24-04:00,umib0zu,,"How'd you meet your flatmates? Whatever person/service connected you might have tipped off their algorithms. Their algorithm is basically to connect people who have a ""meet"", a person who has both of you in your contacts. Also, if the meet has all of you in their cell phone, and they Opt into LinkedIn scanning their contacts, it associates that you possibly know each other. #YourPrivacyWasLostYearsAgo.",4tk1xj,t1_d5i23mw,MysticFurniture,,Reply,1,0,1
d5i1kz8,2016-07-19 08:18:46-04:00,awesomo_prime,,"I've never shared anything on facebook about work, yet somehow under ""suggested friends"" my manager popped up. Creepy.",4tk1xj,t3_4tk1xj,MysticFurniture,,Comment,6,0,6
d5i8yua,2016-07-19 11:37:52-04:00,Windows_98,,Facebook suggested my landlady when I moved into my apartment...,4tk1xj,t1_d5i1kz8,awesomo_prime,,Reply,2,0,2
d5hyhba,2016-07-19 05:45:02-04:00,artillery129,,"I have similar experiences, certainly has nothing to do with IP, I do not share the network with many of these individuals",4tk1xj,t3_4tk1xj,MysticFurniture,,Comment,3,0,3
d5i1kvn,2016-07-19 08:18:39-04:00,twistThoseKnobs,,"These are the creepy things FB and LinkedIn do that I don't like.
I understand that when someone adds your number to their phone, FB syncs it and suggests you as a friend and same on your end.

Do they have your contacts? Are you updating when you change location? Do their recommendations also come with other random people in your area?",4tk1xj,t3_4tk1xj,MysticFurniture,,Comment,3,0,3
d5i274g,2016-07-19 08:40:47-04:00,MysticFurniture,,"Yeah, I don't like those FB friend suggestions based on phone contacts...

No, we do not have our email addresses and we do not have each others phone numbers (I know it does not sound like a friendly flatshare...). 

I also don't have the linked in app. I have not updated my location for past 3 years since I am constantly based in London.

Other suggestions seem to be based on my university and new workplace rather than location per se.

",4tk1xj,t1_d5i1kvn,twistThoseKnobs,,Reply,1,0,1
d5irasj,2016-07-19 18:11:23-04:00,NeoKabuto,,"You haven't updated your location officially, but your IP address would have changed. They can track that to figure out you have a common location.",4tk1xj,t1_d5i274g,MysticFurniture,,Reply,1,0,1
d5iidyq,2016-07-19 14:56:41-04:00,acromulent,,Did you change your address at the post office to your new address? Have bills sent to that address? That information isn't private. They could be mining addresses and see that you both live together.,4tk1xj,t3_4tk1xj,MysticFurniture,,Comment,1,0,1
d5iut4c,2016-07-19 19:33:15-04:00,yes_thats_right,,"Do you have them as contacts in your phone and do you have the LinkedIn app installed?

They both use your contacts list for suggestions.",4tk1xj,t3_4tk1xj,MysticFurniture,,Comment,1,0,1
4tgysl,2016-07-18 15:36:09-04:00,not_norm,Thoughts on Free Code Camp??,"Hey trying to get into web development. I've been jumping from different courses trying to speed learn I guess. Get the basics down then focus on more advance stuff. Should I just stick to Free Code Camp? they have a lot of material seems like they go over everything. 

**My question**: should i just do free code camp courses? has anyone successfully completed the courses then get a job afterwards with the knowledge you learned? ",,,,,Submission,7,0,7
d5hfi4f,2016-07-18 19:02:26-04:00,DesertEagle27,,"I'm a beginner doing FCC. I like it because it's structured and let you build projects. IMO, the best way to learn to code is by building something. I'm on my second project at FCC. It's taking me a while but I'm slowly getting it done. My advice for you is try it out and see how you like it. ",4tgysl,t3_4tgysl,not_norm,,Comment,1,0,1
d5hibij,2016-07-18 20:14:50-04:00,okiyama,,"Jumping from course to course will only hurt you in the long run. Learning the basics over and over is useful, but the real meat of the learning is the more intermediate-advanced level stuff where you start building useful applications. That being said, that makes FCC a good choice since you build a number of apps.

Only complaint I've heard is that their choice is server technologies (node.js with MongoDB) are a little questionable. Not that they are bad to work with, but they are just a bit too trendy for a lot of programmers to put much stock into them, and Mongo in particular has some poor design. It'll be perfectly suitable for learning, and it'll help you *immensely* when it comes to learning other similar technologies.

Don't know about getting a job. Self-taught with no university learning is a tough road to take, but not impossible by any means. You'll mostly need a really impressive Github account to go that route.",4tgysl,t3_4tgysl,not_norm,,Comment,1,0,1
d5hj20o,2016-07-18 20:33:11-04:00,not_norm,,thanks for this its really helpful. I've been thinking about going to school for CS but I will admit im a little nervous I might be behind everyone and get lost in the material ,4tgysl,t1_d5hibij,okiyama,,Reply,2,0,2
d5hj7yv,2016-07-18 20:37:21-04:00,okiyama,,"Where are you in your life? What draws you to CS? I wouldn't worry about being behind. The point of school is to learn and they assume you have zero knowledge coming into it. If you work hard, you'll have no trouble keeping up.

If you're at a point in your life where you want to pursue doing college, there are lots of good options. For CS in particular, it's much more important what you learn than where you go. Sure, a fancy name school like Stanford will open doors but the difference between one state school and another is generally not a huge deal. Even starting out at community college for an associate's before going on to get your Bachelor's at a university works quite well for CS.",4tgysl,t1_d5hj20o,not_norm,,Reply,1,0,1
d5hjedn,2016-07-18 20:41:51-04:00,not_norm,,thanks for that I recently finished high school but have always been interested in CS. I would have long talks with a teacher/ IT at my school which is what he went for. I was worried about starting out at a community college but thank you for that really makes me feel better about going think i'll make an appointment sometime this week for info about enrolling(:,4tgysl,t1_d5hj7yv,okiyama,,Reply,1,0,1
d5hn8su,2016-07-18 22:17:25-04:00,okiyama,,"Ahh I see. Community college is a wonderful option for someone in your shoes! It sounds like you're interested in CS but don't have your heart completely set on it. That makes it good to not sink a ton of money into going to a 4-year university in case it turns out to not be right for you. If you're working on tutorials and looking at things like FCC, you should be ahead of the curve already as well.

Please let me know any other questions you've got. I graduated last year and I've been working since then, so I remember what it was like to be in your position :)",4tgysl,t1_d5hjedn,not_norm,,Reply,1,0,1
d5hngv6,2016-07-18 22:22:57-04:00,not_norm,,"no I really have my heart set on it. college can be expensive so I wanna do 2 years at community college then transfer over to a university if i can. Thanks i guess one question would be what did you do when a you where given an assignment and lost on something?
hows the math for cs?",4tgysl,t1_d5hn8su,okiyama,,Reply,1,0,1
d5hnqbv,2016-07-18 22:29:26-04:00,okiyama,,"That's a good place to be in too! CS is fascinating and really opens a lot of doors. At most community colleges, that won't be a problem at all. Most of them have partner programs with the state universities that let you transfer directly over after 2 years with very little problems. Ask about that when you go to enroll.

One awesome thing about college versus high school is how available professors are. For the most part, the professors really care about motivated students so you can go to them when you have trouble. Obviously, you can't come crying to them with every little thing but if you have tried something for a while, done some googling, maybe even asked other classmates and you can clearly explained what the problem is and what you've tried, they'll be more than happy to help. There's also reddit :P Getting stuck and working through it by using resources available to you like the internet and your professors is a really important skill to have.

Math is generally not so bad, and varies quite a bit based on the school you go to. Usually you need like 3 math classes. They are really useful though, some parts of CS are more theoretical and closer to doing math (like an algorithms class), so having experience in math is useful. Don't worry too much about that at the moment, it should be pretty far down the road.",4tgysl,t1_d5hngv6,not_norm,,Reply,1,0,1
d5hri6s,2016-07-19 00:11:58-04:00,not_norm,,thanks a lot you pretty much made me feel better about enrolling im really looking forward to it now(:,4tgysl,t1_d5hnqbv,okiyama,,Reply,1,0,1
d5i11qe,2016-07-19 07:57:40-04:00,okiyama,,No problem! Good luck and feel free to PM me anytime :),4tgysl,t1_d5hri6s,not_norm,,Reply,2,0,2
4tgezi,2016-07-18 13:45:09-04:00,grottesco,Access to an overloaded website - help?,"Hi guys! My knowledge of computer science is pretty low. That said, I have a specific question, maybe unusual, but extremely important for me. Hope it's not out of place here.

I must register on an governmental website in order to get some essential documents. The problem is that the site allows registrations only once a week, and it lasts just until a limited number of them is reached. Obviously it becomes extremely overloaded, even before ""time x"". My question: is there anything I can do to improve my chances to come through? For instance, does it bring something if I open multiple pages at the same time and compulsively refresh, or it's like each IP-address gets only one attempt at time? What if I have the page already open: do I get a priority? Is there some ´tricks' which might help me?

I know it may sound quite kafkaesque, but it's really so, and this bottleneck is severely affecting my life. I can explain more if needed. 
Thanks a lot
",,,,,Submission,0,0,0
4tcfo1,2016-07-17 20:33:18-04:00,TrenchFurry,Computer Science or Computer Engineering?,"Sorry if I'm posting this to the wrong subreddit, I'm new to Reddit.
Should I go with computer science of computer engineering? What are the pros and cons of both? Which offers more job opportunities? Which is more interesting (Yes I would also like to hear your opinion) and which is easier?",,,,,Submission,5,0,5
d5g8qff,2016-07-17 20:42:11-04:00,tyggerjai,,"Do you like math or transistors?

All of your questions are deeply subjective. One pro of computer science is not having to mess around with hardware. One pro of computer engineering is getting to mess around with hardware. 

You should read the many resources available describing each, and go with whichever one interests you more. ",4tcfo1,t3_4tcfo1,TrenchFurry,,Comment,5,0,5
d5g93jq,2016-07-17 20:52:57-04:00,fostermatt,,"And after college you can probably get the same jobs with either degree too.  

Are you more interested in hardware or software? Either way, while you're in school do outside projects also. Build things (atoms or bits) so you get the experience and you have something to show people about your passion.",4tcfo1,t1_d5g8qff,tyggerjai,,Reply,5,0,5
d5gomkn,2016-07-18 07:34:14-04:00,TrenchFurry,,"Hmm , I go with software , I'm currently doing some programming courses even though I'm in school and I'm learning the very basics , it's not easy but it's something , Thank you so much!",4tcfo1,t1_d5g93jq,fostermatt,,Reply,1,0,1
d5golqc,2016-07-18 07:33:05-04:00,TrenchFurry,,"I prefer math , yeah I guess I'll go research some more into these areas , Thank you so much for the info!",4tcfo1,t1_d5g8qff,tyggerjai,,Reply,1,0,1
d5gztpk,2016-07-18 13:01:03-04:00,high_side,,"Depending on school, transistors may be more EE than CE.  The core of CE shouldn't go any lower than gate-level stuff, though of course you'll have to do satellite work in transistors and physics.",4tcfo1,t1_d5g8qff,tyggerjai,,Reply,1,0,1
d5gatye,2016-07-17 21:43:58-04:00,tylka,,"Computer science allows you to study a broader range of programming topics (machine learning, security, graphics, operating systems, compilers, networking, etc.)

Computer engineering gives you a smaller number of CS electives, but learning about the hardware is extremely useful for things that are used in many fields (low-level programming, optimization, multi-threading, etc.)",4tcfo1,t3_4tcfo1,TrenchFurry,,Comment,3,0,3
d5gonmg,2016-07-18 07:35:42-04:00,TrenchFurry,,"Best reply so far in my opinion , very detailed.
I'm gonna go with computer science since it feels much more interesting , I'm still in school but I am doing some programming courses and learning the very basics even though it's hard , it's something! Thank you so much for the info!",4tcfo1,t1_d5gatye,tylka,,Reply,2,0,2
d5gzqxc,2016-07-18 12:59:19-04:00,high_side,,"Well, the 'allowed' part depends on the school.  Mine let you take whatever courses you want.  The degree just dictated required work.",4tcfo1,t1_d5gatye,tylka,,Reply,1,0,1
d5h4nrg,2016-07-18 14:49:25-04:00,tylka,,"Same, I meant CS majors have more credit hours to use on CS electives vs CpE.",4tcfo1,t1_d5gzqxc,high_side,,Reply,1,0,1
d5gzwa7,2016-07-18 13:02:39-04:00,AmaDaden,,"I graduated with a CS and CE double degree a few years ago. 


>Which offers more job opportunities? 

CS does. Everyone needs programmers these days.

>What are the pros and cons of both? 

I think this is the wrong question. The better question is ""what are the differences?"" CS is software and CE is hardware that runs software. One major con for CS to me is that what we really need to be teaching is Software Engineering, but that's is own rant. If you pick CS, learn as much as you can about design of software. The CS focus on algorithms is nice but only useful for interviewing with most jobs. 

>Which is more interesting (Yes I would also like to hear your opinion) and which is easier?

CS  and CS. There is a endless list of apps and opportunities with CS. I love CE and want to do some work related to it at some point but CS is where it's at

>Should I go with computer science of computer engineering? 

That depends on you. Do you want to make software or hardware? Love math and serious engineering? Go CE. Like all the other stuff I mentioned? Go CS. I will add I think it's much easier to go from CE to CS than from CS to CE. When we interview developers we don't look for degrees, just that they know their stuff. Engineering has the PE exam and you need to have some experience to take that (I think, I still haven't gone after it my self)",4tcfo1,t3_4tcfo1,TrenchFurry,,Comment,3,0,3
d5gpi6j,2016-07-18 08:14:35-04:00,qwerty_danny,,You should take the introductory classes to both and what you like. It's likely your courses will overlap anyways. Don't be afraid to sample: you don't have to make a commitment to CS or ECE immediately! You might even like both. ,4tcfo1,t3_4tcfo1,TrenchFurry,,Comment,1,0,1
d5gzrii,2016-07-18 12:59:41-04:00,high_side,,"> you don't have to make a commitment to CS or ECE immediately

Unless they're impacted majors and you can't switch.",4tcfo1,t1_d5gpi6j,qwerty_danny,,Reply,1,0,1
d5h2ii4,2016-07-18 14:01:36-04:00,monotypical,,might be worth also posting to /r/cscareerquestions ,4tcfo1,t3_4tcfo1,TrenchFurry,,Comment,1,0,1
4tbbg3,2016-07-17 16:10:30-04:00,scrubs2009,I'm trying to test if a program will function in different versions of windows. Will running it in compatibility mode let me figure out how it would function for that version?,Say I'm using windows 10 and the program works fine but it doesn't work in vista. If I run the program in compatibility mode for vista will it work? If so what method should I use for compatibility testing?,,,,,Submission,3,0,3
d5gq94c,2016-07-18 08:44:42-04:00,darthandroid,,"No, you need to install Vista into a VM and test there.

Compatibility Mode enables a bunch of hacks that may improve compatibility with programs designed for older versions of Windows, but there is a lot it doesn't do, and it's not suitable for verifying compatibility with an older version of Windows.",4tbbg3,t3_4tbbg3,scrubs2009,,Comment,3,0,3
d5gu7sd,2016-07-18 10:47:40-04:00,-Hegemon-,,"No way, you need VMs.",4tbbg3,t3_4tbbg3,scrubs2009,,Comment,2,0,2
4t9ttx,2016-07-17 10:25:56-04:00,NorthsideB,Are 3-4 hr 1 day adult ed SQL courses enough to pass the Oracle SQL Associate certification tests?,"I have a quite useless bachelor's in IT from a state University. The one subject I did the best at, and enjoyed the most, was databases. I'd really like to become certified in Oracle SQL Associate level but don't have the time or means to go back to school to get my master's degree. My local college offers a few 1 day 3 hr classes on databases that covers all the basics. Between that and the Oracle 11g study book I own, is that enough to pass both tests to become certified in the associate level? 

BTW, I hate my current job, but don't have enough confidence in my abilities to apply for any jobs in my field. I believe becoming certified is my best chance at getting a job as database admin or related position.",,,,,Submission,17,0,17
d5fqbz8,2016-07-17 12:18:24-04:00,crookedkr,,"Probably. The thing i have found with ""quick"" learning of topics is that you usually either get a good base that you forget most of or you pick a few specific things to take away (I guess these are basically the same). For me, I would try to get the most out of the class by first becoming familiar with what you will learn. You can download Oracle and install it. Try configuring it yourself and loading and ""optimizing"" some TPC datasets and queries. If you do this before taking the course, you will at least know a little about the topics before they are presented. Therefore you will reinforce the topics you found on your own and fill in some gaps that you didn't figure out on your own. This way the things that you take from the course will be the things you can't/didn't learn on your own rather than the basic things.

Another idea is to ask the college: ""what percent of the people taking this course are able to pass the XYZ certification?""",4t9ttx,t3_4t9ttx,NorthsideB,,Comment,7,0,7
d5fx0fo,2016-07-17 15:17:40-04:00,NorthsideB,,"Honestly, I'd do better taking a course or two in database management but I don't know where to go. There are plenty of 2 & 4 yr colleges/universities near me but not many offer more than a basic course on sql. ",4t9ttx,t1_d5fqbz8,crookedkr,,Reply,0,0,0
d5fxoad,2016-07-17 15:35:15-04:00,lonemerc88,,"Check these out. I have seen these posted a couple times in various threads. Standford has mini courses on databases that I believe are free and self paced. https://lagunita.stanford.edu/courses/DB/2014/SelfPaced/about
Also check out http://sqlzoo.net/wiki/Main_Page. 
Lastly, I know.microsoft offers courses  for the certification tests that are for beginners to prep for the tests and learn the basics. They are basically mini boot camps for SQL certs. They are really good for getting started. You can choose to do them in person or online. For the in person ones, I believe these are offered all over. I am not sure if Oracle offers the same but my guess is that they do. ",4t9ttx,t1_d5fx0fo,NorthsideB,,Reply,1,0,1
d5fy0a9,2016-07-17 15:44:05-04:00,NorthsideB,,"Thanks, I'll definitely check it out ",4t9ttx,t1_d5fxoad,lonemerc88,,Reply,1,0,1
d5fx62l,2016-07-17 15:21:53-04:00,lonemerc88,,"That will be a good start. This is a great to get the basics down. I would recommend installing Oracle on a PC or VM to practice/play around with.  A certification in SQL will get you through the door with a good amount of companies, especially if you have a degree in IT already. The associate certifications aren't to hard to get. Once you get that, companies will definitely look at you for entry level database jobs. You will really learn the most when you actually get your foot in the door and working with SQL on a daily basis in a production environment. I'd also recommend looking into other versions of relational database  management systems such as MySQL, MSSQL, PostgreSQL etc. SQL is generally the same across these platforms with some variations.  MySQL and PostgreSQL you can download for free and start playing around right away. MSSQL has a developers copy for $50 dollars which, I believe is the enterprise version for devs. They also have a free express version to get started with. Lastly, if your tight on money and can't afford the classes there are plenty of areas online that you can learn all of this for free. Good luck buddy, if you spend an hour or two a day practicing for the tests you will have the certificate fairly quickly imo.",4t9ttx,t3_4t9ttx,NorthsideB,,Comment,0,0,0
4t8c34,2016-07-17 01:20:32-04:00,VoltaicBlood,Need to interview a few people in a Computer Science field,"I'm currently a college student studying Computer Science and I need the help of 2 or 3 people currently working in my field. I need to ask some questions about communication in your workplace, and would like to send you some questions either through pm or email. If you're interested in helping me, either post here or send me a pm, I would really appreciate it! Thank you.",,,,,Submission,3,0,3
d5frr6c,2016-07-17 12:56:49-04:00,xiongchiamiov,,Why not list the questions here?,4t8c34,t3_4t8c34,VoltaicBlood,,Comment,1,0,1
d5h76e7,2016-07-18 15:46:28-04:00,x_Zoyle_Love_Life_x,,"This inquiry is always posted in this subreddit and I always post this response:

The point of this assignment is to meet people in the CS field, not to get answers to these things. Reach out to friends and family and ask if they know anyone who is in CS. Ask other faculty members or even your professor.",4t8c34,t3_4t8c34,VoltaicBlood,,Comment,1,0,1
d5h7dk8,2016-07-18 15:50:58-04:00,VoltaicBlood,,"I'm sorry, but according to my professor the point of this assignment is learning how to find sources, along with learning how to create professional reports. I'm using reddit as one of my avenues to find sources, not as my only one.",4t8c34,t1_d5h76e7,x_Zoyle_Love_Life_x,,Reply,1,0,1
4t415w,2016-07-16 06:28:16-04:00,kebabenekajsa,Help with taking the minimal closure of a a set of functional dependencies,"Hello, in preparation for my Database systems course, I'm trying to figure out closures and functional dependencies. For this purpose, I have a question:


 * What is the minimal closure of the set of FDs R{**VW -> T, TVW -> R, RS -> Q, T -> R, V -> U, SQ -> P, R -> P, TU -> S**}?

I have been able to conclude that the minimal key of R is {V,W}, so that's something!

I know that the FD **TVW -> R** is redundant, and I would also like to remove **T -> R** and change **VW -> T** to **VW -> R** and **TU -> S** to **RU -> S**. I'm obviously not following an algorithm here, but maybe one of you have a easy to understand algorithm? :)

Edit: I was finally able to get a result,I think that the closure is R+={**VW -> T, T -> RP, V -> U, TU -> S**} Is this correct? 
",,,,,Submission,3,0,3
d5f1mjh,2016-07-16 19:09:20-04:00,None,,"You can test your result using this tool:

http://raymondcho.net/RelationalDatabaseTools/RelationalDatabaseTools

It also output the workings, and does well to show you how the algorithm work.",4t415w,t3_4t415w,kebabenekajsa,,Comment,2,0,2
d5fmjom,2016-07-17 10:19:29-04:00,kebabenekajsa,,"Nice, thanks!",4t415w,t1_d5f1mjh,None,,Reply,1,0,1
4t2i1y,2016-07-15 21:37:23-04:00,BasedKami-sama,Can someone explain pointers to me?,"I get that it stores the memory address of a variable, and by manipulating the address, you can manipulate the variable. The address is notated by &variable. However, some of the notations when the pointers are used to initialize arrays confuse me a lot. Can someone explain to me (or give me some internet resources) so i can understand pointers relating to arrays better?      
      
Edit: This is all in C, BTW",,,,,Submission,30,0,30
d5e64ek,2016-07-15 23:15:04-04:00,PastyPilgrim,,"I actually had a hard time understanding pointers until I learned assembly and how hardware works. For me at least, learning *why* we need/use pointers was the missing link in understanding them.

Your CPU has two* (simplified) places to store information: registers and memory (RAM/cache). Registers are where all the magic happens: they're tiny (e.g. 32 bits) memory cells that are used to perform work. Memory is comparatively massive and is where all of your data goes to wait until it's time to perform work on it (at which point it is loaded into a register). However, most data (arrays, strings, etc.) that you use will be a lot bigger than the size of your register.

This is where pointers come into play. A pointer is just an address that tells you where in memory you're working. Say I want to instantiate and use the string ""hello world"". For simplicity, let's say that we have 8 bit/1 byte registers and each character is 8 bits in size. In memory, Im going to allocate 11 bytes for my string and 1 byte for a null character (so I know where the string ends). The memory manager finds 12 bytes of free space and gives me the address of the start of that memory: let's say... the 100th byte. Now I'm going to load my 12 bytes into addresses 100 through 111. I can't do anything with that whole string because it's much bigger than my 8 bit registers, so in order to do things with my string, I'll need to just save that address, which is 8 bits in size (it's 1100100).

Now I have a ""pointer"" to the start of my string. If I use indexing in my string, all that's happening is that you're adding your index to the pointer address. So str[3] is telling C to load the fourth byte from address 100, and the byte at address 103 gets loaded from memory into a register for you to play with.

In C, when you use the asterisk as part of a variable definition (e.g. `int* x`), you're telling it to reserve the space to store a pointer to a piece of data that is of type integer. When you don't use an asterisk, you're telling it to reserve the space to store an integer. If I wanted to create an array of integers, I would use `int* x` because that is going to create for me an address to the start of my array. This is where malloc and such come into place, because they are used to actually allocate the memory that you need for your array and return for you a pointer to the start of that memory. So `int* x = malloc(10)` is asking malloc to reserve 10 bytes and assign the start of the memory to `x`.

Now, whenever I use `x`, that's going to be the *address* of the start of my array, *not* anything in the array itself. However, I can use `x[3]` to access the fourth integer in my array. Or I can use `*x` to dereference the pointer and load whatever byte is being addressed.

So, what you might see sometimes is that, when looping through an array, I will change `x` and just keep dereferencing it. At the start, `x` points to the first integer in my array. I do something with it using `*x`, then I say `x++` so that x is now the address of the second integer in my array. I do something with it using `*x`, then I increment it to get to the third integer.

What might be confusing is when you start getting into deeper pointers, like `char** foo`, which you might see a lot in C. In that case, you have a pointer to a char pointer. Or, put another way, you have an address to an address to a character. Well, why would anyone ever want to have an address to an address? It's primarily so that a function can change the data without breaking other code.

Let's say that my main calls a function that appends a character to the end of a string. My function is going to need to allocate memory equal to the current size of the string plus 1. And, with a pointer to a pointer, I can change where the start of my string is without needing to change that outer pointer. So A -> B -> the first character in my string. I can change B's address to point to a new string without affecting any other code that is going to use A to access my string.

That's pointers in a nutshell. Feel free to ask any questions.",4t2i1y,t3_4t2i1y,BasedKami-sama,,Comment,27,0,27
d5e9bp1,2016-07-16 01:08:36-04:00,sprocklem,,"Minor correction: malloc takes the number of bytes, not the number of bits.",4t2i1y,t1_d5e64ek,PastyPilgrim,,Reply,8,0,8
d5e6d9q,2016-07-15 23:23:13-04:00,BasedKami-sama,,Thanks for the detailed answer!,4t2i1y,t1_d5e64ek,PastyPilgrim,,Reply,1,0,1
d5e5e81,2016-07-15 22:51:10-04:00,Net_Lurker1,,[Relevant.](http://i.imgur.com/35AzBSK.png),4t2i1y,t3_4t2i1y,BasedKami-sama,,Comment,11,0,11
d5eaky0,2016-07-16 02:02:32-04:00,Madsy9,,"What C-beginners often find confusing is the multiple different uses for the star '\*'. Skipping multiplication (obvious), it's both used for dereferencing pointers and to define/declare a pointer type as opposed to a normal non-pointer variable.

For example:

    volatile int* SOME_MEM_BASE = 0x08000000:

..is a definition of a pointer to int. While

    *SOME_MEM_BASE = 0:

..dereferences the pointer we made and sets the memory it points to, to zero. These are two different uses of \*, so don't confuse the two.

Regarding pointers vs arrays, the difference is not *that* important in practice. The main difference is that arrays such as `int foo[4]` know their own size, while pointers don't know anything about the ""length"" of the data sequence they point to, or even if the memory address they point to is valid. As such, if you have data structures with a small upper bound on size or can pass constant-size arrays to functions, it can be a good idea for both readability and optimization. Compilers love to know the size of things.

But whatever you do, don't read about pointers-to-arrays aka `int (*foo)[256]` Your head will implode.",4t2i1y,t3_4t2i1y,BasedKami-sama,,Comment,3,0,3
d5ejdaq,2016-07-16 10:10:59-04:00,final-getsuga,,You need pointers about pointers? https://xkcd.com/138/,4t2i1y,t3_4t2i1y,BasedKami-sama,,Comment,2,0,2
d5ejdeo,2016-07-16 10:11:04-04:00,xkcd_transcriber,,"[Image](http://imgs.xkcd.com/comics/pointers.png)

[Mobile](https://m.xkcd.com/138/)

**Title:** Pointers

**Title-text:** Every computer, at the unreachable memory address 0x\-1, stores a secret\.  I found it, and it is that all humans ar\-\- SEGMENTATION FAULT\.

[Comic Explanation](https://www.explainxkcd.com/wiki/index.php/138#Explanation)

**Stats:** This comic has been referenced 121 times, representing 0.1023% of referenced xkcds.

---
^[xkcd.com](https://www.xkcd.com) ^| ^[xkcd sub](https://www.reddit.com/r/xkcd/) ^| ^[Problems/Bugs?](https://www.reddit.com/r/xkcd_transcriber/) ^| ^[Statistics](http://xkcdref.info/statistics/) ^| ^[Stop Replying](https://reddit.com/message/compose/?to=xkcd_transcriber&subject=ignore%20me&message=ignore%20me) ^| ^[Delete](https://reddit.com/message/compose/?to=xkcd_transcriber&subject=delete&message=delete%20t1_d5ejdeo)",4t2i1y,t1_d5ejdaq,final-getsuga,,Reply,1,0,1
d5erhm9,2016-07-16 14:07:39-04:00,daV1980,,"Others have given great answers, but let me give an analogy. Imagine that you are at the library. There are tons of books everywhere. In this analogy the books are memory (or RAM). In this analogy then, pointers are cards in the card catalog. They tell you a little bit of information about the book they point at (the type, the location) but in order to get more you need to go see the book itself. ",4t2i1y,t3_4t2i1y,BasedKami-sama,,Comment,2,0,2
d5e4sln,2016-07-15 22:31:55-04:00,riotinferno,,"Welcome to pointers! For the first 6 months of my professional career, I had a post-it on my monitor with pointer notation in it. It can be confusing.

int foo:  //variable that stores a 32-bit value.

int \*pFoo: // also int* pFoo: or int * pFoo: variable that stores a pointer.


pFoo = &amp:amp:foo: // &amp:amp: gets the memory address of the variable foo.

foo = *pFoo: // using the asterisk outside of the declaration gets the value of the memory address.


struct foo_s {
     int bar1:
     int bar2:
}:


foo_s* pStuff:

pStuff->bar1: // -> is the pointer equivalent of the 'dot'

foo_s stuff:

stuff.bar2:


Clear as mud?",4t2i1y,t3_4t2i1y,BasedKami-sama,,Comment,4,0,4
d5evbeh,2016-07-16 15:58:59-04:00,argh523,,"&foo

> pFoo = &foo: // & gets the memory address of the variable foo.

Seems to work, no escaping required. Why are you (*double!*) amp'ing?",4t2i1y,t1_d5e4sln,riotinferno,,Reply,2,0,2
d5fqb42,2016-07-17 12:17:45-04:00,riotinferno,,"I don't follow. Those aren't escapes, they're code comments.",4t2i1y,t1_d5evbeh,argh523,,Reply,1,0,1
d5g2c5v,2016-07-17 17:36:48-04:00,argh523,,"No not the comments, the ""&"". In your source you have 

    pFoo = &amp:amp:foo: // &amp:amp: ...

which displays as

> pFoo = &amp:amp:foo: // &amp:amp: ...

but it works fine with no escaping at all

    pFoo = &foo: // &  ...

> pFoo = &foo: // &  ...",4t2i1y,t1_d5fqb42,riotinferno,,Reply,2,0,2
d5g8pxw,2016-07-17 20:41:46-04:00,riotinferno,,"Thanks for pointing that out.
I had submitted the comment on mobile, and apparently it added the &amp: It didn't show up for me until I viewed it on the web browser. Strange!",4t2i1y,t1_d5g2c5v,argh523,,Reply,2,0,2
d5evr0a,2016-07-16 16:11:47-04:00,-Hegemon-,,"I could, but what's the point?

Hehehe, sorry.",4t2i1y,t3_4t2i1y,BasedKami-sama,,Comment,1,0,1
d5ewx93,2016-07-16 16:46:37-04:00,lordvadr,,"Imagine you have a filing cabinet...or, well, two thousand filing cabinets.  How do you tell someone where the car insurance is?  You tell them, ""cabinet 16, drawer 2.""  But then you have a problem where once drawer 2 is opened, where in the drawer is the car insurance?  You and I can recognize what car insurance looks like, but, say a robot can't.  So you go through all your filing cabinets and you then tape a measuring tape to each drawer so that you can say, ""cabinet 16, drawer 2, 16cm.""

This is a usable mechanism but it has some limitations.  Not all files are 1cm thick. Sometimes you'll have files 10 times as big, sometimes you'll have files that are only a half cm thick.  So you decide that whomever whats to use your filing system has to use it in 1cm widths.  If they want to store 4 files in that cm, that's fine but it's up to them to figure it out when the filing cabinet robot grabs the whole cm. Or, if they want to store a file 10cm thick, you make them tell the filing cabinet robot ""cabinet 16, drawer 2, location 4, and 5, and 6 and 7 and 8.... and 13.""  And it's up to them to figure out what is what.

This is a working example of how physical memory is managed in a modern computer.  It's mostly the **m**emory **m**amangement **u**nit's (MMU's) job to handle the cabinet (DIMM) and drawer (chip) management.

So take it one step further, you simply say 160410 to mean cabinet 16, drawer 4, slot 10.  This is the filing cabinet robot's job (the MMU's job).  160410 is what's called an *address*.  People like to use your home address as an analogy, but I find that analogy lacking.

The filingcabinet robot doesn't care what's there, or what it means, you just say ""160410"" and you get what's at 160410.  It could be car insurance, death star plans, or the number of beers in the fridge.  The robot doesn't care.

Imagine trying to look through a filing cabinet with no labels on the files.  You can't say, ""find me the car insurance""...actually you can, but think of it more basically.  If you say `a=4`, what are you going to ask the robot to do?  ""Fine me '4'?""  There are millions of ""4""'s in memory at any given time.  You can't ask the robot for ""4"".

So when you name a variable, all your doing is asking the compiler to find a location in memory and be consistent with it.  When you say `a=4:` the compiler spits out `160410 = 4` (not really, its a bit more complicated than that, but that's the jist of it.

When you say `c=a*3`, the compiler says ""give me what's at 160410, multiply it by 3 and store that result into 160414 (or wherever it decided to put `c`)"".

So the compiler handles the name<->address problem for you.  It's one of the things that a compiler does, or at least what a higher level language does.  But the CPU only cares about addresses.

In C, you get the address of a variable with the `&`. `&a` means the *Address of a*.  You don't generally care what that address is...in fact, if you do, you're doing something wrong.

But, sometimes, you call a function and instead of it telling you ""4"", it tells you ""here's where you can find the answer.""  Now, where do you store that?  You store that in a pointer.  **A pointer stores the address of a memory location.**  A pointer literally tells you, go find what you're looking for at cabinet 16, drawer 4, slot 16.  One of the poor designs in C is pointers and addresses because the language was written to expose some of the underlying aspects of the CPU to you...and in cases like drivers and such, you HAVE to have pointers.  But it causes a lot of confusion in some places.

So the fist place people get confused, and one of the biggest confusions is scanf.  You see, when you pass a variable in C, it passes its *value*. If you want some function to change your copy of `a`, you have to pass the address of `a`, hence why you have to say `scanf(""%d"", &a)`..you have to tell scanf where in your filing cabinet `a` lives.  You have to give it the *address of a*.

The next thing that confuses people is `scanf(""%s"", string)` and the reason that this is confusing is that when you declare `string[32]`, referencing the variable `string` is **already a pointer**.  All arrays are pointers--again, a bit of a poor design in C.

The reason pointers are ambiguous is that they are simply integers.  While it's not straight forward where the data will live (that's the MMU's job), pointers are simply addresses...like 160410...where the data lives.

So when you don't know what the address is ahead of time, you use a pointer, and then you ask the os/library/whatever ""give me a memory location I can store this in.""  That's the reason malloc returns a pointer.  You've said, ""hey, filing cabinet robot, find me an empty slot **X**cm wide."" And it does.  You then have to tell the compiler ""I don't mean '4' i mean what's at address ""4"".  That's where the asterisk comes in.

This response has dragged out far longer than I intended it to.  If this doesn't explain it well, ask.  PM me if you need to.",4t2i1y,t3_4t2i1y,BasedKami-sama,,Comment,1,0,1
d5eyfot,2016-07-16 17:31:59-04:00,CobaltBlue,,"when i was TA-ing i often suggested students who were struggling with the concept watch this video for the basics before coming to me with specific questions if they still had any (this was for classes after the ones where they should have already learned this)

his teaching style is very accessible :)
https://youtu.be/Rxvv9krECNw",4t2i1y,t3_4t2i1y,BasedKami-sama,,Comment,1,0,1
4t2f1j,2016-07-15 21:13:55-04:00,logoseternal,Online CS bachelor program,"What online CS bachelor program are there these days, besides OSU?",,,,,Submission,7,0,7
d5e3lcs,2016-07-15 21:54:08-04:00,JBLovecrafty,,The University of Illinois Springfield has a pretty solid program.,4t2f1j,t3_4t2f1j,logoseternal,,Comment,2,0,2
d5empkw,2016-07-16 11:53:24-04:00,i_takes,,OSU has an online cs bachelor program? man I just graduated from there this past spring and I had no idea. ,4t2f1j,t3_4t2f1j,logoseternal,,Comment,1,0,1
d5gdsoy,2016-07-17 23:12:56-04:00,logoseternal,,no idea?,4t2f1j,t1_d5empkw,i_takes,,Reply,1,0,1
4t0qa3,2016-07-15 14:59:31-04:00,not_norm,Declaring a variable?,"Sorry not really sure what the right term is but whats the point of declaring a variable and not assigning a value like       var number; versus     var number = 5; just curious also is there a term for this? im learning javascript so if you could tell me why this would be handy in javascript i would appreciate it 
 ",,,,,Submission,3,0,3
d5dryc8,2016-07-15 16:32:06-04:00,Net_Lurker1,,"Let me first explain to you what the difference is in a language that has strong typing, and then onto Javascript.

In C, for example, if I want to store the number 5, I will need an integer. So, we can use ""int number:"" or ""int number = 5,"". The second one is basically doing two steps at once, the first being ""int number:"" and the second ""number = 5:"". On the first step, we are basically telling the computer: ""ok, I want you to set aside a chunk of memory the size of an integer, and associate it with the label *number*"". And the second one: ""Store the value 5 on the variable *number*"".

Now onto Javascript. Since it isn't strongly typed, when you say ""var number:"" all that it does is tell the computer: ""I'm gonna use a variable named number"". When you do ""number = 5:"" the computer says: ""alright, 5 is an integer, so *number* is now an integer, storing the value 5"". 

Hope this helps, hit me up if you have a question or anything.",4t0qa3,t3_4t0qa3,not_norm,,Comment,3,0,3
d5e0acz,2016-07-15 20:13:05-04:00,not_norm,,yea this is a really good break down. could you tell me why you would want to do var someVariable: vs. var someVariable = 5: does it really matter which one you do? im guessing theres times when one is better than the other,4t0qa3,t1_d5dryc8,Net_Lurker1,,Reply,1,0,1
d5e1kun,2016-07-15 20:51:49-04:00,Net_Lurker1,,"Tbh, for variable declarations, it doesn't really matter if you do one or the other. Some people like to put all the instanciations together, for code clarity. Like, putting all your ""var x: var y: var etc:"" together at the beggining of what you're writing, and then saying ""x = 5: ... code that uses x ... y = 20: ... code that uses y ... "". 

Once you get to using objects though, this, the difference between *instantiating* and *assigning* becomes fundamental, and you can do lots of interesting stuff with inheritance and such.",4t0qa3,t1_d5e0acz,not_norm,,Reply,1,0,1
d5e39fl,2016-07-15 21:43:44-04:00,not_norm,,"oh okay. one more thing im trying to teach myself JavaScript. I picked up html and css quickly, I get the syntax of JavaScript and what its capable of I understand loops, functions, arrays, I know everything in JavaScript is basically an object because everything can be stored in a variable. But when im given code to read through I cant understand it unless I run it. i don't know how to use what ive learned any suggestion or resources?",4t0qa3,t1_d5e1kun,Net_Lurker1,,Reply,1,0,1
d5e59h7,2016-07-15 22:46:53-04:00,Net_Lurker1,,"Yeah, well JS can be a bit of a mess, it's not as easy to read as some other languages. There is quite a bit more to JavaScript though, everything relating to the DOM tree, like manipulating html nodes, AJAX, etc. Also, for lots of the more complicated stuff that people want to do in JavaScript they use JQuery, you should definitely learn that too.

Seems like you got the basics, I'd reccomend searching online for Intermediate courses in Javascript, there is tons of great (and free) resources, just find one that you like :)

Edit: [This one](http://htmldog.com/guides/javascript/intermediate/) is a pretty great guideline for stuff you're going to wanna learn.",4t0qa3,t1_d5e39fl,not_norm,,Reply,2,0,2
d5e8qbj,2016-07-16 00:45:31-04:00,not_norm,,"thanks glad to know im not just illiterate when it comes to reading javascript the fact that it can be a mess lets me know i just have to get use to it its only been about a month so i still have to learn about the dom and all that so thanks for that 
",4t0qa3,t1_d5e59h7,Net_Lurker1,,Reply,2,0,2
d5e934q,2016-07-16 00:59:10-04:00,Net_Lurker1,,Best of luck mate. <3,4t0qa3,t1_d5e8qbj,not_norm,,Reply,2,0,2
d5do4h1,2016-07-15 15:05:15-04:00,visvis,,"It binds the variable name to the current scope. If you declare the variable in a function, any later assignments to it are undone once that function returns. If you don't declare the variable, it is in the global scope (or possibly another place it has been declared, for example a function the current function has been nested in) and the value is leaked out of the function.

It's generally good practice to avoid confusion and always be explicit and declare the variable, either in the global scope (outside any function) or in the function you're using it in.",4t0qa3,t3_4t0qa3,not_norm,,Comment,4,0,4
d5e3kw6,2016-07-15 21:53:44-04:00,None,,"sooo idk about javascript but let's look at the following snippet.


    if (true)
       int num = 5:
    else
       num = 6:

    return num:

That looks fine right?
Well, that would not compile. return num would complain that no variable num exists in current scope, that's because num was DECLARED in an if statement, so that variable can ONLY be used in that if block. In order to do what I was trying to accomplish, I'd have to declare the integer above the if statement, and instantiate it inside the if block, and then the compiler wouldn't complain. ex:

     int num:
     if (true) 
       num = 5:
     else
       num = 6:

     return num:

That would work.",4t0qa3,t3_4t0qa3,not_norm,,Comment,1,0,1
4t0oov,2016-07-15 14:51:04-04:00,avaxzat,Can time complexity be a function of something else other than input size?,"To the best of my knowledge, time complexity analysis always aims to quantify the (asymptotic) time requirements of an algorithm as a function of the size (in bits) of that algorithm's input. I've never come across any analysis that quantifies the running time using some other feature of the input besides its size, yet it seems to me like this could sometimes be appropriate.

For example, let's say we have a simple binary counter that just increments any integer we supply to it and returns the result. Clearly not every input to this counter will take the same amount of operations: the worst case for such a counter is to supply it an n bit integer where the first bit is set to 0 and all other bits are set to 1. The best case is an n bit integer where the least significant bit is zero. The former takes n operations, the latter only one. Of course, as is well known, a binary counter can be implemented such that any sequence of n increment operations takes ~~O(1)~~ O(n) amortized time, because the worst cases are very rare compared to the ""good"" cases.

But this example illustrates the basic idea behind my question: given that the number of operations an algorithm needs to perform can vary significantly for inputs of the same size, can it sometimes be appropriate to consider the running time as a function of some other feature of the input besides its size?",,,,,Submission,1,0,1
d5dwy31,2016-07-15 18:38:34-04:00,tholenst,,"One such framework is [Parametrized Complexity](https://en.wikipedia.org/wiki/Parameterized_complexity). A nice example is the FPT algorithm for vertex cover, see e.g. [here](http://web.cs.iastate.edu/~cs511/handout10/FPT_VC.pdf).",4t0oov,t3_4t0oov,avaxzat,,Comment,3,0,3
d5ece60,2016-07-16 03:38:31-04:00,avaxzat,,"Thanks, that's exactly what I was looking for!",4t0oov,t1_d5dwy31,tholenst,,Reply,1,0,1
d5do853,2016-07-15 15:07:29-04:00,visvis,,It sees like you already answered your own question.,4t0oov,t3_4t0oov,avaxzat,,Comment,1,0,1
d5e89qp,2016-07-16 00:28:25-04:00,MengerianMango,,">For example, let's say we have a simple binary counter that just increments any integer we supply to it and returns the result. Clearly not every input to this counter will take the same amount of operations: the worst case for such a counter is to supply it an n bit integer where the first bit is set to 0 and all other bits are set to 1. The best case is an n bit integer where the least significant bit is zero. The former takes n operations, the latter only one. Of course, as is well known, a binary counter can be implemented such that any sequence of operations takes O(1) amortized time, because the worst cases are very rare compared to the ""good"" cases.

I think you're demonstrating a slight lack of understanding. It's only O(1) if the size of input integer is bounded. O(1) doesn't mean literally one operation. It means that we can bound the number of operations by a constant. In this case, we know we can constant bound the number of operations required to increment the integer precisely because it's limited by the size of the integer which we've already assumed to be bounded. Incrementation in arbitrary precision integers *is not* O(1).",4t0oov,t3_4t0oov,avaxzat,,Comment,1,0,1
d5ecckg,2016-07-16 03:35:50-04:00,avaxzat,,"Oh right, I meant O(n) amortized time for a sequence of n increment operations, meaning any single increment operation takes on average a constant number of bit flips.",4t0oov,t1_d5e89qp,MengerianMango,,Reply,1,0,1
4svbin,2016-07-14 16:07:17-04:00,Chappit,Does anyone have a recommendation for a good book or lecture series on computational geometry?,,,,,,Submission,8,0,8
4suu97,2016-07-14 14:35:38-04:00,opendoors1,Linux on Chromebook. What is the point of keeping Chrome OS?,"I bought a Toshiba Chromebook 2 to use for college/learning Linux. 

I also had a 256GB SD card lying around so I popped that in. Is there any point to keeping Chrome OS? I could just run a Linux distro on it? What are the drawbacks? ",,,,,Submission,1,0,1
d5cc08g,2016-07-14 15:11:58-04:00,x_Zoyle_Love_Life_x,,Nothing. Chromium is the least compatible platform in the working world.,4suu97,t3_4suu97,opendoors1,,Comment,2,0,2
d5ckmia,2016-07-14 18:27:12-04:00,martinsa24,,"It runs its own Linux distribution, and you can do commands in chrome, but to learn Linux you might want to Google how to flash a new Linux OS on it.",4suu97,t3_4suu97,opendoors1,,Comment,1,0,1
d5f5vtj,2016-07-16 21:27:03-04:00,alphor_,,"have you tried crouton?

www.github.com/dnschneid/crouton",4suu97,t3_4suu97,opendoors1,,Comment,1,0,1
d6r3kj3,2016-08-21 18:19:04-04:00,wolfchimneyrock,,playing full quality netflix,4suu97,t3_4suu97,opendoors1,,Comment,1,0,1
4sureu,2016-07-14 14:21:19-04:00,bottomlesscoffeecup,First hackathon tomorrow !!,What are the do's and dont's? I really want to impress but honestly I'm so worried I'll make an arse of it. ,,,,,Submission,20,0,20
d5cd1vx,2016-07-14 15:34:20-04:00,dxk3355,,"DO: Bring donuts and coffee unless provided already
DON'T: Play Pokemon Go the whole time",4sureu,t3_4sureu,bottomlesscoffeecup,,Comment,16,0,16
d5ceq8p,2016-07-14 16:12:41-04:00,bottomlesscoffeecup,,"That DON'T there is actually good advice. Will keep my pokemon addiction to a minimum thank you! 
",4sureu,t1_d5cd1vx,dxk3355,,Reply,10,0,10
d5cd1vt,2016-07-14 15:34:20-04:00,gyroda,,"Depends entirely on the event. Generally not disturbing others too much is a good idea, but if they're clearly taking a small break go and have a chat. If there's food, don't be scared to claim some before it vanishes but don't be a pig and take more than your share. ",4sureu,t3_4sureu,bottomlesscoffeecup,,Comment,5,0,5
d5cobhz,2016-07-14 20:00:54-04:00,BrokeDiamond,,"* Have some kind of idea of what you want to work on. Maybe not a concrete idea, but an area you'd like to explore. Especially true if you have teammates.
* Read the theme of the hackathon if there is one, and also pay attention to the different categories and prizes. If any of them seem easy (and if you don't really have a better idea) go ahead and try to win in a category or two, since you can make some decent money or cool gadgets that way.
*  Go around and see all the company booths. Pick up free stuff. Never buy T-shirts again.
* UI is slightly more important than functionality in impressing judges. I've seen plenty of meh projects with great UIs get picked for prizes over great projects with meh UIs.
* Be comfortable. You're going to probably be working for the next 24-36 hours straight, and you're going to feel like shit by the end unless you prepare and schedule accordingly, so try and get the little things right. If you wear contacts, bring glasses too. Feel free to wear comfy slippers instead of shoes. Either bring snacks or know where you can get some. Remember that whatever pace you're going at should be sustainable over the next day, so don't overload on Red Bull and let yourself nap occasionally.
* Have fun. You may or may not finish your project. A lot of people don't. Half of the people who do finish are secretly hoping nobody asks about the edge cases they ignored. Meet people, experiment with new tech, and don't kill yourself.",4sureu,t3_4sureu,bottomlesscoffeecup,,Comment,6,0,6
d5cd11u,2016-07-14 15:33:51-04:00,gurtos,,"I can't really think of anything. Just be cool.

Also, don't be afraid to show your code. If it's not perfect, you will learn something.",4sureu,t3_4sureu,bottomlesscoffeecup,,Comment,7,0,7
d5cigc4,2016-07-14 17:36:18-04:00,orlybach,,"Honestly the most important part is to have fun. Make something you have always wanted to, but never got around to. Try a new technology. Make some new friends. It's not always about winning, but rather enjoying the experience and learning something new. Take advantage of any mentors at the event.

If you have the opportunity to demo your project, always show it off, even if it is unfinished. You spent your time working on it and people want to see it! 
",4sureu,t3_4sureu,bottomlesscoffeecup,,Comment,3,0,3
d5ckzw4,2016-07-14 18:36:02-04:00,NoNotTheDuo,,"DON'T: Be afraid to try something new (language, framework, library, etc)",4sureu,t3_4sureu,bottomlesscoffeecup,,Comment,3,0,3
d5cl3h2,2016-07-14 18:38:29-04:00,livelifedownhill,,"I've found you can go in with one of two different mindsets, which have some overlap, but for the most part are accurate. 

1) you have a pre-planned awesome idea already, you have a team mostly assembled to work on it. You know what languages and frameworks you're going to use to accomplish it, and your team has experience working in these. Your team is made of smart, dedicated people willing to work through the night and miss events and most schmoozing to get it done. You probably win a prize. 

2) you maybe have a couple ideas, maybe a few thoughts on buddies you want to work with. Since you haven't decided on an idea, you haven't thought of languages or frameworks, you don't have a full team, so you can't know what peoples strengths and knowledges are, or how hardworking or smart they may be. So you iron it out there, start on something, probably learning and using some new frameworks or languages you haven't used. go walk around the booths for a while and talk to companies, network, and get swag. Work a bit more. Get some food, go to the demo presentation or class on some new tech. Eventually run out of steam and take a break to sleep for a few hours. Realize you're not going to finish, so give up and go have fun doing other stuff there is to do and be okay with not winning. 

Neither is better than the other, and there can obviously be some overlap between. But I've found them to be pretty true, and honestly I end up in camp 2 more often than not, and have an awesome time and learn a bunch",4sureu,t3_4sureu,bottomlesscoffeecup,,Comment,2,0,2
4soawy,2016-07-13 13:19:19-04:00,PhoKingGr8,Antivirus protection program,"First off, I apologize if this is the wrong sub to be asking this question.  At the moment i have Avast as an antivirus software but it has not been reliable.  I am willing to pay money for a better one.  Can anyone direct me or show me how to find a good antivirus and protection software?  Thank you",,,,,Submission,0,0,0
d5axdaw,2016-07-13 14:44:58-04:00,BrokeDiamond,,"How has Avast not been reliable? That's the one I usually recommend, along with Malwarebytes.",4soawy,t3_4soawy,PhoKingGr8,,Comment,3,0,3
d5bzi6s,2016-07-14 10:45:41-04:00,andybmcc,,"That's exactly what I use, coupled with Spybot S&D.  Between the three, I haven't had any known issues.  They are also my goto for mucked up friends/family computers.  I haven't come across anything that that combo hasn't been able to eradicate.  Make sure to enable the S&D process blacklist, that's mostly what it's there for.",4soawy,t1_d5axdaw,BrokeDiamond,,Reply,1,0,1
d5b8ymi,2016-07-13 19:07:24-04:00,NewtonLawAbider,,Windows defender and malwarebytes and some common sense is all you need,4soawy,t3_4soawy,PhoKingGr8,,Comment,3,0,3
d5bg7x4,2016-07-13 22:26:17-04:00,justlikestoargue,,"If you're using Windows 8.1 or Windows 10 you have Windows Defender automatically. You don't need more than that.

Be careful about risky clicks and downloads no matter what AV you have.",4soawy,t3_4soawy,PhoKingGr8,,Comment,2,0,2
d5ba8fn,2016-07-13 19:41:10-04:00,akingofengland,,A good one I always use that is free is Sophos Home :),4soawy,t3_4soawy,PhoKingGr8,,Comment,1,0,1
d5bg1fn,2016-07-13 22:21:18-04:00,MaShinKotoKai,,Kaspersky or Eset,4soawy,t3_4soawy,PhoKingGr8,,Comment,1,0,1
d5bn097,2016-07-14 02:05:13-04:00,itsmewallis,,"http://www.cbc.ca/news/technology/antivirus-software-1.3668746

Be diligent, Don't let the system protect itself. Just have a copy of Malwarebytes and Spybot Search and destroy to run when things become questionable.",4soawy,t3_4soawy,PhoKingGr8,,Comment,1,0,1
4shufc,2016-07-12 12:19:32-04:00,AbsoluteZenTrash,xaml -- how to make box unchecked,"Hey everyone, 

So I'm working on Microsoft's View-ViewModel-Model and I have a question regarding the GUI part. In the [checkbox](http://imgur.com/MktDGC7) , how do I make it such that when I open the GUI, the checkbox is unchecked. [The corresponding xaml code](http://imgur.com/MvQNrNL) looks like this.

If anyone could help me that'd be great! Thanks",,,,,Submission,0,0,0
d59dh4a,2016-07-12 12:38:33-04:00,huck_cussler,,"    <CheckBox IsChecked=""False"">My Checkbox</CheckBox>

Or, since you're using MVVM it's more likely it will be bound to a variable in your view model, something like:

    <CheckBox IsChecked=""{Binding ShouldBeChecked}"">My CheckBox</CheckBox>

where you have a public boolean variable in your ViewModel called ShouldBeChecked.

This answer ignores a lot of the underlying structure you need for your MVVM project.  I'll assume you have that set up or can check some online resources for how to do so.

**edit:**  Ah, I should have looked at your link first.  As far as the xaml goes you have it right.  Obviously Onlyrecordprocessing has to be public.  There are some other issues that might be happening though.  Have you set your VM as a DataContext in the View?

**edit2:** This post should probably be in /r/learnprogramming or somewhere similar, rather than in /r/askcomputerscience .",4shufc,t3_4shufc,AbsoluteZenTrash,,Comment,3,0,3
d59ejuv,2016-07-12 13:00:47-04:00,AbsoluteZenTrash,,"huck_cussler thanks for replying! I didn't know there was learnprogramming subreddit, I'll ask over there.

To be honest your advice went way over my head, but thanks for taking your time.

-AbsoluteZenTrashLife",4shufc,t1_d59dh4a,huck_cussler,,Reply,1,0,1
d5a7an2,2016-07-13 00:34:43-04:00,videoj,,You can also ask over at /r/csharp and /r/dotnet as well.  ,4shufc,t1_d59ejuv,AbsoluteZenTrash,,Reply,1,0,1
4sh5jx,2016-07-12 10:08:26-04:00,ShortfallOfGravitas,"When I visit my local newspaper site, it says I've reached my page view limit and blocks me. But when I visit in incognito mode, I get full access. How does the site know I've reached my limit?",,,,,,Submission,7,0,7
d59746w,2016-07-12 10:15:09-04:00,Hairshorts,,"It is likely that your browser is storing a cookie for this site. A cookie is a small piece of information stored on your computer which is associated with a website. Your browser sends the current cookie values to the site's server when it makes requests, and the server can send updated cookie values back to the browser when it sends data.

The site could be using a cookie which keeps track of the number of page views. When you switch to incognito mode all your cookies are cleared for that session, so the site does not know that this browser has visited before.

Incognito mode clears cookies because they can be used for tracking users, which would defeat part of the purpose of going incognito.",4sh5jx,t3_4sh5jx,ShortfallOfGravitas,,Comment,12,0,12
d597b9w,2016-07-12 10:20:18-04:00,ShortfallOfGravitas,,"I blocked cookies for the news site domain in Chrome, yet it still says I have reached the view limit. Could there be another mechanism that it tracks me?",4sh5jx,t1_d59746w,Hairshorts,,Reply,2,0,2
d597oj6,2016-07-12 10:29:53-04:00,ThatsAFineRadiator,,Did you clear the cookies too?,4sh5jx,t1_d597b9w,ShortfallOfGravitas,,Reply,3,0,3
d597vx6,2016-07-12 10:35:08-04:00,ShortfallOfGravitas,,"Yeah, clear all the cookies associated with that domain. Is it possible that the tracking cookie could be labelled with a different domain?",4sh5jx,t1_d597oj6,ThatsAFineRadiator,,Reply,1,0,1
d599nkb,2016-07-12 11:17:49-04:00,ThatsAFineRadiator,,Potentially yes. Just clear them all and your cache,4sh5jx,t1_d597vx6,ShortfallOfGravitas,,Reply,1,0,1
d59n573,2016-07-12 16:00:14-04:00,sullage,,"Check out the ""methods"" section here: https://panopticlick.eff.org/about

You can bet newspaper websites are using some of these techniques ",4sh5jx,t1_d597b9w,ShortfallOfGravitas,,Reply,2,0,2
d5ajfhe,2016-07-13 09:30:54-04:00,AmaDaden,,"Yes, lots. Flash has cookies as well. They can also use your IP and browser stats. You can read up on the evercookie and get a better idea of what they could be using http://samy.pl/evercookie/",4sh5jx,t1_d597b9w,ShortfallOfGravitas,,Reply,1,0,1
d59iw84,2016-07-12 14:31:02-04:00,Javadocs,,Could be using local storage maybe.,4sh5jx,t3_4sh5jx,ShortfallOfGravitas,,Comment,5,0,5
d59uvyx,2016-07-12 18:56:52-04:00,Murikk,,This is likely it.,4sh5jx,t1_d59iw84,Javadocs,,Reply,1,0,1
d5v24ql,2016-07-28 17:59:15-04:00,georgebatski,,"Login, cookies or IP. ",4sh5jx,t3_4sh5jx,ShortfallOfGravitas,,Comment,1,0,1
4sggnd,2016-07-12 07:12:57-04:00,ItsMe_RandomNumber,"What are the differences between x86, ARM and PowerPC?","I know most computers use the x86 instruction set and most smartphones use ARM, but how exactly are they different and why are the pros and cons of using each one? Also there are some devices that use PowerPC, like PS3. Is much it different from the others?

And there are devices that do not follow the pattern, like some x86 smartphones. Is there any advantage in doing this? There are also devices that have multiple processors with different architectures like Nintendo Wii, witch uses one ARM and one PowerPC processor. What are the benefits of doing this?",,,,,Submission,2,0,2
d592kf2,2016-07-12 07:38:45-04:00,0b101001111010,,"I don't know anything about PowerPC, but I can tell you some of the differences between x86 and ARM.

x86 is designed to be very fast. It can do really complex instructions entirely using hardware. As a result, x86 processors use a lot more transistors than ARM processors which means that they use a lot more energy. ARM processors are designed to be efficient in terms of energy usage. ARM instructions are very general and usually simple.

This is why it makes sense to use x86 processors in PCs and ARM processors in phones. It's ok if a PC is not very efficient with energy because it's either connected to power or it has a huge battery (compared to a smartphone or tablet). But smartphones need to be energy efficient so they use ARM processors.

tl:dr ARM processors are more energy efficient but slower than x86.",4sggnd,t3_4sggnd,ItsMe_RandomNumber,,Comment,4,0,4
d5952dt,2016-07-12 09:15:03-04:00,andybmcc,,"Yeah, different folks, different strokes.  

I'll comment on PS3 PowerPC.  PS3 isn't just a PowerPC chip, it also has smaller synergistic processing elements (SPEs) connected to the main processor (PowerPC) on a ring bus.  The idea was to have parallel processing power without the cost of a single multi-core chip. The whole package was called a Cell Broadband Engine, and it was an interesting concept that flopped, hard.  This was about the time that general purpose GPU was beginning to be in wide use.  It used a crap ton of very simple cores, instead of the 6 or 8 more powerful cores (plus PPU) in the Cell B.E.  Turns out a pile of small cores with some some shared memory beats the pants off of the Cell architecture.",4sggnd,t1_d592kf2,0b101001111010,,Reply,2,0,2
d597blr,2016-07-12 10:20:32-04:00,thechao,,"I have worked at both x86 & ARM -based hardware shops, although my main work is in GPUs. I have a fair amount of familiarity with PowerPC, as I also work on compiler back-ends.

Anyways, the *major*, *overarching* difference between the three architectures is the licensing agreement that allows you to use the ISAs.

— x86 is controlled (essentially) by Intel, AMD, and Via(?)

— ARM is controlled by ARM

— PowerPC is now, I believe, Open Hardware (OpenPOWER)

Perhaps, in the 80s and 90s, with the rethinking of ISAs for RISC (reduced instruction set computing) there were important differences, but that basically boiled down to:

1. RISC is harder for a human to optimize, but easier for a compiler:

2. CISC requires larger decoders, but is (essentially) a compressed instruction stream.

However, since its 2016, and we can fit, say, 128 decoders in less area than the L0 backing store of a many-core machine, decode is only interesting in terms of how well the instructions can be *compressed*. (Decoders are so cheap, that they don't even consume an entire cycle: they're normally a combinatorial pre-phase during resource fetch & pipeline dispatch, even for x86.)

You can make the implementation of any of the major 3 either a performance-targeted power-hog, or a power-sipping light-weight.

If you want to know why ARM is in all the smart-phones? Intel was blind-sided by the iPhone revolution, and doesn't (functionally) allow 3rd-party licensing of x86. That means if you want to build a smart-phone SOC (system-on-a-chip) your choices boil down to things like ARM, MIPs, PowerPC, etc. ARM was the most popular, and Zipf's law took hold.

The End.",4sggnd,t3_4sggnd,ItsMe_RandomNumber,,Comment,2,0,2
d5ah2mp,2016-07-13 08:14:56-04:00,visvis,,"In addition to the good points already mentioned, it is important to keep in mind that x86 was used in the original IBM PC, which was very important because it gave rise to lots of clones that had a similar setup to be able to run the same programs. Step by step, present-day Windows computers derive from these IBM clones and at every step users wanted to retain backward compatibility to be able to continue to run their old programs. The disadvantage is that this architecture accumulated many features that are no longer really useful and can be quite burdensome in optimizing the design for performance and power usage. When smartphones were introduced, there was no reason to run the old programs so it made sense to use a cleaner design (ARM) instead.

",4sggnd,t3_4sggnd,ItsMe_RandomNumber,,Comment,2,0,2
4sdspr,2016-07-11 18:55:24-04:00,lepriccon22,How do you program a programming language?,I.e. How do you get a computer to understand a language?  How does it actually make the transition from that language into assembly language?,,,,,Submission,44,0,44
d58j5o5,2016-07-11 19:29:28-04:00,simonorono,,"First, you define your language. Define its syntax, its semantics, everything.

Then, it comes the hardest part: developing a compiler/interpreter for it.

A compiler is just a translator, it takes source code from the language you defined and pours the equivalent program in another programming language (generally, with a lower level).

For example, the C compilers, read C code and translate them to assembly, them an assembler converts this assembly code into an executable. ",4sdspr,t3_4sdspr,lepriccon22,,Comment,14,0,14
d591b4r,2016-07-12 06:31:50-04:00,trenchgun,,So C-compiler is written in assembly? And assembler is written in machine code?,4sdspr,t1_d58j5o5,simonorono,,Reply,3,0,3
d5926ay,2016-07-12 07:19:22-04:00,Teslatronic,,"The first compilers/assemblers were. But once you write one C compiler in assembly, you can write another one in C itself, and use the assembly-based compiler to compile the C-based one, and then compile each new version of the compiler with the previous one. This is called bootstrapping.

Edit: see /u/bhrgunatha's comment for more detail.",4sdspr,t1_d591b4r,trenchgun,,Reply,12,0,12
d58kn91,2016-07-11 20:10:02-04:00,PastyPilgrim,,"This is the kind of thing that takes years of study to become proficient at, so it's very difficult to explain in just a handful of words but I'll try.

The core of it is that you need to write a compiler. A compiler takes your code and all of its friendly syntax, and turns it into assembly (the simplest instructions understood by your computer). Then an assembler takes your assembly and turns it into machine code (0s and 1s). The assembler's job is super easy because there's usually a 1 to 1 correspondence between a line of assembly and a line of machine code (each line of assembly becomes precisely one line of machine code and vice versa (with the exception of pseudo instructions)).

So the hard part in writing a language is writing the compiler, but that's usually because your compiler does a *ton* of optimization to make what you write run especially fast on your specific configuration of hardware. It wouldn't be very difficult to write a simple compiler, it just wouldn't be very efficient.

Anyway, assembly itself is already a pretty capable language (in most implementations), you can do any and everything (by definition) in assembly. So the core of your compiler will just be recognizing the syntax you create and translating that into the equivalent assembly code, which is then translated into machine code for whatever machine is running the code.

For example, assembly doesn't really have variables. Instead, you have registers (lightning fast little memory cells that are used to perform mathematical operations) and memory (slow but plentiful long-term storage). You can't keep everything in registers because there aren't enough of them. So what you might do to get variables in your language is to use a lookup table. To do this, you'll create a table in memory that maps a variable name to a type (if desired) and the location in memory where the value is stored. This way, when I create a variable in my language, my compiler will translate that instantiation into the instructions needed to allocate memory for the variable's value and to store the metadata in the lookup table. Then, when I use the variable, my compiler will translate my syntax into the equivalent assembly instructions needed to read the table and then load the value into a register (so that I can do work on it).

That's a compiler in a nutshell. You come up with rules, grammar, and syntax for a language, then you write the necessary code for understanding those rules and translating them into the equivalent assembly code, which is then translated into machine code.

What's interesting, is that in the modern day you don't even need to write your compiler in assembly. Instead, most languages use a language like C to implement their compiler, which has its compiler(s) written in assembly. By using a well-developed language like C, you can exploit the heavy optimization that other people that really know hardware and assembly are doing, and take advantage of the fact that C is easier (and more portable) to write than assembly.",4sdspr,t3_4sdspr,lepriccon22,,Comment,6,0,6
d58o9kl,2016-07-11 21:47:50-04:00,bhrgunatha,,"A famous artefact of early computing is the boot-strapping process where the goal is a self-hosting compiler - which lets you write the compiler for a new language in the new langauge. However to get to that point a lot of earlier innovations were needed.

Take all of this with a pinch of salt - the order and the details may be wildly inaccurate, but the overall ideas viewed from afar give an idea of how we got to the point that we can choose our own language to write a compiler for another language..

To start with, raw binary values had to be set in order to define and run a program. Those raw binary values represent instructions that tell the hardwaer what to do and data that the program needed to operate. This is now usually referred to as **machine code**. 

At first you would enter values into [computer storage using switches](https://en.wikipedia.org/wiki/ENIAC#Programming). 

Since that's so tedious and error prone, [puched cards](https://en.wikipedia.org/wiki/Punched_card) were developed along with the necessary hardware to read them so you could represent lots of values that could be read toagether. They had their own problems but it was a step forward from switches.

After some time symbolic instructions were defined as a shortcut for several machine code instructions - now usually called **assembly language**. For example put the value 8 and store it into a memory location 58  could be written as ST 8, [58]. This might take 3 machine code instructions, one represents the store instruction, one the value 8 and one the location 58. Since now assembly language could be written down it was easier to understand what the computer is being instructed to do. Naturally someone had the bright idea to make that automatic so that for example you could write down the instructions by hand, then create punched cards representing those instructions, convert them to machines code and then run the program. The conversion from the symbolic instructions to machines code was handled by a program called an **assembler** - people still write programs in assembly code and use [assemblers](http://www.nasm.us/) today.

The next logical step is to make the symbolic instructions more useful and less aimed at the mundane, physical processes that tells the computer exactly how to operate and more friendly for people to represent ideas. This is really the birth of **[programming languages](https://en.wikipedia.org/wiki/Autocode#Glennie.27s_Autocode)**. Since programming languages allowed you to do more abstract things  symbolically - like saving the current instructions location, branching off to another part of the same program to return later, the conversion to machine code became more complex.Those programs are called **compilers**.

Compilers allow you to write more useful programs - for example the first program that allowed you to connected a keyboard that lets you enter numbers and characters, one connected to a device to print numbers and characters, then later to display them on another device like a screen. From there you are quite free to write other programs. More languages and their compilers developed that were more suitable to represent more abstract ideas like variables, procedure and functions. 

During the whole process both **hardware** - the physical elctronic machines and devices and **software**, the instructions to get the machines to do useful work - were both developed and that process still continues.

There's a wonderful book called [Code by Charles Petzold](https://www.amazon.com/Code-Language-Computer-Hardware-Software/dp/0735611319/ref=sr_1_1/181-7743827-9922735?ie=UTF8&qid=1468287892&sr=8-1&keywords=charles+petzold+code) that details all of these developments, but actually researched and accurate.



",4sdspr,t1_d58kn91,PastyPilgrim,,Reply,5,0,5
d58ood5,2016-07-11 21:58:52-04:00,shvr,,"> Instead, most languages use a language like C to implement their compiler, which has its compiler(s) written in assembly.

GCC and LLVM, at least, are written primarily in C++. I'm not aware of any major compilers written only in assembly (but I admit to not having much in-depth knowledge of various compilers). Compilers that are written in the same language they compile are known as [self-hosting](https://en.wikipedia.org/wiki/Self-hosting).",4sdspr,t1_d58kn91,PastyPilgrim,,Reply,6,0,6
d58woja,2016-07-12 02:16:43-04:00,lepriccon22,,"Awesome, thanks :)",4sdspr,t1_d58kn91,PastyPilgrim,,Reply,2,0,2
d59q7e2,2016-07-12 17:05:59-04:00,lordvadr,,"We wrote a compiler for one of my CS classes in college.  The language was called YAPL (**y**et **a**nother **p**rogramming **l**anguage).

First thing first, as other's have mentioned, a compiler translates from one language to another...typically assembly...but could be any other language.  Our compiler compiled YAPL, which was a lot like Pascal, into C, which we then fed to the C compiler...which in turn was fed to the assembler.  We actually wrote working programs in YAPL.  For my final project, I wrote a functional--albeit VERY basic--web server.

With that said, it's quite a bit different for an interpreted language, but the biggest part for each is still the same. By far, the most complicated part of a compiler is the **parser**.

The parser is what reads a source code file and does whatever it's going to do with it.  Entire bookshelves have been written on this subject, and PhD's given out on the matter, so parsing can be *extremely complicated.*

In a theoretical sense, higher level languages abstract common or more complicated tasks from the lower level languages.  For example, to a CPU, variables don't have sizes or names, neither do functions, etc.  On one hand, it greatly speeds up development because the code is far more understandable.  On the other hand, certain tricks you can pull of in the lower-level languages (that can vastly improve performance) can be abstracted away.  This trade-off is mostly considered acceptable.  An extra $500 web server (or 100 for that matter) to handle some of the load is far less expensive than 10 extra $100,000 a year x86 assembly developers to develop, optimize, and debug lower level code.

So generally speaking, the parser looks for what are called *tokens*, which is why there are reserved words in languages.  You can't name a variable `int` in C because `int` is a reserved word for a type.  So when you name variable, you're simply telling the compiler ""when I reference this name again, I'm talking about the same variable.""  The compiler knows an `int` is 4 bytes, so does the developer.  When it makes it into assembly, it's just some 4 bytes somewhere in memory.

So the parser starts looking for keywords or symbols.  When it sees `int`, the next thing it's going to expect is a label, and if that label is followed by `(`, it knows it's a function, if it's followed by `:` it's a variable--it's more complicated than this but you get the idea.

The parser builds a big structure in memory of what's what and essentially the functionality.  From there, either the interpreter goes through and interprets the language, or for a compiler, that gets handed to what's called the emitter.  The emitter is the function that spits out the assembly (or whatever other language) equivalent `a = b + c:` happens to be.

This is complicated, but if you take it in steps, it's not really that hard. [This](https://www.amazon.com/Compilers-Principles-Techniques-Alfred-Aho/dp/0201100886/ref=sr_1_2?ie=UTF8&qid=1468357475&sr=8-2&keywords=compilers+principles+techniques+and+tools) is the book we used.  There's a much newer version out now.  If I can find my copy, I'll give it to you if you pay shipping.  PM me.",4sdspr,t1_d58woja,lepriccon22,,Reply,2,0,2
d58ii4o,2016-07-11 19:12:28-04:00,anamorphism,,you write a compiler or interpreter.,4sdspr,t3_4sdspr,lepriccon22,,Comment,3,0,3
d58izh2,2016-07-11 19:24:55-04:00,lepriccon22,,"What does that mean practically?  How do I teach a computer to understand what naming a variable is?  How do I teach it to understand what a for loop is, etc?",4sdspr,t1_d58ii4o,anamorphism,,Reply,1,0,1
d58jria,2016-07-11 19:45:52-04:00,anamorphism,,"you're not teaching the computer anything.

you're basically just writing a translation program.

you would write a program that takes in `for (int i = 0: i < 10: ++i):` and outputs assembly to represent what you want that line of code to do.

something like

- allocate 32 bits of memory for i
- write 0 to all of those bits
- allocate 32 bits for 10
- write 10 to those bits
- declare label 1
- compare i to 10
- jump to label 2 if i is less than 10
- add 1 to i
- jump to label 1
- declare label 2",4sdspr,t1_d58izh2,lepriccon22,,Reply,4,0,4
d58l8qp,2016-07-11 20:25:54-04:00,bhrgunatha,,"That's pretty much the last thing the compiler does. The classic structure for a compiler [has several phases](http://www.tutorialspoint.com/compiler_design/compiler_design_phases_of_compiler.htm). I say classic there because modern compilers can diverge from that. Anders Hejlsberg - who heads th team at Microsoft in charge of C# had an interesting chat about [modern compiler construction](https://channel9.msdn.com/Blogs/Seth-Juarez/Anders-Hejlsberg-on-Modern-Compiler-Construction) and there are other approaches for using many, many very small phases that chain together. 

EDIT: fix my poor typing skills.",4sdspr,t1_d58jria,anamorphism,,Reply,6,0,6
d59cy7h,2016-07-12 12:27:41-04:00,anamorphism,,"sure, but you don't try and teach someone who doesn't know algebra multivariable calculus.",4sdspr,t1_d58l8qp,bhrgunatha,,Reply,2,0,2
d59wlu2,2016-07-12 19:41:55-04:00,giggly_kisses,,That talk with Anders Hejlsberg was very interesting. Thanks for sharing!,4sdspr,t1_d58l8qp,bhrgunatha,,Reply,2,0,2
d58wowa,2016-07-12 02:17:09-04:00,lepriccon22,,"> allocate 32 bits of memory for i
> write 0 to all of those bits
> allocate 32 bits for 10
> write 10 to those bits
> declare label 1
> compare i to 10
> jump to label 2 if i is less than 10
> add 1 to i
> jump to label 1
> declare label 2

How does it know how to understand this, though?",4sdspr,t1_d58jria,anamorphism,,Reply,1,0,1
d59b88h,2016-07-12 11:51:48-04:00,juuular,,"At the end of the day these simple statements like these (move bit A to spot B, etc) boil down to hardware instructions (aka just 1's and 0's). At the very bottom is the circuit board - different electronic parts that interact through electricity. 

The most important part is the transistor - you can apply some voltage to ""set"" it to 1 of 2 states, and you can also apply some voltage differently to get which state it is in. That ""move bit"" command would be converted to an electrical signal to a transistor that makes it output it's state, and then route that signal to a different transistor that gets set.

This is massively oversimplified of course, but at the end of the day these instructions are converted into electrical signals being sent from one place to another and changing the state of different electrical devices.

Theoretically you could implement a CPU (and ""compile"" assembly instructions) by having slaves run around shuffling notecards with different numbers on them, just like we shuffle numbers around on silicon transistors. That would be much slower and more memory inefficient, though.",4sdspr,t1_d58wowa,lepriccon22,,Reply,1,0,1
d58j7lh,2016-07-11 19:30:50-04:00,CQQL,,The book [Structure and Interpretation of Computer Programs](https://mitpress.mit.edu/sicp/full-text/book/book.html) is a great introduction into that topic.,4sdspr,t1_d58izh2,lepriccon22,,Reply,1,0,1
4s7r2e,2016-07-10 18:29:49-04:00,simonthefoxsays,Does the elimination of the front side bus make litecoin easier to mine( and therefore less secure)?,My understanding is that litecoin is based on scrypt so that the difficulty of the problem comes largely from delays in memory communicating with the processor rather than from the calculation itself. Does the elimination of the front side bus and other technological advances significantly reduce the difficulty of that problem?,,,,,Submission,3,0,3
d579jb5,2016-07-10 20:16:48-04:00,ldpreload,,"scrypt has always assumed the (possible) existence of FPGAs, ASICs, etc. with lots of RAM. [The paper](https://www.tarsnap.com/scrypt/scrypt.pdf) in fact starts by designing a hash function that works well on a machine where RAM is fast and uniformly easy to access, and only then acknowledges that real-world machines have to deal with memory latencies. (Moreover, it claims that due to inherent physical limits about where you can put memory and the speed of light being fixed, memory latencies will ultimately _increase_ relative to CPU speeds.)

Its security comes from the fact that calculating a single scrypt hash requires accessing a large _amount_ of memory, and therefore it's hard to parallelize. It doesn't rely on it being slow (or fast) to access that memory: it just relies on the memory all needing to get accessed at some point, and as a result, a parallel scrypt cracker needs to parallelize all of that memory. If you want to build a Bitcoin miner / SHA-256 cracker, you can build a very small circuit that calculates SHA-256, and stick a ton of copies of that circuit on a custom chip. For the cost of a single consumer CPU, you can get many, many times better performance.  If you want to build a Litecoin miner / scrypt cracker, your task is much harder: each parallel unit needs _its own_ bank of a lot of memory. You can build custom chips that are better at the CPU side of scrypt than consumer hardware, and you will get better hash rates, but they won't be tons more cost-effective per dollar than consumer hardware.

In particular, because the scrypt algorithm requires a lot of random access to a lot of memory, the best way to build dedicated scrypt hardware is to give it memory that's random-access, cheap, and fast. But ... those are basically the market requirements for regular, consumer-grade RAM chips. There was a huge unrealized market for hardware that's good at SHA-256, because nobody needed to do that at scale before Bitcoin came out. Any hardware designer could have told you how to build cheap, fast SHA-256 calculators, but nobody would paid them to build it before Bitcoin happened. So hardware Bitcoin miners quickly outpaced miners running on general-purpose CPUs and GPUs. But there isn't a huge unrealized market for hardware that's good at scrypt, because hardware that's good at scrypt is just cheap, fast RAM. Every hardware designer who knows how to do that well is currently employed at a RAM manufacturer.",4s7r2e,t3_4s7r2e,simonthefoxsays,,Comment,10,0,10
d57r8rq,2016-07-11 08:02:06-04:00,simonthefoxsays,,"thank you, I felt like I must have been misunderstanding something.",4s7r2e,t1_d579jb5,ldpreload,,Reply,1,0,1
d57do9z,2016-07-10 22:17:13-04:00,BonzoESC,,"I suspect that it's only easier to mine because it's worth less per dollar of mining hardware and per dollar of mining power than Bitcoin, which means it's mined less than Bitcoin, and goes through fewer difficulty doublings. ",4s7r2e,t3_4s7r2e,simonthefoxsays,,Comment,2,0,2
4s6o7w,2016-07-10 14:26:30-04:00,--yy,Going to a place without internet for a week. What educational stuff/material should I download before I leave,"I'm a CS sophomore, but don't mind learning anything, even if it's not CS related, though that's likely what I'll enjoy.

So far I've got 

CS231N from Stanford.

Tony Alicea's - JS The weird parts (Just finished it though)

and his Learn and Understand NodeJS (halfway done :P )

And a bunch of webdev courses.

I don't mind more theoretical courses as well, like cs231n. I'll have loads of time, and can contribute at least a sustained 8-10 hours each day. This would be more conducive such courses too, only problem being I may burn out.

**Feel free to suggest things you feel we usually don't know enough about but should. Like say some particular math field, or even something like design.**",,,,,Submission,8,0,8
d5752b1,2016-07-10 18:09:25-04:00,bpg542,,Are you going on vacation? Maybe take a break from studying?,4s6o7w,t3_4s6o7w,--yy,,Comment,7,0,7
d57w4gm,2016-07-11 10:39:11-04:00,PM_ME_YOUR_PROOFS,,Honestly I read a lot during Vacation and think about ideas I'm working on during breaks. It's just how I am. I've met a lot of people like that too.,4s6o7w,t1_d5752b1,bpg542,,Reply,2,0,2
d57eurr,2016-07-10 22:51:12-04:00,None,,"Might find something [here](https://github.com/vhf/free-programming-books/blob/master/free-programming-books.md)

I'd recommend building something instead though. Maybe find some decent tutorials but if you get stuck on something then you will wish you had the internet. ",4s6o7w,t3_4s6o7w,--yy,,Comment,2,0,2
d57joii,2016-07-11 01:30:09-04:00,Jon_dhc,,You can use Khan academy's app and download the vídeos from any course that you want ,4s6o7w,t3_4s6o7w,--yy,,Comment,1,0,1
d57pu7d,2016-07-11 06:52:28-04:00,NorthsideB,,I believe you can download all of Wikipedia for only a few gb of space. ,4s6o7w,t3_4s6o7w,--yy,,Comment,2,0,2
d57tov0,2016-07-11 09:31:39-04:00,Murikk,,I use Dash on OSX for offline documentation and I love it!,4s6o7w,t3_4s6o7w,--yy,,Comment,2,0,2
d58gn0h,2016-07-11 18:24:47-04:00,CptMacHammer,,Download the pricessung ide and go through the Nature of Code by Daniel Shiffman. ,4s6o7w,t3_4s6o7w,--yy,,Comment,1,0,1
d57n97m,2016-07-11 04:17:55-04:00,venuswasaflytrap,,Porn,4s6o7w,t3_4s6o7w,--yy,,Comment,1,0,1
d571tay,2016-07-10 16:47:12-04:00,robotreader,,The wikipedia series on algorithms.  Do them all by hand.,4s6o7w,t3_4s6o7w,--yy,,Comment,1,0,1
4s5q81,2016-07-10 10:43:49-04:00,Ipsider,How to answer range queries with a set of count-min sketches?,"Hi there, I know how to answer a single point query using a count min sketch, but since it's bad to just sum up the elements of an interval (bad estimate since the errors add up). How do I use dyadic intervals to answer such a query? I realize I have to keep log(m) count min sketches but then what? I really don't get the description in the original paper (http://dimacs.rutgers.edu/~graham/pubs/papers/cmencyc.pdf)

EDIT: I should clarify that I don't have a problem to understand the idea, rather the implementation!

Thank you guys in advance!",,,,,Submission,3,0,3
4s449z,2016-07-10 01:04:16-04:00,opendoors1,Do I need a laptop with more Ram and processing power?,"I'm going to starting college, studying computer science. 

I'll be needing a laptop but I'm wondering whether I need anything powerful?

I could spend less money and build a more powerful desktop computer. Or I could spend more money and probably not have a new desktop for a while. 

Here are the choices I'm faced with:

_______________
**Macbook Pro**

**Storage:** 128GB

**Ram:** 8GB

**CPU:** 2.7Ghz i5

**Total:** $1,199

I'm not sure how CS is on storage space. But I could easily carry an external SSD in my backpack and use it whenever I would need to. 
   This option would also allow me to build a new PC which I could use for for gaming as well as programming. I would have 32GB RAM and and Intel i5 6600K 3.5Ghz Quad Core CPU. 

Or I could go with one of these two options and probably not be able to build a new PC for a while. 


_______________
**MacBook Pro**

**Storage:** 128GB (Could carry external SSD)

**Ram**: 16GB

**CPU**: 3.1GHZ i7

**Total**: $1,649


_______________
**Macbook Pro**

**Storage:** 256GB

**Ram**: 16GB

**CPU:** 2.9Ghz i5 

**Total**: $1,669

Thank you

Edit: Formatting",,,,,Submission,0,0,0
d56dhcx,2016-07-10 01:28:54-04:00,PastyPilgrim,,"You could get through a CS degree with just a raspberry pi and a 32GB SD card: you really don't need any serious power. I'll give you the same advice that I give everyone asking about what computer to get for a CS program: build a powerful desktop because that's fun, will teach you a little about components, and will let you do anything you'll ever want to do. And buy a chromebook and install Linux with crouton, which will allow you to bring a super lightweight laptop to your classes that will do everything you want (email, text processing, etc on Google's OS and programming on the Linux OS) and will have a battery that will last you all week (seriously, I just charged mine on the weekend).

Since it looks like your budget is <1700$, you could have fun building an awesome PC for $1200 plus maybe a bit more if you want a nice monitor and keyboard, and get yourself a [chromebook for $300](https://www.amazon.com/Acer-Chromebook-Aluminum-Quad-Core-CB3-431-C5FM/dp/B01CVOLVPA/). Then you're set.",4s449z,t3_4s449z,opendoors1,,Comment,8,0,8
d56dl4q,2016-07-10 01:32:48-04:00,opendoors1,,Thanks for the advice. What about storage space on chromebooks? I'm assuming that's where an SSD comes in? Or am I overestimating the amount of storage I am going to need?,4s449z,t1_d56dhcx,PastyPilgrim,,Reply,1,0,1
d56dric,2016-07-10 01:39:40-04:00,PastyPilgrim,,"The one I linked has 32GB of internal storage. Most of your CS projects will be a couple hundred lines. A semester long project may be a couple thousand lines. Regardless, those projects are like 6kB and you could fit *millions* of them in 32GB of storage. And for non-code work (like papers), you'd use Google docs, which is cloud based anyway.

The only reason that you'd need more than 32GB of storage is if you're also going to store music or something on your machine, but I think most people are using streaming services (like Google Play to store their collections or spotify for random music), which don't use any internal storage. And you could always get a 64 or 128 GB SD card for a few bucks, which can just live in your chromebook.",4s449z,t1_d56dl4q,opendoors1,,Reply,3,0,3
d56enzt,2016-07-10 02:17:04-04:00,opendoors1,,"Sweet. I've been taking a look around and the Toshiba Chromebook 2 looks really nice. 

Also, 2 of the biggest reasons on why I am considering a macbook is because pretty much everyone on compsi and programming says that the OS is great for it. And I know that a Macbook is going to last me years. ",4s449z,t1_d56dric,PastyPilgrim,,Reply,1,0,1
d56eygs,2016-07-10 02:29:47-04:00,PastyPilgrim,,"The Toshiba Chromebook 2 is what I have: it's great. I linked that Acer because it's currently the highest rated on Amazon, so I figured that it might be newer and better than the Toshiba CB2. It does have a quadcore CPU and reportedly a better battery life (12hrs v 9hrs) so it may beat out the Toshiba CB2.

A Macbook won't last as long as a desktop that you build yourself. Mac OS is decent because it's Unix-based, but Linux is just as good, but it's free, uses cheaper/better hardware, and will allow you to learn Linux, which you'll encounter in your career as a computer scientist.",4s449z,t1_d56enzt,opendoors1,,Reply,4,0,4
d56f8td,2016-07-10 02:42:59-04:00,opendoors1,,"I was checking that Acer out and it looked perfect until I found out it had no SD slot :(

But yeah I probably won't need more than 32 plus I always have flash drives ",4s449z,t1_d56dric,PastyPilgrim,,Reply,1,0,1
d56fay8,2016-07-10 02:45:43-04:00,None,,[deleted],4s449z,t1_d56f8td,opendoors1,,Reply,1,0,1
d56t4mc,2016-07-10 13:00:11-04:00,opendoors1,,"One more question. 

Would it make a Chromebook faster if I got a 128GB SD card, removed chrome os, and just went with Linux? Or would it not make much of a difference/not be worth it?

After hours of research, I decided to go with the Toshiba cb2 with an i3 since the Acer won't allow me to expand storage. Plus the Toshiba has a backlit keyboard and seems to benchmark higher. 

Thanks again for all the help. ",4s449z,t1_d56fay8,None,,Reply,1,0,1
d56eaiv,2016-07-10 02:01:02-04:00,AnArtistsRendition,,"You'll be fine for most projects without needing any more space, especially if you have a desktop. 

Additionally, I'd recommend installing Linux on your desktop. There are some projects (especially in graphics/ML courses) that require a decently powerful computer and a lot of RAM/hard-drive space. However, they're pretty infrequent so a powerful laptop is overkill. If you have a light, portable laptop and a desktop that you can ssh into (or use TeamViewer), you get the best of both worlds",4s449z,t1_d56dl4q,opendoors1,,Reply,1,0,1
d56lada,2016-07-10 08:40:18-04:00,artillery129,,"I'm going to disagree about the idea that you only need 32 GB of disk space, it depends on which platform you want to target. For example, Xcode alone is 10GB on my disk. If you install dependencies, and many languages, you'll find that you quickly run out of spaces. The other type of development you may be doing may involve VMS, perhaps a setup with vagrant or something, this will quickly consume all the space on your computer. I suggest something similar to the PastyPilgrim, a desktop computer, and an old second hand thinkpad. Why a thinkpad? Better compatibility than shitty chromebook, more options for upgradability, and it'll allow you to actually do your work remote. Many university networks make SSH'ing into your desktop PC very difficult. Why anyone would buy a chromebook when they can get an i5 thinkpad is beyond me (http://www.ebay.com/itm/Lenovo-Thinkpad-X201-12-1-Intel-Core-i5-540M-2-53GHz-4GB-250GB-Win-10-Notebook-/112051323762?hash=item1a16c78372:g:TXgAAOSw-KFXdIVK)",4s449z,t3_4s449z,opendoors1,,Comment,2,0,2
d56n573,2016-07-10 09:59:17-04:00,ukkoylijumala,,Exactly this. A Chromebook is not meant to be a general purpose thing. It is a trade-off: cheaper for the lack of a large persistent storage. This may be fine if there is sufficiently fast WiFi available most of the time. In my experience it is way more convenient to get a normal laptop and keep the files and applications on it and be more versatile and less dependent on a fast internet connection.,4s449z,t1_d56lada,artillery129,,Reply,1,0,1
d57ived,2016-07-11 00:59:44-04:00,jakob_roman,,"Another possibility is getting a good powerful laptop that isn't Apple.

http://www.dell.com/en-us/shop/productdetails/inspiron-15-7559-laptop/dncwpw5722b

That one has better internals than the Mac for only $1300. I have one and it's great. The only thing that's worse than the Mac is the trackpad (the mac's trackpad is outstanding), but it's better than many others out there.

It has a gtx960m graphics card too so it is a pretty decent gaming laptop.

Good luck on your choice.",4s449z,t3_4s449z,opendoors1,,Comment,2,0,2
d56g34y,2016-07-10 03:24:27-04:00,794613825,,"If you want a laptop, don't get a MacBook. They are overpriced, and underpowered. If you don't want Windows, you can just install Linux on a Windows machine, but MacBooks are the last thing you should look at. ",4s449z,t3_4s449z,opendoors1,,Comment,4,0,4
d56m5pb,2016-07-10 09:20:30-04:00,diMario,,"As a professional programmer, I concur. Apple products are overpriced and underpowered. They look slick, they work adequately, and Apple's marketing does a good job of making owners of Apple products appear sophisticated and world wise. What we used to call the Jet Set in my younger days. Which explains why many young persons find Apple's products so appealing.

For onboard storage I think you'll need whatever the operating system needs plus fifteen, maybe twenty gigabytes extra for your current projects and papers. Your notebook should have an SSD, that is a luxury well worth its price. For your backup and secondary storage get yourself an external USB drive, you can buy a 2000 Gb drive for under &euro: 100.

For memory, I'd say that 8Gb of RAM is enough, and especially if you're running linux. Processor wise, I would go for an Intel i5 from the broadwell or skylake range. If money is tight, an I3 will do.

As someone else suggested, you might want to get a desktop as your primary work and relaxation unit and use the notebook (or chromebook) on away missions. In that case I would like to point you in the direction of Shuttle's DH170 micro PC. It is amazingly small, and you can choose which processor you want, how many RAM, what size SSD etc etc. I have one with a i7-6700 and it is very *rapido* indeed. For a monitor you can get any generic full HD (1920 x 1080) for &euro: 150 at most. A wireless keyboard and mouse will set you back another &euro: 30. I currently have a Rapoo E9070 set which looks quite utilitarian and works quite well too.",4s449z,t1_d56g34y,794613825,,Reply,2,0,2
d56lovb,2016-07-10 08:59:31-04:00,MonsieurBanana,,"So far you've had two opinions:

- Buy a good desktop and a cheap chromebook

- If you want a laptop, don't get a mac because they are overpriced and underpowered

I kind of disagree with both. 

**First of all, about the mac :**

They are expensive, but not that overpriced. I know because I have a mac (work provided), I hate it, and have spend a lot of time lately finding a good replacement to buy. Let me tell you, laptops with equivalent build quality and performance are not much cheaper. Look at the thinkpad T460s or the dell xps 13/15, they're both top of the class (ultraportable) laptops and their price are between $1000 and $2500. Sounds familiar? That's the price of a mac. A bit less expensive, but really not that much.

And honestly even then I believe a macbook pro might have better hardware. Apple is pretty much the only one who still has 16:10 screens, much better in my opinion for developer work.

Now about OSX: I hate it. I hate it and I dream of the moment where I'll be able to switch back to linux. But pretty much all the developers I work with prefer OSX to linux, so there's that. Bottom line: if you can afford it, and it seems your budget is enough, and you like OSX, a MBP is a solid choice. But if you don't want top-class new laptop, you have very good ones for much cheaper, more on that later.

**Now about the other opinion, a very good gaming desktop and a chromebook:** 

I think it's fine, if for you gaming is more important than work. Or if you'll work most of the time at home with your desktop, I seldom work at home so for me and for me work is more important, so it's a no-no.

Basically my advice would be to spend less on the desktop, and more on the laptop. You don't need a $1200 desktop. That much money will get you a very very good desktop, and a less-than-average laptop.

Look at this [thread](https://www.reddit.com/r/buildapcforme/comments/4s3igt/700750_pc_for_video_editing_and_gaming/). They have links to two gaming desktop configurations for $750 that can handle any recent game with max, or near max, details. You don't need more, specially if that means having a less good laptop.

As for the chromebook, they are nice, but honestly underpowered. 32gb solid state disk,  **4gb of ram**. That will do the work, but the quality of life won't be there. Maybe you can later upgrade the SSD and RAM, I don't know so I won't comment on that. Used/refurbished thinkpads are the best bang for your buck that you can get: look at [this one on ebay](http://www.ebay.com/itm/Lenovo-ThinkPad-T450s-i7-5600U-FHD-1920x1080-8GB-180GB-SSD-Win-10-Pro-Laptop-/272278603091?hash=item3f6511c553:g:ObgAAOSwLVZVplg9) for $880. Twice the ram, 180gb ssd instead of 32gb. [This is a benchmark](http://www.cpubenchmark.net/compare.php?cmp%5B%5D=2762&cmp%5B%5D=2456) of the chromebook CPU vs the t450s one. The t450s is more than four times as fast than the chromebook. Thinkpads are one of the best laptops you can get. Solid, nice build quality, sadly not very good screens, best keyboard of any laptop, very long battery life, you can change the RAM SSD battery (which you can't with a mac). And you can find a bit older thinkpads, with almost the same performance, for even cheaper, for example $500 for a T440s.

Or of course something else than a thinkpad, but really, you should look into refurbished thinkpads, they are very very good for their price (more info at /r/thinkpad).

$750 for a pretty good desktop and $880 for a very good laptop, that's $1630 for the best of both worlds.",4s449z,t3_4s449z,opendoors1,,Comment,1,0,1
4ryicu,2016-07-08 23:14:30-04:00,Xzillerationer,Has Doom ever be run as an operating system?,"Where when running it's literally the drivers, and doom, and that's literally 100% running on the processor?   
That would be cool as fuck.",,,,,Submission,6,0,6
d555jqy,2016-07-08 23:23:19-04:00,koderpat,,[Someone did turn it into a administration tool](https://www.cs.unm.edu/~dlchao/flake/doom/chi/chi.html),4ryicu,t3_4ryicu,Xzillerationer,,Comment,10,0,10
d557tzw,2016-07-09 00:41:30-04:00,Teddy-Westside,,"Great read, thanks",4ryicu,t1_d555jqy,koderpat,,Reply,1,0,1
d55gzd9,2016-07-09 08:51:02-04:00,BonzoESC,,"The Super Nintendo version didn't depend on an OS. Didn't really have drivers either, since you don't need hardware abstraction when you're only running on a single hardware configuration.  ",4ryicu,t3_4ryicu,Xzillerationer,,Comment,6,0,6
d55yc6m,2016-07-09 17:45:36-04:00,HelleDaryd,,"The DOS version does pretty close to this, there is no other process being run, although some calls are being made to the BIOS in a few cases, but rarely, most will be direct or nearly direct hardware access.",4ryicu,t3_4ryicu,Xzillerationer,,Comment,2,0,2
d55icup,2016-07-09 09:50:37-04:00,umib0zu,,"I'm not sure about Doom, but this was a thing [Electrical Engineers implemented in Minecraft](http://minecraft.gamepedia.com/Tutorials/Redstone_computers) fairly quickly.",4ryicu,t3_4ryicu,Xzillerationer,,Comment,1,0,1
d55l2rm,2016-07-09 11:23:48-04:00,ZenithalEquidistant,,"I've seen a version done as a student project at Imperial College London where Doom was ported to bare-metal code on a Raspberry Pi. Another team implemented 2048, both were very impressive.",4ryicu,t3_4ryicu,Xzillerationer,,Comment,1,0,1
d5584de,2016-07-09 00:51:52-04:00,zefyear,,"It's not immediately clear (although very possible) that Doom *itself* is Turing complete in any meaningful way, although it could certainly be modified to appear to run an operating system.",4ryicu,t3_4ryicu,Xzillerationer,,Comment,1,0,1
d55e5wg,2016-07-09 06:00:10-04:00,farhaven,,"An operating system doesn't need to be turing complete. Turing completeness ""only"" means universal computation which you don't need for some daemon shooting.",4ryicu,t1_d5584de,zefyear,,Reply,6,0,6
d55xz5b,2016-07-09 17:35:07-04:00,zefyear,,"I was about to add after my last statement that some process or set of processes within Doom may be isomorphic to a lower order automaton, but I thought people would remember that the closed-parens problem is evidence enough that an operating system (In the classical sense) requires *at least* nondeterministic state machines, if not Turing completeness (if we're including interrupts and syscall implementation in our definition, Turing completeness is *CERTAINTLY* required)",4ryicu,t1_d55e5wg,farhaven,,Reply,1,0,1
d56iduo,2016-07-10 05:41:51-04:00,farhaven,,"> requires at least nondeterministic state machines, if not Turing completeness
> if we're including interrupts and syscall implementation in our definition, Turing completeness is CERTAINTLY required

I'm not sure I understand why handling interrupts and syscall requires Turing completenes and how NFAs play into the story here. Actually, it'd be a bit worrying if you could achieve universal computation just by doing a few system calls.

Resource management and handling interrupts does not require anything in the OS to implement something that can be bent to behave like a Turing machine. Yes, the thing needs to run on something that behaves like that, but that's it.

*Edit*: Almost forgot. If I remember correctly, the closed-parens-problem does not require an NFA. It requires something like a stack machine that can actually count, which finite automatons can't do.",4ryicu,t1_d55xz5b,zefyear,,Reply,1,0,1
4rstig,2016-07-08 00:06:49-04:00,Plazmatic,"Can you fit a Linear congruential generator to a series of required outputs to get a linear combination of Linear congruence generators, who's values can be derived using Gaussian elimination? [Xpost from /r/AskMath]","Wiki:  https://en.wikipedia.org/wiki/Linear_congruential_generator

So there are a few situations that go along with this, but I will use the base case. [ Origional posted thread](https://www.reddit.com/r/askmath/comments/4rsryg/can_you_fit_a_linear_congruential_generator_to_a/)


Suppose I use the following linear congruential function to generate pseudo random numbers, n's are supposed to be subscripts but doesn't work:

(a*seed_(n)_  + b) % m = seed_(n+1)_ 

I want to solve the following systems of equations

    (a*c + b) % m = W
    (a*W + b) % m = X
    (a*X+ b) % m = Y
    (a*Y + b) % m = Z

where a, c, b and m are all unkown, and W, X, Y, and Z are all known. 

Is this problem possible to solve? Is there a better way to go about this? 

Additionally I should mention that W,X,Y, and Z not being the same is fine, and that unique solutions are not needed. 

What if I change the set of equations to the following

    (a*c + b) % m = S
    (a*S + b) % m = T
    (a*T+ b) % m = U
    (a*U + b) % m = V
    (a*V + b) % m = W
    (a*W + b) % m = X
    (a*X+ b) % m = Y
    (a*Y + b) % m = Z

I know this is an overdetermined system, but solving such a system is required for this set of inputs.  I know it isn't supposed to accomplish anything but would splitting up b actually help at all in this instance to stop it from being an overdetermined system?  

UPDATE:

Found a few things related to this problem,  Chinese Remainder Theorem, though I know Z and don't know M and M is the same for each equation.  Modular Inverse and Euclids algorithm also are relevant, can't seem to relate everything together with what I'm trying to do. This post http://math.stackexchange.com/questions/43948/generating-sequences-using-the-linear-congruential-generator also is very close to what I'm trying to do.

This is also similar to what I'm attempting to do, except I'm trying to create an arbitrary PRNG, not crack an already existing one
http://security.stackexchange.com/questions/4268/cracking-a-linear-congruential-generator.

From my findings I've found that linear congruence generators can't necisarily encode any given sequence of numbers, however I'm willing to use combinational linear congruence as well, and switch to xor shift. 

Additionally even if I couldn't find the PRNG values from a given sequence, it would still be good enough for me to find X values in any given order, as long as those were the only generated numbers within a static frame. 



",,,,,Submission,2,0,2
d546qoj,2016-07-08 09:00:01-04:00,nopantstoday,,"As long as you have no follow up questions, yes. ",4rstig,t3_4rstig,Plazmatic,,Comment,1,0,1
d547119,2016-07-08 09:09:12-04:00,Plazmatic,,"So if I where doing this by hand, how would I solve for any given equation using modulo? I looked it up and no one seemed to have a formalized answer. x % m = 3, there should be multiple answers, but this also turns into a piece wise function.  How would a solver be able to take it in? I use python for short term projects, and I found this http://stackoverflow.com/questions/6789927/is-there-a-python-module-to-solve-linear-equations
second question has a comment that asks about Galois field, which i guess my problem would be, there shouldn't be an infinite number of solutions if you cap off the datatype ranges for a b c and m.  Doesn't modulo decompose into a different equation? 

Additionally can you help with the second part of the question? Can I spoof things, or do I need to deal with an over-determined system in a regular way?",4rstig,t1_d546qoj,nopantstoday,,Reply,1,0,1
d551s4t,2016-07-08 21:27:49-04:00,Plazmatic,,Updated thread after finding new information about the problem.,4rstig,t3_4rstig,Plazmatic,,Comment,1,0,1
d53wpj0,2016-07-08 01:26:25-04:00,ReedJessen,,Is this your Homework?,4rstig,t3_4rstig,Plazmatic,,Comment,-2,0,-2
d53wqrj,2016-07-08 01:27:35-04:00,Plazmatic,,"How the hell would this ever be homework?  I don't even know if this is possible to do, I'm trying to get the function and seed for a psuedo random number generator given a set of outputs so that I may generator those outputs again using said function and seed.  Ideally what I would like to do is have the seed and generator constants take up less space than the sequence of numbers they would generate (second part).  But I have a feeling this isn't possible in the way I would want.  If you've actually had a class or homework problem that actually went over this case I'm all ears.

I could alternatively use a known PRNG but I don't think there is a guarantee that every sequence of numbers, even up to 8 sets of ints can actually be generated by some seed.  If I had such a guarantee I would simply use some one elses light weight PRNG (like built in versions) and do the same thing but with only one unknown.  ",4rstig,t1_d53wpj0,ReedJessen,,Reply,2,0,2
d5r2gnt,2016-07-26 00:26:41-04:00,ReedJessen,,"so...  MS in Financial Mathematics/Discrete Maths courses cover this topic as a standard.

Carnegie Mellon's CDM course covers the topic:  https://www.cs.cmu.edu/~cdm/Homework/cdm-hw-nosol.pdf

As does...

Worchester Polytechnic:
https://www.wpi.edu/Pubs/ETD/Available/etd-050509-105426/unrestricted/jcopp.pdf
W

Maybe these texts I have provided you can help.",4rstig,t1_d53wqrj,Plazmatic,,Reply,0,0,0
4rq4it,2016-07-07 14:29:29-04:00,PorcTree,"Computer Science Degree holders, how does this degree program look","Hello,

I applied to the American College of Greece and was accepted. I really want to get my undergraduate in Greece, but I also want a degree in Computer Science. They don't have a ""Computer Science"" degree, but they do have an IT-Software Development Degree.

I was wondering what people that have gotten a bachelor's degree in Computer Science, or are currently pursuing one, think of the courses that are listed in this curriculum. 

http://www.acg.edu/dereeacg/academics_results.php?major1=64

(If you click the class codes on the left, there is a decent description of what the course covers, and you can also download the syllabus which will show all the content that will be learned in the class).

Is it close to a Computer Science degree or is it plain garbage in your view.

Also one other thing I was wondering? Do employers care where you get your degree from or does it really not matter?

Thanks!",,,,,Submission,7,0,7
d537fqa,2016-07-07 15:11:52-04:00,None,,[deleted],4rq4it,t3_4rq4it,PorcTree,,Comment,2,0,2
d537ot7,2016-07-07 15:16:56-04:00,PorcTree,,"Maybe the ""mathematics for computing"" class covers some of the math?

Description:
Mathematics for Computing - Level 5
Matrices. Vectors in 2-space and 3-space. Euclidean Vector Spaces. General Vector Spaces. Linear Transformations. Eigenvalues and Eigenvectors. Linear Algebraic Codes. The Logic of Compound Statements. Set Theory. Relations on Sets.

Not really sure.

Thanks for the input
",4rq4it,t1_d537fqa,None,,Reply,1,0,1
d537nol,2016-07-07 15:16:20-04:00,DeRickulous,,"The program itself looks fairly legitimate, if a little focused on programming over math, with a few classes I think I would have liked to take in school.

As to whether employers care.. that depends on who the employer is and what they're doing. If what you want to pursue is regular development work, you can get by with something much cheaper and less mathematically intensive than a computer science degree- frankly, I'm still on the fence as to whether mine was the right choice.

If you want the flexibility to enter academia/research development, or if there's a chance you might switch majors, though, college is probably the right path.",4rq4it,t3_4rq4it,PorcTree,,Comment,1,0,1
d5386o3,2016-07-07 15:26:45-04:00,PorcTree,,"I doubt I'll ever get into academia honestly, but I want a degree regardless. 

As far as cost goes, I'm prior military and the GI bill is going to pay my way through school and pay me monthly, so I prefer to go to school get and get an education. Better to get a monthly check to get a degree and go to school, than to just work.

Another concern is I may not be prepared for the the field of work if the curriculum is bad. I'm not the kind of person that just wants a piece of paper. I want to get as much of the skill as possible while attending school. So if higher math is a necessity to know what I'm doing, and do it confidently, I prefer that.

Thanks for your thoughts.",4rq4it,t1_d537nol,DeRickulous,,Reply,1,0,1
d53ace7,2016-07-07 16:08:47-04:00,anamorphism,,"are you interested in doing actual computer science (research, theory-based stuff) or just want to become a software developer?

that degree is what i would expect for a software development/engineering degree.

if you're more interested in the actual computer science academia world, minor in math and take more calculus, linear algebra and discrete mathematics classes to meet your requirements for the minor.

you'll most likely be moving on to get a master's if you want to do computer science anyway, in which case you'd be prepared to apply to another school at that point.

also, when you pick your natural science to meet ""Any Natural Science with a lab"", do yourself a favor and pick one of the math-heavier ones like physics or chemistry.",4rq4it,t3_4rq4it,PorcTree,,Comment,1,0,1
d53cw29,2016-07-07 17:00:08-04:00,PorcTree,,"""are you interested in doing actual computer science (research, theory-based stuff) or just want to become a software developer?""

I honestly wouldn't know if I'd be interested in the research or theory based stuff. I've taken a few programming classes, found them frustrating as hell, but enjoyed them and found them satisfying. So I like that aspect, and I've always liked the idea of studying computers and programming.

Would this curriculum prepare me at all to go for a masters afterwards? This school doesn't really offer high end math classes because it's mostly a business/liberal arts school.

I really wanted to stay in Greece and study, but I'm thinking it's better to go back to the US for my degree thinking I can get a better education.

I'd choose Physics anyways. I love physics, and I love math. Both are some of my favorite subjects. I would major in Physics if it had better job prospects without needing a PhD.

Any thoughts on if it'd be better for me to leave Greece and attempt to go to a better school? Or does this seem like it would suffice.

Thanks for your time",4rq4it,t1_d53ace7,anamorphism,,Reply,1,0,1
d53jzlq,2016-07-07 19:47:51-04:00,anamorphism,,"prepare you for a master's? sure. you just might have to take more classes to catch up on anything you missed.

do whatever you think is best for you. i certainly can't make the decision for you.",4rq4it,t1_d53cw29,PorcTree,,Reply,1,0,1
d53yfay,2016-07-08 02:29:17-04:00,TheTarquin,,"It looks roughly like my degree program was.  As others have said, mine featured more calculus, but honestly, for most programming roles calculus isn't needed much.  Also calculus is definitely a topic that's easy to self-teach from a good text book.

Other than that, it looks pretty standard.  My degree program has a Theory of Comp class, but it looks like most of that material is kind of covered in the two algorithms-oriented courses.

All in all, looks like a fairly normal CS program.  A little different than the one I went through, but seems to cover all the important bases.",4rq4it,t3_4rq4it,PorcTree,,Comment,1,0,1
d535wtq,2016-07-07 14:41:03-04:00,dxk3355,,"Looks similar to what I would of taken at school at the higher levels.  Lighter on Calculus than what I've seen around the US CS programs.  Also lacking chemistry and physics which were required in my school.

Where you got your degree matters quite a bit early on.  Employers hire from colleges and better program schools attract better companies in specific fields.  My school for example has all the major computer companies at all the job fairs.  Also you make connections to people in those companies with your classmates.",4rq4it,t3_4rq4it,PorcTree,,Comment,1,0,1
d537li2,2016-07-07 15:15:05-04:00,PorcTree,,Yeah the connections and interns/job fairs is a concern of mine.,4rq4it,t1_d535wtq,dxk3355,,Reply,1,0,1
d53hxtn,2016-07-07 18:57:15-04:00,achNichtSoWichtig,,"dont worry to much about stuff like this. if you use your time well and study seriously, it is no big deal. yeah, some schools have better connections, which maybe makes things a bit easier but in the end it comes down to what you know and can do.",4rq4it,t1_d537li2,PorcTree,,Reply,1,0,1
4rjw2j,2016-07-06 14:11:02-04:00,sanch_le_redditor,How do I get started ?,"Hey guys, I have recently developed an interest for computer science and was wondering what was the best way to get started ? I already took an introduction to computer science course at college where I learned some of the basics (logic, trees, internet security, binary and even some algorithm (pseudocode)). Now I wanna learn how to programme for real. I am particularly interested in understanding how the internet actually works and how everything is connected, ""where"" are websites, what's so amazing about the cloud, etc. I am not sure what i should do now. Is there a language I should learn first ? (and that I could learn or at least be confortable with by the end of the summer at best).

P.S. sorry if my english isn't the best, i'm french.",,,,,Submission,2,0,2
d51ppu6,2016-07-06 14:30:05-04:00,Crawldragon,,"You can find programming books on Amazon for just about any language that's at least moderately common.  If you don't want to spend any money I couldn't tell you where to learn the absolute basics outside of the ""For Dummies"" series, but websites like Codewars exist which can provide programming ""challenges"" of varying difficulty to help you nail down certain concepts like array sorting and working with strings and stuff like that.  If you don't understand something or can't figure out a difficult solution to a problem, simply Googling it is a surprisingly effective method to learn, especially if StackOverflow has an answer.

Still, for learning practical code, nothing really beats looking at other peoples' code and seeing how it works and fiddling with it.  There are a lot of open-source programs out there that you can just download and fool around with: you can look them up on Wikipedia.

Python is still a good beginner's language, as is BASIC, though I myself learned using Logo and C#.  Nowadays the programming world is diverse enough and has so many languages out there I couldn't even begin to tell you, but C and C++ are very solid languages that are still used for low-level code.  The difficulty curve on those is higher than usual, though, because of their unusual structure, but they've been used as the basis for a lot of other languages, including Python.

If you're going to do programming I would suggest you have a solid grasp of at least one functional programming language and one object-oriented language, since those are the two paradigms you're most likely to end up using.  I would suggest that you try to really get Java and C# down since they're really commonly used, and for web development it's hard to avoid JavaScript and PHP.  From what I understand CoffeeScript is a popular derivative of JavaScript, so you might want to glance over that as well.  When I was going to college everyone was telling me to learn FORTRAN and COBOL since legacy developers were in high demand, but I'm not sure if that's the case anymore and even if it is I would be surprised if it still is in a few decades with how much has been obfuscated by automation.

That's all I've got.  Hopefully some other users will prove more helpful.  Good luck.",4rjw2j,t3_4rjw2j,sanch_le_redditor,,Comment,1,0,1
d51wrdx,2016-07-06 16:49:44-04:00,Valberik,,I can PM you some links to some programming books I have saved on my computer in PDF form in about an hour if you would like. I also have a load of links and such too. ,4rjw2j,t3_4rjw2j,sanch_le_redditor,,Comment,1,0,1
4riqo4,2016-07-06 10:30:59-04:00,kalpof,Computer Science Curriculum,"Hi /r/AskComputerScience! I'm currently doing a free online bachelor in computer science using moocs on coursera, edx and udacity (something like [this](https://ossu.firebaseapp.com/#/curriculum), but a litte bit different). I started it since august 2015, but, after 1 year, i realized that mine curriculum can be bad built (link [here](http://freetexthost.com/6skgjw6ega) ). How can i create a better curriculum? What exams are in the wrong order? What can i delete? There's some important topic that i've missed?

Thanks to everyone and sorry for my bad english. ",,,,,Submission,2,0,2
d549soy,2016-07-08 10:25:34-04:00,visvis,,"This won't actually result in a degree, right? If it does, simply follow the requirements of the institute that grants the degree. If it doesn't, there really is no reason to follow a generic CS curriculum as you won't be getting a degree anyways. Instead, think of what you want to achieve and base your curriculum on that. For example, if you want to be a web dev there is very little reason to do stuff like physics.",4riqo4,t3_4riqo4,kalpof,,Comment,1,0,1
4rid8e,2016-07-06 09:05:13-04:00,PlainclothesmanBaley,"If a site is indexed on search engines, and it has a page which is not linked to elsewhere on the site, how easily can that page be accessed by someone who doesn't already know the link?","So, say you have a website that can be found on google, example.com.  If you then create a page with url example.com/qli380s0/ or whatever, and then you provide no link to your new page anywhere else on your website, how does this fit in?  Are people ever going to find that page?",,,,,Submission,15,0,15
d51gapg,2016-07-06 11:17:26-04:00,None,,[deleted],4rid8e,t3_4rid8e,PlainclothesmanBaley,,Comment,7,0,7
d51luwm,2016-07-06 13:13:05-04:00,hemenex,,What could change it *in practice*?,4rid8e,t1_d51gapg,None,,Reply,2,0,2
d51p54i,2016-07-06 14:18:33-04:00,flebron,,"A webmaster (or anyone, but usually the webmaster) manually adding the page to the search engine. For Google, via https://www.google.com/webmasters/tools/submit-url?continue=/addurl&pli=1",4rid8e,t1_d51luwm,hemenex,,Reply,2,0,2
d51p82y,2016-07-06 14:20:13-04:00,mistled_LP,,"To expand on the ""easily guessable by bots"" item, there are plenty of malicious bots that look for common directories. For example, 'wp-admin' due to WordPress, and 'administrator' due to Joomla. So your page name could run into something like that just due to the coincidence of the name itself.

But for your example of 'qli380s0', I agree that the odds are miniscule. Unless you're facebook or the like, I can't imagine anyone running a bot against you to find such a page.",4rid8e,t1_d51luwm,hemenex,,Reply,2,0,2
d5310z2,2016-07-07 13:03:18-04:00,earslap,,"Maybe the owner accesses the site through his own browser (for instance Chrome) and from then on Google at least gets to know that such a site exists. It's up to them to decide what they are going to do with that information.

Or maybe someone else is sniffing the packets on the network you are in while visiting that URL so that person gets to know the address of the site you are visiting. If he posts the link to anywhere else then boom, it might get indexed.",4rid8e,t1_d51luwm,hemenex,,Reply,1,0,1
d51p6p8,2016-07-06 14:19:26-04:00,AFineTapestry,,"""Security through obscurity is no security at all"".

Does anyone know who said that?",4rid8e,t3_4rid8e,PlainclothesmanBaley,,Comment,4,0,4
d51vxxp,2016-07-06 16:33:19-04:00,MALON,,You did,4rid8e,t1_d51p6p8,AFineTapestry,,Reply,7,0,7
4rgvg4,2016-07-06 00:54:24-04:00,DootyTooth,I Am Just Not Getting it!,,,,,,Submission,0,0,0
d515a0v,2016-07-06 04:19:32-04:00,Bottled_Void,,"What part don't you get?

The original programming provides a menu to return either X * X or X * X * X.",4rgvg4,t3_4rgvg4,DootyTooth,,Comment,1,0,1
4rdbuq,2016-07-05 11:49:09-04:00,ReedJessen,How many levels deep is the deepest directory on the average PC?,"Files are typically arranged in hierarchical structures.  An example might be:

Home -> User -> programs -> Hello_World -> bin -> run.exe

Home -> User -> programs -> Hello_World -> readme.txt

When we think about all the libraries and nested files required to run the average computer, my gut feeling is that one my descend deep down the rabbit hole to get down to the most core file of computing.

Is the total number of hierarchies between the top level and the bottom level an easily measurable thing?  Does this number have significant impact on efficiency or on some other parameter important to the computers function? Does this number differ significantly from architecture to architecture (MacOS to Windows for example)?",,,,,Submission,4,0,4
d5083iq,2016-07-05 13:22:57-04:00,dccr,,"First of all, [this article](https://blog.codinghorror.com/filesystem-paths-how-long-is-too-long/) (and the links therein) probably goes a long way in answering your questions. To try to answer your questions specifically:

&nbsp:

>Is the total number of hierarchies between the top level and the bottom level an easily measurable thing?

That shouldn't be too hard to find out for yourself, just like Jeff did in that article. If you're unfamiliar with [depth-first search](https://en.wikipedia.org/wiki/Depth-first_search) or [breadth-first search](https://en.wikipedia.org/wiki/Breadth-first_search), this could make for a good learning experience. If you think you're up to the task, try implementing one of these algorithms and having it walk through your file system.

&nbsp:

>Does this number have significant impact on efficiency or on some other parameter important to the computers function?

That kind of depends on the function in question. A program being buried deep in a directory path shouldn't run any slower than one sitting in the root directory. But searching for files is likely going to be slower if they're scattered through a complicated directory tree. 

That being said, your search speeds can be improved by indexing important or commonly used files, which I believe most modern operating systems do.

&nbsp:

>Does this number differ significantly from architecture to architecture (MacOS to Windows for example)?

Probably not enough to matter. Improving efficiency based solely on directory depth is pretty low-hanging fruit when we're talking OS optimization.",4rdbuq,t3_4rdbuq,ReedJessen,,Comment,7,0,7
d52zqjl,2016-07-07 12:37:19-04:00,ReedJessen,,Super helpful.  Thanks.,4rdbuq,t1_d5083iq,dccr,,Reply,2,0,2
d50g9vj,2016-07-05 16:14:43-04:00,mastermindxs,,"Directories are treated like file entries in the file system. Say you have some arbitrarily nested directory like '/my/apps/and/stuff' that 'stuff' directory will have an identifier. Any files created inside that directory will have a reference to that directory identifier. So, any one file will have a single reference to its parent directory. Therefore, it's easy for a file to find its relative place in the file system. 

The performance of any program isn't impacted by its place in the file system. 

This is because to the computer, every file is at the same level of nesting, it's flat, with only references to the parent directory being used. And, once the program runs, it doesn't care of where it ran from.

You can count your own file nesting with the use of a bash script, you'd have to recursively dig into every directory in your computer, until no branch (directory) has any further directories, all while keeping count of the nesting level, and then return that level. This won't give you an average globally of course, you'd have to ask a lot of people to do this with you so you could average out the numbers and I don't know of any studies in this regard, it's not an important issue. I can't find anything on Google with these terms so good luck.

If you want to count your own nesting levels, go on stackoverflow.com and ask this there. If you send me a link to your question I will answer with some code.",4rdbuq,t3_4rdbuq,ReedJessen,,Comment,3,0,3
d52zsfj,2016-07-07 12:38:18-04:00,ReedJessen,,"Yeah, I agree that it was probably figured out many years ago and is not minor of an issue now that they don't even discuss it in CS classes.  

Thanks for the feedback.",4rdbuq,t1_d50g9vj,mastermindxs,,Reply,2,0,2
d5087uf,2016-07-05 13:25:25-04:00,justlikestoargue,,"Directories are just a way to categorize and manage files. Think of your computer like a library - it's full of files (books.)

It sucks to have to look for a given book by going one by one through all the books until you get it, so it's easier when you categorize the books and organize them into different sections on shelves.

That's all directories do - the same way that a library is just a room full of books, your computer is just full of files, and the ""locations"" could be entirely arbitrary.",4rdbuq,t3_4rdbuq,ReedJessen,,Comment,2,0,2
d52zpvi,2016-07-07 12:36:57-04:00,ReedJessen,,"Right, but some books are farther away from the reading room and some are farther away from the check-out counter and take longer to fetch.  Sometimes a lot longer. Some files are deeper than others, wouldn't that affect the time it takes to call a function?",4rdbuq,t1_d5087uf,justlikestoargue,,Reply,1,0,1
d56mtx6,2016-07-10 09:47:27-04:00,diMario,,"> wouldn't that affect the time it takes to call a function?

My opinion would be ""no"". In order to execute a function that resides in a program that is located somewhere on your hard disk several actions need to happen:

* The program file must be found, using a filename.        
* The program file must be loaded into memory.        
* The memory structure that is now the program must be *linked* to all external entry points that it refers to.

The speed of the first step, locating the program file, admittedly depends a little bit on the organization of your file system. However, in sheer amount of data being peeled off your hard disk, the second step (reading the file) usually is much larger. The first step only needs to read and interpret very small parts of your hard disk. It will take more reads if your program is buried in a deep subdirectory, because the locating algorithm needs to process more indirections.

But reading the bulk of the program file into memory is by far the more work intensive task. Data is usually read in segments of 4 kilobyte, and these can be all over the place on your disk. So to find a file ten levels deep you'll need to do about twelve or thirteen reads (there is some overhead involved), but to load a modest 400 Kb executable you'll need to do at least a hundred reads.

Also note that modern operating systems have a built in file caching mechanism that uses all memory not taken up by actual programs to store the contents of recently accessed portions of your hard disk. So the first time you search for a file ten levels deep, it will actually have to perform all the disk i/o to find it. The second time you want to use that file, your operating system can find everything it needs in the RAM and thus does not need to perform actual disk i/o. Which makes the second access of your file's content literally hundreds of times faster than the first.
",4rdbuq,t1_d52zpvi,ReedJessen,,Reply,1,0,1
d51814f,2016-07-06 07:01:55-04:00,robotreader,,">my gut feeling is that one my descend deep down the rabbit hole to get down to the most core file of computing

It's a good intuition, but wrong in this case.  Directories nest, when they're well organized, from most general to most specific.  Core files are actually in their own top-level folder, and generally only a few levels deep in that folder.",4rdbuq,t3_4rdbuq,ReedJessen,,Comment,2,0,2
d52zkyz,2016-07-07 12:34:09-04:00,ReedJessen,,Cool.  Thanks.,4rdbuq,t1_d51814f,robotreader,,Reply,1,0,1
d504gn5,2016-07-05 12:07:14-04:00,iamtoe,,I'm pretty sure it doesn't work like that at all.,4rdbuq,t3_4rdbuq,ReedJessen,,Comment,4,0,4
d5057u7,2016-07-05 12:23:06-04:00,ReedJessen,,What doesn't work like what?,4rdbuq,t1_d504gn5,iamtoe,,Reply,1,0,1
d50n0pz,2016-07-05 18:47:00-04:00,okmkz,,"As an addendum to OPs question, what (if any) limits exist to this value for common filesystems?",4rdbuq,t3_4rdbuq,ReedJessen,,Comment,1,0,1
d56n0rm,2016-07-10 09:54:39-04:00,diMario,,"I seem to recall that there was a limit in the total length that a pathname (directories plus filenames) could have in some older file systems used by DOS and early Windows. So the limit was not so much in the nesting depth itself, but in the total length taken as a result of all the subdirectory names in the complete path to the file.

[Here](https://msdn.microsoft.com/en-us/library/aa365247\(VS.85\).aspx#maxpath) is some outdated documentation explaining the limits that used to exist.",4rdbuq,t1_d50n0pz,okmkz,,Reply,2,0,2
4rbg1z,2016-07-05 02:52:48-04:00,Lotton,What OS do you prefer?,Ever since I upgraded to windows 10 my virtual machine has been causing me to blue screen along with blatant performance issues so i have decided it is time to move on and get a better computer. This would be purely for programming and classes. Would I be better off using a Mac or buying a cheaper computer and installing Linux. Even if the answer is subjective I just want to hear thoughts to help give me a final push into an OS and not regret spending a lot of money,,,,,Submission,7,0,7
d4zugbs,2016-07-05 07:00:05-04:00,Stonegray,,"Personally, I prefer mac. It's unix based, so it has many of the upsides of linux. Control of the system from the shell is amazing: there's a command for anything you can do with a mouse. It doesn't have a package manager out of the box, but there are a few (I like Brew) that work great. The hardware has it's upsides and downsides. You mostly get what you pay for. 

I switched to Mac and Sublime 3 years ago, never looked back. Hard to imagine how I lasted so long on Windows and Notepad++ now.",4rbg1z,t3_4rbg1z,Lotton,,Comment,9,0,9
d559ukm,2016-07-09 01:59:23-04:00,roboborbobwillrobyou,,"> there's a command for anything you can do with a mouse

Do you even terminal bro?",4rbg1z,t1_d4zugbs,Stonegray,,Reply,1,0,1
d5034no,2016-07-05 11:39:14-04:00,dxk3355,,Personally I prefer windows 7.  10 isn't bad but I haven't used it much.  Linux just annoys me with half-baked applications for my day to day tasks.  And mac is too expensive for me even though I'm making ~75k a year ,4rbg1z,t3_4rbg1z,Lotton,,Comment,4,0,4
d50stmv,2016-07-05 21:13:57-04:00,None,,"Thanks for telling us how much you make a year, very pertinent information.",4rbg1z,t1_d5034no,dxk3355,,Reply,-3,0,-3
d50gnuu,2016-07-05 16:22:54-04:00,not_an_evil_overlord,,I prefer linux (I use mint) for work and osx for my home stuff. But I also like gaming so I have windows too. I welcome all operating systems.,4rbg1z,t3_4rbg1z,Lotton,,Comment,3,0,3
d50sfpt,2016-07-05 21:03:52-04:00,dkHD7,,I feel you. The only reason I haven't ditched windows entirely is because of my steam library. Lol. ,4rbg1z,t1_d50gnuu,not_an_evil_overlord,,Reply,5,0,5
d4zqldt,2016-07-05 03:10:48-04:00,dkHD7,,"It's really all about preference. If you're considering Linux, it depends on what you want. If you prefer stability, look for the debian based and Ubuntu based linux OSes, like Linux Mint or Solydxk. If you want more of an OS to tinker with and be on the cutting edge of package releases, arch Linux or Gentoo is the way to go. For me personally, Manjaro is my favorite. It has the ease of use like Ubuntu, but the cutting edge software of an arch OS (as Manjaro uses arch repos). Don't be afraid to disto-hop either. If you're using a Linux OS, though, make sure any kind of software you use for school has a Linux version or variant.",4rbg1z,t3_4rbg1z,Lotton,,Comment,5,0,5
d50mz0e,2016-07-05 18:45:48-04:00,Houly,,Antergos uses arch repos. Manjaro uses their own.,4rbg1z,t1_d4zqldt,dkHD7,,Reply,1,0,1
d4zxawj,2016-07-05 09:05:51-04:00,king3730,,"+1 for Manjaro. Been using it since it's early alpha days. The package manager extensions are quite nice, they have done a bunch of work on the installation process is a plus and the `mhwd` (Manjaro Hardware Detection) saves lives. ",4rbg1z,t1_d4zqldt,dkHD7,,Reply,1,0,1
d4zs1xw,2016-07-05 04:32:10-04:00,bartturner,,DEC - VMS,4rbg1z,t3_4rbg1z,Lotton,,Comment,2,0,2
d50m5s3,2016-07-05 18:26:31-04:00,cipherous,,I use windows 10 and ubuntu (on Virtual box).  I used to use OSX on my macbook but opted to go the PC route.,4rbg1z,t3_4rbg1z,Lotton,,Comment,2,0,2
d50p1x7,2016-07-05 19:37:48-04:00,rfinger1337,,Windows 7.,4rbg1z,t3_4rbg1z,Lotton,,Comment,2,0,2
d50iipu,2016-07-05 17:02:45-04:00,yendrdd,,"I was able to get a used Thinkpad x220 with an 2nd gen i5, 4GB of RAM, and 128 GB SSD off of eBay for less than $120.

It's extremely light. But the computer is 5 years old so it doesn't have a 1080p screen, and you'll be limited to OpenGL 3.1.

I run Linux on this machine but I have Windows on anther laptop and my gaming rig. I find programming to be easier on Linux.",4rbg1z,t3_4rbg1z,Lotton,,Comment,2,0,2
d5033ak,2016-07-05 11:38:27-04:00,marco262,,"Like others here, I CA  confirm that it's very easy to distro-hop in Linux, which makes experimenting much easier. Most distros I've messed with have a Bootable USB image option, and most Linux distros come with a USB writer application to make creating that Bootable USB easy. 

However, I work mostly in an old copy of Windows 7, since I do a lot of work on embedded systems and third party hardware, Windows is required for compatibility with a lot of those. I'd you can get your hand on Windows 7, I hear there are some unscrupulous folk out there who have figured out how to use the 30-day trial indefinitely. That's not something I would ever do though, not at all. ",4rbg1z,t3_4rbg1z,Lotton,,Comment,1,0,1
d50884e,2016-07-05 13:25:34-04:00,notUrAvgITguy,,Lenovo x230 with Ubuntu MATE. I love it.,4rbg1z,t3_4rbg1z,Lotton,,Comment,1,0,1
d50ydea,2016-07-05 23:43:58-04:00,justlikestoargue,,Windows 10,4rbg1z,t3_4rbg1z,Lotton,,Comment,1,0,1
4razma,2016-07-05 00:32:59-04:00,SoulOfAMachine,What networking layer(s) do operating systems work with?,"As I understand the basic picture looks like this:

networking device <--> device driver <--> OS <--> application

Where the networking device/driver handles layer 1 and 2 tasks. My question is, what does the OS actually send and receive to/from the driver? Is it IP packets or layer 2 frames? Does the device know the IP address of the computer?
",,,,,Submission,4,0,4
d4zxwjt,2016-07-05 09:25:50-04:00,elpantalla,,"I believe you are looking for this
https://en.wikipedia.org/wiki/OSI_model",4razma,t3_4razma,SoulOfAMachine,,Comment,3,0,3
d50f36q,2016-07-05 15:49:24-04:00,MyKingdomForAShip,,"I believe the NIC drivers do all the layer 2 stuff for you, so the OS would pass the IP packet down for encapsulation. (And the NIC would strip off the frame headers then pass the IP packet up in the other direction).

The NIC only needs to know its MAC in order to function so it wouldn't store the machine's IP in a basic implementation.  ",4razma,t3_4razma,SoulOfAMachine,,Comment,2,0,2
4r5np9,2016-07-04 01:04:42-04:00,zSilverFox,Do integrals of time complexity functions tell us anything?,"Kind of specific, I know integrals give the area under a curve. Say something like bubble sort has an upper bound of O(n^2 ). Does the integral of that tell us anything useful like the amount of time or information computed? Are there any applications of integrals to algorithm analysis at all?",,,,,Submission,11,0,11
d4zauu5,2016-07-04 18:16:00-04:00,drummyfish,,"I've never heard about integrals used in this context, but I'm not super into complexities.

Integral of a function that takes amount of data as input and outputs time gives you a quantity whose unit is data * time (in a same manner that for example integral of a function of power over time gives you work = power * time). I don't know what data * time is - it's something that tells you that either a lot of time was spent computing or a lot of data was processed. Whatever that is, you'll get an upper limit of that for given algorithm, as big O notation is an upper boundary.

The values of definite integrals don't make much sense to me in this context because, for example, if you have a function with O(1) complexity and a function of O(2) complexity and you integrate these functions from 0 to 1, you'll get 1 from the first one and 2 for the other, even though O(1) is a same complexity as O(2). So basically I would only be interested in forms of indefinite integrals. These will be interlinked with complexity functions via more or less bijection:

* 1 => x, i.e. constant to linear
* 2 => 2x, i.e. constant to linear
* x => x^2 / 2, i.e. linear to quadratic
* log(x) => x(log(x) - 1), i.e. logarithmic to linearithmic
* ...

So basically integration spits out a function that belongs to a class of more complex functions. That's what integrals do, they raise the degree of polynoms and basically make functions more ""complex"". But you'll still have the same complexity classes, you'll just be calling them differently.

So basically I can't think of a use for integrals here, because complexity functions are here to tell us classes which we can compare, not concrete number values. But I really don't know, maybe some knows more.",4r5np9,t3_4r5np9,zSilverFox,,Comment,3,0,3
d4yhi1n,2016-07-04 01:13:03-04:00,VideotapeReturn,,I don't remember the exact problem but one time in my algorithms class we upper bounded the sum of an infinite series with an integral (I believe we had to first prove some regularity condition to show it was valid). Solving the sum analytically was impossible afaik. ,4r5np9,t3_4r5np9,zSilverFox,,Comment,2,0,2
d52ky9n,2016-07-07 04:26:48-04:00,qwerty_danny,,"I think a problem is that our function T(n) for computational complexity operates on integers. We take input sizes and output the amount of ""work"" i.e. instructions we need to perform, which is also an integer. Integration typically works on real numbers.  
  ",4r5np9,t3_4r5np9,zSilverFox,,Comment,1,0,1
d4yschh,2016-07-04 10:21:26-04:00,RamsesA,,"If the function describes the complexity of a single step n in a larger iterative process, then I believe the integral is the complexity of the larger process.",4r5np9,t3_4r5np9,zSilverFox,,Comment,1,0,1
4r41fy,2016-07-03 17:58:41-04:00,jdorje,What is the inverse of latency?,"""Latency"" is measured in seconds, i.e. 10 milliseconds.  The multiplicative inverse would be per-seconds, i.e. 100 hz.  But what do we call this?  Normally hz would be associated with ""frequency"" but that term seems considerably off in this context.

(Edited for clarity.)",,,,,Submission,5,0,5
d4y6xfs,2016-07-03 19:23:07-04:00,human_tendencies,,Responsiveness.,4r41fy,t3_4r41fy,jdorje,,Comment,18,0,18
d4y55r4,2016-07-03 18:31:26-04:00,None,,[deleted],4r41fy,t3_4r41fy,jdorje,,Comment,3,0,3
d4y5b2x,2016-07-03 18:35:34-04:00,jdorje,,"Well you want lower latency. 

But normally higher numbers are good.

So it'd be convenient to say ""you want higher antilatency"".",4r41fy,t1_d4y55r4,None,,Reply,2,0,2
d4y5o2o,2016-07-03 18:46:09-04:00,None,,[deleted],4r41fy,t1_d4y5b2x,jdorje,,Reply,4,0,4
d4y5qwx,2016-07-03 18:48:26-04:00,jdorje,,"Well instead of 1 microsecond to 1320 seconds, it would be 1 megahertz to 750 millihertz.

Yeah it makes little sense to measure it that way, but makes some sense to convey the frequency.",4r41fy,t1_d4y5o2o,None,,Reply,2,0,2
d4y6ro6,2016-07-03 19:18:21-04:00,themeaningofhaste,,"Not necessarily. The frequency only really makes sense if it's cyclic. So in a very simple example, if you have one bit of data being transferred with some time, a frequency really isn't well defined in this case even though the time is. The units might work out but aren't justified in this case, so I would then argue that it also doesn't make sense to convey the frequency unless you start talking about a maximum frequency, which carries with it some other assumptions as well.",4r41fy,t1_d4y5qwx,jdorje,,Reply,1,0,1
d4y73o2,2016-07-03 19:28:16-04:00,tyggerjai,,"Exactly. Frequency isn't the inverse of time, it's the inverse of *period*. The time between a syn and an ack may be latency, but it's not periodic, even over multiple packets. 

In this case, there's no inverse, it's just ""minimising latency"".",4r41fy,t1_d4y6ro6,themeaningofhaste,,Reply,5,0,5
d4y7ie1,2016-07-03 19:40:36-04:00,jdorje,,"Yeah frequency isn't the right term.

I do like responsiveness.

Mathematically the number I'm talking about is 1/latency.  Whether it makes sense to actually use that value could depend on the context, sure.",4r41fy,t1_d4y6ro6,themeaningofhaste,,Reply,4,0,4
d4y5gbr,2016-07-03 18:39:48-04:00,big-blue,,"More/higher speed or throughput? This kind of depends on the question, whether you mean latency in I/O hardware, networking, etc.",4r41fy,t1_d4y5b2x,jdorje,,Reply,1,0,1
d4y5o8s,2016-07-03 18:46:18-04:00,jdorje,,"Throughput is not the same meaning at all.

Maybe I'll just stick with antilatency.  I don't see that it depends on which latency you mean.  Latency has a specific meaning, the time between events, and its inverse has a specific meaning too - but not, apparently, a specific name.",4r41fy,t1_d4y5gbr,big-blue,,Reply,1,0,1
d4y9g8a,2016-07-03 20:39:32-04:00,bamacal,,"If you are considering latency as in the time between events, you might use inter arrival rate as the inverse?I would be thinking along the same lines as the previous poster and consider latency in terms of I/O completion times (coming from a storage background), so latency is the given time for an operation to complete and throughput would be the rate of command completions over a given time period.",4r41fy,t1_d4y5o8s,jdorje,,Reply,1,0,1
d4y9xed,2016-07-03 20:54:27-04:00,jdorje,,"""arrival rate""?  Maybe.  But ""rate"" sounds a bit too much like throughput, perhaps.

Throughput and latency are completely different things, as you say.  ",4r41fy,t1_d4y9g8a,bamacal,,Reply,1,0,1
d4ytuir,2016-07-04 11:07:36-04:00,alecbenzer,,"> Throughput is not the same meaning at all.

I thought of 'throughput' when reading the question. https://www.reddit.com/r/AskComputerScience/comments/4r41fy/what_is_the_inverse_of_latency/d4ytrs2",4r41fy,t1_d4y5o8s,jdorje,,Reply,1,0,1
d4z3vn0,2016-07-04 15:11:32-04:00,jdorje,,"Generally I would say latency is the delay before the first event, while throughput is the rate of events happening after the first.  It does depends on the field though. ",4r41fy,t1_d4ytuir,alecbenzer,,Reply,1,0,1
d4yd5vt,2016-07-03 22:40:17-04:00,Hairshorts,,"> normally higher numbers are good

That's not true. Latency, golf score, error level, number of turns, path length, and waste are all things you might want to minimize. Priority queues are often implemented as min-heaps by default.

Minimizing when the absolute minimum is 0 is also nice because you can tell how far you are from optimal.",4r41fy,t1_d4y5b2x,jdorje,,Reply,1,0,1
d4ydtvs,2016-07-03 23:02:14-04:00,antonivs,,"If a request, query, or transaction takes 10 milliseconds, then (in an idealized sequential scenario) you can perform 100 sequential [requests per second](https://en.wikipedia.org/wiki/Web_server#requests_per_second), [queries per second](https://en.wikipedia.org/wiki/Queries_per_second), or 
[transactions per second](https://en.wikipedia.org/wiki/Transactions_per_second).

These are all measures of [throughput](https://en.wikipedia.org/wiki/Throughput).

In the simplest, idealized case - a single server processing requests sequentially - latency and throughput may be direct inverses of each other, and in that context ""throughput"" seems to be an answer to your question.

However, in general, the relationship between latency and throughput is dependent on the architecture of the system in question, and is affected by factors like concurrency, load, request complexity, network latency, distance between nodes involved in the request, etc.
",4r41fy,t3_4r41fy,jdorje,,Comment,3,0,3
d4ytrs2,2016-07-04 11:05:33-04:00,alecbenzer,,"I think qps/throughput is the best answer, the main difference being that you don't generally talk about single request's ""throughput"", but you can talk about its latency.

> concurrency

You can clarify and say throughput of a particular thread.

> load, request complexity

In aggregate, this should affect latency and throughput equally, no?

> network latency, distance between nodes involved in the request

These shouldn't be a factor as long as you're comparing apples to apples, right? (ie, end-to-end throughput vs. end-to-end latency, or server-side throughput vs. server-side latency)",4r41fy,t1_d4ydtvs,antonivs,,Reply,2,0,2
d4zgt2u,2016-07-04 21:12:13-04:00,antonivs,,"> You can clarify and say throughput of a particular thread.

Sure. Part of what I was thinking of is that if you look at a service from the outside, the fact that say a Google request has 100ms latency doesn't tell you anything useful about the throughput of Google as a whole, so in a sense the inverse relationship breaks down, even though it may still exist at some level.

Your other points are valid. I was trying to cover a lot of different scenarios without getting specific. As things get more complex, typically all you can do is talk statistically in terms of averages, distributions, etc., and put bounds on latency and throughput.
",4r41fy,t1_d4ytrs2,alecbenzer,,Reply,1,0,1
d4ybe6m,2016-07-03 21:42:03-04:00,gameplace123,,"* “Instantaneous”
* “0 Latency“
* “Virtually no wait”
* More variations of the word 'instant'.",4r41fy,t3_4r41fy,jdorje,,Comment,1,0,1
d4ybi67,2016-07-03 21:45:46-04:00,jdorje,,"I think you misunderstand the question.  I mean the multiplicative inverse of the latency value, not the act of having no latency.",4r41fy,t1_d4ybe6m,gameplace123,,Reply,2,0,2
d4yqe48,2016-07-04 09:08:14-04:00,gameplace123,,"I didn't misunderstand the question. What you're asking for doesn't exist. I was giving you a way to describe the lack of latency as you stated in one of your replies.

> So it'd be convenient to say ""you want higher antilatency"".

If you're describing a project or product to someone, saying 'you want higher *antilatency*' is ridiculous. Instead, use the terms I gave you.",4r41fy,t1_d4ybi67,jdorje,,Reply,1,0,1
d4z3mb6,2016-07-04 15:05:10-04:00,jdorje,,"I get your point that there's no term for it and no mathematical reason you'd want to use it,  but the multiplitive inverse of latency certainly exists: you just divide 1 by the latency.  

My context is in giving two numbers to a layperson to represent two latency values.  Trying to always explain the lower value is better then leads into an explanation that the actual unit is nanoseconds, and hence to confusion. If I could say one value is just twice the responsiveness of the other it's still a mathematically sound representation,  that nonetheless makes more sense in normal English. 

So yeah,  gonna go with responsiveness. ",4r41fy,t1_d4yqe48,gameplace123,,Reply,1,0,1
4qv1ah,2016-07-01 21:00:54-04:00,CuntFaggotAssRape,Trying to write a regex to validate urls,Hey all just trying to validate some urls with a regex expression. Any help would be appreciated even if it is just telling me what to look out for. ,,,,,Submission,0,0,0
d4w4id2,2016-07-01 21:29:24-04:00,None,,[deleted],4qv1ah,t3_4qv1ah,CuntFaggotAssRape,,Comment,8,0,8
d4wpncv,2016-07-02 12:34:52-04:00,darthandroid,,"This link has all the information you need: TL:DR: Validating URLs is insanely complex, and a proper regex is over 500 characters long.",4qv1ah,t1_d4w4id2,None,,Reply,3,0,3
d4x3iu8,2016-07-02 19:20:20-04:00,okiyama,,"I'm a regex noob. Does shorter generally mean faster? The Stephen Hay solution looks good enough for my own personal purposes, but if it doesn't make much difference may as well use the one with full coverage. ",4qv1ah,t1_d4w4id2,None,,Reply,1,0,1
4qtvnd,2016-07-01 16:33:38-04:00,Tara_ntula,Advice on pursuing a job/career in programming without college education?,"This question probably gets asked a lot here but as I'm trying to search for answers I'm having a hard time finding what I'm looking for.

My boyfriend has has to struggle throughout childhood. He came from an abusive and poor household. College was a very last-second decision for him senior year of HS and thus we ended up at the same university.

However school has never been easy for him and so he dropped out of the university. He took a few classes in software development at a community college in my college town and thoroughly enjoys it, but has stated he can't pursue a degree right now because he is too stressed with trying to survive without any parental help and 2 very inconsistent jobs.

He wants to get certification in the software development realm so he can get a better paying job and start working towards a career. 

I was wondering if this subreddit had any advice on what types of certification he could look into, the types of jobs he can apply for/look for/work towards, some essential things he could start learning on his own, etc. your own life experiences would help greatly, as well.

Thanks",,,,,Submission,3,0,3
d4w1lch,2016-07-01 19:58:55-04:00,Frekkon,,"This question is better suited for r/cscareerquestions
",4qtvnd,t3_4qtvnd,Tara_ntula,,Comment,6,0,6
d4wm9f5,2016-07-02 10:48:54-04:00,trenchgun,,He could do Harvard CS50 Edx version. I found it to be a good and inspiring resource. https://www.edx.org/course/introduction-computer-science-harvardx-cs50x,4qtvnd,t3_4qtvnd,Tara_ntula,,Comment,1,0,1
d4wh325,2016-07-02 06:50:45-04:00,None,,"This sub is more for science related questions but ya, he can definitely pursue it. Some people do better learning on their own than in school. There is a lot of demand for web developers and you don't need a certification. You just need to be make websites. He should learn the basics like html, css, javascript, jquery, and a serverside language like php or ruby and then he should learn a framework like laravel or rails. 

This takes time and it sounds like he is busy surviving but some people do well having a goal to pursue on the side. it gives them hope. 

Check out [codeacademy](https://www.codecademy.com/) for a basic intro and then he can find other courses and tutorials as he decides what path he wants to follow.",4qtvnd,t3_4qtvnd,Tara_ntula,,Comment,1,0,1
4qtnhr,2016-07-01 15:47:23-04:00,odhran666,How far can you go in Machine Learning with limited mathematical knowledge?,"I've applied for a graduate course in artificial intelligence. For my undergrad I did some neural net and genetic programming stuff and managed to get by well enough with only a basic understanding of the mathematics involved. 

However, machine learning seems more mathematical than anything I've done before, and if the CalTech course is anything to go by, I'm totally lost by the [second lecture](https://www.youtube.com/watch?v=MEG35RDD7RA) where he discusses [Hoeffding's inequality](https://en.wikipedia.org/wiki/Hoeffding%27s_inequality).

Is ML all about working out formulas and manipulating functions? Can I do any non-trivial work with only a knowledge of the algorithms?",,,,,Submission,1,0,1
d4vzxqi,2016-07-01 19:12:14-04:00,zkxs,,"From my personal experience, just far enough to pass the drop-out deadline, forcing you to withdraw and get a big W on your transcript.",4qtnhr,t3_4qtnhr,odhran666,,Comment,3,0,3
d4wujx3,2016-07-02 14:58:35-04:00,odhran666,,"Ouch, W is one of my least favorite letters. Was there anything in particularly you struggled with?",4qtnhr,t1_d4vzxqi,zkxs,,Reply,1,0,1
d4wty20,2016-07-02 14:40:53-04:00,HeraclitusZ,,"The way I have found it, there are kind of 3 sides to the coin. 

On the one, AI typically refers to the very mathematical CS field. You cannot get anywhere here without learning the math. It involves things like game theory. If you don't expect to be doing a lot of math in CS, you don't expect to be doing CS.

On the next, machine learning typically refers to the data science field. This will involve math as well, but more contained in statistics and a little closer to application. It involves things like data mining for prediction.

Finally, there is the software engineering side, which just uses knowledge of the algorithms and understanding of their usefulness to get them into real world application. It involves things like getting driverless cars working.

And of course, these distinctions are nowhere strict so they flow into each other all over the place: so much so that you could easily refer to any of them as AI/machine learning, and most related projects will involve pieces of them all. And there are some closely related topics don't clearly belong to any one of them but are definitely part of the general area, like genetic programming and computer vision.

So the only way that jumps out at me that you can do non-trivial work (in the field, not *about* the field) with only a modicum of math is if you are doing some highly software engineering work like searching for new applications of existing methods or testing exiting methods against each other to determine empirical effectiveness. 

But if you are going to grad school in the field, there is no way they won't provide opportunity to learn the necessary math for what you want.",4qtnhr,t3_4qtnhr,odhran666,,Comment,2,0,2
d4wvgb0,2016-07-02 15:25:37-04:00,odhran666,,"Thanks, that is a good breakdown. The frustrating thing is, if I could just see a code sample I'd understand what was happening, but courses online and some books I've seen rely totally on math notation. I think if I keep going through online courses for the terminology, and read through some open source ML libraries I'd be in a good position for September. ",4qtnhr,t1_d4wty20,HeraclitusZ,,Reply,1,0,1
d4w1kq5,2016-07-01 19:58:24-04:00,tyggerjai,,"I would say at the graduate level you need the math. ML tools for working with data are becoming more ""user friendly"", but if this is a course on implementing ML and AI, then it's going to be innately math heavy - statistics, calculus and linear algebra. ",4qtnhr,t3_4qtnhr,odhran666,,Comment,1,0,1
d4yfa44,2016-07-03 23:51:24-04:00,Hairshorts,,"Machine learning classes are usually about learning the math behind the algorithms, the usefulness and limitations of different algorithms and techniques, and implementing some of the algorithms. Implementing the algorithms yourself for a class is about getting a better sense of how the algorithms are implemented in general, not about producing a great machine learning library.

If you are going to *use* machine learning, there are well-tested libraries out there that implement most of the algorithms you'll want to use. If you're using someone else's implementation it's more important that you understand how it works on a mathematical level rather than understand the specifics of the implementation.

If you're going to do research into machine learning itself then you need to understand both the math behind it and how to implement any new contributions you come up with.",4qtnhr,t3_4qtnhr,odhran666,,Comment,1,0,1
4qtbxw,2016-07-01 14:44:28-04:00,ArbitraryPotato,Is it possible to put my android studio app onto my iPhone without jailbreaking it?,"Title, I made an app and want to put it on my iPhone.",,,,,Submission,0,0,0
d4vpy0x,2016-07-01 15:11:21-04:00,lhamil64,,"Even if you jailbreak it, its not really possible. They are two completely different platforms, and use completely different languages and APIs to write the apps. But you can develop in your app's core in something like C++ that can be natively compiled on both platforms, and make the porting process easier. See [this StackOverflow question](http://stackoverflow.com/questions/5296545/is-there-an-easy-way-to-convert-android-application-to-ipad-iphone) for more details. ",4qtbxw,t3_4qtbxw,ArbitraryPotato,,Comment,1,0,1
4qrfdl,2016-07-01 08:16:07-04:00,Puddleglum567,What foreign language would be advantageous to know as someone going into the software engineering field?,"This question was asked a few years ago and people said Arabic or Mandarin, or possible Japanese, but I do not know if this is still the case 

Thanks :)",,,,,Submission,5,0,5
d4v9v3o,2016-07-01 09:07:48-04:00,-Hegemon-,,"1) English

2) English

...

8) English

9) German

10) Russian",4qrfdl,t3_4qrfdl,Puddleglum567,,Comment,26,0,26
d4vf6h7,2016-07-01 11:21:55-04:00,yes_thats_right,,"Spoken language only really helps you to understand books, articles, tutorials etc which you will look at when you need help solving particular problems. Knowing English is sufficient to get all the help you need. You will get practically no advantage by learning another language.",4qrfdl,t3_4qrfdl,Puddleglum567,,Comment,3,0,3
d4vwism,2016-07-01 17:41:17-04:00,smellyrobot,,"Best spoken language: Klingon. Use it to impress people in interviews and at meetups!

Best written language: Emoji. With the rise of Slack based ChatOps a mastery of pictographs is, in my opinion, a practical necessity.",4qrfdl,t3_4qrfdl,Puddleglum567,,Comment,4,0,4
d4vczch,2016-07-01 10:31:50-04:00,dxk3355,,"English is a wider margin.  Somewhere down the line is Mandarin & Russian.  Good luck learning any of these Mandarin is tonal and Russian is considered really difficult to learn.

Nobody needs to know Arabic, the middle east doesn't generate code.  Don't bother with anything Indian like Hindi, they speak so many languages in India that it's going to default to English.",4qrfdl,t3_4qrfdl,Puddleglum567,,Comment,3,0,3
d4v9s6y,2016-07-01 09:05:16-04:00,dejmjin,,"English. (since all online documentation is in this language)

Not sure why Arabic might help in software engineering. (Source: speak Arabic..)",4qrfdl,t3_4qrfdl,Puddleglum567,,Comment,3,0,3
d4viq1d,2016-07-01 12:37:36-04:00,Adnotamentum,,"Where would you like to work? Most first world countries have good demand for software engineers. If you want to move to Germany, Sweden, or France, learn their languages - but bear in mind that that's probably not going to be a requirement to work there. If not, don't bother.",4qrfdl,t3_4qrfdl,Puddleglum567,,Comment,2,0,2
d4vk4wq,2016-07-01 13:07:31-04:00,Filmore,,"English in general

Russian if you want foreign contractors who are good

Mandarin if your company is super cheap and focuses on cost over quality",4qrfdl,t3_4qrfdl,Puddleglum567,,Comment,2,0,2
d4vb0fh,2016-07-01 09:41:46-04:00,segfaultbear,,"Well, about the only Spanish has been good for is connecting with my boss. ",4qrfdl,t3_4qrfdl,Puddleglum567,,Comment,1,0,1
d4w1mp2,2016-07-01 19:59:59-04:00,-Hegemon-,,"Yeah, I'm a native Spanish speaker and never use it for IT stuff",4qrfdl,t1_d4vb0fh,segfaultbear,,Reply,2,0,2
d4w2j0g,2016-07-01 20:27:31-04:00,xisx,,"Hindi? I work with many folks from India and while they speak English, there are heavy accents. I think I could understand them better with a lot more exposure. ",4qrfdl,t3_4qrfdl,Puddleglum567,,Comment,1,0,1
d4wza5j,2016-07-02 17:15:24-04:00,yybb,,"The only way it will help at all is if you want to work in a particular non English speaking country. Do you want to live in China or Taiwan some day?  Then learn Mandarin.  Generally though it won't help. Learning a language is a huge time commitment and your time is better spent studying technical topics.

If you're interested in learning a foreign language for its own sake, just pick one that interests you. ",4qrfdl,t3_4qrfdl,Puddleglum567,,Comment,1,0,1
d4viw02,2016-07-01 12:41:08-04:00,brennanfee,,Chinese (Mandarin).  Lots of manual testing is now outsourced to China.,4qrfdl,t3_4qrfdl,Puddleglum567,,Comment,0,0,0
4qpd21,2016-06-30 22:10:43-04:00,WACMM,"Any good audiobook styled IT, Comp Sci lesson to listen to?","Im looking to start listening to books to help me fall asleep, zone out when exercising, or just listening to something while working..But with a twist!

I want it to be educational or a course of some sorts that would teach me new things or help me remember old things related to computers and programs and such.

Basically like a training course but one I can just listen to and actually learn something.

Have any of you suggestions?",,,,,Submission,6,0,6
d4v1r17,2016-07-01 02:29:48-04:00,singham,,Most of the stuff taught in Comp Sci would requires you to pay attention to a white board or slides. Just audio only conceptual exposition is difficult.,4qpd21,t3_4qpd21,WACMM,,Comment,1,0,1
d4v81ex,2016-07-01 08:02:43-04:00,987f,,Thus the question.,4qpd21,t1_d4v1r17,singham,,Reply,4,0,4
d4v8bvc,2016-07-01 08:14:17-04:00,dxk3355,,I would disagree: most professors seem to use shitty power points now and just read off it.  Those could make audiobooks.,4qpd21,t1_d4v1r17,singham,,Reply,1,0,1
d4vnxmc,2016-07-01 14:27:43-04:00,WACMM,,"I understand where you're coming from, I guess I should have worded it better.

I should have also mentioned I am a comp sci major with my bachelors degree, in which I already have some experience in some areas. I just was trying to find a refresher lesson or something along those lines.

You are right though. They do require some attention to boards and diagrams and such.",4qpd21,t1_d4v1r17,singham,,Reply,1,0,1
4qnqwa,2016-06-30 16:14:18-04:00,Sniksder16,Need to learn how to use Vizuly and was told I would need to learn JavaScript and HTML which would be a better starting point?,"So I am tasked with making stuff in vizuly but I only know Java no scripting languages, I asked on the companies forums and was told I would need to learn HTML, CSS, and JavaScript and was wondering what would be a better starting point? ",,,,,Submission,2,0,2
d4ugp5g,2016-06-30 16:35:51-04:00,yooman,,"Start by learning basic HTML, and learning about the concept of the Document Object Model (DOM) and how pages on the web are structured. Then learn CSS, and how it is used to apply styles to the page. Then learn basic JavaScript, and how it can be used to manipulate the DOM and its styles, among many other things.  Finally, learn about jquery and other popular JavaScript frameworks to see what you can do with it. Then go try to apply your knowledge to the task at hand.  Good luck and welcome to web programming!",4qnqwa,t3_4qnqwa,Sniksder16,,Comment,2,0,2
4qj506,2016-06-29 21:28:24-04:00,kitfreddura,What would you like in a programming language?,"Hey all,

I just started a subreddit for a project I think would be both fun and interesting to work on. A crowd sourced programming language where every feature is proposed, discussed, and implemented by the community. If you're interested check out /r/openlang!",,,,,Submission,8,0,8
d4tjodl,2016-06-29 23:52:03-04:00,panderingPenguin,,Turing Completeness.,4qj506,t3_4qj506,kitfreddura,,Comment,14,0,14
d4twhy1,2016-06-30 09:13:52-04:00,taniaelil,,"Not absolutely necessary, Coq and Idris are two languages that trade Turing completeness for guaranteed termination.",4qj506,t1_d4tjodl,panderingPenguin,,Reply,3,0,3
d4tqb13,2016-06-30 04:24:41-04:00,antiprosynthesis,,Lol.,4qj506,t1_d4tjodl,panderingPenguin,,Reply,1,0,1
d4vday3,2016-07-01 10:39:29-04:00,dxk3355,,"Something like JavaDocs for the APIs.

Compiler that tells you what the fuck is wrong in a meaningful way.

",4qj506,t3_4qj506,kitfreddura,,Comment,3,0,3
d4ulkj0,2016-06-30 18:26:03-04:00,idontchooseanid,,Low level!,4qj506,t3_4qj506,kitfreddura,,Comment,1,0,1
4qi4w7,2016-06-29 17:45:52-04:00,Cant_Think_For_Shit,Did anyone go into their major not knowing anything about computer science? How did you turn out?,"I am curious because I start in the fall. I went to a high school that focused on different trades. Mine was computers. I learned some basic coding and networking etc. I consider myself an expert in basic computer use.

On the side I enjoy coding in HTML and python. I've known a little bit about ruby and php as well.

",,,,,Submission,8,0,8
d4t7ymm,2016-06-29 18:32:04-04:00,minimim,,"Most people that go into CS courses are in this situation.

Do you like math?",4qi4w7,t3_4qi4w7,Cant_Think_For_Shit,,Comment,15,0,15
d4t8if7,2016-06-29 18:45:30-04:00,Cant_Think_For_Shit,,Love math. I am transferring to a college that offers me a CS major. But right now I am a math tutor,4qi4w7,t1_d4t7ymm,minimim,,Reply,5,0,5
d4t8l8w,2016-06-29 18:47:25-04:00,minimim,,"CS is a course about Math. If you like it, you'll like Computer Science.",4qi4w7,t1_d4t8if7,Cant_Think_For_Shit,,Reply,4,0,4
d4tnz9n,2016-06-30 02:28:39-04:00,thedboy,,I did. Currently inpatient at a psychiatric hospital undergoing treatment for major depression and anxiety issues. It could have gone better I suppose. ,4qi4w7,t3_4qi4w7,Cant_Think_For_Shit,,Comment,10,0,10
d4ti7hi,2016-06-29 23:08:15-04:00,panderingPenguin,,"Almost everyone does. I did and I got a job at Microsoft straight out of school. Doesn't matter where you start, it matters how hard you work.

Also just a sidenote, isn't this kind of an oxymoron?

>I consider myself an expert in basic computer use.",4qi4w7,t3_4qi4w7,Cant_Think_For_Shit,,Comment,6,0,6
d4tty83,2016-06-30 07:43:54-04:00,Cant_Think_For_Shit,,"lol I was waiting for someone to say that :p Thanks for all the help everyone, it is appreciated. ",4qi4w7,t1_d4ti7hi,panderingPenguin,,Reply,1,0,1
d4t8prw,2016-06-29 18:50:28-04:00,francispoop,,"I did. In high school we have a choice of what to focus on before going to college so I did architecture/drafting (I like to draw). Then moved to a different country and ended up taking CompSci in College/University. Got my degree and I work as a full stack web developer now, and honestly it was the best decision I've ever made.",4qi4w7,t3_4qi4w7,Cant_Think_For_Shit,,Comment,4,0,4
d4u4yp9,2016-06-30 12:29:15-04:00,SuperSerialSlap,,"I had no idea what I wanted to do. I just crossed Computer Science on the application because it sounded interesting. But after 3 years in the program I've become the president of the university's ACM, presented research at academic summits and even worked with NASA.

I think that if you have the will and the passion you can definitely make it. The fact that you enjoy coding on the side tells me you already have the right stuff. Just never stop learning.",4qi4w7,t3_4qi4w7,Cant_Think_For_Shit,,Comment,2,0,2
d4u9gc0,2016-06-30 14:04:09-04:00,Cant_Think_For_Shit,,Thank you for your insight!! Very inspiring ,4qi4w7,t1_d4u4yp9,SuperSerialSlap,,Reply,1,0,1
d4ty49z,2016-06-30 09:57:25-04:00,codythisguy,,"I thought you said ""out of your major"" and I was mildly amused at the thought of someone sleeping through 4 years and still graduating",4qi4w7,t3_4qi4w7,Cant_Think_For_Shit,,Comment,1,0,1
d511x3r,2016-07-06 01:41:58-04:00,xxkid123,,"Went in with the basic ability to fix computers and run Linux without typing rm -rf in the root directory. 

I'm only a year in so I'm not much of a software engineer, but I can write a pretty mean linked list if you ask me.",4qi4w7,t3_4qi4w7,Cant_Think_For_Shit,,Comment,1,0,1
d4u0j2y,2016-06-30 10:54:20-04:00,Pardomatas,,You go to college to learn things,4qi4w7,t3_4qi4w7,Cant_Think_For_Shit,,Comment,1,0,1
4qi2mw,2016-06-29 17:33:19-04:00,__october__,Pumping Lemma: is this proof correct,"So I have to show that the language L is not regular. Here's the [definition of the language along with my proof sketch](http://i.imgur.com/VnSnq3T.gif).

Now, my argument is that the last step reveals a contradiction to the statement that s' is in L, since there will always be more a's than b's (k+p > p), which goes against the definition of the language. Thus the language does not satisfy the pumping lemma's conditions.

Is my reasoning correct or am I missing something?",,,,,Submission,2,0,2
d4thrly,2016-06-29 22:55:31-04:00,_--__,,"Your reasoning is correct, but you certainly need to add more details to your sketch - in particular:

* what is p? 
* why are u and v necessarily of the form a^q and a^k ? 
* why is q+k≤p? i.e. why must w end with b^p ?
* why is s∈L?

Of course, all these things are trivial, but it is important to realise that a proof is an *argument* and without these details what you have cannot be considered a proof.",4qi2mw,t3_4qi2mw,__october__,,Comment,2,0,2
4qfiyv,2016-06-29 09:33:37-04:00,FreddyClement,"Why are underscores often sorted to be before upper case letters, despite underscores being lexicographically greater than upper case letters?","For example, why is the file _Test.txt sorted above Text.txt in [this example](http://i.imgur.com/KiSFIgm.png). I imagine it may have something to do with windows not having case sensitive characters?

Another example is SSID names, it's fairly common for businesses to prefix their WiFi access point name with an underscore so it appears at the top of the list. What method of sorting is used to show this above upper case letters.

'_' has the value of 95 while 'A' through 'Z' have the values between 65 and 90.",,,,,Submission,12,0,12
d4snvlu,2016-06-29 11:42:09-04:00,dk-,,"The sorting in Windows is not lexicographical at all. They are using a natural sort order. Note for example how files with the filenames ""1.txt"", ""2.txt"" ... ""10.txt"", ""11.txt"" is sorted in that order and not ""1.txt"", ""10.txt"", ""11.txt"", ""2.txt"" and so on. As for why they have decided that ""_"" should come before letters I don't know.",4qfiyv,t3_4qfiyv,FreddyClement,,Comment,10,0,10
d4sx9su,2016-06-29 14:47:01-04:00,ACoderGirl,,"I assume it's a generic ""symbols come before letters/numbers"" idea. Which makes perfect sense to me. After all, it lets you bring important stuff to the top of the list. Who hasn't put a symbol in front of a particularly important file so that it would be at the top of the list?

I'm too lazy to check properly, but a quick test makes it look like symbols are all sorted by unicode code point. Then presumably it's a culture independent sorting for the letters (eg, so that ""Ñ"" is sorted with ""N"").",4qfiyv,t1_d4snvlu,dk-,,Reply,4,0,4
d4u7743,2016-06-30 13:15:45-04:00,dk-,,Yup. I found a couple of documents online with recommendations on sorting orders that also recommended symbols before letters.,4qfiyv,t1_d4sx9su,ACoderGirl,,Reply,1,0,1
d4tfcrg,2016-06-29 21:47:49-04:00,GoldenDragonXIV,,"I did a bit of internet sleuthing as I was curious to know the answer to this as well, and came across [this](https://answers.microsoft.com/en-us/windows/forum/windows_7-files/windows-7-file-name-sort-order/b6bb0847-4b98-4f6b-85da-fe6b65516153?auth=1) post from 2011 - more specifically, the reply from 
¡Firedog.

It seems that microsoft has their own sort order (i.e, not based on the order of ASCII characters) that changes from version to version, where 'special' characters are sorted before alphanumeric ones.",4qfiyv,t3_4qfiyv,FreddyClement,,Comment,2,0,2
4qdzyq,2016-06-29 01:45:34-04:00,Hawker_G,"MTP Library,OpCodes, Datacodes","Recently I have been looking into MTP(Media Transfer Protocol) and upon realizing that there is poor java library support for transferring files from a MTP device to a Windows computer, and back, I thought I would attempt to make my own library. Now I am an beginner to intermediate Java programmer so I may be biting of a bit more then I can chew but I am learning as I go and am enjoying the process.

So recently I stumbled upon a pdf document explaining MTP from the USB implementers Forum. In this they seem to have a ton of methods and ways of getting information from a MTP device. They have DataCodes, Op Codes, and many more ways of getting information from MTP devices. The thing is I have never worked with OpCodes or Datacodes and would have no idea how to even send the codes to my device to get this information, run these methods, or what that would entail. Any help would be appreciated as far as, how I can send these codes to my device to execute these methods and working my way back up from the low level machine code to implementing something for Java.

EDIT:  Figured it would help if you saw the pdf I was talking about so here is a link to the wiki where you can download it:
https://en.wikipedia.org/wiki/Media_Transfer_Protocol#cite_note-spec1.1-4

TLDR:
I stumbled upon OpCodes and Data Codes for Media Transfer Protocol that gives information and or transfers it. How do I use these codes to interact with a MTP device. Then once doing that use that information to make a Java Library for transferring files on a MTP device.",,,,,Submission,1,0,1
4qco1p,2016-06-28 20:12:35-04:00,defblind,What data mining method can I use to select only specific texts to read out of a pool of thousands and thousands of pages?,"Absolutely tech illiterate person here, so bear with me please:

I'm in charge of locating documents in which an institution of my country analyzes past year's public budget to detect the misuse of public funds. These documents are extremely long and dissect the inner functioning of our public administration, so they are full of technicalities. 

However, after reading hundreds and hundreds of pages I started to note that the parts of the documents in which they refer to possible corruption cases all look alike. That is, the words and the phrases used are repetitive and the structure of the sentences is similar.

So I was wondering if there is any method I could implement to just feed a program all these documents, to detect which ones have a similar structure to those which I already know contain possible instances of corruption.

The documents are in spanish, by the way",,,,,Submission,8,0,8
d4s0p7u,2016-06-28 21:45:35-04:00,tRfalcore,,"machine learning / Information Retrieval with clustering algorithms or similarity algorithms (cosine similarity, bayes similarity)  are what you want.  check out this free book from stanford http://nlp.stanford.edu/IR-book/



You feed the algorithm an [active learning](https://en.wikipedia.org/wiki/Active_learning_\(machine_learning\)) data set where you tell it what text looks like that is fraudulent, then let them go search the rest of the documents.",4qco1p,t3_4qco1p,defblind,,Comment,2,0,2
d4s1r88,2016-06-28 22:15:04-04:00,defblind,,"This sounds like super useful! Thanks a lot!
",4qco1p,t1_d4s0p7u,tRfalcore,,Reply,2,0,2
4q7vf6,2016-06-28 01:41:22-04:00,AaronWardHere,How do I create a traffic simulation using specific parameters?,"Hi, I am looking to create a traffic simulation application where I can program when the lights should be red, green. Random vehicles coming in the roads, specific parameters that I can set in the traffic lights when specific kinds of vehicles come to an intersection. Is there a particular software that I can use to do this or should I start from scratch? If I do start from scratch, any ideas on how to? I am good with Java but not so much with the graphical UI to create this. Any help is appreciated, thanks.",,,,,Submission,9,0,9
d4rwhpr,2016-06-28 19:49:33-04:00,NearSightedGiraffe,,AnyLogic is simulation software that is decent for a large number of simulations. I have used for my course to simulate maintenance proceedures as well as project scheduling. It has built in 3D models for many common objects you are likely to need.,4q7vf6,t3_4q7vf6,AaronWardHere,,Comment,1,0,1
d4s0xtm,2016-06-28 21:52:06-04:00,redditpirateroberts,,"You're going to need to find and learn some UI library for Java do that the UI element of this. Can't advise their as I haven't used java since i took a class in it. I'm sure plenty of options exist that people here, or google, can tell you about.

In terms of the logic that you will use in conjunction with your UI code, it really depends on the specifics and detail you want. If you want the traffic lights to sync up, that would be an additional layer of complexity, but here is an example of a more basic way to do this. 

You could have a traffic light class, described by the following psuedo code:

    class TrafficLight {
    enum currentState = enum(green, yellow, red)
    int secondIndex 
    double x_coordinate
    double y_coordinate

    func init() {
        currentState = green 
        secondIndex = 0 \
        x_coordinate = *set based on UI library youre using*
        y_coordinate = *set based on UI library youre using*
    }
    //called every second
    func secondTimer() {
        if(secondIndex > 14) {
            secondIndex = 0 
            currentState = green
        }
        else {
               if currentIndex == 7 {
                   currentState= yellow
               }
               if currentIndex == 10 {
                   currentState = red
               }
               currentIndex += 1
         }

    }
    }
This would create a very basic traffic light class that you can init and which will work on a 15 second cycle, being green for 8 seconds, yellow for 3 seconds, and red for 4 seconds. Obviously you would need to use it in conjunction with a UI library, but I hope this kinda shows how to do this on a very basic level. I would imagine you would want to add more complexity in. 

Sorry if this doesn't make sense. I am very high. 
",4q7vf6,t3_4q7vf6,AaronWardHere,,Comment,1,0,1
d4rbh74,2016-06-28 11:53:04-04:00,justlikestoargue,,Check out Cities: Skylines,4q7vf6,t3_4q7vf6,AaronWardHere,,Comment,-2,0,-2
4q6hi9,2016-06-27 19:56:53-04:00,Aviontics,Good CS News Website?,"Hello all. I am in my 4th year of my CS degree and I made a pact to myself to start reading at minimum 1 computer science ""tech"" article every day until I graduate.... however I for the life of me can't find a very good and consistent tech-news website. Does anyone have any recommendations?",,,,,Submission,10,0,10
d4qlp54,2016-06-27 20:28:25-04:00,davidthefat,,"https://news.ycombinator.com/

http://www.phoronix.com/

http://arstechnica.com/

",4q6hi9,t3_4q6hi9,Aviontics,,Comment,6,0,6
d4qnuiy,2016-06-27 21:25:28-04:00,Aviontics,,Well thank you good sir! I will check them out. :) Much appreciated. ,4q6hi9,t1_d4qlp54,davidthefat,,Reply,2,0,2
d4qsyrt,2016-06-27 23:48:46-04:00,blufox,,http://lobste.rs,4q6hi9,t1_d4qnuiy,Aviontics,,Reply,2,0,2
d4qtkze,2016-06-28 00:08:47-04:00,Madsy9,,"What is your definition of ""tech"" article? Just technology in general? Or related to computer science? If it is the latter, are you looking for publications on new algorithms and such, or ""light-weight"" IT news like Ars?

For technology and programming in general, [Ars Technica](http://arstechnica.com/) and http://slashdot.org/ are okay I suppose, though you get some fluff with the good stuff. For new publications, get a subscription to ACM or similar. You also have https://arxiv.org/",4q6hi9,t3_4q6hi9,Aviontics,,Comment,3,0,3
d4qzppy,2016-06-28 04:45:05-04:00,Aviontics,,Mainly tech in general yes. As of right now I'm active duty military (I separate from the Navy in 1 month) and until then I am a day and night diesel engine mechanic... no real tech talk or culture in my day to day life as no one has similar views at all really. That is why I just want to start showering myself with articles and all sorts of information to get just a more well versed knowledge of the industry as a whole before my degree gets finished up.,4q6hi9,t1_d4qtkze,Madsy9,,Reply,1,0,1
d4qx247,2016-06-28 02:23:36-04:00,ImmaculateDissection,,"Are there more good sites/blogs ? I'm not a fan of twitter but sites/blogs to specific topics would be neat, e.g. Databases, Web development, Enterprise Applications, new Trends etc. I'm having a Hard time finding good resources except r/programming which can be very polarizing",4q6hi9,t3_4q6hi9,Aviontics,,Comment,1,0,1
4q2xwa,2016-06-27 07:52:45-04:00,Ipsider,How is PageRank implemented in the MapReduce paradigm?,"Hi there,
I am learning the PageRank algorithm and I understand the principles of it but I get confused when it comes to formulating it by using the MapReduce paradigm.

I have seen some pseudo code of the map and the reduce functions but I can't get my head around this special output pair, where the node and the list is just forwarded.

I think I am confusing input and output links here.

Can someone do an ELI5 here?

Thank you!",,,,,Submission,9,0,9
d4q6cu4,2016-06-27 14:39:46-04:00,umib0zu,,"I'm assuming you've read the Brin/Page paper set. So really, pagerank is only an Eigenvector problem where you do a MapReduce matrix multiplication over and over again until you get a vector that doesn't change. In MapReduce, a sparse vector (or matrix) is just a list of defined elements in the vector, so a matrix multiplication is a relatively simple program. [Mining of massive datasets](http://infolab.stanford.edu/~ullman/mmds/book.pdf) has some discussion about it and implementation details.",4q2xwa,t3_4q2xwa,Ipsider,,Comment,2,0,2
d4sguya,2016-06-29 08:48:35-04:00,Ipsider,,"> where you do a MapReduce matrix multiplication

I do understand how the PageRank algorithm works but as I said I can't get my head around the implementation of the map and reduce functions. Of course I have to do matrix multiplication but how? :D",4q2xwa,t1_d4q6cu4,umib0zu,,Reply,1,0,1
d4skl94,2016-06-29 10:32:02-04:00,umib0zu,,"Again, it's in the Mining of Massive Datasets book. First or second chapter if I remember it correctly. Intuitively for small Matrix * Vector problems, you keep a copy of the vector on every reducer, then map every element of the matrix with a key that is the row index of the element. Then you just multiply every row element of the matrix by its associated vector element and generate the row element of the output vector.",4q2xwa,t1_d4sguya,Ipsider,,Reply,1,0,1
d4sxa0z,2016-06-29 14:47:09-04:00,Ipsider,,"oh boy, I am sorry. I overread your source. Sorry for my impatience. Thank you!",4q2xwa,t1_d4skl94,umib0zu,,Reply,1,0,1
4q0gbp,2016-06-26 19:57:51-04:00,0joshuaolson1,Is Terra the only system programming language able to compile code at runtime?,"I stumbled across [Terra](http://terralang.org/) last year, and the most similar language I can think of is [Julia](http://julialang.org/). Terra is inspired by and uses Lua for templating/metaprogramming instead of being derived from e.g. the Lisp or ML families, but like many JITted language implementations it blurs the line between compile-time and run-time.

Are there any other low-level (no VM, small runtime, etc.) programming languages I'm missing that can compile and run code at runtime (besides assembly)? Maybe Forth-likes, or [Extempore's](http://digego.github.io/extempore/philosophy.html) xtlang? Terra calls this [(multi-)staged programming](http://www.cs.rice.edu/~taha/MSP/).",,,,,Submission,7,0,7
d4pdq4i,2016-06-26 23:04:43-04:00,adipisicing,,"How about Lisp?

[Lisp Machines](https://en.m.wikipedia.org/wiki/Lisp_machine) used to run Lisp as a systems language, and I can't think of a better example of blurring the line between compile-time and run-time.",4q0gbp,t3_4q0gbp,0joshuaolson1,,Comment,4,0,4
d4pf62k,2016-06-26 23:42:27-04:00,0joshuaolson1,,"Huh, apparently they existed into the 1980s. Still, it's definitely a family of languages now. Besides lisp machine lisps, some FPGA programming languages might fall into a similar category.",4q0gbp,t1_d4pdq4i,adipisicing,,Reply,1,0,1
d4q31ym,2016-06-27 13:30:30-04:00,ixampl,,"Please correct me if I'm wrong but my understanding is that Terra is not a self-contained language. I skimmed their PLDI paper and it seems Terra relies on Lua as follows: A user writes a program generator in Lua that creates Terra types and functions. The generated code then is indeed compiled and can be executed, or saved as obj file. However the ""compiling at runtime"" seems to occur on the Lua level and relies on LLVM for compilation. So I am not sure how the runtime could be considered small.

From looking at the implementation section, this combo runs on and relies on LuaJIT, so to me it would seem that Terra itself does not fullfil the characteristics you are looking for.

Along those lines of high-level language that is used to write code generators whose programs are compiled to a lower level and natively executed, there is for instance LMS in Scala, and I believe MetaOcaml also has/had support for native compilation output.


",4q0gbp,t3_4q0gbp,0joshuaolson1,,Comment,2,0,2
d4q662l,2016-06-27 14:35:57-04:00,0joshuaolson1,,"Here's how I see it. Lua uses LLVM to compile already low-level Terra code to a target. Terra can run without them, in which case it can't instruct them to compile anything at runtime.

Technically yes, as I worded the vague question, Terra can't compile code itself, but I believe it can run without Lua or LLVM after arbitrarily mixed runtime code generation stages.

For things like LMS, MetaOCaml, MetaML, and MetaHaskell or whatever it's called, are either the compilable subset (arguably the kind of system programming language I'm asking about) or the generated output vm-free and (nearly) runtime-free?

So you're right, but I would naively think any low-level language generating self-contained code would either have the same problem of the bloat of a compiler, or such compilation would be limited to what some language implementations already do with partial application and specialization.",4q0gbp,t1_d4q31ym,ixampl,,Reply,1,0,1
d4qpe3f,2016-06-27 22:07:16-04:00,ixampl,,"Yes, on your last point. I also believe that would not really be feasible. 

Regarding the other MPS languages/frameworks, some of them allow targetting to C and CUDA, but don't ask me how well any of this works in practice.",4q0gbp,t1_d4q662l,0joshuaolson1,,Reply,2,0,2
d4r65cf,2016-06-28 09:41:04-04:00,0joshuaolson1,,"If they can generate CUDA, I doubt they'd require garbage collection for it, for example. This area really needs to be explored more, especially as LLVM itself can target backends besides assembly.",4q0gbp,t1_d4qpe3f,ixampl,,Reply,1,0,1
d4pdbm2,2016-06-26 22:55:19-04:00,0joshuaolson1,,"So far I've found [`C](http://loome.cs.uiuc.edu/CS498F10/readings/tcc-toplas.pdf) and [E-Code](http://www.cc.gatech.edu/systems/projects/ECL/manual.pdf). They're both modifications to C, and the first one in particular might be responsible for the Tiny C Compiler we have today.",4q0gbp,t3_4q0gbp,0joshuaolson1,,Comment,1,0,1
4q0b1p,2016-06-26 19:23:04-04:00,AnyhowStep,Resources for Graph Search Algorithms?,"I'm working on a personal project. The goal is writing AI to generate ""perfect play"" for levels of a certain game.

The game is single player, has various items appearing in different places at different times and the player has to perform actions in reaction to these items. Each ""level"" is... deterministic. Always the same items at the same place at the same time. Also, there are never any cycles. Item #1 always goes to Item #2 always goes to Item #3, etc.

However, the number of ways to react to each item opens about 20 nodes or more! And each level can have hundreds to a thousand items! So, the search space is up to 20^1000, right?

""Perfect play"" would be the most efficient way to react to each item and clear the level. Human players are pretty good at this and can make decisions about reacting efficiently in a few ms (no kidding).

I've made my own attempt over the past year on and off but I've run into problems over and over. I gave up a few months ago but I'm gonna' try again.

I don't know enough about graph search algorithms to do this properly, it seems. My attempts were always:

+ Heuristics

""Perfect play"" might be the wrong word. There is a general consensus about the most efficient way to clear a level but some levels allow for variations and there's no real true answer. And there are a lot of factors affecting the efficiency of your next action, including the action taken previously. So, heuristics got me pretty close but there were always edge cases I couldn't handle.

+ Iterative deepening (?)

Not too sure what it is called. Basically, the search space is so huge, I can't possibly generate them all at once. So, I generate the paths as I process each node.

+ Splitting level into regions

Not too sure what the term for it is. Some (not all) levels can be considered a combination of smaller levels. So, I solve for the most efficient way to solve smaller-level-A, then use the end-state of smaller-level-A as the start-state of smaller-level-B.

+ Best-first-search

Like A*, I guess. Every open node is put into a sorted queue and I process the one with the lowest cost so far. I also limit the number of opened nodes to 10,000. If more than 10,000 nodes are open, I discard the lowest scoring ones to make space.

+ Pruning(?)

If state X and state Y lead to state Z and (state Y -> state Z) costs less than (state X -> state Z), state X and its child nodes are immediately discarded.

The problems I have with the above approaches are:

+ Slow. It can take up to a few minutes to solve each level.
+ Inaccurate. Ignoring the potential for erroneous heuristics, I've seen cases where a path that would end up being better down the road get pruned/discarded before it's processed because of the limit on the number of nodes. Increasing the 10,000 limit makes the slow algorithm even slower.

So... Yeah, TL;DR, I suck at graph search algorithms. All my attempts suck. I'm looking for a graph search algorithm ""bible"" or ""bibles"".",,,,,Submission,1,0,1
d4pemq4,2016-06-26 23:28:05-04:00,theobromus,,"Take a look at the minimax algorithm (it's not too clear from your description if there's another player in the game or not) and alpha-beta pruning. That will give you a way to start.

The real trick is determining how many moves forward you can look, and how to evaluate your state at each possible mid-game position. I'd say the state of the art right now in that is things like Deep Mind's AlphaGo, which uses neural nets and Monte-Carlo Tree Search.",4q0b1p,t3_4q0b1p,AnyhowStep,,Comment,1,0,1
d4pqkdt,2016-06-27 07:59:55-04:00,AnyhowStep,,It's single player. There is no other player. Just one player and him performing possibly up to 20 actions in response to each thing that happens in the level.,4q0b1p,t1_d4pemq4,theobromus,,Reply,1,0,1
d4q4tm7,2016-06-27 14:07:52-04:00,theobromus,,"The other thing that wasn't too clear to me - is there some randomness in the different ""things"" that happen each level?

Basically could you pick your strategy (which action in each level) entirely at the beginning? Or do yo need to respond to something that happens outside your control?",4q0b1p,t1_d4pqkdt,AnyhowStep,,Reply,1,0,1
d4qq6zc,2016-06-27 22:28:54-04:00,AnyhowStep,,"No randomness. Deterministic. Always the same. No surprises. Yeah, you can pick your strategy entirely at the beginning. The goal is to just be as efficient as possible, lowest cost route. The problem is that you could have two identical sequences of items that require different sequences of response to be ""efficient"" because the sequences before also affect how efficient they'll be. So, I can't really cheat by finding ""known"" patterns.",4q0b1p,t1_d4q4tm7,theobromus,,Reply,1,0,1
d4qv1ph,2016-06-28 00:59:38-04:00,theobromus,,"Yeah I'm still really confused by your description of the game. Normally you'd think of an action as any thing the player can do at a given state, but it sounds like you combine an action with an item.
I'm assuming you know which items and actions you have from the beginning.

In that case some kind of best first search is really where you have to go. The real trick is still in have a heuristic to evaluate which potential pathway is most promising.",4q0b1p,t1_d4qq6zc,AnyhowStep,,Reply,1,0,1
d4s80ub,2016-06-29 01:31:24-04:00,AnyhowStep,,"Yeah, you combine an action with an item. And, yeah, there are a fixed number of items and actions, all of which are known (just a really huge number of them).

I guess I'll stick to what I have, then, and continue tweaking the heuristics >< It's so time-consuming. Do you happen to have any recommendations for pruning or optimizations?

Thanks!",4q0b1p,t1_d4qv1ph,theobromus,,Reply,1,0,1
4px8ho,2016-06-26 06:37:27-04:00,trojanrob,What should I learn in the Summer?,"Hi,

I'm going to University in September and know absolutely nothing about programming.

I have a summer school coming up in two weeks ( it lasts a week ) and I am meant to be taught Python and create a small program or something through the summer school ( it is basically something that can lower my Uni entry criteria ).

The degree mostly starts off with Java and some other languages. 

What can I do in this time to learn programming. Can anyone provide a link to some long online courses I can take? I have thought of the following:

* Intro to CS50x
*  Learn Python the Hard Way

What else should I look at to help me understand what I'm getting into. Also, are there any tips to how I should use these resources, like should I write in pen and paper or just observe videos?

Thank you all. I am sorry if this has been asked before but I am hoping there are some updated resources now! :)",,,,,Submission,15,0,15
d4oj2iz,2016-06-26 08:02:02-04:00,MOnsDaR,,"Codeacademy.com is a good way to do your first steps in programming. It just teaches you the basics, but it's easy and fast to get through. Besides some other languages they also have a python module.",4px8ho,t3_4px8ho,trojanrob,,Comment,7,0,7
d4p20da,2016-06-26 17:45:24-04:00,SirSourdough,,"It's actually just [codecademy.com](http://www.codecademy.com).
",4px8ho,t1_d4oj2iz,MOnsDaR,,Reply,4,0,4
d4pmncc,2016-06-27 04:26:46-04:00,MOnsDaR,,"I'm currently just on mobile, thanks for getting the correct link",4px8ho,t1_d4p20da,SirSourdough,,Reply,1,0,1
d4ooq2y,2016-06-26 11:38:07-04:00,ThatsAFineRadiator,,Algorithms and data structures. You can know all the languages and frameworks you want but nothing trumps a solid base in algorithms and data structures ,4px8ho,t3_4px8ho,trojanrob,,Comment,3,0,3
d4opp37,2016-06-26 12:07:09-04:00,trojanrob,,Thank you. What courses would you recommend for a 0 to hero king of approach into beginners Algos.,4px8ho,t1_d4ooq2y,ThatsAFineRadiator,,Reply,3,0,3
d4p2aux,2016-06-26 17:53:20-04:00,zSilverFox,,"Udacity and Coursera both have good courses, but they also assume knowledge of python and java respectively. You can probably follow the logic, but I'd recommend a beginner tutorial of either language first... Also available on either site.",4px8ho,t1_d4opp37,trojanrob,,Reply,2,0,2
d4p2fcv,2016-06-26 17:56:45-04:00,SirSourdough,,"I haven't worked through all of the material, but [Algorithms: Design and Analysis (Part 1)](https://www.coursera.org/learn/algorithm-design-analysis) tought by Tim Roughgarden from Stanford is supposed to be a very solid class. 

That said, if you are truly starting from zero, I probably wouldn't go straight into that class. Fortunately, it doesn't start until July 11, so in the meantime, I would look for an introduction to programming that you could work on between now and then. The [codecademy.com](http://www.codecademy.com) recommendation is good (if basic) as is [CS50](https://cs50.harvard.edu/).

I'm not a big fan of Learn Python the Hard Way, but it's free and accessible so that part is nice. There are tons of other introductory Python tutorials online, so browse through them and find one that you enjoy working on. For the basics, it hardly matters where you learn, it just matters that you stick with it and keep programming. Understanding will come with time. ",4px8ho,t1_d4opp37,trojanrob,,Reply,2,0,2
d4oj2tl,2016-06-26 08:02:28-04:00,bctama,,"When I was learning how to program, the first thing I started with was Learning Python The Hard Way. I thought it was pretty neat: handy advice and programming practice hand in hand. How well received it was by the CS community I wouldn't know, as I'm in the same boat. CS50, on the other hand, is excellent. If you're going to use either resource, take some notes, but actually typing out programs and seeing what they do etc is also important if you've never done any programming. You mentioned Java- I found that once you learn a couple languages, things get very familiar in others and you'll pick them up faster. ",4px8ho,t3_4px8ho,trojanrob,,Comment,1,0,1
4pvtwg,2016-06-25 22:57:12-04:00,bctama,Going to college for CompSci soon...,"...and I've been scouring the internet for books to read, things to work on. Suddenly I'm reading all this stuff about making my own website and blog, doing online coding/hacking competitions, hackathons, meetups, helping people on Stack (People ask questions that I don't understand in the slightest) and I feel like so overwhelmed. I mean, I've written some scripts, have my own laptop I've setup with Arch Linux, done a few really small things with an Arduino and RPi, but nothing that I could even begin to consider full-blown projects like what I've read (Some of which I've put on github, though I imagine someone would rather see quality over quantity). 

One of the things suggested on the net was to contribute to open-source- I've looked at Mozilla and the AUR, but just like what happened above, I didn't understand any of it. What the heck have I been doing my entire life? Is it alright to feel so anxious? Should I have been doing this stuff right when I started high school? What should I be doing now to keep up?

Any advice would be appreciated to keep me from imploding.

On a side note, sorry if this doesn't belong here.",,,,,Submission,8,0,8
d4oamvm,2016-06-26 00:17:27-04:00,PastyPilgrim,,"You're fine. Most people in your CS program will never have written a line of code prior to enrollment, let alone done any computer science related things (math and theory) unless they took AP CS. You're most likely ahead of the curve.

Spend your summers trying to do projects or research related to what you learned in the past year, build your skillset, and try and find some co-ops after your second or third year.",4pvtwg,t3_4pvtwg,bctama,,Comment,11,0,11
d4oaqih,2016-06-26 00:21:08-04:00,Anuglyman,,"Relax, man. You haven't even started college yet. You're not supposed to know any of that stuff. Relax. Enjoy your summer. Don't even worry about that right now. You're about to dive into college for the next 4 (maybe more) years. Enjoy this time you have now to chill. You will learn that stuff in class in the fall and on. No one expects you to be a silicon valley ready programmer on day 1 of comp sci college. If you want to get some familiarity with some stuff that's fine, but keep the scope limited. It is a very broad topic that can take you on many tangents and start to overwhelm. If I were you though, I'd just enjoy what is probably the last summer of freedom. After this, it is probably internships, classes and work for the rest of your summers. ",4pvtwg,t3_4pvtwg,bctama,,Comment,3,0,3
d4oi12z,2016-06-26 06:58:05-04:00,ProfessorAlgorithm,,"Have some fun.  Want to make video games?  Check out a game library (e.g. pygame) and learn by trying to make something simple, like pong or space invaders.

More interested in AI?  Check out a library (e.g. scikitlearn) and start with some tutorials.  Try some variations of your own after the tutorials.  Don't skip this last step.  It proves that you know how to do it now: you weren't just typing what the tutorial told you to type.

These examples are python, but pick something that will give you as much of a headstart as you can manage.  

The fun is really important.  Not many of your courses will allow you to work fun stuff, but being passionate can make a huge difference in how much you enjoy your field, and how successful you are in it.  Plus, if you skip ahead and learn about conceptual concepts, you'll either:

1. Be bored in your classes, tempting you to skip and potentially missing something important.

2. Struggle with those concepts because you are also simultaneously learning programming syntax, best practices, etc.  Imagine learning about mechanics in a physics class without knowing what an equation looks like.

You've got the right attitude that I've seen in some of my best students.  If you have the confidence to be a leader, helping other students along the way, even better.

",4pvtwg,t3_4pvtwg,bctama,,Comment,1,0,1
d4oks4e,2016-06-26 09:25:06-04:00,dragid10,,"Dude I've you've installed Arch Linux, you're probably ahead of the majority of your class already. Most people have barely written a line. You're going to be fine. Just remember that internships and projects are important to do. Projects look good on resumes as well as internships. As you go through the year you'll learn and discover what type of projects you want to do ",4pvtwg,t3_4pvtwg,bctama,,Comment,1,0,1
4pnvdr,2016-06-24 12:47:47-04:00,PyroFox123,"Hi, I'm looking for CS books.","Hi, I'm going straight to the point. I like CS, programming, troubleshooting, all that fun stuff. Any CS books you would recommend? I have lots of programming ones but feel free to suggest more! [If you know any about troubleshooting, tell me. I would read those ones ASAP.]",,,,,Submission,0,0,0
d4mgd5l,2016-06-24 13:04:25-04:00,EnterprisePaulaBeans,,"[This blog post](https://blog.codinghorror.com/recommended-reading-for-developers/) is pretty great and has a good list of books. Of the books on the list, I'd personally highly recommend *Introduction to Algorithms* and *Code Complete*.",4pnvdr,t3_4pnvdr,PyroFox123,,Comment,1,0,1
4pnf5i,2016-06-24 11:23:04-04:00,TissueReligion,Trying to plot a CS self-study -- what resources complement nand2tetris?,"I really wish there was a ""hacker's mode"" for nand2tetris that would let you go into much greater depth with pipelining, threading, building a filesystem, etc. In the absence of that... I'm thinking:

1) Go through nand2tetris to get a broad high-level view of what computers are all about. Hopefully that gives me the background to branch out into areas that nand2tetris doesn't go into depth in.

2) Fill in the digital logic design fundamentals (I've done a bit of this before, so not too concerned).

3) Find some way to get experience with the processor architecture bits that nand2tetris doesn't go into.

4) ??? Find a good operating systems resource.

5) ??? Find a good compilers and languages resource.

I just want to have a somewhat in-depth understanding of design from transistor to compiler. No career goal or anything.

Ideas for ways to complement nand2tetris? Thanks!",,,,,Submission,3,0,3
d4n42xd,2016-06-24 22:57:48-04:00,SakishimaHabu,,"If you know, or are learning, some C language then computer systems: a programmers perspective would be a good follow up.",4pnf5i,t3_4pnf5i,TissueReligion,,Comment,1,0,1
d4nh4h4,2016-06-25 09:29:37-04:00,TissueReligion,,Wow this looks magical. Thanks for the tip. Must have overlooked this.,4pnf5i,t1_d4n42xd,SakishimaHabu,,Reply,1,0,1
4pmysf,2016-06-24 09:51:52-04:00,DrTestBender,"Why do some websites allow non-alphanumeric characters (#@!$%) as part of a password, while others do not?",Is it especially difficult to allow those characters in a password? It seems like allowing for their use could allow users to generate stronger passwords. ,,,,,Submission,21,0,21
d4m9tsd,2016-06-24 10:37:54-04:00,roomzinchina,,"Most websites hash passwords, so any character can be entered. Others (do not trust these websites) do not, and if the developer has not properly designed the login system, special characters could lead to SQL Injection. In these cases, it is simplest to ban special characters rather than fix the underlying problem. Disallowing certain characters is usually a reliable indication the site is storing the passwords in plain text, and should not be used.",4pmysf,t3_4pmysf,DrTestBender,,Comment,26,0,26
d4mh2ue,2016-06-24 13:19:50-04:00,RecursiveProgrammer,,"For more information on SQL Injection around passwords I would suggest this video.

https://www.youtube.com/watch?v=ciNHn38EyRc",4pmysf,t1_d4m9tsd,roomzinchina,,Reply,4,0,4
d4ngofa,2016-06-25 09:10:53-04:00,crookedkr,,"Meh, if you know how to ban the characters then you know which need to be escaped to prevent injection.",4pmysf,t1_d4m9tsd,roomzinchina,,Reply,1,0,1
d4nhmkf,2016-06-25 09:49:03-04:00,umib0zu,,Yup. No one said banning characters actually works. It just means the moron developer *believes* it works.,4pmysf,t1_d4ngofa,crookedkr,,Reply,1,0,1
d4mbbs7,2016-06-24 11:13:02-04:00,BonzoESC,,"Password policies are easy to understand and an easy thing for people outside the development organization to have an opinion about. If you know management is going to have an opinion about one thing, it's easier to let that thing be the password policy than something that matters like the database schema or JavaScript frameworks or…

Some fields may have actual password strength policies decided by legal or even an entity outside the company.",4pmysf,t3_4pmysf,DrTestBender,,Comment,7,0,7
d4mrzw8,2016-06-24 17:22:50-04:00,brennanfee,,Because those that don't are coded and written by morons.,4pmysf,t3_4pmysf,DrTestBender,,Comment,8,0,8
4pmc1o,2016-06-24 07:09:04-04:00,FJ-BSN,Want to fill knowledge gaps in an Information Systems degree.,"I've nearly completed my degree in information systems but find myself interested in learning more of the technical side of Computer Science.

My degree taught programming in a higher level language(the courses software engineers take not computer science majors) and focused primarily with databases (T-SQL mostly but an overview of noSQL and other types). The rest of my subjects were more system analyst and IT management focused.

I'm looking for good sources to learn the other more technical aspects of computer science but find most online CS courses dull because they tend to focus on programming. I'm looking more for courses on algorithms (i know the basics), the kind of math notation I see sometimes and more theoretical or lower level kinds of programming (like machine code, how compilers work, etc.) And the kinds of math I'll need to understand computer science proofs (like the ones I've seen in here about P vs NP or graph isomorphism). I have a semester of discreet math and first year calculus with two years applied statistics I'd that helps narrow recommendations down. Not math shy I really enjoy it.

I have no money to my name so MOOCs or free e books would be amazing but ill look into other books as i get the resources.

I've tried Nand to tetris and found it amazing, particularly logic gates and I've watched MIT youtube videos on algorithms which I've also enjoyed if that helps you gauge my interest.

Lastly thank you Compsci and AskComputerScience for all the amazing content you provide and all the fascinating hours I've whiled away here :)

TLDR; Studied CIS, found out too late I'm only interested in CS, and want to fill the knowledge gaps.",,,,,Submission,1,0,1
4pjrqa,2016-06-23 19:40:22-04:00,Zimmrum,.zip file question,So for my last assignment we were supposed to submit a .zip file of our folder and executable etc. But I got deducted 5 points because my .zip file created a new folder instead of just revealing all the contents onto the current directory.  How would I go about doing that??,,,,,Submission,0,0,0
d4ljipf,2016-06-23 20:06:10-04:00,Lifelong_Throwaway,,"I'm not sure this question is really appropriate for this sub... but I can answer you I suppose.

Most likely, you selected the folder containing the files and did whatever you did to turn it into a zip (for ex. on Windows, right clicking it and hitting ""send to zip""). This will, of course, make a zip file containing that folder. To avoid this, you can select all the FILES in your folder at once (not the folder itself), and then right click and ""send to zip"" (or the equivalent for 7zip if you use that)",4pjrqa,t3_4pjrqa,Zimmrum,,Comment,2,0,2
4pjm7u,2016-06-23 19:05:31-04:00,simonli2576,What happens if you pull the USB when you're...,"1. reading files from it?

2. writing/modifying files on it?

3. deleting files on it?

4. formatting it?

5. doing nothing to it but you didn't choose to safely reject it beforehand?",,,,,Submission,7,0,7
d4livhn,2016-06-23 19:50:41-04:00,You-asked-for-it,,"1. Reading stops. Files should be fine. Program that was reading them may crash or fail.

2. Depends on how they are written. If the program is streaming information to a file (writing as it goes) or writing a big file it will probably fail and corrupt the files it was writing or modifying.

3. Depends on how you delete. Sometimes the computer just tells the flash drive ""hey don't worry about that file"". It doesn't replace or remove the file on the drive it just shows the free space. This is why you can usually recover photos and videos immediately after you delete them if a new file doesn't take its place. 

4. Format will fail, you will have to start formatting again. USB drives should be OK still though, portable hard disk drives could have a potentially fatal error.

5. If the system is not reading or writing from the drive it is OK. Ejecting the drive usually is the OS telling the rest of the programs on the computer to stop using the device and then cuts off access.

",4pjm7u,t3_4pjm7u,simonli2576,,Comment,9,0,9
d4lqgbl,2016-06-23 22:59:07-04:00,Syde80,,"> portable hard disk drives could have a potentially fatal error.

What is your thinking behind this?  I'm having a hard time determining how an HD will get turned into a paperweight from this.",4pjm7u,t1_d4livhn,You-asked-for-it,,Reply,3,0,3
d4ltrve,2016-06-24 00:26:12-04:00,Jimga150,,"I would imagine it has to do with the fact that a spinning hard disk has more to risk when the power is abruptly cut from its needle and motor, a data black my be corrupt and unwritable, making the HDD a dead one. Just a guess though.",4pjm7u,t1_d4lqgbl,Syde80,,Reply,2,0,2
d4lw0ml,2016-06-24 01:33:35-04:00,None,,[deleted],4pjm7u,t1_d4ltrve,Jimga150,,Reply,3,0,3
d4lyv7u,2016-06-24 03:23:27-04:00,lordvadr,,"Modern hard drives (I looked up western digital blacks to double check) can tolerate about 350g's worth of shock while operating, which amounts to about a 6 foot fall to a concrete floor. And 1000g's while not operating. You obviously don't want to push your luck but picking up and moving a running hard drive isn't going to do anything to it.",4pjm7u,t1_d4lw0ml,None,,Reply,3,0,3
d4m2mau,2016-06-24 06:34:25-04:00,Jimga150,,Does that mean it can withstand 350g unharmed entirely or it can withstand it without shattering the case?,4pjm7u,t1_d4lyv7u,lordvadr,,Reply,3,0,3
d4mimo0,2016-06-24 13:53:05-04:00,lordvadr,,"I'd have to read the warranty closer, but I imagine it means without catastrophic data destruction. Keep in mind the shock can vary somewhat wildly based on how it lands (corner vs flat, for example). I would imagine that if you sent a hard drive in with a badly buggered up corner they will tell you it wasn't covered, but I don't know. It would also probably be wise to decommission any drive that got dropped while spinning.

Anecdote, I recently accidentally dumped out a box of a dozen or so to-be-shredded drives that fell probably 5 feet onto a concrete parking lot and then tumbled down the ramp a few feet. It was a horrific sound, but when I picked them up, you'd have been hard pressed to identify any damage. Obviously not operating, but i think the case is quite a bit tougher than people might imagine.",4pjm7u,t1_d4m2mau,Jimga150,,Reply,2,0,2
d4m7qae,2016-06-24 09:44:52-04:00,You-asked-for-it,,I had one come unplugged during a quick format once. It was a small USB powered one. When I reattached it it was able to be recovered and the file were still there. However I could no longer write to the disk. Found an error on the device that said the issue was hardware related. I think it's more of a power was lost issue then a unplugged during formatting issue. It was also getting old. So many things led the the devices failure and it wasn't necessary for me to determine what it specifically was. Probably the power loss or a combination of many things.,4pjm7u,t1_d4lqgbl,Syde80,,Reply,1,0,1
d4lyl7j,2016-06-24 03:11:12-04:00,Devvils,,"What will go wrong? if you are using soft updates, not much,

https://en.wikipedia.org/wiki/Soft_updates",4pjm7u,t3_4pjm7u,simonli2576,,Comment,1,0,1
4pcqry,2016-06-22 16:08:22-04:00,AsiansInc,WPF MVVM Datagrid Does Not Hide All Rows When Bound to a Variable,"I am using a MVVM framework, not mvvmLight, and have databinding setup on a datagrid. I have the datagrid bound to a list that contains a 'RowVisible' property. If 'RowVisible' is false then the datagrid will hide the row, and vice-versa. The issue is it won't hide all of my rows when I change the variable at run-time. 

I could break MVVM and have my front-end view refresh its own datagrid but again this would break MVVM. 

My other option, which i have had no success in, would be to force my datagrid to re-bind whenever I change the entire list. 

I would much prefer to just hide/un-hide the rows and preserve data-integrity as well as it is easier on the processor.
Thanks.",,,,,Submission,0,0,0
4pbi1l,2016-06-22 12:19:48-04:00,vape-jesus,"If HDMI is digital and VGA is analog, then how are there unpowered HDMI to VGA cables?","[SOVLED] I'm using one right now. HDMI is 0s and 1s, but vga is values based off of voltage... somehow the screen is receiving a vga signal though. How is this possible without a digital to analog powered ocnverter?",,,,,Submission,11,0,11
d4jkbds,2016-06-22 12:43:06-04:00,minimim,,"HDMI cables have power on them, to power the digital to analog converter. They just made it small enough to fit into the bulky VGA connector.",4pbi1l,t3_4pbi1l,vape-jesus,,Comment,8,0,8
d4jkfp1,2016-06-22 12:45:32-04:00,vape-jesus,,I thought about that and I assumed that was the reason but my computer says it didn't have power over hdmi. But that's the only real explanation so my computer must've lied. Thanks!,4pbi1l,t1_d4jkbds,minimim,,Reply,2,0,2
d4jkk84,2016-06-22 12:48:06-04:00,minimim,,"All HDMI cables have a +5V rail on them, exactly for this application.",4pbi1l,t1_d4jkfp1,vape-jesus,,Reply,9,0,9
d4k7z20,2016-06-22 21:25:32-04:00,FaithForHumans,,"There's not really a thing such as ""power over HDMI"" to my knowledge. As /u/minimum mentioned, part of the HDMI spec is to provide a 5V rail, but it's only required to carry 50mA. It's really only used for active converters (such as HDMI to VGA) that draw minimal current.

Here's a pinout if you're interested. http://www.marcodesalvo.it/wp-content/uploads/2009/11/hdmi_pinout-300x254.jpg",4pbi1l,t1_d4jkfp1,vape-jesus,,Reply,6,0,6
d4jkn55,2016-06-22 12:49:48-04:00,CoopNine,,"Either you have an active HDMI to VGA adapter, or the video card/device you're using supports converting the signal to analog. but there's no way to just connect the pins to the right places like a DVI to VGA converter essentially does.  (DVI actually has an analog signal as well as the digital)",4pbi1l,t3_4pbi1l,vape-jesus,,Comment,4,0,4
d4jkxfe,2016-06-22 12:55:40-04:00,minimim,,Not every DVI output carries the analog signal: http://lowprofilevideocards.computer-parts-store.com/images/Nvidia_Geforce_GT630_2_Gb_DDR3_Videocard_3_large.jpg,4pbi1l,t1_d4jkn55,CoopNine,,Reply,2,0,2
d4jlgsu,2016-06-22 13:06:50-04:00,CoopNine,,I should have said DVI-I has analog as well as digital.  ,4pbi1l,t1_d4jkxfe,minimim,,Reply,5,0,5
d4jkdre,2016-06-22 12:44:26-04:00,Javadocs,,"Do you have an example (brand/model number)? All the HDMI to VGA converters/cables I see have a ""power input"" (usually USB).

",4pbi1l,t3_4pbi1l,vape-jesus,,Comment,1,0,1
d4jkk0s,2016-06-22 12:47:59-04:00,vape-jesus,,"http://www.bestbuy.com/site/insignia-hdmi-to-vga-adapter-black/1577417.p?id=1219503843232&skuId=1577417


It has a micro usb slot but I can unplug it and it works fine. Apparently as someone else said, there is power over hdmi now (even though I'm pretty sure my computer doesn't have that, it's the only explanation).",4pbi1l,t1_d4jkdre,Javadocs,,Reply,1,0,1
d4jkv61,2016-06-22 12:54:25-04:00,Javadocs,,"The ones I was looking at were VGA to HDMI. Like /u/minimim said, HDMI has a +5V pin. It's not enough really to power a whole output device, but enough to power a converter.

http://www.hdmi.org/images/inside_hdmi_cable.jpg",4pbi1l,t1_d4jkk0s,vape-jesus,,Reply,3,0,3
4p9s7o,2016-06-22 05:17:58-04:00,rads712,Hey guys! I'm new to the field of IoT and I would love to understand the implementation of the same in a University environment. I need to gather public response as part of my internship. Please help me out.,"The processes in a university starting from admissions can be optimized by IoT. But how?
I suggested making the ID card universal within the campus (to issue library books, track a student within campus, pay for food and stuff bought within campus, logging in attendance and so on)
What kind of ideas do you guys have?",,,,,Submission,6,0,6
d4j8efn,2016-06-22 07:22:31-04:00,noknockers,,"Why do I have this little niggling voice in the back of my head which screams 'university assignment', rads712?",4p9s7o,t3_4p9s7o,rads712,,Comment,2,0,2
d4je1wc,2016-06-22 10:24:51-04:00,rads712,,"Haha no it's not an assignment. Don't worry Reddit, I'm not making you do my homework.",4p9s7o,t1_d4j8efn,noknockers,,Reply,1,0,1
d4jebjc,2016-06-22 10:31:24-04:00,BlueFootedBoobyBob,,"That is not really IoT(and I pitty you if your Uni doesn't have it.)

Imo examples would be NFC payment in the Cafeteria, being able to look at your balance from home, displays like there are 5 free tables in that Computer Lab and 4 in that one.",4p9s7o,t3_4p9s7o,rads712,,Comment,1,0,1
d4jg84f,2016-06-22 11:15:31-04:00,BonzoESC,,"My roommate made a [toaster that tweets pictures of toast](https://twitter.com/toastercam) with a toaster and a Raspberry Pi. If prospective students going through the admissions process eat toast, that could help.",4p9s7o,t3_4p9s7o,rads712,,Comment,1,0,1
d55bozd,2016-07-09 03:27:27-04:00,ImmaculateDissection,,"The processes... I guess there are countless processes going on in the University. From which perspective do you look at the processes? admin, finance, student, facility management, ...? this would be the first thing to consider. next would be to identify certain processes and define+describe them clearly. Such a task is incredibly huge so it needs to be broken down into little bits and pieces. Also IoT is a vague term and a wide field... e.g. do you only consider Software automization? Hardware? Both? What is the goal for each of the relevant use cases of the Processes and What is the goal of the optimization? Economical aspects :)? sry for the long Post I got lost",4p9s7o,t3_4p9s7o,rads712,,Comment,1,0,1
4p8uei,2016-06-22 00:20:22-04:00,doge_ucf,PLEASE help: could this series of audio and visuals be being auto generated? How could I find a pattern?,"If you haven't heard of **unfavorable semicircle**, I would suggest not to even waste your time getting sucked into the abyss of wonder, BUT it is the most intriguing, confusing, frustrating thing I've ever seen so if you're looking for something to boggle your mind, *this is it*. Below are links to the youtube and twitter where the videos are posted, and the subreddit if you wish to familiarize yourself more with the background information.

Quick rundown: 
* There are obscure (appearingly)nonsensical videos being posted to these accounts, at an average rate of 3 per minute, but recently much, much more on the twitter page. (Today they posted over 1000 in a 12 hour period)
* Most are 3-5 seconds long, but some are hours
* They mostly are flashing images and inaudible words
* Automated test operations have been ruled out; youtube banned the original channel so they started posting most of them on twitter instead of their new youtube channel.


Can anyone tell me if these could be auto generated videos (as if almost completely random) that a user wouldn't have to do anything but press a button and they would just start making variations? Or if there is a way to start looking for a pattern in these? **Anything** would help at this point. I just don't know where to get started. I don't know if this is exactly where to even ask but it seemed like a good option. 

https://twitter.com/unfavorablesemi
https://www.youtube.com/channel/UCLEBJyqL1KKsKKz_aBqfPaQ
https://www.reddit.com/r/UnfavorableSemicircle/",,,,,Submission,10,0,10
d4j4bed,2016-06-22 03:31:29-04:00,earslap,,"These usually turn out to be C&C messages for controlling botnets. Various platforms regularly and automatically ban accounts posting gibberish (which are generally C&C messages) so video + audio encoded approach might be there to deter the auto-detection capabilities of the platforms. There also are a few subreddits that are being used this way.

As far as patterns go, you might not find any, especially if the messages behind them are encrypted (and the keys reside in the botnet softwares). Any data you might be able to extract will not be different from pure randomness in the absence of the key, in that case.",4p8uei,t3_4p8uei,doge_ucf,,Comment,6,0,6
d4j4xef,2016-06-22 04:05:17-04:00,doge_ucf,,"Wow that's really interesting. If you solved this everyone's gonna flip a shit (in a good way). I've never heard of C&C messages and didn't come up with much when I googled it. Would you mind expanding? 

Also, are you saying these don't have any meaning behind them and are just spamming? Or is there an actual purpose behind it, and if so what is it? ",4p8uei,t1_d4j4bed,earslap,,Reply,1,0,1
d4j52rr,2016-06-22 04:13:49-04:00,earslap,,"An example proven to use reddit: https://www.intego.com/mac-security-blog/iworm-botnet-uses-reddit-as-command-and-control-center/

One of the many subreddits I could find (this seems to be dead but I bet there are many active ones running right now): /r/strawmen/

Some explanation behind the technique: https://zeltser.com/bots-command-and-control-via-social-media/

For more info search google for ""social media botnet command and control"" and similar terms.",4p8uei,t1_d4j4xef,doge_ucf,,Reply,4,0,4
d4j5i51,2016-06-22 04:38:43-04:00,doge_ucf,,"Holy fuck my mind is BLOWN. You are literally a life saver for answering this question. Or at least time saver. This has been driving me absolutely nuts trying to understand, well, ANYTHING about it.

I just want to clarify though, from what I read in the iworm article it said that its just using those sites to communicate and now spread the virus.... So I didn't open up my computer wide up for it to infect it right? ",4p8uei,t1_d4j52rr,earslap,,Reply,1,0,1
d4j5tgc,2016-06-22 04:57:35-04:00,earslap,,"No don't worry, you won't get infected by visiting those.",4p8uei,t1_d4j5i51,doge_ucf,,Reply,1,0,1
d4j5ytv,2016-06-22 05:06:29-04:00,doge_ucf,,"Okay good and thank you so much. You should go check r/unfavorablesemicircle though in a few days to see if you, indeed, did ""solve"" it. People have gone so extensively in depth into certain theories and dedicated so much time to solving it, they'll probably go ape shit if that's what this actually is lol",4p8uei,t1_d4j5tgc,earslap,,Reply,1,0,1
d4j65ng,2016-06-22 05:18:09-04:00,earslap,,"People look for patterns in things that doesn't contain any. That tendency is a part of being human. For instance, the digits in the number PI are statistically random but that doesn't stop people trying to do things with it.

The subreddit you linked is not the first one looking for something similar. See for instance: /r/Solving_A858/

Or: https://www.reddit.com/r/f04cb41f154db2f05a4a (see how many people are subscribed)

See here also: https://www.reddit.com/r/solving_reddit_codes/ (see the sidebar)

/r/unfavorablesemicircle is not unique in any way, there are thousands of similar sightings all over the web. It is a neat way for separate pieces of software to communicate without using a single implicating server. People are having fun searching for meaning in them so godspeed to them. Never seen something interesting come out of them though.",4p8uei,t1_d4j5ytv,doge_ucf,,Reply,3,0,3
d4j1brk,2016-06-22 01:17:22-04:00,wafflestealer654,,"Let's rule out the idea that the user is fucking with us (just so we can be curious).

There's an infinite amount of methods this user could be using to generate these videos.

I would open the audio stream using Audacity or some similar program and see what it sounds like when you modify the audio.",4p8uei,t3_4p8uei,doge_ucf,,Comment,3,0,3
d4j2r89,2016-06-22 02:16:32-04:00,doge_ucf,,"People have tried pitch modification and alternate speeds without any real results. Someone tried to look into if it was an FSK encoded data stream, and another said they thought it sounded like data streams on short wave radio (link describing what they used below). Nothing seemed to become of it but I think I may try and follow their approach. Do you know anything about this?


https://www.reddit.com/r/UnfavorableSemicircle/comments/4b3c5n/comment/d15vygn",4p8uei,t1_d4j1brk,wafflestealer654,,Reply,2,0,2
d4j2sr3,2016-06-22 02:18:23-04:00,doge_ucf,,"Also please excuse if I'm making absolutely no sense and sounding like a dumbass, I'm completely new to this. ",4p8uei,t1_d4j1brk,wafflestealer654,,Reply,2,0,2
d4j362o,2016-06-22 02:35:06-04:00,wafflestealer654,,"Oh no, you're all right. I haven't heard of this user before seeing your post. I'm a student about to head into a comp. engineering degree so I'm very new at data analysis.

I don't know of anything else to look for yet.",4p8uei,t1_d4j2sr3,doge_ucf,,Reply,2,0,2
d4j4r72,2016-06-22 03:55:21-04:00,doge_ucf,,Lol well thank you for your help though!,4p8uei,t1_d4j362o,wafflestealer654,,Reply,1,0,1
d4j3n04,2016-06-22 02:57:08-04:00,rlcute,,"> Can anyone tell me if these could be auto generated videos (as if almost completely random) that a user wouldn't have to do anything but press a button and they would just start making variations? 

I'm in computer graphics so my first thought was that he made a tool that generates the images, reads the pixel values and adds a soundbite depending on the position (or colour) of the pixel (and maybe play a specific clip if x number of pixels are adjacent to each other in a specific pattern). Could be done with one operation, he'd just need to find different soundbites to use.
But some of the image-audio combinations don't really make sense so I don't know.",4p8uei,t3_4p8uei,doge_ucf,,Comment,2,0,2
d4jxdjx,2016-06-22 17:07:01-04:00,scarab13,,"This is no C&C or bots, there is an elaborate attempt to say something in these. Otherwise it would look random and not like this: http://i.imgur.com/1oZn5RN.jpg

",4p8uei,t3_4p8uei,doge_ucf,,Comment,2,0,2
d4k3va5,2016-06-22 19:41:13-04:00,doge_ucf,,Damn. I really wanted it to be so I could finally move on from this ridiculousness lol. I'm consumed with curiosity. Do you have any thoughts on what it could be then? ,4p8uei,t1_d4jxdjx,scarab13,,Reply,1,0,1
4p6gmp,2016-06-21 15:29:48-04:00,ssps1138,Interactive Display Software,"Hey everyone! So I'm trying to develop some sort of interactive display using the XBOX Kinect. Does anyone how I could get started writing my own program? I would like it to be used something along the lines of UBI Interactive so that if needed, I could use it to show powerpoint slides and other work.

Thanks!",,,,,Submission,0,0,0
d4j3cnj,2016-06-22 02:43:34-04:00,rlcute,,"Look into using Leap Motion instead of kinect, it's designed for hand gestures and it's also cheaper and no additional hardware is required (just a laptop). You would still need to write your own software to do the things you need it for, but [here's an example project](https://developer.leapmotion.com/gallery/the-ultimate-interactive-presentation) for an interactive presentation that should help you out.",4p6gmp,t3_4p6gmp,ssps1138,,Comment,1,0,1
4p5iii,2016-06-21 12:35:46-04:00,Phoola,What's different about the analog to digital cables which requires a modem?,"I understand the need for a modem, but why aren't phone lines capable of handling digital signals?",,,,,Submission,2,0,2
d4i739m,2016-06-21 13:06:57-04:00,visvis,,"There are filters on the line allowing only certain frequencies through, crippling any digital signals trying to go through.",4p5iii,t3_4p5iii,Phoola,,Comment,3,0,3
d4irpdb,2016-06-21 20:46:30-04:00,s3sebastian,,"Modern data transmission over phone lines is only digital (for internet services as well as for calls). For example here in Germany a standard called [Annex J](https://en.wikipedia.org/wiki/G.992.3_Annex_J) is used, it's all digital.
There are no analog or digital cables, you can use a cable for both, but you have to take the attenuation, noise etc. of the cable into consideration when you want to transmit on certain frequencies with certain modulation types and protocols.

There's also [digital modulation](https://en.wikipedia.org/wiki/Modulation#Digital_modulation_methods) btw, for example WiFi  among others often uses quadrature amplitude modulation.",4p5iii,t3_4p5iii,Phoola,,Comment,3,0,3
4p4xfo,2016-06-21 10:40:53-04:00,mybabysbatman,Please help! I need help with one line of code!,"Hey so when it comes to computer science I pretty much no nothing. But because I'm the youngest guy at my company everyone assumes I'm the tech wiz. So my company has asked me to write a line of code that will take the date and time of a printer and to have that printer write that everytime it prints. Now I've gone back and forth through this manual several times but I'm still lost. Can anybody offer advice. Here's the printer's manual. 

https://www.zebra.com/content/dam/zebra/manuals/en-us/software/zpl-zbi2-pm-en.pdf",,,,,Submission,7,0,7
d4i2as0,2016-06-21 11:24:27-04:00,dejaWoot,,"Holy Hell. It's own language.
I'd recommend r/programminghelp, there's not much here which falls under the realm of computer science except at a very low level.

EDIT: there's an [emulator](http://labelary.com/viewer.html) here. Also has some sample code.

EDIT2: Found it. Page 1357 has examples.

    ^FC%
    ^FD %m/%d/%y-%H:%M:%S^FS

You'll have to tweak for desired formatting though, this is the plain jane version.",4p4xfo,t3_4p4xfo,mybabysbatman,,Comment,17,0,17
d4i2imm,2016-06-21 11:29:14-04:00,mybabysbatman,,"You guys are awesome! I'm working from home and I don't even have the printer in front of me so I had no way of testing if what I was doing was correct. This emulator is a life saver.
",4p4xfo,t1_d4i2as0,dejaWoot,,Reply,5,0,5
d4i8xl9,2016-06-21 13:45:29-04:00,mybabysbatman,,You guys are my hero's seriously. Tha k you so much!,4p4xfo,t1_d4i2as0,dejaWoot,,Reply,4,0,4
d4i23cd,2016-06-21 11:19:55-04:00,MisoSoup,,"Start at Page 1351 of the manual and the following pages.

It provides some sample scripts to set and read from the RTC.

Looks like a complete brain fuck btw. Good luck!",4p4xfo,t3_4p4xfo,mybabysbatman,,Comment,3,0,3
d4i2jfg,2016-06-21 11:29:43-04:00,mybabysbatman,,Haha yeah it's starting to give me a headache.,4p4xfo,t1_d4i23cd,MisoSoup,,Reply,1,0,1
4p2svb,2016-06-21 00:14:07-04:00,dsfsfdsfdsfs,Pumping lemma for context free languages,"So this is an example of a CFL pumping lemma proof: http://i.imgur.com/ZdUehmr.png

I understand why it's not a CFL, but what does the m+2i-2 indicate? I get m, the 2i-2 not so much?",,,,,Submission,1,0,1
d4ho231,2016-06-21 01:39:48-04:00,HeraclitusZ,,"It's kind of hard to read because there is very little context, but I imagine it refers to the case where both pumped substrings are within the ""a"" section.

When a CFL gets pumped, it gets pumped in 2 places spanning a certain distance. That distance could be contained in just one section (i.e., all in ""a""), or across 2 different sections (like the ""ab"" case presented as ""similar cases""). So it is just simplifying 2(i-1) to 2i-2. 

The reason for the i-1 choice is probably to emphasize you are also allowed to pump down once instead of up any number of times, so if we have i-1, we can consider all i >= 0, which is a nicer bound in some sense than -1. ",4p2svb,t3_4p2svb,dsfsfdsfdsfs,,Comment,1,0,1
d4ht6ba,2016-06-21 06:19:41-04:00,dsfsfdsfdsfs,,What does 2(i-1) mean? The distance between v and y? I don't see it.,4p2svb,t1_d4ho231,HeraclitusZ,,Reply,1,0,1
d4hye7z,2016-06-21 09:50:16-04:00,HeraclitusZ,,"They are exponents. A string is mathematically represented as a monoid where multiplication is the same as concatenation. So if you pump the same substring multiple times, like going from ""a"" to ""aa"", that is written as going from a to a^2 . Here, we start with a^m . If we pump some substring of length n>0 some number of times i-1 for i>=0, we have a^m+n(i-1).

In addition, that kind of touches on error in this proof. The author has chosen specific substrings to pump, namely single letter ones. That is not allowed. *All* substrings of the necessary length must be considered for pumping. But the idea will be the same when that is done, also considering the clear pattern breaking that occurs if you pump strings across 2 letter sections (e.g. a^m (ab)^i b^m c^m ).",4p2svb,t1_d4ht6ba,dsfsfdsfdsfs,,Reply,1,0,1
4p0u30,2016-06-20 16:33:13-04:00,InarticulateAtheist,Can you suggest a really good book (or online resource) to learn algorithm design?,"I tried using CLRS, but found it a bit tough since I'm a beginner.",,,,,Submission,6,0,6
d4h9n4j,2016-06-20 18:54:10-04:00,umib0zu,,[SICP](https://mitpress.mit.edu/sicp/full-text/book/book.html). Then try to read CLRS again.,4p0u30,t3_4p0u30,InarticulateAtheist,,Comment,2,0,2
4oulr8,2016-06-19 14:54:42-04:00,mahalo1984,What is the bible for compliler/language design?,,,,,,Submission,5,0,5
d4fp0n3,2016-06-19 15:51:18-04:00,NinjaViking,,"[The Dragon Book!](https://en.wikipedia.org/wiki/Compilers:_Principles,_Techniques,_and_Tools)",4oulr8,t3_4oulr8,mahalo1984,,Comment,6,0,6
d4ftihd,2016-06-19 17:49:16-04:00,I_Do_Not_Abbreviate,,"[The list I first heard that featured in is a little outdated, but I still like it.](https://www.youtube.com/watch?v=4U9MI0u2VIE)",4oulr8,t1_d4fp0n3,NinjaViking,,Reply,2,0,2
d4g8igf,2016-06-20 00:05:29-04:00,Devvils,,lol. I did one of Jeff Ullmans's courses on Automata.,4oulr8,t1_d4ftihd,I_Do_Not_Abbreviate,,Reply,1,0,1
d4g0twy,2016-06-19 21:09:13-04:00,bekroogle,,This. Though I'm not sure how much it goes into designing a language as much as implementing one.,4oulr8,t1_d4fp0n3,NinjaViking,,Reply,1,0,1
d4iallk,2016-06-21 14:20:05-04:00,east_lisp_junk,,"It doesn't really discuss/teach language design at all, but I've found a lot of people don't notice the difference between designing a language and designing a compiler for that language. It's also not nearly comprehensive enough to be ""the bible"" of compiler design (e.g., it includes almost nothing on SSA).",4oulr8,t1_d4g0twy,bekroogle,,Reply,1,0,1
d4inci1,2016-06-21 18:53:15-04:00,bekroogle,,"Maybe that's one of the reasons I don't recall ever running across SSA. =/.

Got any good recommendations for texts that deal with it?",4oulr8,t1_d4iallk,east_lisp_junk,,Reply,1,0,1
d4jcilj,2016-06-22 09:45:02-04:00,east_lisp_junk,,"For an introductory text that includes SSA, there's Cooper & Torczon's ""Engineering a Compiler."" If you really want to get into optimization, it's worth looking at Muchnick's ""Advanced Compiler Design and Implementation"" (fairly broad coverage of optimization) and Kennedy's ""Optimizing Compilers for Modern Architectures"" (focuses a bit more on loop transformation stuff, which was Kennedy's research group's bread and butter for years).",4oulr8,t1_d4inci1,bekroogle,,Reply,1,0,1
4otzhw,2016-06-19 12:30:42-04:00,burn_and_crash,"What programming language (syntax) features define it as a ""modern"" style language?",,,,,,Submission,16,0,16
d4fj416,2016-06-19 13:11:13-04:00,visvis,,Many old languages are updated with new features occasionally so I don't think and syntax features could allow you to distinguish modern programming languages (and how would you define what a modern language is anyways?).,4otzhw,t3_4otzhw,burn_and_crash,,Comment,3,0,3
d4fj9ra,2016-06-19 13:15:45-04:00,burn_and_crash,,"It's mostly the reason why I asked the question: I see the term flying by every so often, but always wonder what sets it apart from a ""classic"" language. I mean, standard libraries apart from design seem quite consistent nowadays in terms of their feature set, so that doesn't appear to be a relevant measure to me. 

But I don't really know either what _does_ set them apart.",4otzhw,t1_d4fj416,visvis,,Reply,1,0,1
d4foeen,2016-06-19 15:35:03-04:00,zombarista,,"In my mind, multi-paradigm languages are the most mature. If you want to go functional, you can. If you want to stick with blocks, you can. ",4otzhw,t3_4otzhw,burn_and_crash,,Comment,3,0,3
d4fu7jw,2016-06-19 18:08:19-04:00,PumpkinSeed,,"Generics, lambda expressions, asynchronous operations, pattern matching... That's all I can think of off the top of my head. ",4otzhw,t3_4otzhw,burn_and_crash,,Comment,4,0,4
d4gn3kh,2016-06-20 10:21:51-04:00,xcombelle,,"except async, all are in haskell which is pretty old ?",4otzhw,t1_d4fu7jw,PumpkinSeed,,Reply,3,0,3
d4gnj39,2016-06-20 10:32:59-04:00,PumpkinSeed,,"Functional programming is pretty old, but many the concepts are only just now seeing wide spread adoption. Broadly, I think the combination of OOP and Functional paradigms are what make a ""modern"" language these days. ",4otzhw,t1_d4gn3kh,xcombelle,,Reply,1,0,1
d4geysv,2016-06-20 04:16:35-04:00,Devvils,,"Unification in its many forms.


eg https://en.wikipedia.org/wiki/Unification_(computer_science)#Application:_Type_inference",4otzhw,t3_4otzhw,burn_and_crash,,Comment,1,0,1
d4gbydm,2016-06-20 01:57:52-04:00,koorb,,"Typically when we say madden language we mean one that supports Object Orientated Programming, but I don't think that is anything we need to worry about now as they all do, unless it isn't relevant.",4otzhw,t3_4otzhw,burn_and_crash,,Comment,-4,0,-4
4oskvg,2016-06-19 04:54:32-04:00,coconutscentedcat,Would I be able to find a good job if I drop out after 2yrs of CS?,"If I'm ever at a point in my life where I absolutely need to be financially independent, and cannot continue school for the next 2 years of my life even if I wanted to, would I still be able to find a junior position somewhere? Will I still be able to get a good job?


 I'm going into my first year of CS as a mature student, and I'm worried that it may come to that at some point during my 4yr of studies. There's a chance that I will be forced to drop out in my 2nd or 3rd year.

Right now, my employability isn't great. I can build simple good looking websites (html, css, some js & php) - but not much more than that (I took 1yr of web design at community college). I was considering doing 1 more year of college and at least getting a diploma, but I'm really looking forward to studying comp science.",,,,,Submission,7,0,7
d4fe0j1,2016-06-19 10:34:55-04:00,BillionExplodingSuns,,"Can you get your associates first? At the very least I would consider that a step up from boot camps. You can get junior web developer off an associates, jobs that require Java, C and the like will want you to have your B.S. Oh, and work on a portfolio, starting now!",4oskvg,t3_4oskvg,coconutscentedcat,,Comment,4,0,4
d4fwijm,2016-06-19 19:14:04-04:00,Bottled_Void,,"Your title could just as easily be, ""Would I be able to find a good job without a degree"".


Yeah, sure, why not? But having that piece of paper does make getting that job a damn sight easier. 


Without the piece of paper you're still up against all the people that are self-taught. And some of those people are pretty talented too.",4oskvg,t3_4oskvg,coconutscentedcat,,Comment,2,0,2
d4fxb8x,2016-06-19 19:36:58-04:00,coconutscentedcat,,"Would I still stand a chance if I have a nice web dev portfolio? I can build a good looking portfolio site with a few good examples. I'll also try to intern during my first summer year.
",4oskvg,t1_d4fwijm,Bottled_Void,,Reply,2,0,2
d4g1182,2016-06-19 21:14:04-04:00,Bottled_Void,,"For web development, most places don't care too much about the degree for a junior position. Is that where you saw yourself working?


The variety of jobs that are relevant to computer science is pretty huge. Some doors would probably be closed to you without a degree and some are already open.


Automotive Software Engineer, they'd probably like to see that piece of paper. But there are a few other routes you could take.


Webdesign - If you can do it, and it looks and loads fine, you'll probably be fine. Might be harder to set into the SSL/Payment/Database end of things though.


IT - Okay, so you'd probably start out just installing Windows off an ISO over and over to start with. But once you've trained up in network infrastructure you can really make a difference to a large company. (Of course they'll still think you just install Windows over and over).


Software support - Running a helpdesk might not seem that great. But some places will let you get more involved with the products letting you give feedback and suggestions as to things they could change.


Installation - Guy in a van putting boxes in houses and fault finding. Probably not where you're thinking of being.


I mean something else might just be getting a job at a software house. Even if you're just filing papers. It might give you an in for getting more involved in the work. This might be how you start out working as an intern.


But in short, I'd look at the job listings. Look at the requirements they've listed. You don't always need everything they put down, but it will give you an idea of what sort of options are available to you. And more importantly, which ones aren't if you don't finish your course.",4oskvg,t1_d4fxb8x,coconutscentedcat,,Reply,1,0,1
d4f7lsj,2016-06-19 05:00:22-04:00,SolarShrieking,,Student loans + 2 jobs while in school could be feasible. It's what I'm doing.,4oskvg,t3_4oskvg,coconutscentedcat,,Comment,4,0,4
d4f7n98,2016-06-19 05:03:03-04:00,coconutscentedcat,,"You must manage your time REALLY well. I'm very likely not as focused and productive as you, so I doubt that would be an option for me. Keeping good grades at the university level and learning some stuff on my own will unfortunately use up all of my time :)",4oskvg,t1_d4f7lsj,SolarShrieking,,Reply,4,0,4
d4f7oxy,2016-06-19 05:06:14-04:00,SolarShrieking,,"One of my jobs is in the IT Dept. at my college, so it helps with scheduling shifts that bookend my classes. When you consider third shift and weekends, your time really opens up. Hell, I'm at work right now. (5a local time)

I have my days scheduled out in half hour increments, and I try to stick to it as well as possible. I'm a very weak-willed person, and it's so easy for me to just be like ""eh, I'll just stay home"" if I don't stick to my schedules and routines.

Edit: you mentioned learning stuff on your own. I teach myself programming in my off time, and make a bit of side money from it. Definitely possible, and currently rocking a 4.0 :)",4oskvg,t1_d4f7n98,coconutscentedcat,,Reply,1,0,1
d4ffaw3,2016-06-19 11:16:33-04:00,None,,"Get a credit card. Live frugally. Get a job. Go to a cheaper school if you can. A degree will help a lot. Unless you know somebody that can get you a job,  and provide you with 2 years of experience which would work as well. But I'd advise finishing school. 

Getting a job without a bachelors and no experience is possible,  but much much harder. ",4oskvg,t3_4oskvg,coconutscentedcat,,Comment,0,0,0
d4fdsh5,2016-06-19 10:27:29-04:00,Reinhold_Messner,,I completed my degree in Economics in 2005 but dropped my CS double after two years. I'm a senior architect now. I self taught C#. ,4oskvg,t3_4oskvg,coconutscentedcat,,Comment,-1,0,-1
4okssj,2016-06-17 15:00:18-04:00,hoomei,Are there any computer science concepts that are also interesting on a philosophical level?,"I was chatting with a philosophy department head of a university. I told him, ""programming is formal logic on steroids."" He asked me for an example. I tried to think of a CS problem that would be interesting to him, as a professor of philosophy, but I blanked. Since both philosophy and CS deal with logic, are there CS concepts that reach into philosophy?",,,,,Submission,23,0,23
d4dh588,2016-06-17 16:00:22-04:00,bekroogle,,"I'd say GEB (Gödel Escher Bach) is full of them. I particularly find the concept of ""meta"" and recursion worth contemplating for hours on end.

Though, to understand the philosophical aspects of recursion, you must first understand the philosophical aspects of recursion...",4okssj,t3_4okssj,hoomei,,Comment,15,0,15
d4dp5ln,2016-06-17 19:19:18-04:00,The_Popes_Hat,,Yeah that whole book is the answer to op's question,4okssj,t1_d4dh588,bekroogle,,Reply,2,0,2
d4dv4vp,2016-06-17 22:22:07-04:00,YourFavoriteBandSux,,You must also understand the philosophical aspects of tail recursion.,4okssj,t1_d4dh588,bekroogle,,Reply,1,0,1
d4ec2w1,2016-06-18 11:09:45-04:00,hoomei,,Meta is so meta.,4okssj,t1_d4dh588,bekroogle,,Reply,1,0,1
d4dk2kb,2016-06-17 17:06:47-04:00,shamankous,,Proof theory and type theory dovetails nicely with logic and philosophy. Discussions of semantics and provability show up on both sides.,4okssj,t3_4okssj,hoomei,,Comment,11,0,11
d4dnlnm,2016-06-17 18:35:49-04:00,UncleMeat,,"This is the right answer, in my opinion. A lot of computer science *applications* like Strong AI have interesting philosophical implications, but Types as a proof system is the most direct application to philosophy. ",4okssj,t1_d4dk2kb,shamankous,,Reply,6,0,6
d4ec3tg,2016-06-18 11:10:32-04:00,hoomei,,Would you be able to point me to some good beginner info on these concepts?,4okssj,t1_d4dk2kb,shamankous,,Reply,1,0,1
d4emqn2,2016-06-18 16:26:27-04:00,shamankous,,Proofs and Types by Girard is where I'd start. ,4okssj,t1_d4ec3tg,hoomei,,Reply,1,0,1
d4dpd9v,2016-06-17 19:25:16-04:00,Madsy9,,"Well, it depends on where CS ends and Mathematics begins. All of CS is also mathematics, and there's plenty of problems with a philosophical twist in mathematics. [Kolmogorov Complexity](https://en.wikipedia.org/wiki/Kolmogorov_complexity), [Turing's halting problem](https://en.wikipedia.org/wiki/Halting_problem), [Church's Lambda Calculus](https://en.wikipedia.org/wiki/Lambda_calculus) and [Gödel's incompleteness theorems](https://en.wikipedia.org/wiki/G%C3%B6del%27s_incompleteness_theorems) all have something to say about philosophy I think, because they say something about the very foundation of what can be proven, computed or compressed/optimized. In fact, Gödel's incompleteness theorem, Church's lambda calculus and Turing's halting problem are all equivalent when it comes to proving undecidability. I.e all of them are convertable to all the others.

Or to make an analogy to philosophy: Modern philosophers such as Ludwig Wittgenstein did a lot of philosophical work on what can be expressed and conveyed through language separated from things that can only be experienced first-hand or felt. That is, he tried to figure out the very core of what 'meaning' means and how far knowledge and meaning can be explained and conveyed to other people through language. In a way, I think that's philosophy's attempt to answer what Turing, Church and Gödel did for computer science.",4okssj,t3_4okssj,hoomei,,Comment,7,0,7
d4dpd8o,2016-06-17 19:25:15-04:00,big-blue,,"Multi-Agent-Programming. Generally, you program agents using the BDI model (Belief-Desire-Intention), which has its roots in philosophy, having been developed by the American philosopher Michael Bratman.

Using i.e. Jason to program an agent is an entirely different approach to programming than imperative, object-oriented or functional programming and is incredibly interesting, also to philosophers, since it tries to mimic reasoning as known from living creatures.",4okssj,t3_4okssj,hoomei,,Comment,3,0,3
d4e258g,2016-06-18 02:52:04-04:00,WillKhitey,,P vs NP,4okssj,t3_4okssj,hoomei,,Comment,2,0,2
d4e5oc4,2016-06-18 06:24:55-04:00,bacondev,,"Interesting, yes. But I'm not entirely sure how it's related to philosophy.",4okssj,t1_d4e258g,WillKhitey,,Reply,2,0,2
d4e925y,2016-06-18 09:24:58-04:00,ammicha,,In some sense (and under a bunch of wild philosophical assumptions about e.g. in what ways human thinking can be modeled by a computer) it's the question of whether creating something is fundamentally more difficult than understanding it.,4okssj,t1_d4e5oc4,bacondev,,Reply,2,0,2
d4dhiec,2016-06-17 16:08:40-04:00,MCPtz,,"The Iterated Prisoner's Dilemma. 

Something any student can implement in simple terms to show the results of finite iteration and indefinite iteration with variations on tactics and conclusions of strategies.",4okssj,t3_4okssj,hoomei,,Comment,1,0,1
d4dj027,2016-06-17 16:42:02-04:00,milk131,,"For a philosophical discussion I would say that AI would be an interesting subject, especially with the growth of autonomous cars. Although I'm not sure if it fits with discussing CS purely under formal logic.

A topic I've heard discussed is ""When does your car get to kill you?"". Say an autonomous vehicle is placed in a situation whereby the two options available are to drive off of a cliff and almost certainly killing you, the driver. Or to veer onto the pavement killing a crowd of people. It's similar to the trolley problem already discussed in philosophy, but with some interesting additions such as would different manufacturers prioritise the driver or the absolute lowest number of causalities, and advertising based on this? Government legislation? Moral issues surrounding the programmers who write the code that could ultimately clash with their own notation of life?

Having said that, an argument could also be made that if a machine was placed in such a situation then it may have little control of the outcome in the end and so the moral question is somewhat mute. But even so I've found it and interesting concept to discuss with people, even outside of a CS background.",4okssj,t3_4okssj,hoomei,,Comment,1,0,1
d4dnok0,2016-06-17 18:37:59-04:00,Mukhasim,,"This is really just ethics, it doesn't have much to do with AI. As you say:

> would different manufacturers prioritise the driver or the absolute lowest number of causalities

So, the issue here is about what decisions *humans* should make.

As such, (for me) this kind of answer to the question is like an admission that the interesting bits are the bits that are not CS.",4okssj,t1_d4dj027,milk131,,Reply,1,0,1
d4egvzp,2016-06-18 13:33:47-04:00,PM_ME_YOUR_PROOFS,,Oh man yea. These are my favorite subjects. AI has all sorts of philosophy of mind problems. Some Decision Theory problems and problems in epistemology relay on robots that make decisions. Computability is a veritable cornucopia of philosophical content.  Complexity Theory IMO hasn't been utilized enough in philosophy but it will get there. Honestly computer science and philosophy are like peanut butter and jelly.,4okssj,t3_4okssj,hoomei,,Comment,1,0,1
d4f042x,2016-06-18 23:23:28-04:00,qwerty_danny,,I like how Objects and Classes are analogous to [Plato's Theory of Forms](https://en.wikipedia.org/wiki/Theory_of_Forms). I think it can help people learn inheritance and OOP. ,4okssj,t3_4okssj,hoomei,,Comment,1,0,1
d4e6bms,2016-06-18 07:06:16-04:00,bacondev,,"I was writing a library that allows the programmer to do perform operations on physical quantities (e.g. mass, volume, force, etc.). The trouble lied in the fact that SI units are defined as measurements. Therefore, units of measurements can be used as measurements. Conversely, measurements can be used as units of measurement (e.g. a light year). So how do you distinguish the two concepts? Or are they even different concepts? This one puzzled me for days.",4okssj,t3_4okssj,hoomei,,Comment,0,0,0
d4dgj7j,2016-06-17 15:47:05-04:00,Hallucinaut,,"How about what equality means?  How some values are equal no matter how they're derived (primitives in Java, say) but others are equal if and only if they refer to exactly the same object (Objects).

Or what 'null' is. How an empty set differs from null, which differs from 0 or something that hasn't even been defined in a particular scope.

I really have no idea how to answer this question, but it was weird enough to induce a ramble!
",4okssj,t3_4okssj,hoomei,,Comment,-1,0,-1
d4dlv3f,2016-06-17 17:50:19-04:00,stangelm,,"Turing Test.  At what point does the ability to perform logical functions become intelligence?  Will competes become sentient?  Should an intelligent, sentient computer have rights? ",4okssj,t3_4okssj,hoomei,,Comment,-1,0,-1
4ok2x9,2016-06-17 12:36:38-04:00,aterrorcatsandwich,Uncrackable?,"Hi!

Total layman and I had a question. Is there any way to make a program completely uncrackable? Like if you were to try to look at the code, it would delete itself or auto corrupt.

I have more questions if someone want to PM me.

Thanks!",,,,,Submission,0,0,0
d4d8gft,2016-06-17 12:56:42-04:00,None,,[deleted],4ok2x9,t3_4ok2x9,aterrorcatsandwich,,Comment,3,0,3
d4d966i,2016-06-17 13:11:58-04:00,aterrorcatsandwich,,"Thank you for your reply. Follow-up question, does that mean the self destruction aspect would necessarily be embedded in the OS? I am trying to ground a sci fi script I'm working on.

I am going for a super computer that has a hidden purpose. Sort of a decoder ring situation. It does one thing normally, but when presented with certain inputs it has a secret purpose. is there a way that this hidden purpose could be hidden from someone who has been working on it?

",4ok2x9,t1_d4d8gft,None,,Reply,1,0,1
d4da36y,2016-06-17 13:30:54-04:00,None,,[deleted],4ok2x9,t1_d4d966i,aterrorcatsandwich,,Reply,3,0,3
d4db0m2,2016-06-17 13:50:10-04:00,scordata,,Side chain attacks. ,4ok2x9,t1_d4da36y,None,,Reply,1,0,1
d4dq9dt,2016-06-17 19:51:06-04:00,aterrorcatsandwich,,Thank you for the thorough reply. That really helps!,4ok2x9,t1_d4da36y,None,,Reply,1,0,1
d4damc1,2016-06-17 13:42:07-04:00,veeberz,,"There are malware out there that uses techniques to hide itself (e.g. exists only in memory and persists not as a file, but somewhere else), obfuscate the naughty code, and mutate itself. Maybe look into techniques used in malware, since those can be considered a program with a secret purpose.

Code obfuscation is a technique to make reverse engineering difficult. Just for fun, check out this defcon talk about halfway through:
https://youtu.be/HlUe0TUHOIc",4ok2x9,t1_d4d966i,aterrorcatsandwich,,Reply,1,0,1
d4dqaa0,2016-06-17 19:51:51-04:00,aterrorcatsandwich,,"Very interesting. I understood maybe 20% of that video but the""creepy malware"" at the end is scary",4ok2x9,t1_d4damc1,veeberz,,Reply,1,0,1
d4d95qh,2016-06-17 13:11:42-04:00,Bottled_Void,,"No. Not completely.


But once a program is compiled and linked into a binary executable it's not really that easy to read anyway. Sure you can go through the trouble of running it through a disassembler and find out what every instruction is doing at the machine/driver level, but it would still be a lot of effort.


Here is what ping.exe looks like if you open it up in notepad:


http://i.stack.imgur.com/PxG2J.png


And this is what the original source would probably look like:


http://www.codeforge.com/read/134002/Ping.asm__html


What you can do though is have a executable that performs some operation, (e.g. delete system32) and then delete itself. Or if you're fancy, you could have an executable that modifies part of it's code as it runs to make it harder to detect. If you're really fancy, you could copy some pretty mean tricks from hardware developers and install the executable into the firmware of some hardware. That way even wiping the entire drive clean means it still comes back.


But since you're asking if you can make a program uncrackable, then no, there is pretty much always someone better out there that could figure it out (if they had the desire). That's if you're talking a executable file, sitting on a disk drive, that hasn't been executed.


Hardware on the other hand, that's a different story. But that wasn't your question.",4ok2x9,t3_4ok2x9,aterrorcatsandwich,,Comment,1,0,1
d4dqaoo,2016-06-17 19:52:11-04:00,aterrorcatsandwich,,Thank you. That's very helpful in clarifying.,4ok2x9,t1_d4d95qh,Bottled_Void,,Reply,2,0,2
4oj9tl,2016-06-17 09:41:43-04:00,electricquill,Educational resources for a better low level understanding,"Hi all, I've been programming for years using high level programming languages but have never had any sort of formal education. I have little to no understanding of how things work at the low level, how data is stored/modified, terms like 'stack', 'heap', 'registers' etc.

Are there any books or resources you would recommend to me?",,,,,Submission,1,0,1
d4d1v10,2016-06-17 10:31:47-04:00,actinium89,,perhaps http://www.nand2tetris.org/ could be intresting for you?,4oj9tl,t3_4oj9tl,electricquill,,Comment,1,0,1
4oj9sf,2016-06-17 09:41:29-04:00,dor442,Help with Data Structures question,"Hi i need help :)

We know that the algorithm for finding a median in an array is:
T(n)<=T(3n/4)+T(n/5)+O(n)

Prove that:
the formula T(n)=1 for every n0>=n
and
T(n)=T(an)+T(bn)+n for every n0<n, a+b<1
their solution is T(n)=O(n)

They hinted us that we may have to prove using induction that T(n)<=cn for an appropriate c

thanks to anyone who may help!",,,,,Submission,0,0,0
4ogsr8,2016-06-16 21:15:04-04:00,Zerocare,What is the best HUMAN language to learn for a career in cyber security?,"Aside from English, what is in high demand now adays? Russian? Chinese? Arabic?",,,,,Submission,15,0,15
d4cgfgp,2016-06-16 21:40:03-04:00,None,,[deleted],4ogsr8,t3_4ogsr8,Zerocare,,Comment,6,0,6
d4cpvcb,2016-06-17 02:00:20-04:00,Rokichet,,"I am currently studying CS with a minor in Japanese. Are you currently working as a developer? If so, we're you able to turn Japanese into a selling point to get hired? ",4ogsr8,t1_d4cgfgp,None,,Reply,2,0,2
d4cil30,2016-06-16 22:31:55-04:00,dandrino,,"I will also say that if you do not consistently keep up and practice with a difficult language there is a good chance you will lose it. I have learned both Spanish and Chinese, and I can say that Spanish has largely stuck with me whereas Chinese has significantly slipped away since I learned it.",4ogsr8,t1_d4cgfgp,None,,Reply,1,0,1
d4d2iwh,2016-06-17 10:47:31-04:00,visgean,,"The Russian forums are very good source for studying security. But overall I don't think its a good strategy to learn language just because of ""high demand"". Russia is a prominent cyber threat now, but who knows how will the landscape of industry change in 10 years. 

But I don't think it's such an impossible feat to learn any of these languages - if you really want to learn the language you can go to live to one of these countries. So the question is really about where would you want to live at least for a few years: https://en.wikipedia.org/wiki/Russian_language#Statistics or https://en.wikipedia.org/wiki/List_of_countries_where_Arabic_is_an_official_language , I did not find this kind of list for Chinese but I presume its mainly used in mainland china. 

",4ogsr8,t3_4ogsr8,Zerocare,,Comment,1,0,1
4odvkf,2016-06-16 11:25:49-04:00,breadypotter,Any Awesome CS Thesis Ideas Out There?,Hey guys...was hoping you could help me out. Can you suggest any cs-related ideas that I could use for my undergrad thesis? Your responses would really be appreciated.,,,,,Submission,4,0,4
d4bysui,2016-06-16 14:53:50-04:00,iknighty,,"I suggest you go talk to some professor, they always have some ideas that they don't have time to explore further or implement, and if it's their idea they're more likely to be involved and help you as a supervisor.",4odvkf,t3_4odvkf,breadypotter,,Comment,10,0,10
d4c28co,2016-06-16 16:02:22-04:00,Bottled_Void,,"I quite like the idea of doing something with Evolutionary Algorithms.


https://en.wikipedia.org/wiki/Evolutionary_algorithm


What areas of your course did you like?",4odvkf,t3_4odvkf,breadypotter,,Comment,3,0,3
d4hzrgb,2016-06-21 10:25:28-04:00,patnoy1,,A.I Lingerie Matching ,4odvkf,t3_4odvkf,breadypotter,,Comment,1,0,1
4obftu,2016-06-15 23:45:00-04:00,pnotp,"Looking for paper: Harrison, Jones ""Semantics of Implicit Time Determinate Choice"" 1992","The paper is cited by Hoyler, Carter ""Deterministic Concurrency"" 1994 but all I could find is this citeseer page:

http://citeseerx.ist.psu.edu/showciting?cid=3532341

Sometimes I can find papers on the websites of the authors, though the citation only uses first initials so I don't have enough to go on. The publication it's from seems to be in the catalog of a few university libraries but the nearest is over 200 miles from me.",,,,,Submission,3,0,3
4o8jy5,2016-06-15 13:39:27-04:00,Tanger68,Reformatting a Windows RT tablet,"Just got into electrical stuff with a solid base of coding experience and operating system knowledge. So far, I haven't found anybody who's managed to do it and write about it. Any ideas on a starting point, CompSci? I think the device is bootlocked, so that's a kick already, but I'm optimistic",,,,,Submission,0,0,0
4o8j88,2016-06-15 13:35:53-04:00,SkyewardSword,"Is it worth studying a joint mathematics / computer science degree, or should I just go for a pure computer science degree instead?",Would the mathematics / computer science degree detract from the computer science side at all? Would the maths I learn from the mathematics side be applicable to a job in digital security or forensic computing (the fields I want to go into)? Are both halves of the degree as enjoyable as each other? I have so many questions,,,,,Submission,16,0,16
d4am7a3,2016-06-15 15:46:21-04:00,videoj,,"> digital security or forensic computing 

Neither of these fields is math intensive, unless your interested in cryptography.  Forensic analysis involves understanding the low-level architecture of computers and file systems.   

However, having good in-depth knowledge of math is a plus in most STEM fields.  Whether you would enjoy it depends on much you enjoy math.",4o8j88,t3_4o8j88,SkyewardSword,,Comment,6,0,6
d4aouex,2016-06-15 16:40:38-04:00,apendleton,,"The coursework might or might not be useful depending on what you end up wanting to do. If it's fundamental crypto, or something heavy on linear algebra like 3D rendering, maybe. If it's building web apps, maybe less so. For what it's worth, I don't think of forensics as being that math-y.

The *credential* (piece of paper that says ""math"" on it) will probably not be more useful to you than one that just says CS unless you want to go into academia, in a more theory-oriented direction of research. Again, doesn't seem too likely from your stated interests.",4o8j88,t3_4o8j88,SkyewardSword,,Comment,4,0,4
d4alws1,2016-06-15 15:40:31-04:00,Bottled_Void,,"Put it this way. I did Computer Science. There was a degree for combined Computer Science / Mathematics.


I did the exact same course as you could have done for the combined degree. The only difference was the dissertation.


Of course your university may be different, but the only real difference for me is what was written on the paper I got at the end.",4o8j88,t3_4o8j88,SkyewardSword,,Comment,4,0,4
d4b69qk,2016-06-16 00:02:19-04:00,MajorCS,,"For me, this question heavily depends on what the university covers. Eg. My computer science major really neglected math but provided a games technology/graphics second major to cover and fill any gaps. So maybe speak to their department heads or review what's covered in their course structure. 

Secondly, whether or not it's enjoyable comes down to whether you enjoy it. We really cannot say. 

I know this didn't directly answer your questions but I thought I'd give you my 2 cents. ",4o8j88,t3_4o8j88,SkyewardSword,,Comment,2,0,2
d4ma6ee,2016-06-24 10:46:17-04:00,TissueReligion,,"Stray opinions:

1. Most people will say don't bother. They say that because most advice is tailored to the lowest common denominator, and most people neither need the extra math nor can handle the extra courseload. If you are exceptional, don't take advice intended for other people.

2. You can still take extra math classes for pleasure even if you won't get a minor/major out of it. The extra major probably won't have a significant impact on your career prospects, *but* the knowledge and enhanced ability absolutely might.

3. Self-studying once you leave college is very hard. A 8-10 hour work day drains most people and makes it hard for them to be productive afterwards. Its absolutely doable, but if there are sets of classes that you feel like you must to be a self-respecting nerd/cs guy, it is sooo much easier to do it in college, when you actually get the most productive hours of your day to yourself to learn.

4. One of the most important things to be able to take from undergraduate math coursework is the mathematical maturity to get to the point where you *can* effectively self-teach yourself. Theoretically I could throw Baby Rudin at a smart 12th grader and he could digest it and self-study it, but if there's no outside reference (teacher, graded tests/assignments) then he will probably think he understands much more and better than he really does. But if you're able to use college as a way to get the mathematical maturity and perspective you need, you'll be able to self-study properly afterwards.",4o8j88,t3_4o8j88,SkyewardSword,,Comment,1,0,1
4o8akp,2016-06-15 12:51:27-04:00,coen244throwaway,Need help understanding some Java concepts,"* Static methods and variables: I'm having trouble understanding what the following statement is telling:

""A static method has no this, so it cannot use an instance variable or method that has an implicit or explicit this for a calling object""

* Static variables: Once they are initialized, they become accessible to the whole class. So any static method that makes use of these variables are then able to change its value. Does this change become permanent to the whole class? If another static method were to use the same variable to change its value, it would now modify the value previously adjusted from the other static method?

* Wrapper classes: They convert any primitive type representation to  a string representation. So they're mostly useful as arguments of the toString method? 

Thank you!",,,,,Submission,3,0,3
d4afh2c,2016-06-15 13:29:13-04:00,lneutral,,"A static variable belongs to the class the way an instance variable belongs to an object. There's only one instance of the static variable, period - so if you change it, anything that accesses it inside or outside of that class's methods will be accessing that same variable.

As for wrapper classes, that term is broad. I assume you mean classes like Integer, Float, etc.? Anything that requires an object instead of a scalar will need them. You can't put scalars like float inside a generic class, so you'd use them for that too. There are other more subtle uses, but scalars and objects do not necessarily have the same ""rules"".",4o8akp,t3_4o8akp,coen244throwaway,,Comment,2,0,2
d4ah6ul,2016-06-15 14:04:12-04:00,coen244throwaway,,"mhm I see..

EDIT - was able to answer my own question, thanks for the above clarification though!",4o8akp,t1_d4afh2c,lneutral,,Reply,1,0,1
d4ai23b,2016-06-15 14:21:42-04:00,lneutral,,"Okay, that's complicated. Let's say we have:

    public class A {
      public int myInt:
      public static float myFloat:
    }

You must have an instance of A to get myInt (since it's an instance variable), but you do not use an instance to get myFloat. So:

    class B {
      public void myFn() {
        A a = new A():
        myInt = 0: //WRONG
        myFloat = 0.0f: //WRONG
        A.myInt = 0: //WRONG
        a.myInt = 0: //Correct
        A.myFloat = 0.0f: //Correct
        a.myFloat = 0.0f: //WRONG
      }
    }",4o8akp,t1_d4ah6ul,coen244throwaway,,Reply,3,0,3
d4ajng9,2016-06-15 14:54:17-04:00,coen244throwaway,,"awesome, this helps visualize some of the concepts - thanks!",4o8akp,t1_d4ai23b,lneutral,,Reply,1,0,1
d4ai3g4,2016-06-15 14:22:29-04:00,DoItForMom,,Make the variable public or rather make a public static method that returns the variable,4o8akp,t1_d4ah6ul,coen244throwaway,,Reply,1,0,1
4o4qvz,2016-06-14 20:56:19-04:00,redmprog,help for studying computer science,"I want to become computer expert in all aspects 
I know programming
I study Electrical Engineering in university and I love computers
I want to know computers in basic level
If I want to learn basic about computers what work should I do?
between data structures , operating system , computer networks and other lessons what order should read?
and what topic from math should read?
I almost know list of lessons but don't know exactly order for study them
I need professional full description :)",,,,,Submission,2,0,2
d49xlda,2016-06-15 04:11:29-04:00,Mirakodusd,,Maybe r/cscareerquestions be helpful to you too,4o4qvz,t3_4o4qvz,redmprog,,Comment,3,0,3
d49oxof,2016-06-14 22:47:07-04:00,BoSsManSnAKe,,"I'm like you. Have you considered switching from electrical engineering to computer engineering? Start by learning a language. If learning on your own, I'd say python. Once you know enough to the point you know about object oriented programming, learn about data structures and algorithms. Then you can learn about operating systems and networking.

It's hard to learn about everything on your own, I'd suggest another degree or switching majors. Once you know a lot, you'll also realize that there is a lot more to learn.

As for math, just know a couple semesters of calculus to begin with. Assuming electrical engineering, you'll learn a good amount anyway. Math is just good for algorithms and once you know how to program this becomes more apparant.",4o4qvz,t3_4o4qvz,redmprog,,Comment,1,0,1
d49thf2,2016-06-15 01:05:32-04:00,Crawldragon,,"As a former computer science student currently in a relationship with a maths student, it's amazing how many mathematical concepts you can accidentally learn studying computers without ever even thinking about it like math.",4o4qvz,t1_d49oxof,BoSsManSnAKe,,Reply,1,0,1
d49xu41,2016-06-15 04:25:08-04:00,redmprog,,I don't have conditions for switching my majors or probably forced to switch :|,4o4qvz,t1_d49oxof,BoSsManSnAKe,,Reply,1,0,1
d4a5n58,2016-06-15 09:57:18-04:00,singham,,Quora would be of more help to you in this regards. I have read lot of good answers there.,4o4qvz,t3_4o4qvz,redmprog,,Comment,1,0,1
4o3xdn,2016-06-14 17:53:43-04:00,zmacker34,Transferring old files/pics to new computer,"I have a realllllly old mac (circa 2008), that I'm afraid is going to bite the dust with all of my ancient files and pics on it. What is the best way to save or move all of my files without the risk of damaging them? ",,,,,Submission,0,0,0
d4e6mhq,2016-06-18 07:24:27-04:00,diMario,,"If the old mac is still in working order and you can boot it until you can login to it, the most obvious way is to buy (or borrow) an external USB drive (either HDD, or USB stick) and copy files from the old mac to the external drive. Then copy them from the external drive to your new computer. I am assuming the mac has a USB interface of course.

Should this be not feasible, the next thing to consider is to take the hard disk(s) out of the old mac and place it in an external USB enclosure. These are available for both 2.5 and 3.5 inch form factors. Although the ones for drives with parallel ATA (PATA) interfaces will be fairly rare nowadays (because SATA has become the standard) you should be able to find them. These external enclosures usually are USB devices themselves, so you can plug them into your new computer and slurp the contents like you would do with any other external USB drive.

One consideration: the file system type used by Apple in the mac probably will not be recognized by your average Windows O/S, and possibly neither by a freshly minted linux install. If your new computer is a mac too, then it should pose no problem.",4o3xdn,t3_4o3xdn,zmacker34,,Comment,2,0,2
4o3fq7,2016-06-14 16:18:34-04:00,this9000,My goal is to work at Google next year. I'm looking for study partners for interview prep.,"I find that study partners help me to maintain a better study schedule and have more fun. 

I recently completed a BSc in CS. I will begin a MSc program in September in Bay Area. I have 2 months of free time now that I will be dedicating to interview prep (+30hrs/week). My goal is to be an intern at Big N next summer.  My IRL friends from CS department don't share a similar goal which is why I'm here. I'm mostly free all day and I can probably arrange my schedule according to yours. 

I'm looking for one or more of the followings: Brainstorming, improving thought process, competing with each other, helping each other memorise/learn/troubleshoot things, mock interviews at pramp. I'm open to suggestions. I'd like this partnership to include some friendship as I believe it is important to have more fun and also to maintain a sense of duty to help each other.

If this sounds good, please PM me.",,,,,Submission,0,0,0
d4aczmc,2016-06-15 12:38:38-04:00,panderingPenguin,,">I have 2 months of free time now that I will be dedicating to interview prep (+30hrs/week).

Just saying, I don't think that's anywhere close to necessary.  If I were you, I'd just chill a little, sure interviews can be hard and stressful sometimes but it's nothing to go crazy over.  Just do enough to keep the material fresh and probably prepping the week before will be fine. Best of luck to you.",4o3fq7,t3_4o3fq7,this9000,,Comment,3,0,3
d4hudaz,2016-06-21 07:26:50-04:00,bartturner,,"But it is not like what you are studying does not have value after the interview.

Really, I mean really knowing different algorithms will help you throughout your career.

I am old but for example I learned Paxos fundamentals (VMS Clustering originally) a zillion years ago and recently actually been leveraging



",4o3fq7,t1_d4aczmc,panderingPenguin,,Reply,1,0,1
d4ib0cy,2016-06-21 14:28:43-04:00,panderingPenguin,,"Doing ""interview prep,"" which probably involves lots of whiteboarding, toy problems with little tricks, a few open ended problems, etc is probably not super relevant in the long run except for other interviews. Sure he'll spend some time brushing up on CS fundamentals which is great, but that doesn't require the kind of time OP was talking about.",4o3fq7,t1_d4hudaz,bartturner,,Reply,1,0,1
d8pdeio,2016-10-12 17:23:02-04:00,danshalev,,"Try Pramp. It's a free p2p live mock interview practice platform. You'll get the closest experience to a real code interview, until you ace it.
Highly recommended! ",4o3fq7,t3_4o3fq7,this9000,,Comment,1,0,1
4o2fe0,2016-06-14 13:04:44-04:00,ArtifIcer54,looking for a solid intro to embedded programming that's mac-compatible,"Hey all!

I'm trying to teach myself embedded programming for my job.  I've been working on my current project in mbed so far, which makes things a lot easier/simpler, but doesn't have quite the breadth of support that I need, and makes low-level control difficult.  I want to redo everything (my needs are actually pretty basic) in a setting where I have finer control over the board's operations.

I'm working on a mac, programming an NRF51, and I'd love some advice on how to get started.  What development environment, if any, should I use?  What toolchain?  What tutorials are good for beginners, etc.  I've been searching around online for a while but most of what I've found isn't mac-friendly.  Thanks!",,,,,Submission,4,0,4
d4968o3,2016-06-14 15:33:05-04:00,elpantalla,,"Try this:
https://developer.nordicsemi.com/

You essentially want to be developing using the GCC toolchain for your specific architecture. ",4o2fe0,t3_4o2fe0,ArtifIcer54,,Comment,2,0,2
d4aezvp,2016-06-15 13:19:31-04:00,ArtifIcer54,,"What exactly do you mean ""for my specific architecture?""  Are you talking about the nrf?  Also, won't I need another program to load the code onto the board itself?  I remember when I was dual-booting windows I used gcc (I think it was gcc at least) to compile, and then Keil uvision to load the .hex file onto my board.  Is there a keil equivalent for mac that you would recommend?

Thanks for all the help.  I'm kind of a scrub in all this, but I really want to get better. :)",4o2fe0,t1_d4968o3,elpantalla,,Reply,1,0,1
4o1a0n,2016-06-14 09:02:06-04:00,Veson,ELI5: interval tree clock,"I understand how vector clocks detect concurrent events, but they are designed for a fixed number of nodes, so we can't easily add and remove nodes. And with interval tree clock we can. Could you help me grasp how they work: [Interval Tree Clocks: A Logical Clock for Dynamic Systems](http://gsd.di.uminho.pt/members/cbm/ps/itc2008.pdf). I don't understand the graphic notation.",,,,,Submission,5,0,5
4ny1wi,2016-06-13 17:50:23-04:00,TheWeebles,How to download linux virtual machines on windows 8,"I've tried virtual box and ubuntu, after I've downloaded from the sites, I can't install them. Is there any guide for this?",,,,,Submission,0,0,0
d47xk8r,2016-06-13 18:21:23-04:00,101C8AAE,,How about you describe what problem you encounter?,4ny1wi,t3_4ny1wi,TheWeebles,,Comment,1,0,1
d4836w7,2016-06-13 20:39:52-04:00,TheWeebles,,"right now im trying to get vmware player and ubuntu, but when I try to extract the vmware player i get an error message saying:

**setup failed to extract the files necessary to install vmware player**",4ny1wi,t1_d47xk8r,101C8AAE,,Reply,1,0,1
d4e6udl,2016-06-18 07:36:54-04:00,diMario,,"(1) Check that the partition (usually the C: drive) where Windows places your temporary files has enough free space. Setup extracts individual files from the vmware archive you dowloaded and wants to store them in a temporary location before starting the installation dialog. If there is not enough free space this fails.

(2) less likely: the file you downloaded from VMWare is somehow corrupt. This happens on occasion. Between the VMWare server sending a data packet and your client PC receiving it something went wrong but the error was not detected at that time. But now that you try using the downloaded file the error manifests itself and causes errors when trying to extract individual files from the downloaded archive.
One way to prove this hypothesis and overcome the problem is to download the installer a second time from VMWare and try again.",4ny1wi,t1_d4836w7,TheWeebles,,Reply,1,0,1
d4857qt,2016-06-13 21:29:15-04:00,sullage,,The guide on the virtualbox site looks pretty extensive: https://www.virtualbox.org/manual/ch01.html#intro-installing,4ny1wi,t3_4ny1wi,TheWeebles,,Comment,1,0,1
4nxj0l,2016-06-13 16:05:38-04:00,EggMcFuckin,Help with a CS homework question,"The assignment I'm working on asks us construct various design models (class model, physical data flow diagram, etc.) for a given scenario. Basically, it's FitBit for a University:

* There is a device that is attached to your wrist the tracks your activity (steps, heart rate, etc.)
* There is a smartphone app associated with the device
* The device and the app communicate with each other
* You can use the app to sign up for fitness classes at one of the University's fitness centers
* There is an RFID in the device that allows you to enter one of the University fitness centers without needing to provide other identification

What I'm really struggling with is building the network data flow diagram in Visio. I know very little about things like routers, servers, firewalls, how devices communicate, standard protocols, etc.
I'd be happy with anything that could point me on the right path, or help me visualize what the solution should look like.",,,,,Submission,7,0,7
d48dq60,2016-06-14 01:02:57-04:00,Pseudofailure,,"I don't know what kind of class in which you are enrolled, but it sounds like you might be over-thinking some of the question. When I hear ""class diagram"" or ""data flow diagram"", I generally assume abstract concepts, like `[fitbit] ---(sends message)---> [central server]`, so I assume you wouldn't need to know network protocols and whatnot. 

However, the ""physical"" modifier of the later might change that. If that's the case, in order to not give you answers, I'd encourage you to learn to google better. Like ""wireless device communication"" ""rfid"" or ""internet protocols"", possibly combined with queries about use cases. You should be able to spend an hour googling and get a decent idea of the protocols involved here, or at least their names. 

Following that note, just figure out the clients and servers involved in this, then figure out how they're communicating. Google some more for examples of this, like ""IoT device networking"" or ""mobile app to server communication"". 

The final diagram would be a bunch of arrows representing communication between each client and/or server. Consider which ones have access to each other. ",4nxj0l,t3_4nxj0l,EggMcFuckin,,Comment,3,0,3
4nwupk,2016-06-13 13:55:09-04:00,pumpinglemma16,Question about proof of Pumping Lemma theorem,"I'm learning the pumping lemma, and on most proofs (like [this](http://www.cs.sfu.ca/~kabanets/308/lectures/lec4.pdf) one) they usually start with the pigeonhole principle, if there's a length of n but there's n+1 visited states in the sequence, must be a repeated state. Then they say that there's a string w = xyz for 3 states q1 -> x -> q2 (loops with y) -> z -> q3, where y is the one that repeats. They also mention x and z can be  empty but y must be nonempty. Why is this? Why can't I loop y with the empty string?

They usually say its because of distinct time steps or i < j, but it doesn't make sense to me. 

Thanks for any help.",,,,,Submission,2,0,2
d47pm98,2016-06-13 15:28:12-04:00,lacunahead,,"The short of it is that a DFA by definition cannot move on no input, i.e. there are no epsilon-transitions in a DFA. 

Suppose we had a DFA with an epsilon transition from state Q1 to Q2. Then upon arriving at Q1 we have a choice to make as to whether we want to stay in Q1, or move to Q2 with no input. But this means our DFA is actually an NFA, since we have a non-deterministic choice.",4nwupk,t3_4nwupk,pumpinglemma16,,Comment,1,0,1
d47t1t1,2016-06-13 16:39:57-04:00,pumpinglemma16,,"Can you explain why epsilon is a non-deterministic choice even if it's the only choice from q1 to q2 ie there would be some manner of determinism? For instance if in w = xyz, if x can be epsilon and z can be epsilon and episilon is the only choice from q1 to q2 as in my above explanation, then wouldn't it be an NFA? Furthermore why is it only that pumping lemmas are for DFAs? Can we not have regular languages in NFAs, which pumping lemma is used to prove that a language is regular?",4nwupk,t1_d47pm98,lacunahead,,Reply,1,0,1
d4lw2zv,2016-06-24 01:35:46-04:00,_--__,,"> Furthermore why is it only that pumping lemmas are for DFAs? Can we not have regular languages in NFAs, which pumping lemma is used to prove that a language is regular?

Thanks for the shoutout /u/lacunahead (I've been on reddit hiatus for a while). 

The Pumping Lemma applies to *regular languages* (if L is regular then there exists p such that ...). Its *proof* uses DFAs precisely because things behave very nicely with DFAs (e.g. no epsilon loops complicating things).

And to answer your original question - note that the condition in the PL that |y|>0 is stronger than the condition |y|≥0.  In other words, by not letting y be ε you learn a lot more from the consequences of the theorem.",4nwupk,t1_d47t1t1,pumpinglemma16,,Reply,2,0,2
d47tzap,2016-06-13 16:59:18-04:00,lacunahead,,">Can you explain why epsilon is a non-deterministic choice even if it's the only choice from q1 to q2 ie there would be some manner of determinism?

Even if epsilon is the only transition available at a state, we still have the choice of following it or not. Whereas with any other valid input to a DFA, we have no choice as to whether we follow it or not. If Q1 -> Q2 on input ""a,"" we can't stay at Q1 on input ""a."" But if Q1 -> Q2 has an epsilon transition, we can, upon reaching Q1, either stay at Q1 or optionally transition to Q2.

>For instance if in w = xyz, if x can be epsilon and z can be epsilon and episilon is the only choice from q1 to q2 as in my above explanation, then wouldn't it be an NFA?

It's important to clarify that epsilon is not an input for a DFA or an NFA. So neither x, nor y, nor z can be epsilon, and they cannot contain epsilon, because x, y, and z consist of input strings to the automaton.

>Furthermore why is it only that pumping lemmas are for DFAs?

That's a good question which I'm not equipped to answer at any level of theoretical sophistication. I know /u/_--__ has a better understanding of this than me, so maybe they can help if they have the time.

>Can we not have regular languages in NFAs, which pumping lemma is used to prove that a language is regular?

The pumping lemma can never prove that a language is regular, only that a language is not regular.",4nwupk,t1_d47t1t1,pumpinglemma16,,Reply,1,0,1
d47v8nc,2016-06-13 17:27:38-04:00,zardeh,,">Even if epsilon is the only transition available at a state, we still have the choice of following it or not. Whereas with any other valid input to a DFA, we have no choice as to whether we follow it or not. If Q1 -> Q2 on input ""a,"" we can't stay at Q1 on input ""a."" But if Q1 -> Q2 has an epsilon transition, we can, upon reaching Q1, either stay at Q1 or optionally transition to Q2.

They seem to be asking a slightly different question, which is that what if the DFA is

    Q1 -> e -> Q2

with no other paths out of Q1. The answer to this is twofold:

1. that is exactly equivalent to a DFA with no Q1. 
2. In a DFA, every state must have an exit transition for all posisble values, so this would be invalid.",4nwupk,t1_d47tzap,lacunahead,,Reply,2,0,2
4nwioh,2016-06-13 12:47:52-04:00,Mixedload,Where can I write code and have it show up for JavaScript?,"I learned a little JavaScript and I want to use it, but I can't find anywhere to make programs and see the results immediately. Is there any where I can do this? ",,,,,Submission,4,0,4
d47ivvi,2016-06-13 13:09:20-04:00,NiceOneAsshole,,"Also, [jsfiddle](https://jsfiddle.net/)",4nwioh,t3_4nwioh,Mixedload,,Comment,6,0,6
d47i2dl,2016-06-13 12:51:52-04:00,sullage,,Try your browser ,4nwioh,t3_4nwioh,Mixedload,,Comment,5,0,5
d47ivio,2016-06-13 13:09:08-04:00,wafflestealer654,,"This, and also try the console provided with many modern browsers. If you're using Firefox or Chrome, then press F12 to find the JavaScript console.",4nwioh,t1_d47i2dl,sullage,,Reply,3,0,3
4nu8s2,2016-06-13 01:31:41-04:00,loba333,Considering a degree in computer science.,"Firstly sorry if this is the wrong sub to ask this.

I want a degree in CS. I've always loved programming and can code in python. But i want to learn industry practice standards and be able to construct far more complex programs than i'm currently capable of. 

I have no idea how to approach this though. I already have a degree in physics from Sheffield, England. I'm currently living in Japan, it would be cool taking a degree here but I would be willing to move. 

Can any one recommend a service to review courses ? or recommend how to approach this?",,,,,Submission,5,0,5
d478s1k,2016-06-13 09:08:27-04:00,None,,"You probably won't learn ...industry.."" from a CS degree.",4nu8s2,t3_4nu8s2,loba333,,Comment,2,0,2
d47hfmy,2016-06-13 12:38:07-04:00,faxekondikiller,,"If you ""just"" want to learn industry standards and be able to work on big and complex software projects, it sounds like software engineering is more suited for you. At least in Denmark, computer science is much more theoretical oriented. Computer science still teaches you programming, but it is used more as a tool to explore the rest of the computer science field, where (as far as I know) software engineering have programming as the main focus, and teaches things such as how you organize big software projects and how you code well.",4nu8s2,t3_4nu8s2,loba333,,Comment,1,0,1
d47l5za,2016-06-13 13:57:49-04:00,self_raising,,"This. 

I did computer science in the UK and I went into software engineering after graduating but I would do a software degree if you know that that's what you want to do.",4nu8s2,t1_d47hfmy,faxekondikiller,,Reply,1,0,1
d47mfab,2016-06-13 14:23:11-04:00,dorkus,,"There is nothing like doing. If you're a good enough programmer, get a job. That's where I think you'll learn what you're looking for. 

You could also do side projects, and learn from others (answer questions on StackOverflow, nothing like teaching!) 

Also, subreddits like /r/dailyprogrammer  /r/learnpython and /r/learnprogramming could we quite useful as sources of inspiration for stuff to work on and learn.

As others have said, CS degrees are lots of theory. You'll learn to code certainly, but don't expect to be an expert in any one thing depending on the program. I think I probably coded in 50+ languages by the time I was done with my degree. Solid in theory, but don't ask me to code a Parallaxis program :D",4nu8s2,t3_4nu8s2,loba333,,Comment,1,0,1
4ntxqh,2016-06-12 23:58:22-04:00,ReallyAttract,"If end-to-end encryption is possible, why aren't we widely using it?","Not necessarily in favor of encryption, just an interesting topic I would like to know more about. Is it speed concerns? Usability? Realizing it is even available to begin with?",,,,,Submission,15,0,15
d46wp0w,2016-06-13 00:08:00-04:00,Steve132,,Usability and technical literacy,4ntxqh,t3_4ntxqh,ReallyAttract,,Comment,19,0,19
d46zkby,2016-06-13 01:46:57-04:00,brtt3000,,Money. Not encrypting doesn't cost developer/ops time to setup and maintain.,4ntxqh,t3_4ntxqh,ReallyAttract,,Comment,7,0,7
d4755h2,2016-06-13 06:34:23-04:00,5225225,,"Actually, on the other hand, end to end encryption means as long as the clients are secure, you don't need to maintain strong security on your server. The most an attacker could do is drop messages, they can't actually read them.

The security of a PGP message doesn't depend on the email servers they're held on.",4ntxqh,t1_d46zkby,brtt3000,,Reply,3,0,3
d4763qe,2016-06-13 07:24:03-04:00,roomzinchina,,It's incredibly dangerous to advocate for end-to-end encryption on the premise that infrastructure security can be ignored.,4ntxqh,t1_d4755h2,5225225,,Reply,10,0,10
d47699h,2016-06-13 07:31:21-04:00,5225225,,"*if the end to end works flawlessly* (probably won't), there shouldn't be any security issues with having a compromised server, other than denial of service attacks.

Yes, you still should take security seriously. But the people who use your apps aren't completely fucked (At least not easily) if you get cracked. They *are* if you're just storing messages in plaintext.",4ntxqh,t1_d4763qe,roomzinchina,,Reply,2,0,2
d47gw92,2016-06-13 12:26:24-04:00,xiongchiamiov,,"It's not the default, and people haven't been asking for it.

It also restricts what you can do server-side, which limits features people want. For instance, Hangouts chats get integrated into gmail's search results, and you can't do that if Google never knows what you said.",4ntxqh,t3_4ntxqh,ReallyAttract,,Comment,6,0,6
d47kguu,2016-06-13 13:42:53-04:00,Rangsk,,"I was basically going to say the same thing. Google hangouts is actually a really good example, because they allow you to read and search your entire message history from any device. This wouldn't really be possible with end-to-end encryption.

Beyond that, they also use your message contents to target ads, which is pretty much their major source of revenue.

Remember: when an online service is free, you aren't the customer. You're the product.",4ntxqh,t1_d47gw92,xiongchiamiov,,Reply,2,0,2
d47outk,2016-06-13 15:12:29-04:00,OHotDawnThisIsMyJawn,,This is why WhatsApp Desktop and Web still need your phone.  Otherwise there would be no way to sync the messages between your devices.,4ntxqh,t1_d47gw92,xiongchiamiov,,Reply,1,0,1
d4702b8,2016-06-13 02:06:45-04:00,visvis,,The technology is certainly easily available and the performance really is not an issue anymore. It may sound crazy but I think one of the major reasons for large companies is that governments don't want this because they can no longer intercept internet traffic this way. The fact that these companies cooperate with PRISM makes this seem a very likely reason.,4ntxqh,t3_4ntxqh,ReallyAttract,,Comment,2,0,2
d490s1l,2016-06-14 13:42:50-04:00,dxk3355,,"Even when enabling encryption today there's also the issue of the different cipher suites.  Suppose your 'box' (router, phone, widget) only supports TLS 1.2 with TLS_ECDHE_RSA_WITH_AES_256_CBC_SHA but the program or server you're talking to only supports TLS_RSA_WITH_AES_256_CBC_SHA.  Well now someone needs to support the other's standard.  Add to this the constant changes that have been happening in the last few years and the whole thing is a mess.  If you get a cheap home router web page from like 5 years ago I bet Chrome will throw a fit because of the certificate length if you enabled HTTPS.  Nobody has standardized the suites (TLS doesn't) and the new issues are being discovered too fast lately.",4ntxqh,t3_4ntxqh,ReallyAttract,,Comment,2,0,2
d4702n2,2016-06-13 02:07:08-04:00,maq0r,,"Is not as simple as it sounds really. Although it has gotten easier. 

Encryption was a messy subject a few years ago and for the most part it still is it just has been hidden from you: Your phone for example may have generated a private key and the password is you ""unlocking"" it. So, years ago you'd have to know how encryption worked to be able to use it, you'd generate a pair of keys using openssl, upload them to a keyserver and go to signing parties and exchange keys. It was a very technical process that didn't have yet a level of user friendliness, and it didn't need to in a pre-Snowden area. There was no real demand (for the general public that is) to use encryption.

Now, users have demanded that products and services guarantee privacy from intrusion, so a demand for total encryption has been met with a rollout of tools and services that are now boasting encryption by default. ",4ntxqh,t3_4ntxqh,ReallyAttract,,Comment,1,0,1
d47c26o,2016-06-13 10:38:13-04:00,TlalocII,,Everyone using iOS iCloud Keychain is using end to end encryption.,4ntxqh,t3_4ntxqh,ReallyAttract,,Comment,1,0,1
d47hxyt,2016-06-13 12:49:14-04:00,yet-another-user,,"There was little demand from mainstream public before Snowden, and it complicates things.",4ntxqh,t3_4ntxqh,ReallyAttract,,Comment,1,0,1
4ntr74,2016-06-12 23:08:13-04:00,hexualhealing,Help with an interview assignment?,"Hey! I have a class project on career exploration and I need to interview a professional in the field. My intended major is computer science but I don't know anyone in a related profession. If there's anyone here who wouldn't mind filling out a few interview questions, I'll really appreciate it. Thank you!

These are the questions:

- Title
- Job responsibilities

1. How did you choose your field?
 
2. What were the steps in your career development?  (How did you end up doing what you are doing?)

3. What advice would you give to a person just starting out?  

4. What is the favorite part of your job?

5. What parts of the job do you like the least?

6. Would you pursue the same career if you had a chance to do things over (why or why not)?",,,,,Submission,6,0,6
d46vyae,2016-06-12 23:46:03-04:00,elykl33t,,"1. For some reason when I was younger I wanted to make my own flash game and started to Google stuff. I didn't learn much, but I decided ""I wanna be a programmer!"" since I liked computers. It worked out.

2. I'm just out of school, so for me it's mostly just getting my bachelors in CS. I also did a LOT of extracurriculars, since I enjoyed them, but it really built leadership and interpersonal skills which I feel are underrated by most CS people.

3. Again, for me, it's develop your interpersonal and communication skills. If you're in the field, you know your stuff well enough, clearly. But you won't just be coding. You'll be doing meetings, navigating bureaucracy, working with teams of people including non-programmers and non-technical members. Programming is only part of the job.

4. Right now, for me, it's just the feeling of contributing. Making a change to a massive code base, submitting it, and seeing it merged into the next build is just a good feeling. ""I did this."" Previously, as the head of my one programmer team (me being the one programmer) it was the autonomy.

5. The stress that sometimes hit around deployment time. Fortunately currently I don't have to deal with this, but on my older projects there was a lot of ""Sweet works perfectly for me and the testers. Oh shit wait it doesn't work on prod?""

6. Yes. I don't love coding like some people do, I don't have my own personal projects or anything. But I enjoy it enough, and I believe programming is the thing my brain was wired for. Plus it allows me to indulge myself in hobbies and such in my non-working hours because of the pay.",4ntr74,t3_4ntr74,hexualhealing,,Comment,2,0,2
d4704ta,2016-06-13 02:09:33-04:00,hexualhealing,,Thank you for helping!,4ntr74,t1_d46vyae,elykl33t,,Reply,1,0,1
d473dsx,2016-06-13 04:50:26-04:00,artillery129,,"Title: Graduate Student


Job responsibilities: School


How did you choose your field?


Field was chosen from a young age. I always liked programming. It started with geocities when I was around 8 years old.


What were the steps in your career development? (How did you end up doing what you are doing?)


The steps in my career development went from geocities to html, to basic, to visual basic, to java, to php.. etc etc, jumping from language to language as the project necessitated.


What advice would you give to a person just starting out?


If you don't like programming now, you won't like it later. This is not the kind of job one does 'for the money'.


What is the favorite part of your job?


Finally seeing the result of months or possibly years of programming.


What parts of the job do you like the least?


Cryptic bugs that have absolutely no mention anywhere on the internet. As well as poor logging/ bad error messages.


Would you pursue the same career if you had a chance to do things over (why or why not)?


Yes I would. I enjoy my job and it has given me many possibilities for future jobs/fields. It's like learning a tool that can be used almost anywhere. Then again, that could be the proverbial 'if you have a hammer, everything starts to look like a nail'.
",4ntr74,t3_4ntr74,hexualhealing,,Comment,2,0,2
d485kjy,2016-06-13 21:38:03-04:00,sullage,,"Senior Java Engineer (I am not an Engineer as I did not get that degree (much respect), that is just my title)

Features and bugs, Mentoring (aka babysitting)

1. Was failing the physics program

2. pitfall for the ti81 -> botting on aol -> slacking in college -> telesales -> project center contractor -> engineer

3. Get an internship. Every summer if possible.

4. Fixing bugs other people fail to solve. Green field features/projects.

5. Legacy maintenance.

6. I would be here in a heartbeat. The pay is good and I'd be doing a lot of this in my spare time if I was an accountant.

*edit: formatting*",4ntr74,t3_4ntr74,hexualhealing,,Comment,1,0,1
d46xfi0,2016-06-13 00:31:00-04:00,x_Zoyle_Love_Life_x,,No excuse. Go out and meet someone in your field using LinkedIn. That's what this assignment is meant for. ,4ntr74,t3_4ntr74,hexualhealing,,Comment,0,0,0
d473brr,2016-06-13 04:47:02-04:00,artillery129,,"I think he's being the classical programmer e.g. lazy :D 


already fits right in!",4ntr74,t1_d46xfi0,x_Zoyle_Love_Life_x,,Reply,3,0,3
4npj9v,2016-06-12 04:58:10-04:00,iNeverHaveNames,Algorithm for determining sequence of matrix in iteration,"suppose you have a 2x2 matrix, [ [0,0] , [1,0] ] .
Each value in the inner arrays can be from 0 to 2.

Assuming we've got an algorithm to generate all possible matrices within those constraints in some sort of logical, sequential fashion.
e.g.  
0: [ [0,0] , [0,0] ] 
1: [ [1,0] , [0,0] ] 
2: [ [2,0] , [0,0] ] 
3: [ [0,1] , [0,0] ]
4: [ [0,2] , [0,0] ]
5: [ [0,0] , [1,0] ]
...

I want to be able to input the matrix, and output it's position in the sequence.

Any advice is helpful. I think machine learning can help, but I figured there's probably a better approach that I'm not aware of.

Thankyou!",,,,,Submission,8,0,8
d45txch,2016-06-12 05:32:54-04:00,thewataru,,"I assume that allowed values are integers. Some meaningful order is lexicographic order. And you can forget about matrix structure and tread it as a vector of 4 elements, or 1x4 matrix.

Now, you have to simply treat that vector as a base 3 number.

So [[0,0], [0,0]] or [0,0,0,0] is simply 0. [0,0,0,2] is 2 and [1,0,0,0] is 27. Note that these positions are counted from zero. 

Formally, if matrix is [[a,b],[c,d]] then your answer is 1+d+3c+9b+27a",4npj9v,t3_4npj9v,iNeverHaveNames,,Comment,8,0,8
4np7bk,2016-06-12 02:41:03-04:00,AaronTooNerdy,"Is using the ""proper"" typing method optimal?","Been meaning to ask about this for a while, but is the ""proper""/""traditional"" method of typing the most optimal for programming? Currently I just type and program without really paying attention to what fingers I use for each key.",,,,,Submission,0,0,0
d45rjur,2016-06-12 03:07:21-04:00,tyggerjai,,"A) Programming and Computer science - not as closely related as you might think. 

B) typing - probably the least important aspect of programming. 

Beyond ergonomic considerations, type however you like. It will make very little difference. ",4np7bk,t3_4np7bk,AaronTooNerdy,,Comment,3,0,3
d466ex2,2016-06-12 13:16:40-04:00,dmazzoni,,"Here's an interesting essay that argues typing fast is definitely important:

http://steve-yegge.blogspot.com/2008/09/programmings-dirtiest-little-secret.html?m=1

I'm not sure how much it matters if it's proper or not, but efficient is important.",4np7bk,t3_4np7bk,AaronTooNerdy,,Comment,1,0,1
d46oc8u,2016-06-12 20:28:22-04:00,lgastako,,"There are different styles of programming and different styles of programmers, but I find that typing fast is an asset because I often do exploratory programming in a REPL and if I am typing too slow (eg with one hand because I'm manipulating something with the mouse) then it often interferes with ""flow"" and being able to remember everything I was thinking while typing.",4np7bk,t3_4np7bk,AaronTooNerdy,,Comment,1,0,1
4nkthc,2016-06-11 05:41:39-04:00,moeseth,What's the most optimal way to segment sentence using a dictionary?,"Hi, 

Let's say I have a dictionary like 

dictionary = [""London Mayer"", ""visited"", ""Yangon"", ""home"", ""my""]

And a sentence like ""London Mayer visited my home.""

I would like the result to be 

London Mayer|visited|my|home|

How can I write optimize code for that in term of space and complexity?

Thanks",,,,,Submission,6,0,6
d45n8bb,2016-06-12 00:02:17-04:00,chromaticgliss,,"You need a lot more details about what you're trying to accomplish to get a reasonable answer.

What do you mean by ""segment""? 
Is the sentence fed into the program or just the dictionary? 

Are you just trying to convert spaces to pipes based on words in the dictionary being ""separators"" so to speak?

Or are you trying to generate a sentence based on a dictionary of words?",4nkthc,t3_4nkthc,moeseth,,Comment,1,0,1
d45vgi9,2016-06-12 07:11:55-04:00,moeseth,,"the former: ""convert spaces to pipes based on words in the dictionary being ""separators""""

",4nkthc,t1_d45n8bb,chromaticgliss,,Reply,1,0,1
d47ma7e,2016-06-13 14:20:20-04:00,kobortor,,"You can use KMP or a similar algorithm to find where the words match. For each match at i with a string of length k, draw a directed edge (on a linear graph) from i to i + k. After you're done, simply use depth first search or something similar to see if there is a path to the end of the sentence.",4nkthc,t3_4nkthc,moeseth,,Comment,1,0,1
d4564hl,2016-06-11 15:15:22-04:00,veeberz,,"What are you trying to accomplish? I need more details, like where the list of words come from. Do you plan to have a huge list of words to split an arbitrary sentence like that? ",4nkthc,t3_4nkthc,moeseth,,Comment,1,0,1
d45vgv5,2016-06-12 07:12:32-04:00,moeseth,,"I have a list of words : 20,000.

I need to segment sentence based on the dictionary. Longest matching method.",4nkthc,t1_d4564hl,veeberz,,Reply,1,0,1
4nkhe6,2016-06-11 02:57:47-04:00,pas43,What the top 3 CS technologies you want to learn?,"For me it is to learn how to use Weka also learn about CNN neural networks and the ELK stack. What about you? Any software, data structures, databases or ai?",,,,,Submission,21,0,21
d44whjz,2016-06-11 10:46:47-04:00,trenchgun,,"QA-automation. So that I can be like my idol, sixyear-nowork guy.",4nkhe6,t3_4nkhe6,pas43,,Comment,7,0,7
d45qg65,2016-06-12 02:10:53-04:00,znick5,,"Not really a technology per say, but it has been on my mind lately: I want to learn more testing standards and practices. My current job doesn't do shit to test our patches. After each iteration we do ""testing"" which is just where the business analyst spends a day clicking stuff and saying ""yup it works""... I try to write some unit tests and my manager asks me what I am wasting my time on... Can't wait to finish my degree and get a better job.",4nkhe6,t3_4nkhe6,pas43,,Comment,1,0,1
d45vdwr,2016-06-12 07:07:16-04:00,Toxicable,,"It's the same at my work, twice now my manager has brung up that our testing methods aren't working and that we need something better so both times I say that we use unit testing but she thinks that we'd spend more time testing and rewriting tests than writing code in the first place and no one else will back me up, as a junior dev on a web based service still in uni
",4nkhe6,t1_d45qg65,znick5,,Reply,1,0,1
d4hugs9,2016-06-21 07:31:32-04:00,bartturner,,"Been there and done that.    It is really, really hard to get people to change.",4nkhe6,t1_d45vdwr,Toxicable,,Reply,1,0,1
d44tq4n,2016-06-11 09:01:56-04:00,IAmNotMyName,,"At the moment it's Scala, Docker and Ansible.",4nkhe6,t3_4nkhe6,pas43,,Comment,1,0,1
4nj3nl,2016-06-10 19:41:26-04:00,bacondev,Are quantum computers more susceptible to soft errors (e.g. background radiation) than their classical counterparts? Why or why not?,"By ""soft errors"", I mean [this](https://en.wikipedia.org/wiki/Soft_error).",,,,,Submission,9,0,9
d44qnec,2016-06-11 06:02:17-04:00,The_Serious_Account,,"On one hand quantum information is certainly more sensitive to outside influence, but in some way I'd say less. A fully functional quantum computer would expect there to be errors all the time and would hence implement some form of constant error correction on its qubits. It's simply expected that qubits randomly shift states and it will be implemented in a way that handles that. The source of these errors is not particularly relevant. 

",4nj3nl,t3_4nj3nl,bacondev,,Comment,2,0,2
d4c3qv8,2016-06-16 16:33:29-04:00,bacondev,,"Sorry for the late reply. So if I understand you correctly, the probabilistic nature of quantum algorithms would mean that most (if not all) fluctuations of quantum information would be negligible?",4nj3nl,t1_d44qnec,The_Serious_Account,,Reply,1,0,1
d4c6sg0,2016-06-16 17:37:41-04:00,The_Serious_Account,,"No, it has nothing to do with the probabilistic nature of the algorithms. My point was fairly simple and maybe over-explained. Any implantation of a quantum computer expects a lot of errors, because it's extremely sensitive. ""Unexpected"" sources of errors would simply drown in the expected ones. ",4nj3nl,t1_d4c3qv8,bacondev,,Reply,2,0,2
4ni8l5,2016-06-10 16:18:24-04:00,Zackeezy116,"How should I make a resume to apply for a CS internship if I have little to no job experience, only University?","I've had one job and I've finished two years of college in computer science and someone told me to send them my resume so they could forward it to HR, but I don't know what to put on my resume.  I know I should list the computer science classes I've taken and the job I had last summer, but what else would be good to include?  I don't really have much else experience.  I have high school achievements and some community service stuff, but not much else.  Any advice would be appreciated. ",,,,,Submission,12,0,12
d445xr1,2016-06-10 17:22:54-04:00,mastermindxs,,"Include in your intro that you are looking for an internship, with room to grow into the position you want. Leave experience blank. And then list your education. Add in any extracurricular work you've done. Side projects for example (you do have at least one, right?) and that should be it. 

When I look at an intern's or junior's application and resumé, I'm mainly looking for side projects they work on and how enthusiastic they are during the interview. For an intern position we don't care how much experience you have, but how much you love doing what you do and will be diligent in learning more. 

Source: VPE that does technical interviews.",4ni8l5,t3_4ni8l5,Zackeezy116,,Comment,5,0,5
d446w6s,2016-06-10 17:47:28-04:00,Zackeezy116,,"I have several side projects going but none of them have been finished. One in particular might be a joint effort.  I work on the technical side while my friend works on the aesthetics.  Most of the coding I've done has been for class.  I did love doing it, though. ",4ni8l5,t1_d445xr1,mastermindxs,,Reply,1,0,1
d44j0dm,2016-06-10 23:41:36-04:00,Afro_Samurai,,"If your school has a career center, they can offer resume samples, critiques, and interview help. ",4ni8l5,t3_4ni8l5,Zackeezy116,,Comment,2,0,2
d44j59j,2016-06-10 23:45:38-04:00,Zackeezy116,,"I'm out of state from my University, and idk if I can get that stuff online.",4ni8l5,t1_d44j0dm,Afro_Samurai,,Reply,1,0,1
d44j79s,2016-06-10 23:47:20-04:00,Afro_Samurai,,"Check their website, that's were I would expect to find samples and general/somewhat specific advice. ",4ni8l5,t1_d44j59j,Zackeezy116,,Reply,2,0,2
d44j85p,2016-06-10 23:48:04-04:00,Zackeezy116,,I'll look. Thanks for the idea!,4ni8l5,t1_d44j79s,Afro_Samurai,,Reply,1,0,1
4nhdri,2016-06-10 13:22:11-04:00,rakup_master,MS Abroad,"I am kinda unaware of these stuffs. I am an Information Science student from a very reputed college in Chennai, and am planning for my MS abroad. Being an IS student, can I enroll myself for ""MS in Computer Science"" or is it just plain ""Master of Science in Information Technology"" or can I choose any specialization relating CS/IS - maybe Data Analytics, Software Engineering??

Edit : Grammar lol",,,,,Submission,2,0,2
d43w0tp,2016-06-10 13:38:25-04:00,visvis,,"This is something you should ask at the international office of whatever institution you intend to study. Requirements for enrollment and acceptance of foreign degrees is different everywhere and often judged on an individual basis. It will also depend on the prerequisites for the courses they offer themselves, which again heavily depends on the university. When you ask them, it's best to include a list of the courses you took including topics and study load.",4nhdri,t3_4nhdri,rakup_master,,Comment,1,0,1
4neuxk,2016-06-10 01:16:11-04:00,yendrdd,Applications of DE in CS,"I finished my first year of college and asked /r/cscareerquestions a while ago about ""[Advanced Math in CS.](https://www.reddit.com/r/cscareerquestions/comments/3p0v9f/advanced_math_in_cs/)"" I took Differential Equations this Spring because Calc 3 and Linear Algebra didn't fit in my schedule, but I really enjoyed it. I can see applications of ODE to model real world stuff, and Euler's Method being used in numerical analysis.

The teacher I took this Spring hasn't taught DE in like 30 years, so he was very unprepared. I was able to learn everything in the curriculum, but I haven't been able to apply what I've learned outside of math. Kind of like I've learned what the eight derivative is but I don't know how to apply it. I just look at it and ask, ""Why did we do this?""

So, how do the concepts in Differential Equations fit into the field of Computer Science?

Thanks in advanced!",,,,,Submission,2,0,2
d445qxe,2016-06-10 17:18:09-04:00,None,,I would imagine it's useful in graphics and physic engines. Other than that it's not a required skill but logic reasoning and problem solving is. So think of it as practice. ,4neuxk,t3_4neuxk,yendrdd,,Comment,3,0,3
4ne1wj,2016-06-09 21:30:24-04:00,KasottyBlogCom,[At the END of the post!] FREE ticket to world’s biggest online event for authors and entrepreneurs! June 12-22. Reserve your spot before it's too late!,,,,,,Submission,0,0,0
4ndjyp,2016-06-09 19:26:17-04:00,macticzul,File Sharing Web App,"Hey! 
I want to build a website, where users can share PDF files. In this website I would like to have: a login system, file storage (for PDFs), a reward system (user get virtual coins for uploading now files), ability to search, categorize and tag files. 

Should I use PHP for the login system?
Also I have no idea what to use for the other parts. 
Could anybody help me with information, tutorials anything? 

Thanks a lot! :)",,,,,Submission,2,0,2
d43qmfq,2016-06-10 11:38:12-04:00,visvis,,"PHP with some database back-end (usually mysql) should work fine for all of this. The only thing you need to do that is not straightforward is to extract text from the PDFs and there should be libraries available to do that.

PHP tutorials are [not hard to find](https://www.google.nl/search?q=php+tutorial).",4ndjyp,t3_4ndjyp,macticzul,,Comment,1,0,1
d448n64,2016-06-10 18:35:03-04:00,macticzul,,"Thanks so much. For the categories I'm thinking of rather than extracting text, I could instruct the user to select one of a predefined category. That should be easier to create and control, what do you think?",4ndjyp,t1_d43qmfq,visvis,,Reply,1,0,1
d44nlbm,2016-06-11 02:37:27-04:00,visvis,,"Sure that would be easier, but the search would be much less useful that way.",4ndjyp,t1_d448n64,macticzul,,Reply,1,0,1
4ndfij,2016-06-09 18:57:34-04:00,zefcfd,"""Synchronous programming models require lightweight threads"" - Hillary Clinton. Did she get it mixed up? If she's right, why?",,,,,,Submission,20,0,20
d434wjn,2016-06-09 22:03:49-04:00,roxven,,I watched this happen live. People suspected she had someone in her ear piece and mostly enjoyed it. It doesn't answer the question but it is correct.,4ndfij,t3_4ndfij,zefcfd,,Comment,7,0,7
d43fa8x,2016-06-10 04:50:06-04:00,zombiezs,,"100% had someone in her ear. They asked Obama about sorting algorithms and he had a somewhat articulate answer as well.

Someone is feeding them lines.",4ndfij,t1_d434wjn,roxven,,Reply,8,0,8
d43sfgl,2016-06-10 12:19:25-04:00,argh523,,"> somewhat articulate answer

I think he made a joke about Bubbleshort beeing his favourit or something. Not sure if that qualifies :)",4ndfij,t1_d43fa8x,zombiezs,,Reply,3,0,3
d44kgeq,2016-06-11 00:27:23-04:00,zombiezs,,"""What is the most efficient way to sort a million 32 bit integers?""

[""I think bubble sort is the wrong way to go.""](https://www.youtube.com/watch?v=k4RRi_ntQc8)

Guess he was being cheeky.",4ndfij,t1_d43sfgl,argh523,,Reply,2,0,2
d43srb7,2016-06-10 12:26:39-04:00,zefcfd,,"yeah that part is pretty obvious to me, i just thought it'd be funny if she got it wrong. i cant figure out why synchronous programming models require light weight threads. ",4ndfij,t1_d43fa8x,zombiezs,,Reply,1,0,1
d433pes,2016-06-09 21:31:16-04:00,piguy123,,I didn't watch the video but just the statement you posted is correct. In a synchronous environment native threads aren't supported so in order to simulate threading and to avoid waiting on things you can use light weight or green threads. This is how things like node and JavaScript work. ,4ndfij,t3_4ndfij,zefcfd,,Comment,9,0,9
d43lz61,2016-06-10 09:45:45-04:00,cogman10,,"A synchronous programming model does not require lightweight threads.  Any threading implies an asynchronous programming model, green or otherwise.",4ndfij,t1_d433pes,piguy123,,Reply,3,0,3
d43skjq,2016-06-10 12:22:31-04:00,zefcfd,,i'm feeling pretty strongly about this as well.  hopefully more people will chime in on this one.,4ndfij,t1_d43lz61,cogman10,,Reply,1,0,1
d43bvic,2016-06-10 01:50:51-04:00,zefcfd,,Node is an asynchronous programming model though?,4ndfij,t1_d433pes,piguy123,,Reply,3,0,3
d43ec2q,2016-06-10 03:55:05-04:00,piguy123,,"It's asynchronous but single threaded I believe. In a single threaded environment nothing is ever truly asynchronous (that is, running at the same time), it's just that you're able to simulate it by not waiting on most operations. The term synchronous used in the statement you posted is not totally accurate which is probably the source of all the confusion. ",4ndfij,t1_d43bvic,zefcfd,,Reply,0,0,0
d43e7h5,2016-06-10 03:47:38-04:00,devDoron,,Can you share the timestamp for when she says this? Video is over an hour long.,4ndfij,t3_4ndfij,zefcfd,,Comment,2,0,2
d43sd81,2016-06-10 12:18:02-04:00,zefcfd,,"> that is, running at the same time

shoulda been timestamped already but it's at 55:20",4ndfij,t1_d43e7h5,devDoron,,Reply,3,0,3
d43un2c,2016-06-10 13:08:06-04:00,Zackeezy116,,RES doesn't skip to the time stamp.,4ndfij,t1_d43sd81,zefcfd,,Reply,2,0,2
d57jbjt,2016-07-11 01:16:26-04:00,jakob_roman,,"She's one of the most pandering people I've ever seen. That question was a definite plant. 

Here's a real cringe-worthy one.

https://youtu.be/8q29F1jPvNk",4ndfij,t3_4ndfij,zefcfd,,Comment,1,0,1
d42y2bm,2016-06-09 18:58:28-04:00,zefcfd,,"I was just listening to this, and can't help but think she got things backwards? If so, it's hilarious that an entire audience at google humoured her / didn't know.",4ndfij,t3_4ndfij,zefcfd,,Comment,1,0,1
4nb50n,2016-06-09 10:52:22-04:00,alazi,Is there a critical mass for PIs where they outperform enterprise systems?,"At which point would an array of smaller computers begin to outperform enterprise computing hardware, e.g. Dell Poweredge?

Assumptions:

1. each small computer would be operating in parallel
2. the computation is small, akin to lookup and routing
3. the bus of each small computer is 64bit
4. the enterprise hardware would have 6 cores. 

Please help me improve this question if I've left something out that you think is relevant!

edit: the enterprise hardware have 16 cores.. thanks /u/gstuartj

",,,,,Submission,3,0,3
d42t89e,2016-06-09 17:00:19-04:00,splenetic,,"It depends entirely on the task. Some tasks can easily be split into parallel threads of execution. Many can't. Among those that can there can be a requirement for communication between nodes that then leads to a dependency on interconnects. This is also true for access to storage for local memory, shared memory (if used) and for bulk storage such as disks.

For what it's worth there have been attempts at building supercomputers out of huge numbers of small processors. The [Connection Machine series](https://en.wikipedia.org/wiki/Connection_Machine) were a number of systems that had tens of thousands of low-power processing nodes optimised for parallel processing. ",4nb50n,t3_4nb50n,alazi,,Comment,4,0,4
d43l4y6,2016-06-10 09:22:41-04:00,alazi,,"Interesting and thank you for the reference. 

The task here would mostly be simple lookup and routing where the routing tables (as well as the instruction itself) would be distributed only periodically so there is no need for real-time inter-communication between the PIs

I know of a [Beowulf cluster](http://www.zdnet.com/article/build-your-own-supercomputer-out-of-raspberry-pi-boards/) (bramble evidently) that was built to allow someone to perform larger calculations that someone was impatient to wait for university computer time.. ",4nb50n,t1_d42t89e,splenetic,,Reply,1,0,1
d42cgdw,2016-06-09 11:01:04-04:00,gstuartj,,"Limiting the enterprise hardware to only 6 cores biases the question. A minimum of 12-16 cores is not unusual for servers. More, for the beefy tasks.",4nb50n,t3_4nb50n,alazi,,Comment,2,0,2
d42cm0p,2016-06-09 11:04:32-04:00,alazi,,edited.. Thank you!,4nb50n,t1_d42cgdw,gstuartj,,Reply,2,0,2
4n94cz,2016-06-09 00:01:26-04:00,windysands,Why doesn't data bending a video affect the audio?,"I 'databent' an mp4 file in audacity by applying the echo effect to the file's data. The result was expectedly very messed up imagery. However, besides changing pitch in a few places, the audio was about the same.",,,,,Submission,4,0,4
4n7crt,2016-06-08 17:04:53-04:00,unmeilleurmoi,"Help: thousands of x,y points defining objects, how do I find/define space","If you'd imagine an XY scatter plot of three rooms, where the points are in a room and there are areas without points identifying walls or doors. Is there really a good way to algorithmically divide the space into these three areas and properly categorize which space each point exists in?",,,,,Submission,3,0,3
d422u0h,2016-06-09 05:08:44-04:00,visvis,,This is called [cluster analysis](https://en.wikipedia.org/wiki/Cluster_analysis) and there are many different algorithms to do this.,4n7crt,t3_4n7crt,unmeilleurmoi,,Comment,2,0,2
d426j16,2016-06-09 08:18:50-04:00,unmeilleurmoi,,Wow! Yes! This is exactly what I need! Do you have any recommendations for an algorithm or library to use? Python is easiest.,4n7crt,t1_d422u0h,visvis,,Reply,2,0,2
d427hsf,2016-06-09 08:53:11-04:00,visvis,,"Unfortunately I have no hands-on experience but ISTM there should be plenty of software out there to do this as this is widely used. Moreover, even if the formulae look scary at first, many of the algorithms should actually be pretty easy to implement yourself (for example for k-means clustering, which is probably a good fit for your use case).",4n7crt,t1_d426j16,unmeilleurmoi,,Reply,2,0,2
d42d17z,2016-06-09 11:13:47-04:00,videoj,,Try asking over at /r/machinelearning they can point you in the right direction.,4n7crt,t1_d426j16,unmeilleurmoi,,Reply,2,0,2
4n71ck,2016-06-08 16:02:48-04:00,throwmeawaaey,Broadening my horizons?,"Hi guys
I would like a few tips on broadening my horizons. I just started Python through my high school class and I would now like to step out a bit. What should I do to get as thorougly versed in the topic as possible? I'm tech savvy but I think that more can be done. Basically, just put what you think it would be beneficial in the slightest (or just enjoyable) and I'll try and do it.

Thanks!",,,,,Submission,2,0,2
d41dq4s,2016-06-08 16:24:26-04:00,visvis,,"There are many different directions, both in the sense of subfields of computer science to learn about and languages to learn and get experience in. There is no way to tell what will be fun or useful for you without more info.",4n71ck,t3_4n71ck,throwmeawaaey,,Comment,1,0,1
d42gh17,2016-06-09 12:27:59-04:00,throwmeawaaey,,"I enjoy the nitty-gritty, getting into the heart of a PC. That sort of stuff.",4n71ck,t1_d41dq4s,visvis,,Reply,1,0,1
d42gxm6,2016-06-09 12:37:54-04:00,visvis,,"Sounds like you might want to look at operating systems then. You could follow a course or explore the topic by yourself. A good start would be to learn C and maybe even some assembly and then look at a simple understandable OS, for example [MINIX3](http://www.minix3.org/), to see what happens.",4n71ck,t1_d42gh17,throwmeawaaey,,Reply,1,0,1
d42h4bw,2016-06-09 12:42:00-04:00,throwmeawaaey,,Thank you :),4n71ck,t1_d42gxm6,visvis,,Reply,1,0,1
4n6qg6,2016-06-08 15:04:53-04:00,The_most_okay,Taking CS1 and Discrete Structures 1 in fall. I'm nervous. What can I read or look at to get an idea about what I will be learning about?,"After the semester is over in fall , I have to take a computer science foundation exam.  Here is a link to past exams.  This exam kind of scares me because for the most part I'm not sure how to begin to answer the questions.  http://www.cs.ucf.edu/registration/exm/",,,,,Submission,6,0,6
d41k23n,2016-06-08 18:50:14-04:00,CptMacHammer,,"Start studying now. /r/learnprogramming for CS 1, Khan Academy and related sites for Discrete Structures. You can also try and find a practice test. Top schools upload their lectures and stuff to youtube. Maybe you can try to make something fun like a game or a small arduino project. If you like art try looking up Processing fot a good time.",4n6qg6,t3_4n6qg6,The_most_okay,,Comment,2,0,2
d41mwo0,2016-06-08 20:02:37-04:00,The_most_okay,,"Thanks for the help.  What sections of khan academy for discrete structures?  Is it the algorithms section of the computer science category?  I would have no idea on how to start an arduino project, the only programming course Ive taken is programming with C (an intro class really)",4n6qg6,t1_d41k23n,CptMacHammer,,Reply,1,0,1
d42rd9u,2016-06-09 16:19:14-04:00,CptMacHammer,,"The modular arithmetic portion and beyond of the Journey into Crypto should be useful. The Algorithms portions would be great would be great. Don't worry about not knowing how to make an Arduino project. I was just suggesting that it would be easier to learn if you working on a project that you found interesting. There are a lot of great resouces out there they just take some looking to find. 

Also I hope this might help. 
https://github.com/vhf/free-programming-books/blob/master/free-programming-books.md#graphical-user-interfaces",4n6qg6,t1_d41mwo0,The_most_okay,,Reply,1,0,1
d41p1jb,2016-06-08 20:58:13-04:00,eric101995,,Make sure to take Arup for CS1. He writes the foundation exam and his tests will probably cover what you need to know.  (had to check I wasn't on the UCF subreddit for a second lol),4n6qg6,t3_4n6qg6,The_most_okay,,Comment,1,0,1
d41ptzd,2016-06-08 21:18:11-04:00,The_most_okay,,lol I know thats funny theres another Knight amongst this subreddit.  Thanks for the info.,4n6qg6,t1_d41p1jb,eric101995,,Reply,1,0,1
4n3d2q,2016-06-08 00:48:52-04:00,youreeka,Is it possible to use CPU power remotely? E.g. a graphics card,"Could you have a setup whereby I'm on a crappy laptop with a high speed internet connection. I have a much better pc at home with more processing power and a graphics card etc.

My keystrokes are sent to the home pc, which crunches the data, and then streams back HD video of the output, which I can stream using my internet connection.

So from my point of view, I'm using the software seamlessly.",,,,,Submission,3,0,3
d40isgv,2016-06-08 01:29:23-04:00,xiongchiamiov,,"Sure. Most commonly this is done over ssh, but some companies have built sophisticated software to distribute work across servers for very particular applications.",4n3d2q,t3_4n3d2q,youreeka,,Comment,5,0,5
d40iw24,2016-06-08 01:32:52-04:00,wescotte,,[Cloud Gaming](https://en.wikipedia.org/wiki/Cloud_gaming) does exactly that.,4n3d2q,t3_4n3d2q,youreeka,,Comment,3,0,3
d40j33h,2016-06-08 01:39:56-04:00,Rangsk,,"Steam has this built in, called In-Home Streaming.

http://store.steampowered.com/streaming/",4n3d2q,t3_4n3d2q,youreeka,,Comment,3,0,3
d40lrcz,2016-06-08 03:29:02-04:00,youreeka,,"uh yeh - that's exactly what I was thinking about...... nice one!

If only I thought about it 10 years ago

are there servers or whatever that you can rent or subscribe to? It'd be awesome to be able to login anywhere you have a decent wifi connection and play high end pc games that are hosted by killer pc machine hub server things...",4n3d2q,t1_d40j33h,Rangsk,,Reply,1,0,1
d40rcoj,2016-06-08 08:11:15-04:00,TashanValiant,,"There was a service called OnLive a few years back but I think that may have gone belly up. 

There is also the Nvidia Shield. 

Also PSN Remote Play, which I think does not require being on the same network. I think you just have to log in to your PSN account and have the hosting device set to wake via network. 

And I'm pretty sure you could set up Steam In Home Streaming through the internet via some tunneling or vpn tools. It might not be reliable, but it could be possible. ",4n3d2q,t1_d40lrcz,youreeka,,Reply,2,0,2
d40lj28,2016-06-08 03:17:53-04:00,panderingPenguin,,"Yep, exactly what remote desktop software is. Also, your phone is offloading things to servers frequently without you ever noticing ",4n3d2q,t3_4n3d2q,youreeka,,Comment,2,0,2
d42b6j6,2016-06-09 10:31:51-04:00,wacki86,,"TL:DR Yes but with possible lag


Cloud/remote play is possible(same concept as many enterprise computer setups). However depending on many factors there might be a lag between your keystroke(s) and the gaming PC receiving said keystroke(s).
    
If within the same network, consider the following:

* 1) are you connected over wifi or ethernet?
* 2) is the gaming pc wifi or ethernet?
* 3) Router/Switch/Hub Config
* 4) Cables used (Cat 5 vs Cat 6)
* 5) Ethernet card max speeds
* 6) other network activities using up/down bandwidth at the time you wish to play

I have a PS4 and they came out with Remote play recently and its pretty neat except for the millisecond of a delay between pressing jump and having the character jump. This delay wont matter for most games but for some it can make a world of a difference(more or less the same reason people buy gaming mice and/or keyboard to reduce this type of delay)


EDIT:Formatting, my markdown skills SUCK",4n3d2q,t3_4n3d2q,youreeka,,Comment,2,0,2
d40vslw,2016-06-08 10:19:07-04:00,kush1234,,I would be concerned with latency obviously you are not concerned about FPS,4n3d2q,t3_4n3d2q,youreeka,,Comment,1,0,1
4n1x2s,2016-06-07 19:06:22-04:00,Teofill,Best way to transfer huge data US to EUROPE in 24 hour cycles?,"Howdy ask CS!

I am wondering what methods are best for moving a TB or more a day from an HDD/SSD in US to Europe.

I am working on a film project and I'm generating about a TB of data each day, images, gps files, imu files, etc.  and I need the data to available for my clients next day for review in Europe.  Aside from mailing the Full HDDs im fairly lost when it comes to data transfer standard practices.  

Not sure I'm explaining this very well but hopefully you guys can ask me the right questions!! :D

",,,,,Submission,1,0,1
d405j1n,2016-06-07 19:40:21-04:00,Merad,,Something like Azure cloud storage may be an option: https://azure.microsoft.com/en-us/pricing/details/storage/,4n1x2s,t3_4n1x2s,Teofill,,Comment,1,0,1
d406fu4,2016-06-07 20:03:30-04:00,anamorphism,,dropbox.,4n1x2s,t3_4n1x2s,Teofill,,Comment,1,0,1
d40ewgz,2016-06-07 23:33:18-04:00,videoj,,"[Google Drives has a number of plans](https://support.google.com/drive/answer/2375123?hl=en)

Also, check your internet plan to make sure you don't have upload limits that'll prevent you from moving that much data.

",4n1x2s,t3_4n1x2s,Teofill,,Comment,1,0,1
d416mmu,2016-06-08 14:05:52-04:00,proskillz,,"Most small and medium size companies simply don't have the bandwidth to move that level of data in a single day. How fast is your upload pipe? This will give you some idea of how long it might take.

http://www.t1shopper.com/tools/calculate/downloadcalculator.php

Many companies do resort to mailing a hard disk as it's faster than pulling it down over the wire.",4n1x2s,t3_4n1x2s,Teofill,,Comment,1,0,1
4n0168,2016-06-07 12:58:55-04:00,guifroes,"As a developer, what is the biggest challenge you’re facing now? What do you wish you could do better?",,,,,,Submission,14,0,14
d3zvgb6,2016-06-07 15:47:59-04:00,anamorphism,,"gathering requirements, as always.

there's nothing i find particularly difficult about coding or designing a system that works if i know what it's supposed to do. it may not be the best but it'll be relatively solid and function as intended.

the hard part is putting multiple people in a room with different ideas on what they want and distilling that down into something that meets all of their needs.",4n0168,t3_4n0168,guifroes,,Comment,10,0,10
d3zxe3f,2016-06-07 16:28:59-04:00,Feroc,,"Same for me. In addition I also still have troubles knowing the responsibilities in my company, even after almost two years. So many teams with so many systems, all communicating with each other somehow. 

Coding really isn't the hard part where I work (granted, our code isn't rocket sciences), it's knowing who to talk to and how to coordinate everyone. ",4n0168,t1_d3zvgb6,anamorphism,,Reply,3,0,3
d40n76r,2016-06-08 04:44:35-04:00,BlueFootedBoobyBob,,"Yes. This is the absolute biggest problem. Even if the ""customer"" is in the next room.",4n0168,t1_d3zvgb6,anamorphism,,Reply,1,0,1
d416eqz,2016-06-08 14:01:20-04:00,fragglerock,,"What strategy have you tried? 

Look at Gojko Adzic book impact mapping. It is great. Also user story mapping by Jeff Patton.",4n0168,t1_d3zvgb6,anamorphism,,Reply,1,0,1
d3ztadx,2016-06-07 15:02:45-04:00,None,,"Wondering if learning all the web frameworks and tech is more worth than learning machine learning. Both are strikingly interesting to me, but not sure where I can go with after learning either.",4n0168,t3_4n0168,guifroes,,Comment,7,0,7
d405ep4,2016-06-07 19:37:15-04:00,zefcfd,,"This is apples to oranges, of course machine learning is more important. Now if you said "" should I learn the Internet in great specificity or machine learning"" that'd be a fair comparison. ",4n0168,t1_d3ztadx,None,,Reply,2,0,2
d3zx35z,2016-06-07 16:22:35-04:00,Madsy9,,"Planning, architecture design, better deadline/sprint estimation. In that order.",4n0168,t3_4n0168,guifroes,,Comment,4,0,4
d41pwpr,2016-06-08 21:20:02-04:00,guifroes,,Do you feel that your trouble with architecture design has something to do with the difficulty in estimation? Is it the case that your code is hard to maintain/implement new features?,4n0168,t1_d3zx35z,Madsy9,,Reply,1,0,1
d3zxokc,2016-06-07 16:35:10-04:00,danltn,,SAT being NP-Complete.  Annoying.,4n0168,t3_4n0168,guifroes,,Comment,2,0,2
d40z4qm,2016-06-08 11:33:00-04:00,Teemperor,,"[deleted]  
 ^^^^^^^^^^^^^^^^0.3393 
 > [What is this?](https://pastebin.com/64GuVi2F/26959)",4n0168,t3_4n0168,guifroes,,Comment,1,0,1
d411hy4,2016-06-08 12:22:19-04:00,arsnl,,My biggest challenge is dealing with the incredibly unstable development environment which has different versions/releases of the project. Make one configuration change and everything just breaks,4n0168,t3_4n0168,guifroes,,Comment,1,0,1
d41pxoh,2016-06-08 21:20:41-04:00,guifroes,,Do you have automated tests?,4n0168,t1_d411hy4,arsnl,,Reply,1,0,1
4mnhlp,2016-06-05 09:57:52-04:00,-_WANDERER_-,How do software control hardware?,"Software, or ""app"" as people today call it, are basically commands made of binary values, zeroes and ones. How, then, do those zeroes and one's translate to physical, electrical events? 

For example, you click shutdown and the computer shuts off the monitor, fan, hard drives, etc.

UPDATE: 

Say what?! My mind just went like the virtual world in Tron trying to visualize and comprehend these concepts. ಠ_ಠ 

I'm googling some information about low-level computer language. Close-to-hardware Machine codes.. Am I going in the right direction?

UPDATE:

I just started watching a YouTube videos on how to build a computer from the lowest level. Hopefully this will give me a better understanding. ",,,,,Submission,22,0,22
d3x0kdg,2016-06-05 13:32:02-04:00,Ward574,,"As a programmer turned electrical engineer, I understand the confusion.

To make a very simplified example, hardware registers are mapped to memory locations maintained by the OS. Only the OS can change these, and it does so via drivers (which is simply software with extra privileges and no GUI. Not precise, but trying to keep it simple).

When you buy and install a new piece of hardware, it is the drivers which tell the OS what is available, and provides a set of functions for software to talk to it. Those functions receive the base address of the hardware and then can read and write to the registers stored on the hardware itself. Kind of like every piece of hardware comes with a tiny bit of dedicated RAM, where every bit is assigned a specific behaviour.

Software talks to devices by making IO calls. These calls are very similar to disk IO, but are directed at the specific device, ie, instead of writing to ""C:\"", they'll attempt to write to ""LPT:\"" for printer operations. These calls are intercepted by the OS which redirects it to the appropriate driver. The driver then accepts the data or commands from the software and directly changes the values of the device registers just like changing a value in memory.

This in turn affects the behaviour of the hardware which is dependant on the values held in those various registers. It varies from device to device. A single register can switch on or off a function, a collection of registers can act as a buffer for IO with an external device, such as a printer.

There is a whole set of rules and protocols to follow. That's what the driver is for. The OS doesn't care how it works. It sees the driver advertises a standard function for receiving IO requests, and so when software makes an IO request towards that device, it invokes the matching function in the driver.",4mnhlp,t3_4mnhlp,-_WANDERER_-,,Comment,16,0,16
d3xz0g0,2016-06-06 08:34:58-04:00,yes_thats_right,,"This is a good answer for how the OS talks to hardware but I interpreted his question as being ""how can computer code make physical things happen"" which I suspect you are also in a good position to answer from an electrical engineering point of view.
",4mnhlp,t1_d3x0kdg,Ward574,,Reply,3,0,3
d3y8n3g,2016-06-06 12:45:24-04:00,Ward574,,"I googled the question and saw a lot of answers like what you're describing, a lot like the posts already made. I wanted to be different and share a little programmer based perspective. :)",4mnhlp,t1_d3xz0g0,yes_thats_right,,Reply,2,0,2
d3yyu7e,2016-06-06 22:48:14-04:00,-_WANDERER_-,,"I think I understand a bit better now. I had to do some googling on the history and evolution of computing, including how storage works. I was born  in '85 and all I know about computers are what I see on tv like hackers movie, tron, that sort of stuff. I started using computers around year 2000, the most I learned in school was C programming. ",4mnhlp,t1_d3y8n3g,Ward574,,Reply,2,0,2
d3x0gwo,2016-06-05 13:29:25-04:00,lneutral,,"Much of computer hardware uses two strategies for communicating between devices.

One approach is to use special instructions (usually some permutation of the names IN and OUT) to send signals to ""ports."" Special hardware maps from ports to devices, and generates the appropriate signals along the appropriate wires to represent that information in a way the devices can use. This can be things like flipping switches, or more complicated, like transmitting graphics information.

Another approach is to use special memory locations, which represent different devices. Then a computer can use the same set of instructions to access devices as it does to access any other memory it uses.

If you think of the devices like monitor, fan, hard drives as having switches that can be electrically controlled, it's not a huge leap to think that doing one of the two things above could control the switches that enable or disable hardware.

To answer /u/CARGLE, yes, some devices *do* have their own sort of instruction sets, separate from the one CPUs use. This is pretty common, in fact.

But is that really all that surprising? Even CPU instruction sets are just collections of numerical values (represented as bits) that select from a bank of possible circuits the correct operation to be applied to values inside and outside the CPU.",4mnhlp,t3_4mnhlp,-_WANDERER_-,,Comment,6,0,6
d3x133p,2016-06-05 13:45:49-04:00,AmaDaden,,"So just as software has levels of abstraction, hardware does as well. To make how this works crystal clear we have to first talk about some details of these other levels so you can see how the components work. 

At nearly the lowest you have transistors. Input enough voltage and current flows through, not enough and nothing flows. Importantly, you can have situations where 1 volt can turn on a transistor and allow the amount of voltage flowing to change from 0 volts to 10 volts. This is how a small signal can drive something as comparatively power hungry as a fan.

Moving up an abstraction level, you can put transistors together to form logic gates. These are basic binary logic elements like 'and', 'or', and 'not'. These binary values of true and false are really just two different voltage levels. For example 5 volts and 0 volts. True and false are also considered binary 1 and 0, meaning you frequently have a 5 volt signal representing a 1. The value of 5 volts here is dependent on the how small the components are, chip makers are always fighting the fundamental laws of physics in order to minimize power consumption, see the [Intel tick-tock cycle](https://en.wikipedia.org/wiki/Tick-Tock_model). 

Moving up again, gates can be put together to make things like adders, and flip flops. Flip flops can be triggered to hold a value as long as they have power (not a signal but just having the computer plugged in). Every bit of RAM is a flip flop. Adders are components that add binary numbers. You can chain them together to add bigger numbers. The difference between a 32-bit machine and 64-bit machine is the number of flip flops and adders used in the components. 

If you ever heard the word 'word' used in CS this is the level where that comes in to play. 64-bit machines move data around in 64 bit long chunks and 32-bit machines use 32 bit chunks. Both are called a 'word'. On that note there are also components called multiplexers or mux for short. These are just used to select one word from a set of words. This is how RAM accesses stored data. The RAM has a huge number of flip flops stored as groups of X-bit long words and uses a mux to read from what ever word it is asked to retrieve.

That whole trip was necessary to talk about registers, the memory in the CPU. RAM has levels: the closest to the CPU are the registers, other memory in the processor is known as the cache, other RAM is out side of the processor and is what people commonly think of as RAM. The cache is valuable because the smaller the group you're selecting from the quicker it can be done. The CPU works by writing values to special memory locations in the CPU called registers. These are locations that are both storage and hooked up to other components. When a CPU runs an instruction, it gets that instruction from the register hardwired to store that. At the same time a different register that points to the next instruction in memory copies that instruction from the location it's pointing to in the cache in to the next instruction register. It's a complex little dance but the core point here is that some memory locations are used for special automated purposes and do things with out being told to because they are how things are 

There are lots of unique registers in the CPU to control running code but most components have their own processing units and their own specialized memory locations. So somewhere there is a bit in a memory location in the PC that is connected to a transistor that powers the fan. Write a 1 to it and the fan gets power, write a 0 and the power stops. The CPU controls these by sending a command to the component. The hard dive has it's own internal processor to service requests from the CPU. It would receive a message to power down and it would then do whatever it needs to in order to do that. That would mean running a small program hardwired in to the chips of the hard drive that makes sure that everything shuts down properly.",4mnhlp,t3_4mnhlp,-_WANDERER_-,,Comment,5,0,5
d3xegp2,2016-06-05 19:41:53-04:00,wallet_idiot,,"Others have mentioned device drivers and low-level software interactions, but I think what you're asking is how your ""virtual commands"" (text on the screen or in a file) translate into real world actions. If not, then please ignore :-).

The virtual world is then, as its name implies, not actually real. The electrical signals are the only things that exist. Text on the screen is just specific electrical signals passed to each pixel on the monitor such that it creates a pattern. Your keyboard is a more direct example: a physical action translates to an electrical signal that's passed to the CPU/memory. Commands to the CPU are electrical signals passed to the pins of a CPU such that it's triggered to carry out specific, pre-set commands (combine/compare these electrical signals, for example).

When viewed from a high level it's a lot to take in, which is why we abstract it all away. Remember, graphical interfaces have advanced A LOT since the early days of computing. Just having a command line terminal was a huge step forward (punch cards FTW!), so it's no surprise there's such a disconnect between what we see and what is physically occurring.

Perhaps a more manageable, practical introduction would be to look into assembly language. Each command translates fairly transparently to 1s and 0s (electrical highs and lows) passed to the CPU. Low-level is a pretty deep rabbit hole if you're curious/masochistic: CPU->operation->logic_gates->transistors->semiconductors->[atomic_weirdness].",4mnhlp,t3_4mnhlp,-_WANDERER_-,,Comment,2,0,2
d3xg0r6,2016-06-05 20:26:09-04:00,-_WANDERER_-,,I think I start slipping into that rabbit hole when I read about Konrad Zuse's computing machine. ,4mnhlp,t1_d3xegp2,wallet_idiot,,Reply,1,0,1
d3xyfu2,2016-06-06 08:12:53-04:00,-_WANDERER_-,,"Maybe I'm going at this the wrong way? In my current understanding, ones and zeroes exist in the virtual world.. But the virtual world doesn't really exist? Only electrical signals? That it's actually those electrical signals going through a circut, logic gates, storage,....and ending up as audio/video/physical output....? Did I understand that correctly?",4mnhlp,t1_d3xegp2,wallet_idiot,,Reply,1,0,1
d3xz31v,2016-06-06 08:37:45-04:00,yes_thats_right,,"That is more or less correct, which is why computing power is largely constrained by how small we can make the various electrical components (and hence how many can fit in one place).",4mnhlp,t1_d3xyfu2,-_WANDERER_-,,Reply,1,0,1
d3xzkw6,2016-06-06 08:55:27-04:00,wallet_idiot,,"Yes, that's about right. For example, with a 5V power supply, if you see 5V on a wire you can think of it as a 1, and seeing 0V as a 0. Putting these voltages to the inputs of a logic gate (which is just a few transistors arranged in a certain way) produces the logic gate output. For a NOT gate, seeing 5V at the input puts 0V at the output, and seeing 0V at the input puts 5V at the output.

Linking logic gates together allows more higher level operations like addition, multiplication, multiplexing, etc. We combine the simple stuff to make the complicated stuff.",4mnhlp,t1_d3xyfu2,-_WANDERER_-,,Reply,1,0,1
d3xkq3r,2016-06-05 22:23:39-04:00,ollee,,"I had an entire class called Computer Organization in my 3rd year of CS. I'd say roughly 75% of the entire set of information I learned in an entire semester from that class is contained in this thread. 

In addition we touched on the basics of compiler design, encoding numbers into binary, and machine states. That's it.

Interesting stuff, but a total let down of a class.",4mnhlp,t3_4mnhlp,-_WANDERER_-,,Comment,2,0,2
d3wze2v,2016-06-05 13:01:14-04:00,KyleRochi,,"At a low level its a bunch of commands issued to your CPU, the CPU reads these commands and executes them. For booting, when you give the mechanical input ""on"" most CPUs boot up memory and read from 0xFFFFFFF0 (at least on x86s). From then on it just reads a bunch of instructions and executes them. For shutoff, imagine there are lots of toggle switches that can be either on or off, if you switch the right order of them, a mechanical shutoff signal will run and everything will turn off. 

CPUs are made of mind bogglingly big arrays of tiny transistors. These transistors work together to take the 1s and 0s and translate them from code to ""real life""",4mnhlp,t3_4mnhlp,-_WANDERER_-,,Comment,2,0,2
d3x7wap,2016-06-05 16:42:19-04:00,-_WANDERER_-,,My mind went like the virtual world in Tron trying to visualize and comprehend these concepts. ಠ_ಠ ,4mnhlp,t3_4mnhlp,-_WANDERER_-,,Comment,1,0,1
d3wuvp3,2016-06-05 10:52:51-04:00,CARGLE,,"As a 3 year student I still struggle with how this is actually done. On a very basic level I think it's all electric currents. I'm not sure if the cpu does this or some other device on the motherboard but a 0 would be interpreted maybe as a low current and a 1 would be a high. When you put a sequence of those together the cpu will interpret it as an instruction set for something to be done on the hardware side. 

I'm so sorry if I'm giving misinformation. So far in school i've only gotten a little bit of info about this stuff in a computer systems class and that was from a software perspective. There may be a better subreddit you could ask this question in. ",4mnhlp,t3_4mnhlp,-_WANDERER_-,,Comment,1,0,1
d3xktw1,2016-06-05 22:26:09-04:00,ollee,,"Read the other replies in this post, the top 2 give a good basic understanding that is enough to give you a direction to Google down the rabbit hole.",4mnhlp,t1_d3wuvp3,CARGLE,,Reply,1,0,1
d3x09ro,2016-06-05 13:24:17-04:00,rfinger1337,,"oh, you will have so much fun learning this! Get a prototype board like an arduino or raspberry pi and do some basic projects.

http://readwrite.com/2014/04/21/easy-arduino-projects-basics-tutorials-diy-hardware/

To over symplify the answer:    If ( something == something else) { allow voltage to a pin }

^ this is software controlling hardware.  It's literally using a gate to complete a circuit that includes the bit of hardware.

If you get an arduino and do some simple programs (there are step by step's online) you will start to get a feel for it.

This is on the macro level, but ultimately, all computing is letting current through one path or another. So in the smallest part of a processor there is a gate that allows electricity to pass, or not.  ",4mnhlp,t3_4mnhlp,-_WANDERER_-,,Comment,1,0,1
d3wx4v5,2016-06-05 12:00:00-04:00,soguesswhat,,"Your application uses some programming language to interact with the computers operating system, which in turn interacts with the hardware on your computer through device drivers. 

There are a wide variety of hardware devices that do very different things. But, for example, a hard disk drive turns the electric impulses into mechanical energy to spin magnetic plates. ",4mnhlp,t3_4mnhlp,-_WANDERER_-,,Comment,0,0,0
4mmr39,2016-06-05 05:55:46-04:00,PyroFox123,How far are we from making AI?,"How far are we from making AI? Something that can think, feel, talk, et cetera.",,,,,Submission,1,0,1
d3wrpwo,2016-06-05 08:51:44-04:00,bacondev,,"I suppose that you are asking about [artificial general intelligence](https://en.wikipedia.org/wiki/Artificial_general_intelligence) (AGI). AGI already exists but not to the extent that you're asking. For example, Google's [AlphaGo](https://www.deepmind.com/alpha-go.html) beat Lee Sedol, the world's greatest player of Go, four out of five times. In fact, this is the first time that *any* computer program has ever beat a professional Go player. That in and of itself is quite a feat. But what's even more extraordinary about this is that the underlying algorithms are general in nature. That is, they could be tasked to do something other than Go with relative ease. For example, it could be trained to predict the outcome of every game in an NCAA men's basketball tournament (i.e. March Madness) or it could learn to detect objects, faces, etc. in pictures. This could all be done with few (if any) changes.

With that said, there are three main reasons that this is not in commercial products yet:

- We need faster algorithms. There's still so much to be discovered with AI that there is room for another full-time Einstein on the topic.
- We need faster hardware. The last hardware configuration for AlphaGo that I have heard about involved 1,920 CPUs and 280 GPUs. Obviously, that entails financial and spatial barriers.
- We need quantum computing. As hardware continues decreasing in size, it is approaching subatomic sizes. This is still decades away, but we're getting to the point at which [quantum tunneling](https://en.wikipedia.org/wiki/Quantum_tunnelling) will become a major issue. In other words, electrons could pass through barriers which is not possible according to classical physics. This would mean that transistors would be largely ignored in a circuit. We need quantum computers so that we can truly achieve faster hardware. Not only is it faster due to size, it's faster due to properties of quantum physics. Many quantum algorithms could be implemented with a time complexity of the square root of the time complexity of the equivalent classical algorithm.

To actually answer your question, nobody really knows for sure. It could be in three decades. It could be in three centuries. It all just depends on the rate of progression of research. If you're thinking about something such as the operating systems in the movie [Her](http://www.imdb.com/title/tt1798709/), then unfortunately, my guess is that you will likely be dead before that happens.",4mmr39,t3_4mmr39,PyroFox123,,Comment,3,0,3
d3wusye,2016-06-05 10:50:29-04:00,ebix,,"I'd like to point out that AlphaGo was not *that* general. The neural net structure was tuned for Go specifically, even though neural nets in general can be used for lots of tasks. Also, despite the fact that it did unsupervised learning (played lots of games against it's self), it was seeded with a great deal of supervised learning. I forget who the quote was but something along the lines of: ""If intelligence is a cake, supervised learning is the icing, and unsupervised learning is the rest of the cake""...

We still have a looong way to go in the field of unsupervised learning. ",4mmr39,t1_d3wrpwo,bacondev,,Reply,2,0,2
d3wrw5y,2016-06-05 08:59:49-04:00,PyroFox123,,"Wow, thats helpful. Nice research, and 1,920 CPU's. I wish my computer had that amount of processing power. Try me now, Fallout 4  recommended specs. Anyway, AGI was exactly what I meant. Thanks.",4mmr39,t1_d3wrpwo,bacondev,,Reply,1,0,1
d3ws48o,2016-06-05 09:10:00-04:00,rfinger1337,,"Ann Perkins! (listen to the clip of ""Her"" and you will understand.)",4mmr39,t1_d3wrpwo,bacondev,,Reply,0,0,0
d40sljc,2016-06-08 08:53:29-04:00,floider,,8.552 miles,4mmr39,t3_4mmr39,PyroFox123,,Comment,1,0,1
4ml8dt,2016-06-04 21:05:51-04:00,emond24,CS advice,So long story short I decided to go back to college to get a CS degree. I'm very interested in computer and such. I will be going back to a community college just to freshen up a bit on my math. When I start taking actual CS courses. Could I get some advice on studying and being ready for it? I'm afraid that I won't get any of it or I will struggle. Afraid I won't be to knowledgable on it. But this is the career id like to pursue. Are there video to watch or something?,,,,,Submission,2,0,2
d3whf09,2016-06-05 00:00:32-04:00,crookedkr,,"You probably won't get much response here. Nearly this exact question is posted like once a week. The short answer is try it you don't have anything to lose by trying. If it doesn't work out do something else. Also, google...there are 1000 tutorials on everything you could want to know about coding, theory, algorithms, etc.",4ml8dt,t3_4ml8dt,emond24,,Comment,1,0,1
d3wnzel,2016-06-05 05:03:15-04:00,WalrusForSale,,What kind of job do you want? ,4ml8dt,t3_4ml8dt,emond24,,Comment,1,0,1
d3x5dot,2016-06-05 15:37:13-04:00,emond24,,That's just it I'm not sure. I wanna do something with computers. People have said my best option is a CS degree,4ml8dt,t1_d3wnzel,WalrusForSale,,Reply,1,0,1
d3xicmr,2016-06-05 21:26:30-04:00,WalrusForSale,,That seems like the best question to start with,4ml8dt,t1_d3x5dot,emond24,,Reply,1,0,1
d3xlr4n,2016-06-05 22:49:10-04:00,emond24,,"I'm assuming you have a CS degree or you're pursuing one correct? How much of what you do in the course you take will you do for the job someone decides to take. Example. I think I'll struggle hardcore in programming. How much of programming with will jobs with CS degrees focus. Maybe im over thinking it and worrying my self to much. If I pass my classes. I'm more worried I won't know anything for the jobs( I hope you get what I mean) will jobs be easier? Sorry for the book I just wrote. Just getting some info thanks

",4ml8dt,t1_d3xicmr,WalrusForSale,,Reply,1,0,1
4ming4,2016-06-04 11:00:45-04:00,hpfan5,Is there a compositing video program similar to how panorama pictures are made - but for videos?,"please and thank you ...something like google maps cars who map the streets and buildings, but how are multiple videos grafted together?",,,,,Submission,5,0,5
d3vzg7w,2016-06-04 14:56:39-04:00,PastyPilgrim,,"I don't know of any software that can stitch together multiple video streams because that would be immensely computationally intensive. There are some research projects into this (namely, I can remember Microsoft detailing this idea at some point), but no consumer software that I know of.

However, 360 degree video is definitely something that exists: it's used for VR all the time. But, as far as I know, all of that content comes from multiple cameras fixed to a single point (or one super fish-lens camera) where you can mathematically determine overlap and crop that out. Determining overlap and stitching together multiple independent cameras without a shared reference frame would be so difficult.",4ming4,t3_4ming4,hpfan5,,Comment,2,0,2
d3wgx8t,2016-06-04 23:44:10-04:00,theobromus,,"Yes, the only solutions I'm aware of that work well essentially assume a fixed arrangement of cameras (or at least a very small variance in their positions). E.g. https://vr.google.com/jump/
But to my knowledge, no one is releasing such software (facebook has promised to open source it's software but has not yet done so, and I'm not sure on the quality of the results: https://facebook360.fb.com/facebook-surround-360/).",4ming4,t1_d3vzg7w,PastyPilgrim,,Reply,2,0,2
d3w1fez,2016-06-04 15:55:57-04:00,drgalaxy,,https://www.disneyresearch.com/publication/panoramic-video-from-unstructured-camera-arrays/,4ming4,t3_4ming4,hpfan5,,Comment,2,0,2
4mh7d9,2016-06-04 02:40:39-04:00,TissueReligion,Suggestions for self-studying programming language theory?,"I've seen a few book suggestions floating around (Programming Language Pragmatics comes up a lot...), and I've been trying to find syllabi of classes on the Internet, but I don't have a good enough feel for the material to evaluate a syllabus.

Any book / syllabus recommendations?",,,,,Submission,11,0,11
d3vk01g,2016-06-04 04:39:55-04:00,icendoan,,"I believe the books to look for are *Software Foundations* and *Types and Programming Languages*, and a text on compilers might be helpful as well, such as the Dragon book.",4mh7d9,t3_4mh7d9,TissueReligion,,Comment,1,0,1
d3vkg8i,2016-06-04 05:10:18-04:00,kirang89,,This might be exactly what you're looking for: https://github.com/steshaw/plt,4mh7d9,t3_4mh7d9,TissueReligion,,Comment,1,0,1
4mdt4d,2016-06-03 12:51:56-04:00,natselrox,Why does the '&' symbol sometimes show up as '&amp:' in a lot of places?,"You'll often see that someone copied a test from a site and when that is posted on another site, the '&amp;' symbol is changed into '&amp;amp;' or somesuch. Why is that? Thanks.",,,,,Submission,12,0,12
d3v3bx9,2016-06-03 18:47:18-04:00,lordvadr,,"In HTML the & sign is a special character.  Specifically it got it's start because HTML tags start and end with < and >, so it was decided that to print an ACTUAL < or >, you had to use what's called an *escape sequence*. In HTML, it's ampersand (&) followed by either a code or a short word describing what you want.  In the '<' and '>' cases, those are **l**ess-**t**han and **g**reater-**t**han symbols, so they were escaped &amp:lt: and &amp:gt: respectively.  As is the case with most escape sequences, you then need to escape the escape sequence.  So an actual & got an &amp:amp: escape sequence to render an actual & because early parsers couldn't know if the & was the start of an escape sequence or just a literal &.

There are many, many more.

So when automated escapers and descapers do their thing with inputs and outputs, they sometimes screw up and will escape **&amp:**amp: as **&amp:amp:**amp: which causes the next iteration to render as &amp:amp:.

[Full List of HTML escape sequences here](http://www.theukwebdesigncompany.com/articles/entity-escape-characters.php)",4mdt4d,t3_4mdt4d,natselrox,,Comment,5,0,5
d3uoy2a,2016-06-03 13:13:29-04:00,Mines_of_Moria,,This will probably be illuminating: https://stackoverflow.com/questions/9084237/what-is-amp-used-for,4mdt4d,t3_4mdt4d,natselrox,,Comment,4,0,4
d3upw58,2016-06-03 13:33:34-04:00,natselrox,,"This helps but it's not entirely clear to me.

Some sites have the option to embed tweets and I often see the '&' in the tweet being shown as '&amp:' on the site. Is it because Twitter uses the '&amp:' to represent the ampersand sign and when it is copied, the other site displays it without converting it into the '&' sign?",4mdt4d,t1_d3uoy2a,Mines_of_Moria,,Reply,2,0,2
d3urwev,2016-06-03 14:16:49-04:00,xiongchiamiov,,"Fyi, those all show up as the same thing unless you put them in backticks.

Generally it's because a site is escaping something that doesn't need to be escaped. This is bad, but not as bad as the alternative, which is opening yourself up to xss.",4mdt4d,t1_d3upw58,natselrox,,Reply,5,0,5
d3uqiab,2016-06-03 13:46:43-04:00,smellyrobot,,"It's usually because '&' has been twice escaped. Most editors know HTML entities like ampersands, quotes, greater than and less than symbols need to be escaped, so they do this automatically. So '&' becomes '&amp:amp:' and '""' becomes '&amp:quot:' and your browser knows to convert these to '&' and '""' when displaying text. When text is directly copied from HTML these entities are escaped twice, so '&amp:' becomes '&amp:amp:amp:' which is displayed by the browser as '&amp:amp:'.

You must've seen this happen when you wrote your question since reddit does some automatic conversion for HTML entities.",4mdt4d,t3_4mdt4d,natselrox,,Comment,6,0,6
d3uraqx,2016-06-03 14:03:47-04:00,987f,,"Because sometimes & is used in code so when it is used in text, we have to use a different symbol.",4mdt4d,t3_4mdt4d,natselrox,,Comment,4,0,4
d3usinw,2016-06-03 14:30:12-04:00,Natanael_L,,&inserttext: is a way to encode special characters for display without making the browser parse them as part of the code or something else. Amp stands for ampersand = &. Sometimes the source encoding becomes visible to the user for whatever reason.,4mdt4d,t3_4mdt4d,natselrox,,Comment,2,0,2
d3vantx,2016-06-03 22:18:17-04:00,EvgeniyZh,,"My RSS reader showed a header as:

*Why does the '&' symbol sometimes show up as '&' in a lot of places?* 

Got stuck for a second.",4mdt4d,t3_4mdt4d,natselrox,,Comment,2,0,2
4m9fef,2016-06-02 17:10:53-04:00,dezball,How to approach computer science math,"I am one of those who graduated with an unrelated major years ago (biology) and am now making the plunge to computer science.  

A week ago I started a six week discrete math course at the university where I'm doing a compsci post-bac, and I consistently feel so incredibly lost in class.  This is the first math class I've taken in 7+ years, and it doesn't help that the last math class I took was calc I, so I've never been exposed to proofs before and my professor is really not that great :(

I am willing and able to spend every free minute I have trying to get a hold of the information, and any advice from all of you experts would be greatly appreciated. 

I started a bit behind, but should be caught up by the end of the weekend.  My goals for the remainder of the class include: (a) read the textbook before class so that it is not completely foreign while he's teaching. (b) Re-read the book after if I still don't get it 100%, and supplement this with online resources.  (c) I found this MIT online course, where I can perhaps watch the corresponding video http://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-042j-mathematics-for-computer-science-fall-2010/ while I'm at the gym or something. (d) I've found a study partner, so we'll be getting together after we've each studied on our own to go over problem sets and new concepts. (e) start the problem sets as soon as they are assigned to give myself the time I need to internalize/understand it.

One thing I am scared of is not being able to think on the spot for the exams - most HW questions I've done so far I've been able to complete because I've given myself enough time to think things through without pressure.

The other thing is I've read through here the importance of discrete math and math in general in computer science.  Because I am striving to be the best computer scientist I can be, I do truly want to understand the topic, and maybe even continue taking related math courses. So any advice on how to proceed with compsci related math classes are welcomed (and a side question: is it necessary to go back and take calc I, II and III? [I'd also have to take calc I since it's been many many years and I do not remember much at all])

THANK YOU!!",,,,,Submission,10,0,10
d3tqaii,2016-06-02 18:46:39-04:00,PastyPilgrim,,"You don't need to retake calculus. Other than that, it's kind of hard to just tell someone how to be better at math, especially when you're someone that's already educated and already knows how you learn best. Perhaps your mistake was taking discrete pretty early on in your development: I had already been programming for 4 or 5 years and studying computer science for 2 or 3 years when I took discrete, so I was able to absorb the logic in the context of the utility that it provided. But I can't say how useful that is when I never got to experience learning discrete prior to computer science.

As a side topic, have you considered studying bioinformatics instead? Already having a degree in biology, you could go to a graduate program for bioinformatics where you'll enhance/apply your knowledge of biology while learning how to program to solve problems and analyze data. Very little math required. I'm not one to stand in the way of someone that wants to learn computer science because, as a computer scientist, I think CS is the greatest thing in the world, but you already have the toolset, knowledge, and interest in a subject to become highly skilled at something not many people are skilled at. Plus, if you take the math out of what you're studying (which is what you're struggling most with) and add in your biology (which is what you're best at), you're pretty much left with bioinformatics.",4m9fef,t3_4m9fef,dezball,,Comment,2,0,2
d3v7kvt,2016-06-03 20:50:15-04:00,dezball,,"Thanks for the advice.  I am actually trying to steer away from bioinformatics, so will be trying extra hard to do well and understand this math course. : )

I may come back once it's over and post some general advice if it happens to go well, if anyone happens to be interested. ",4m9fef,t1_d3tqaii,PastyPilgrim,,Reply,1,0,1
d3u0iyd,2016-06-02 23:16:26-04:00,nilleo,,"Discrete math is one of the hardest courses over taken, no doubt. Don't beat yourself up too much. Study, study, study. If you don't understand something, ASK QUESTIONS. Trust me, not understanding stuff in that class compounds up really fast.",4m9fef,t3_4m9fef,dezball,,Comment,2,0,2
d3v7hvq,2016-06-03 20:47:51-04:00,dezball,,Thanks for the encouragement- I will definitely be spending every free minute trying to truly learn the material!,4m9fef,t1_d3u0iyd,nilleo,,Reply,1,0,1
4m33gc,2016-06-01 15:37:37-04:00,underscore_frosty,Which mathematics course should I take next fall?,"So, I'm wanting to take some math courses at my university to fill out my next academic year (I'm mostly done with my CS program requirements, just have a few more classes to take). Ultimately I'd like to take a course that has the most relevancy to theoretical computer science.

So far I've narrowed it down to Rings and Polynomials, Complex Analysis, and Formal Logic (I'm taking a statistics class as well, but it's a requirement for my CS program). In your opinion, which one of those would be most relevant to the theory aspect of computer science?

For background I'm planning on going to grad school and specializing in algorithms/theoretical computer science (and potentially cryptography/applied cryptography).",,,,,Submission,11,0,11
d3s9ixe,2016-06-01 17:18:08-04:00,mpdehnel,,"Formal Logic, absolutely 100%.",4m33gc,t3_4m33gc,underscore_frosty,,Comment,8,0,8
d3sceve,2016-06-01 18:27:02-04:00,kurtms,,"While the others are certainly interesting and will come in handy eventually, formal logic is a concept you need to know extremely well (especially for algorithms) and will help in the other classes as well.",4m33gc,t1_d3s9ixe,mpdehnel,,Reply,4,0,4
d3sifqy,2016-06-01 21:02:12-04:00,TheCommador,,"I've never heard of formal logic before, is it safe to assume it's boolean algebra or some variant/derivative? ",4m33gc,t1_d3sceve,kurtms,,Reply,1,0,1
d3smju4,2016-06-01 22:50:57-04:00,underscore_frosty,,"It's really a concept. Basically, it is the application of various logical systems to mathematics. Look up mathematical logic which is what ""formal logic"" basically is (in this context).",4m33gc,t1_d3sifqy,TheCommador,,Reply,1,0,1
d3t5r2q,2016-06-02 11:08:47-04:00,icendoan,,"You should consider which courses you'd enjoy, especially if they're not required for your course. University isn't just about the bit of paper at the end.

That said, formal logic is great if you have a taste for that sort of thing.",4m33gc,t3_4m33gc,underscore_frosty,,Comment,1,0,1
d3t7yrx,2016-06-02 11:58:21-04:00,underscore_frosty,,Formal Logic and Advanced Formal Logic (2 quarter sequence) count as electives for my CS program so I'm more inclined to take those plus I actually do have a knack for logic.,4m33gc,t1_d3t5r2q,icendoan,,Reply,1,0,1
d4jludc,2016-06-22 13:14:11-04:00,andybmcc,,"Number/group theory style courses such as your ""Rings and Polynomials"" are pretty much required for an understanding of cryptography.  I can't imagine trying to wrap your head around something like public key encryption without at least a basic understanding of concepts such as Galois fields.",4m33gc,t3_4m33gc,underscore_frosty,,Comment,1,0,1
4lztb6,2016-06-01 02:05:00-04:00,Guacamula,Is This Career Becoming Oversaturated?,"Ok so I'm close to taking the plunge and going all-in on learning programming but I see some comments that cause me to hesitate. I have a bachelor's in industrial engineering, but I think programming would be more interesting and better suited for me. 

Starting at 27 years old, am I going to be able to compete with the army of coders coming up? I hear junior development work isn't as easy to find as it was 5+ years ago. Entry level positions are being outsourced to other countries for cheap, working remotely is becoming more popular. Bootcamps are pumping out junior devs, and cs jobs overall are going to continue to grow in popularity. Hell, it's popular here on reddit /r/programming outnumbers /r/engineering 6:1. 

I'm worried if I self teach diligently for the next year or so, I still won't be relevant in 2018 applying for entry level positions.",,,,,Submission,12,0,12
d3rgv2m,2016-06-01 03:23:53-04:00,nuclearqtip,,"Depends on what you're after. There are different talent levels to consider. If ""coding"" to you means making a CRUD website using the latest buzzword technology, then yes it's saturated like hell. However if ""coding"" means problem solving, design, architecture, planning, implementing, deploying, running, etc using the tools fit for the job, then the market is wide open for you.

There are plenty of low quality devs. There aren't a lot of high quality ones.",4lztb6,t3_4lztb6,Guacamula,,Comment,22,0,22
d3ro3v1,2016-06-01 09:14:42-04:00,Guacamula,,"Do you think it is possible to become a ""high quality"" developer in a couple years when I'm starting from scratch? Or will I be one of these code monkeys until I get a few years work experience and then be able to contribute in a more meaningful manner.",4lztb6,t1_d3rgv2m,nuclearqtip,,Reply,3,0,3
d3rroyi,2016-06-01 10:48:23-04:00,nuclearqtip,,"In my experience the difference between the two comes down to attitude. Low quality devs tend to only learn the bare minimum to do their jobs. They crave familiarity and routine procedure. And while this can sometimes yield immediate results for the company, the developer's ability to think and ask critical, probing questions (e.g. ""why"" rather than ""how"") is stunted, which hurts them and the company in the long run.

Really good developers are constantly learning. They understand that every decision involves tradeoffs, and they know how to resolve ambiguity in order to properly choose between those tradeoffs. They choose the right tools and solution for the job at hand, and they're wary of hype. They understand that most languages are roughly equivalent, and so they tend to be able to pick up new ones pretty fast (a few weeks to a month usually). They don't view code or computers or large projects as magical or unapproachable -- they know that all code was written by human beings and therefore they can grasp it too. Good devs aren't ""oracles of knowledge"", they just have a sound approach to problem solving, a thirst for knowledge, an awareness of tradeoffs, and a confidence in themselves to be able to overcome whatever challenges are thrown at them.

I have no idea how long it takes to get there. There's really not a solid cutoff. Years? A decade? It really depends on so many factors. If you surround yourself with good devs (or get a great mentor) AND you have the attitude AND you have the foundational knowledge (e.g. CS) AND the company is encouraging of you AND you have the time and energy, then yes probably just a couple years.",4lztb6,t1_d3ro3v1,Guacamula,,Reply,8,0,8
d3sbssf,2016-06-01 18:11:41-04:00,Guacamula,,Ok I'm going to try my best. Hopefully I'll have a success story to post one day.,4lztb6,t1_d3rroyi,nuclearqtip,,Reply,2,0,2
d3rs7ab,2016-06-01 11:00:18-04:00,ehead,,"It doesn't matter what quality dev you are, it matters what quality dev people perceive you to be. I doubt you will have much luck applying for jobs that require 5 year minimum experience, so you will likely have to start with an entry level job.    

Presumably if you are talented you will be able to move around and get more experience and work at better gigs. The kind of more advanced engineering or architectural software work that was mentioned above may be something you could do after a few years experience, combined with hard work and of course good luck (as always), provided you manage to leverage your networking skills and navigate the career path properly.",4lztb6,t1_d3ro3v1,Guacamula,,Reply,3,0,3
d3w8lnk,2016-06-04 19:32:07-04:00,cugamer,,This.  There is a metric shitload of crummy developers out there who are somehow conning their way into good paying jobs and a smaller number (perhaps an assload) of quality devs who aren't able to market themselves well and aren't working.,4lztb6,t1_d3rs7ab,ehead,,Reply,1,0,1
d3rl9pp,2016-06-01 07:32:30-04:00,elpantalla,,"There are different sub fields in software. I think a lot of the web programming is a little more saturated, since that's what's *in* these days. 

Demand is high for embedded engineers, and will only grow in the future. Embedded has a steeper learning curve, and it's not as easy to get started as web development is. 

There's also backend stuff if you like being in the web space. 

Bootcamps don't churn out engineers, they churn out code monkeys that only know 1 or 2 frameworks and don't have the skills to do anything else. All they do is create resumes for employers to (mostly) throw away. 

",4lztb6,t3_4lztb6,Guacamula,,Comment,7,0,7
d3rnr22,2016-06-01 09:03:55-04:00,Guacamula,,Do you suggest I stay away from the web development side? Should I lean towards back end development or this embedded you refer to?,4lztb6,t1_d3rl9pp,elpantalla,,Reply,1,0,1
d3rs965,2016-06-01 11:01:31-04:00,elpantalla,,"I wouldn't go that far. I would suggest you learn a little bit about backend development and embedded development. Maybe get a raspberry pi and some sensors and play around. Whatever you enjoy, get good at. 

If you're a good developer, you won't have to worry about finding a job. If you're passionate and enjoy what you do and take pride in it, you will be attractive to employers. ",4lztb6,t1_d3rnr22,Guacamula,,Reply,2,0,2
d3rtsz0,2016-06-01 11:36:46-04:00,worldDev,,"The reason it's saturated is because it has so many self-starter resources. The cause of it's saturation actually makes it easier for you to start with because you have so many beginner oriented recipes / tutorials that hit need to know info to produce something that functions. The problem is when people stop there and base their skills off of following instructions instead of knowing what the tools they are using do well or poorly, and what programming patterns can cause issues with performance, maintainability, and scalability.

Truth is you can find people using any language that are stuck outside of following tutorials, it's just more prominent in web because of available resources for people with no proper education on the subject and the overwhelming demand for highly abstracted level rapid development solving the same crud issues.",4lztb6,t1_d3rnr22,Guacamula,,Reply,1,0,1
d3rkcyb,2016-06-01 06:49:37-04:00,james4765,,"Business programming always needs people who are comfortable working in the business world - the tech side isn't enough, knowing the *whys* of what a business application is being written for is really important and can mark you for advancement.

Code monkeys are relatively easy. Someone who shows curiosity about what the thing they're working on does, and is able to ask the right questions to head off problems in the future, is far more valuable than someone with tech chops and no care taken.

The medical field faces the same thing - there's diploma mills churning out entry level medical types like there's no tomorrow, but a fair bit of those new graduates are entirely unsuitable for employment...",4lztb6,t3_4lztb6,Guacamula,,Comment,4,0,4
d3rnz4n,2016-06-01 09:10:45-04:00,CoopNine,,"Where are you, and where are you willing to go?  You're going to find different situations depending on where you are.  I'm in the central US, and finding qualified programmers of all levels is difficult.  The vast majority of applicants out of college we see require sponsorship, which we prefer not to do, but end up having to do because the applicant pool is so small.  I don't see that army of coders coming up, at least here.  I've talked to other people at some career fairs around the area and it seems very similar throughout the central/upper midwest.  

You will find different situations depending on where you are, but if you're willing and able to relocate, I think we are far from being over saturated. 

 

",4lztb6,t3_4lztb6,Guacamula,,Comment,2,0,2
d3rpnok,2016-06-01 09:57:46-04:00,Guacamula,,"Currently in northeast Ohio, I think ideally I'd like to live in the Pittsburgh area. I might be willing to relocate for a few years depending on the conditions.",4lztb6,t1_d3rnz4n,CoopNine,,Reply,1,0,1
d3rt5pw,2016-06-01 11:22:21-04:00,CoopNine,,"I can't speak for that area...  The area I'm most familiar is the upper midwest, west of Chicago, east of Denver, north of Dallas.  KC, Omaha, MSP, and some smaller towns like Sioux Falls and Des Moines all have pretty good markets for people looking for jobs doing full stack web development either in .NET or Java using MVC.  

What you might do is attend some Java or .NET user groups to start networking, and learning more about what is used in practice.    These are often sponsored by recruiters.  This is also a good avenue to understanding the job market.  Seeking out a quality and respected recruiter in your area and asking questions on skill sets can help a lot if you're trying to figure out what you should do.

Finally if you have work experience in industrial engg, that's going to at least look interesting to people reviewing your resume.  The company I work for hires all kinds of engineers as well as we're stupid big and diverse, so that experience can be helpful in more ways too.",4lztb6,t1_d3rnz4n,CoopNine,,Reply,1,0,1
d3s1t1c,2016-06-01 14:30:09-04:00,Guacamula,,Could you expand a bit on Java or .NET user groups? Where could I find these groups or the recruiters you mentioned?,4lztb6,t1_d3rt5pw,CoopNine,,Reply,1,0,1
d3s4o7i,2016-06-01 15:31:30-04:00,CoopNine,,".net user groups in your general area https://blogs.msdn.microsoft.com/jennifer/2009/06/11/net-user-groups-in-michigan-ohio-kentucky-and-tennessee/

Java user groups https://java.net/projects/jugs

As for recruiters, companies like teksystems or C&A associates are probably in your area.  Might just do a search for tech recruiters in your city.",4lztb6,t1_d3s1t1c,Guacamula,,Reply,1,0,1
d3s53r0,2016-06-01 15:40:51-04:00,apendleton,,"One anecdote: my company still has a hard time hiring as fast as we'd like to, even with all the bootcamps and whatnot (and we do consider candidates fresh out of school or bootcamp programs). There's some lag effect there, probably, but at least as far as we've seen we're not at a saturation point of good people yet.",4lztb6,t3_4lztb6,Guacamula,,Comment,2,0,2
d3sfdf3,2016-06-01 19:42:08-04:00,rfinger1337,,"The need for developers is increasing a rate that can't hope to be matched by the number of developers in the market.  According to some studies, the vast majority of developers in the market right now have 6 years experience or less.  In a mature or dying industry, those numbers would be 4x or 5x that many.  

(manufacturing postions for example)

This is a safe industry, if you enjoy it.",4lztb6,t3_4lztb6,Guacamula,,Comment,2,0,2
d3sig7d,2016-06-01 21:02:33-04:00,KronktheKronk,,No.,4lztb6,t3_4lztb6,Guacamula,,Comment,2,0,2
4lz492,2016-05-31 22:53:57-04:00,moeseth,Does HTTP browser Cache-Control work across different website?,"Hey,

For example, I have one file that has Cache-Control of 1 year like http://example.com/test.jpg.

I used it in example.com and assume it has been cached there.

If I use the same link in another website, anotherwebsite.com, will it need to be cached again?
",,,,,Submission,1,0,1
d3tgbd5,2016-06-02 14:58:43-04:00,cowmandude,,"Ultimately it's up to the browser, but typically if its coming from the same source the cache is good regardless of which domain the actual page is on.",4lz492,t3_4lz492,moeseth,,Comment,1,0,1
4lyr8t,2016-05-31 21:29:49-04:00,emma413,PLEASE help me write code using python,"I need to do this using Python :
Create a function word_lengths(words) that accepts a list of strings and

returns a list of ints, where each int is the length of one of the words. An example

of the function is shown below. The example also shows you how to remove the

punctuation from a long passage of text and chop it into a list using the spaces

between words.

In []:

phrase = 'Take any passage from any document.'

In []:

words = phrase.split(' ')

In []:

word_lengths(words)

[4, 3, 7, 4, 3, 8]",,,,,Submission,0,0,0
d3r7e4h,2016-05-31 21:52:18-04:00,wafflestealer654,,"I'm sorry, we don't do homework on this sub. If this really is a homework assignment, try /r/HomeworkHelp.

Otherwise the people over at /r/learnpython can help you. But, you should provide any codes you have written.",4lyr8t,t3_4lyr8t,emma413,,Comment,7,0,7
d3rhmc7,2016-06-01 04:06:28-04:00,arphod,,Figure it out. That's what programmers do. All the time. Don't like it? Try dancing.,4lyr8t,t3_4lyr8t,emma413,,Comment,2,0,2
d3ra1rv,2016-05-31 23:05:06-04:00,motheryaar,,"def word_lengths(phrase):
    
    words = phrase.split(' ')
   
    lengths = []
    
    for word in words:
       
        lengths.append(len(word))
    
    return lengths",4lyr8t,t3_4lyr8t,emma413,,Comment,-3,0,-3
4lynmb,2016-05-31 21:06:57-04:00,emma413,intro to computer science questions please help!,"Using python 
Describe a real life situation in which your thoughts follow the accumulator pattern. 

Describe a real-life situation using the aggregator pattern. 

Describe a real-life situation in which your thoughts follow the best-so-far pattern. 

thank you!! ",,,,,Submission,0,0,0
d3r7qno,2016-05-31 22:01:44-04:00,darkhelmet41290,,You might be looking for Chegg or some other place that does your homework without any displayed effort.  ,4lynmb,t3_4lynmb,emma413,,Comment,7,0,7
4lx5mu,2016-05-31 15:53:01-04:00,UnifiedHearts,What should every programmer know/have?,"This can include useful apps on the laptop, Tips and tricks when starting computer science classes, Skills you should have when finishing your bachelors and looking for a job (can include what languages you should have a good understanding of).. etc. 

I suppose I should include some background about myself. 
Just finished my associates degree and now transferring to the university for two more years of school. I'm taking Data Structures and Statistics in the summer (starting class next week). Planning on being a Programmer or maybe a Software Developer. As far as languages, I've dabbled in a few languages such as C# and Python, but Java is what i most used and have good understanding of (practicing problems on Project euler). Plan on learning C++ next then after that some other types like HTML and others.

Sorry for bad grammar and long length.

Edit: Thanks guys. Some of these tools you all mentioned i haven't heard before and will be looking into it. Special shout out to the people who provided their answers in a list type form that really helps.",,,,,Submission,22,0,22
d3qxjbk,2016-05-31 17:37:47-04:00,For_Iconoclasm,,"Version control!

I was never taught anything about version control in college until a younger advanced systems software instructor spent a day on it, because he wanted us to use it when we collaborated on projects, and that was many semesters _after_ I started using it at my internship. In the time between my internship and the aforementioned systems class, my attempts at group collaboration via version control software failed: they were ""comfortable"" emailing file updates to each other.

Perhaps it takes working on a project of actual import, with actual stakes at hand, to desire the benefits of version control, or maybe it takes the characterization of a piece of software that occurs in the longer lifetime of a project in a professional environment which is missing from the flash-in-the-pan school projects. I just cannot imagine going back to timestamped WIP comments in files in shared directories or emailing new versions around. Version control makes it much easier to see who changed what, where and when, and they _should_ write why. And it's not only for collaboration: I use version control on all of my personal projects so that I can track progress over time and find the changes that caused bugs as quickly as possible.

Version control is not strictly a practice of _computer science_, but it is invaluable to the practice of _software engineering_.",4lx5mu,t3_4lx5mu,UnifiedHearts,,Comment,22,0,22
d3r6phf,2016-05-31 21:33:58-04:00,TheCommador,,"It's difficult to justify using it on, as you said, flash in the pan school projects. Do you recommend a certain resource for learning git, or other version controls?",4lx5mu,t1_d3qxjbk,For_Iconoclasm,,Reply,6,0,6
d3r8005,2016-05-31 22:08:58-04:00,For_Iconoclasm,,"I forgot to mention that git is, indeed, my weapon of choice, but it's also the only one I ever learned.

It being a first-class citizen of my workflow is what allows me to justify using it, even for small or short projects. Of course, learning it in the first place is the point of friction. Unfortunately, I don't have a fantastic resource for learning git quickly and effectively. The [book on the official site](https://git-scm.com/book/en/v2) is where I got a lot of my initial knowledge, but additional knowledge was accrued piecemeal through experience and Google searches. Truth be told, I still Google complicated operations if I ever need to use them, but the basics stick well and work well.",4lx5mu,t1_d3r6phf,TheCommador,,Reply,5,0,5
d3rdafm,2016-06-01 00:47:08-04:00,Afro_Samurai,,"I like the Atlassian tutorial to start:

https://www.atlassian.com/git/tutorials/",4lx5mu,t1_d3r6phf,TheCommador,,Reply,4,0,4
d3renxx,2016-06-01 01:40:42-04:00,stumblelightly22,,"!Remindme on June 3, 2016",4lx5mu,t1_d3rdafm,Afro_Samurai,,Reply,1,0,1
d3reo8h,2016-06-01 01:41:02-04:00,RemindMeBot,,"I will be messaging you on [**2016-06-03 05:40:57 UTC**](http://www.wolframalpha.com/input/?i=2016-06-03 05:40:57 UTC To Local Time) to remind you of [**this link.**](https://www.reddit.com/r/AskComputerScience/comments/4lx5mu/what_should_every_programmer_knowhave/d3renxx)

[**3 OTHERS CLICKED THIS LINK**](http://www.reddit.com/message/compose/?to=RemindMeBot&subject=Reminder&message=[https://www.reddit.com/r/AskComputerScience/comments/4lx5mu/what_should_every_programmer_knowhave/d3renxx]%0A%0ARemindMe!  on June 3, 2016) to send a PM to also be reminded and to reduce spam.

^(Parent commenter can ) [^(delete this message to hide from others.)](http://www.reddit.com/message/compose/?to=RemindMeBot&subject=Delete Comment&message=Delete! d3reo8h)

_____

|[^(FAQs)](http://np.reddit.com/r/RemindMeBot/comments/24duzp/remindmebot_info/)|[^(Custom)](http://np.reddit.com/message/compose/?to=RemindMeBot&subject=Reminder&message=[LINK INSIDE SQUARE BRACKETS else default to FAQs]%0A%0ANOTE: Don't forget to add the time options after the command.%0A%0ARemindMe!)|[^(Your Reminders)](http://np.reddit.com/message/compose/?to=RemindMeBot&subject=List Of Reminders&message=MyReminders!)|[^(Feedback)](http://np.reddit.com/message/compose/?to=RemindMeBotWrangler&subject=Feedback)|[^(Code)](https://github.com/SIlver--/remindmebot-reddit)|[^(Browser Extensions)](https://np.reddit.com/r/RemindMeBot/comments/4kldad/remindmebot_extensions/)
|-|-|-|-|-|-|",4lx5mu,t1_d3renxx,stumblelightly22,,Reply,1,0,1
d3reqd7,2016-06-01 01:43:35-04:00,darthandroid,,"I use it all the time for personal/school projects. Having the ability to quickly checkpoint/bookmark your work is amazing. That moment when you suddenly prototype something and get it working, you can commit your workspace and then start iterating to improve it, without worrying that you'll break it and not know how to get back to where you were.",4lx5mu,t1_d3r6phf,TheCommador,,Reply,2,0,2
d3rgmch,2016-06-01 03:11:24-04:00,kirang89,,[Git for Computer Scientists](http://eagain.net/articles/git-for-computer-scientists/) and [Git from Bottom Up](http://ftp.newartisans.com/pub/git.from.bottom.up.pdf) are my goto resources.,4lx5mu,t1_d3r6phf,TheCommador,,Reply,1,0,1
d3r1xnj,2016-05-31 19:26:09-04:00,rfinger1337,,"www.stackoverflow.com
",4lx5mu,t3_4lx5mu,UnifiedHearts,,Comment,12,0,12
d3qvpoi,2016-05-31 16:56:10-04:00,dirks74,,Basic knowledge about regular expressions.,4lx5mu,t3_4lx5mu,UnifiedHearts,,Comment,9,0,9
d3r1xqa,2016-05-31 19:26:12-04:00,Bottled_Void,,"You can save yourself a whole bunch of time if you get good at it.

    (/\*([^*]|[\r\n]|(\*+([^*/]|[\r\n])))*\*+/)|(//.*)

Is a pretty common one that strips all comments from C.


But you can write a few simple ones to format data and such pretty easily. Grouping is really useful part of it to know.",4lx5mu,t1_d3qvpoi,dirks74,,Reply,5,0,5
d3r3c65,2016-05-31 20:02:52-04:00,Mukhasim,,You can also save a bunch of trouble if you know about alternative delimiters.,4lx5mu,t1_d3r1xqa,Bottled_Void,,Reply,5,0,5
d3r1yqt,2016-05-31 19:26:54-04:00,rfinger1337,,"I have a basic knowledge of them, but have almost never used them.  Why of all things would you point out this one?

If I had to pick one thing, I would say collections.",4lx5mu,t1_d3qvpoi,dirks74,,Reply,5,0,5
d3quclt,2016-05-31 16:26:36-04:00,hemenex,,"Ascii table always available, even in space.",4lx5mu,t3_4lx5mu,UnifiedHearts,,Comment,20,0,20
d3qvt5g,2016-05-31 16:58:17-04:00,mightychip,,Bet Johansson was pissed she left hers on Mars.,4lx5mu,t1_d3quclt,hemenex,,Reply,10,0,10
d3qywp8,2016-05-31 18:10:09-04:00,FondSteam,,I was seriously trying to artsy one up and get it tattooed,4lx5mu,t1_d3quclt,hemenex,,Reply,4,0,4
d3qyr7m,2016-05-31 18:06:27-04:00,Madsy9,,"A good programmer is not defined by his/her tools or even any particular ~~knowledge~~ facts. In my opinion you can't say ""well Alice knows C++, C# and F# and that's why she is a programmer"". The learning never stops, and the *programming* part transcends any particular programming language. If you know and understand the semantic powers that programming languages have, then learning a new language is just about learning a new syntax.

However in my opinion, programmers *should* know the optimal or close to optimal solution for problems they encounter in their daily work. Which means they should also be able to discover when they don't. And when they lack some knowledge to solve a problem, they should have a strong enough math and comp-sci background to find, read and understand the technical papers they need. Whether it's a whitepaper from ACM, SIGGRAPH or one of the W3 specifications. And with the point above, it follows that a programmer should enjoy learning and learn on the job even when not explicitly told so.

I would never hire a programmer on the sole background of knowing the MSDN or Javadocs by heart, or because they know language X, Y or API Z. I would hire them based on their problem solving skills and attitude towards the profession.

In shorter words: You don't have to know how to implement QuickSort, but you should be able to figure out how to when/if required. And even when not knowing how to implement QuickSort you should know that Bubblesort is awful when sorting a huge dataset and that O(n log n) sorting algorithms are better.",4lx5mu,t3_4lx5mu,UnifiedHearts,,Comment,10,0,10
d3r2epu,2016-05-31 19:38:22-04:00,rfinger1337,,"I agree entirely, and would add that there are some basics that everyone should know (collections, flow control, how to use source control, what clean code is and why it's important.)",4lx5mu,t1_d3qyr7m,Madsy9,,Reply,3,0,3
d3r2tid,2016-05-31 19:49:03-04:00,Madsy9,,"Collections, data structures and flow control goes under ""semantic powers that programming languages have"" :)

Same thing applies to type systems, macros, closures, lexical/dynamic/block scope, pattern matching, continuations (and special cases of continuations such as coroutines), multiple dispatch, and so on.

And you won't learn about all such semantics by learning only one language, simply because no single language exposes all of them.",4lx5mu,t1_d3r2epu,rfinger1337,,Reply,3,0,3
d3r32a1,2016-05-31 19:55:32-04:00,rfinger1337,,"Yep, that's a valid point.",4lx5mu,t1_d3r2tid,Madsy9,,Reply,1,0,1
d3r33zy,2016-05-31 19:56:48-04:00,EnterprisePaulaBeans,,"A bunch of good programming books, ESPECIALLY *Code Complete*. It's such a great book: I read a few chapters at least every month.

That said, Jeff Atwood has [a complete list](https://blog.codinghorror.com/recommended-reading-for-developers/).",4lx5mu,t3_4lx5mu,UnifiedHearts,,Comment,6,0,6
d3r6vlx,2016-05-31 21:38:40-04:00,TheCommador,,Every month? It's nine hundred pages.,4lx5mu,t1_d3r33zy,EnterprisePaulaBeans,,Reply,2,0,2
d3r6yla,2016-05-31 21:40:53-04:00,EnterprisePaulaBeans,,"You found me out!

I should clarify: I reread at least a few chapters of it every month.",4lx5mu,t1_d3r6vlx,TheCommador,,Reply,5,0,5
d3r4jvf,2016-05-31 20:35:43-04:00,yes_thats_right,,"They should know:

5) Test Driven Development. Not because they need to use this methodology but because developers should have the right mindset to always be considering edge cases.

4) SLDC. You are just one piece of the puzzle, not everything.

3) Data structures. 90% of what you do is manipulating these, make sure you are using the right ones.

2) Algorithms. This sorts the great developers from the average.

1) Know what your customer needs and learn how to communicate with them. Building the wrong thing is useless even if they asked for it. Similarly, don't assume they don't understand what they need, often it is you who doesn't understand.",4lx5mu,t3_4lx5mu,UnifiedHearts,,Comment,6,0,6
d3rblle,2016-05-31 23:50:24-04:00,doverthere,,"A good IDE. A good IDE is built to save you lots and lots of time. And this can make all the difference in the world when developing, especially when there are many deadlines to meet and you need all the spare time you can get. And if you're working on big projects like designing frameworks for your software, you'll want to look for a premium IDE with lots of support and lots of features. Lightweight IDE's won't always cut it.
  
I'm developing mainly in Php and our company provides licenses for Phpstorm. It's an amazing IDE with plugin support for many other languages like Javascript, HTML, Bash, Perl, etc and supports VCS integration and database connections. Another of my favorite features of phpstorm is the ability to create deployment paths. You can insantly deploy files or directories or your entire project to your local or remote dev environments. It evens shows a diff between the file(s) being deployed and the ones currently on the server. This saves me from having to manually transfer files or needing to write my own transfer scripts.",4lx5mu,t3_4lx5mu,UnifiedHearts,,Comment,2,0,2
d3rgvwc,2016-06-01 03:25:01-04:00,kirang89,,"* A decent idea of how all the internal parts of a computer fit together. I'd recommend [Computer Systems: A Programmer's Perspective](https://www.amazon.com/Computer-Systems-Programmers-Perspective-3rd/dp/013409266X/ref=sr_1_1?s=books&ie=UTF8&qid=1464765819&sr=1-1&keywords=computer+systems+a+programmer%27s+perspective+3rd+edition) and [Code: The Hidden Language of Computer Hardware and Software](https://www.amazon.com/Code-Language-Computer-Hardware-Software/dp/0735611319/ref=sr_1_1?s=books&ie=UTF8&qid=1464765789&sr=1-1&keywords=code+the+hidden+language+of+computer+hardware+and+software)
* A good book to solidify your principles like [Code Complete](https://www.amazon.com/gp/product/0735619670/ref=pd_lpo_sbs_dp_ss_2?pf_rd_p=1944687742&pf_rd_s=lpo-top-stripe-1&pf_rd_t=201&pf_rd_i=020161622X&pf_rd_m=ATVPDKIKX0DER&pf_rd_r=DPADY8EP5EB7CY0GMVM4) or [Pragmatic Programmer](https://www.amazon.com/Pragmatic-Programmer-Journeyman-Master/dp/020161622X?ie=UTF8&*Version*=1&*entries*=0).
* Really understanding the nuts and bolts of a text editor that can preferably work with any language and is cross-platform (Emacs, Vim, Sublime Text to name a few)
* A good understanding of a version control system like Git. I'd recommend [Git for Computer Scientists](http://eagain.net/articles/git-for-computer-scientists/) and [Git from Bottom Up](http://ftp.newartisans.com/pub/git.from.bottom.up.pdf)",4lx5mu,t3_4lx5mu,UnifiedHearts,,Comment,2,0,2
d3qylgz,2016-05-31 18:02:38-04:00,myfavoriteanimal,,Touch typing,4lx5mu,t3_4lx5mu,UnifiedHearts,,Comment,4,0,4
4lx09f,2016-05-31 15:24:34-04:00,Graysless,"What is the best way to learn programming, and what language should I start with?","I am going to school for electrical engineering in the fall and I wanted to learn a little programing over the summer. What are the best programming languages for beginning electrical engineering majors? Also, how should I go about learning this language?",,,,,Submission,1,0,1
d3qu47n,2016-05-31 16:21:37-04:00,theseldomreply,,"I would suggest learning C. As an EE you will probably spend some time with arduinos which use a variant of C.

Also if you think you will stick with EE I would just buy an arduino now and start playing around with it. You will simultaneously learn the basics of some simple circuitry and a bit about c.

If you can learn c you will understand the basics of computing, memory control, etc. However you will probably only be writing small command line utilities with c. If you want to do something more software oriented try out python or java maybe.",4lx09f,t3_4lx09f,Graysless,,Comment,1,0,1
d3qy1sl,2016-05-31 17:49:46-04:00,Graysless,,"Thanks, what resource would you suggest for learning c? ",4lx09f,t1_d3qu47n,theseldomreply,,Reply,1,0,1
d3vu4u3,2016-06-04 12:23:09-04:00,Ki1103,,"Hey I've noticed that you haven't had a response yet so I'll step in.  I personally haven't learned c (yet) but I've repeatedly been recommended CS50x which is a online version of Harvard's intro to computer science course

",4lx09f,t1_d3qy1sl,Graysless,,Reply,1,0,1
d3qyyjc,2016-05-31 18:11:23-04:00,Madsy9,,"The ""best"" way to learn programming is the method which is the most efficient for you. There is no single way of learning which is optimal for everyone.

As far as languages go, it generally doesn't matter which one you start with. But since you want a language useful for EE, I'd say C or C++ as well as assembly for the computer architectures you use. Be it ARM9, ARM Cortex, Atmel Atmega, etc.",4lx09f,t3_4lx09f,Graysless,,Comment,1,0,1
4lsxas,2016-05-30 22:12:25-04:00,csGradStudent647,Upcoming Adv. computer architecture exam: Grad student seeking help.," So, I will be writing Advanced comp architecture in a few days. 

&nbsp;
The prescribed text book is Computer Architecture, 5th Edition A Quantitative Approach- Hennessy and Patterson .
&nbsp;
 I am having difficulty in understanding the concepts from the textbook. The professors and TA are not being helpful. I don't want to fluke in this exam. 
&nbsp;
Are there any websites or online resources or online tutors that would be helpful?
",,,,,Submission,1,0,1
d3qd5mg,2016-05-31 09:52:55-04:00,zefcfd,,    i believe there are pretty good MIT lectures online for computer architecture,4lsxas,t3_4lsxas,csGradStudent647,,Comment,1,0,1
4lrwo6,2016-05-30 18:10:01-04:00,nyqn,How to get most out of distributed systems papers?,"I'm reading papers like [GFS](http://static.googleusercontent.com/media/research.google.com/en//archive/gfs-sosp2003.pdf), and [Borg](http://static.googleusercontent.com/media/research.google.com/en//pubs/archive/43438.pdf) and something doesn't seem right. No matter how many I read, I feel like I'll be no better at designing my own systems. And also I never get ""right, that is a genius idea."" It feels like a lot of implementation details that make me think why didn't they do it some other way. Am I spinning wheels here or is this normal for a beginner?",,,,,Submission,8,0,8
d3q1da3,2016-05-31 00:41:35-04:00,theobromus,,"I would actually start at a lower level and look at the basic algorithms, rather than specific implementations of them. In particular, you can read about:

* [Paxos algorithm](https://en.wikipedia.org/wiki/Paxos_(computer_science\))

* [Atomic commits](https://en.wikipedia.org/wiki/Atomic_commit), the [Two Generals Problem](https://en.wikipedia.org/wiki/Two_Generals%27_Problem), and [Two-Phase Commit](https://en.wikipedia.org/wiki/Two-phase_commit_protocol).

* Replication and [Eventual Consistency](https://en.wikipedia.org/wiki/Eventual_consistency) and perhaps also the [CAP Theorem](https://www.infoq.com/articles/cap-twelve-years-later-how-the-rules-have-changed).

As far as designing your own systems, GFS and Borg are systems you shouldn't be reinventing, all of the major cloud providers have this as part of their base infrastructure (and believe me it's actually complicated and hard to get right). Mostly I think appreciating their complexity makes it clearer why you want to to build on a system that handles this for you.

It is useful of course to understand how they work and what the basic problems of distributed computation are.",4lrwo6,t3_4lrwo6,nyqn,,Comment,2,0,2
d3q1dbo,2016-05-31 00:41:37-04:00,WikipediaPoster,,"**[Paxos (computer science)](https://en.wikipedia.org/wiki/Paxos_\(computer_science\))** 

 ---  

>Paxos is a family of protocols for solving consensus in a network of unreliable processors. Consensus is the process of agreeing on one result among a group of participants. This problem becomes difficult when the participants or their communication medium may experience failures.

 >Consensus protocols are the basis for the state machine replication approach to distributed computing, as suggested by Leslie Lamport and surveyed by Fred B. Schneider. State machine replication is a technique for converting an algorithm into a fault-tolerant, distributed implementation. Ad-hoc techniques may leave important cases of failures unresolved. The principled approach proposed by Lamport et al. ensures all cases are handled safely.

 

 --- 

 ^I ^am ^a ^bot. ^Please ^contact ^[/u/GregMartinez](https://www.reddit.com/user/GregMartinez) ^with ^any ^questions ^or ^feedback.",4lrwo6,t1_d3q1da3,theobromus,,Reply,1,0,1
d3q1ddh,2016-05-31 00:41:41-04:00,WikipediaPoster,,"**[Paxos (computer science)](https://en.wikipedia.org/wiki/Paxos_\(computer_science\))** 

 ---  

>Paxos is a family of protocols for solving consensus in a network of unreliable processors. Consensus is the process of agreeing on one result among a group of participants. This problem becomes difficult when the participants or their communication medium may experience failures.

 >Consensus protocols are the basis for the state machine replication approach to distributed computing, as suggested by Leslie Lamport and surveyed by Fred B. Schneider. State machine replication is a technique for converting an algorithm into a fault-tolerant, distributed implementation. Ad-hoc techniques may leave important cases of failures unresolved. The principled approach proposed by Lamport et al. ensures all cases are handled safely.

 

 --- 

 ^I ^am ^a ^bot. ^Please ^contact ^[/u/GregMartinez](https://www.reddit.com/user/GregMartinez) ^with ^any ^questions ^or ^feedback.",4lrwo6,t1_d3q1da3,theobromus,,Reply,1,0,1
d3pzcro,2016-05-30 23:37:49-04:00,instant_cat_torque,,"Yeh that's pretty normal. While a lot of stuff never really clicked for me until after I had read a lot, make sure you are reading older papers for understanding why things are done in. Historical context. For instance Borg is a recent paper but from a research perspective it has been influenced by decades of systems and papers before it. ",4lrwo6,t3_4lrwo6,nyqn,,Comment,1,0,1
d3qdlz4,2016-05-31 10:05:37-04:00,bacondev,,"I don't have much to say beyond what has already been said, but research papers aren't generally written with a beginner audience in mind.",4lrwo6,t3_4lrwo6,nyqn,,Comment,1,0,1
4lrs4k,2016-05-30 17:41:10-04:00,rakup_master,What are some good computer science journal with high acceptance rate and good impact factor?,I want to publish my first research paper. But I am not sure if it is good enough to get selected in the top journals. I want to publish it in a good journal as it's an original idea. What are some good journals paid or free that accept easily?,,,,,Submission,11,0,11
d3po0a2,2016-05-30 18:42:31-04:00,mpdehnel,,"I think you've missed the point of ""high impact factor"" if you also want it to have a v high acceptance rate. Secondly, why a journal? Most CS research is (at the first instance) published in conferences. ",4lrs4k,t3_4lrs4k,rakup_master,,Comment,19,0,19
d3psv70,2016-05-30 21:00:45-04:00,sandwichsaregood,,"Yeah, OP should look at publishing it in a conference proceedings instead of a journal. A lot of (most?) conference proceedings are peer reviewed (which is the most important thing) while being a little more lenient and IMO are a much better avenue for a first paper. Journals can be pretty elitist, especially those with high IF: if you aren't at least a PhD candidate they'll often turn you away.

Plus conferences are much more fun. You get to travel and meet people. Journals are stuffy and uptight.

Edit: conference proceedings can also get your foot in for a journal publications. A lot of the high profile journals have associated conferences and will invite the best summaries and presentations to publish a full length article. For OP's benefit, conference proceedings papers are usually 1-5 pages (depending) plus a 15 minute presentation if accepted. Journal articles run from 6-40 depending on the journal, though 15 or 40 pages are probably the most common formats.

Conference proceedings get you a lot more attention unless you are well enough known that people actually pay attention to what you publish. They also aren't any less important and many top researchers publish and present in conferences.",4lrs4k,t1_d3po0a2,mpdehnel,,Reply,9,0,9
d3qcrub,2016-05-31 09:41:44-04:00,worst,,">A lot of (most?) conference proceedings are peer reviewed (which is the most important thing) while being a little more lenient and IMO are a much better avenue for a first paper.

Yes and no. *All* good conferences will be peer reviewed. A non peer reviewed conference proceeding holds zero weight.

To that end, I find that it is way easier  to get into a journal just because of the revision process (although it takes a lot longer!) Most conferences your submission is evaluated and you are either in or out. Journals give a chance for revisions.


>Journals can be pretty elitist, especially those with high IF: if you aren't at least a PhD candidate they'll often turn you away.

Never seen this honestly. It's probably just more of a case that non-PhDs are much less likely to submit publishable work?

>Edit: conference proceedings can also get your foot in for a journal publications. A lot of the high profile journals have associated conferences and will invite the best summaries and presentations to publish a full length article.

From experience, fast tracked papers are almost *always* those that were nominated or won best paper awards.

I.e., cream of the crop to begin with.

>For OP's benefit, conference proceedings papers are usually 1-5 pages (depending) plus a 15 minute presentation if accepted. Journal articles run from 6-40 depending on the journal, though 15 or 40 pages are probably the most common formats.

Ehhh. Most conference proceedings are going to be 10-14 pages. Workshop papers will usually be 5-6.

Journal papers are all over the place, but at least some ACM journals have recently started capping submissions at like 14 pages. Kinda weird...

>Conference proceedings get you a lot more attention unless you are well enough known that people actually pay attention to what you publish. They also aren't any less important and many top researchers publish and present in conferences.

Honestly, minus a few specific journals (e.g., TON) most of my colleagues value conferences higher and often don't even bother for a journal extension unless we just have 20% new work kinda laying around.
",4lrs4k,t1_d3psv70,sandwichsaregood,,Reply,5,0,5
d3qfhuc,2016-05-31 10:53:47-04:00,sandwichsaregood,,"I'm speaking mostly from experience with the more mathematically inclined journals and conferences, so I will defer to you since it seems you have more experience with the pure CS ones.

Re peer review for proceedings, I was mostly referring to abstract-only conferences. IEEE for instance often prefers to take one page abstracts. They are reviewed and filtered, but not formally peer reviewed. Those are the ones that tend to be more elitist in my experience as well.",4lrs4k,t1_d3qcrub,worst,,Reply,2,0,2
d3qlnks,2016-05-31 13:15:54-04:00,worst,,">I'm speaking mostly from experience with the more mathematically inclined journals and conferences, so I will defer to you since it seems you have more experience with the pure CS ones.

To be fair, I'm mostly involved in systems work, and there are likely subtleties in fields I don't mess with. This is probably one of them.

>Re peer review for proceedings, I was mostly referring to abstract-only conferences. IEEE for instance often prefers to take one page abstracts. They are reviewed and filtered, but not formally peer reviewed. Those are the ones that tend to be more elitist in my experience as well.

Ah. Idk about this at all. I rarely publish IEEE (these venues have started to get kind of a bad name as opposed to ACM venues). I didn't even know IEEE had abstract only venues to be honest. There are definitely short papers/extended abstracts/posters at ACM venues but in my experience the bar is way lower and there tends to be greater representation from ""lower tier labs"" (I.e., less famous people).

There definitely is a ""mafia"" at certain venues though, and in non double blind venues having the right name on your paper can certainly sway reviewers, but I've also been on PCs where big name authors get rejected without a bat of the eye.

Bottom line though: OP is looking for a venue with two mutually exclusive properties: good and easy to get in to :)

It would certainly help if OP gave a hint as to the field his work is in...
",4lrs4k,t1_d3qfhuc,sandwichsaregood,,Reply,2,0,2
d3qlzbz,2016-05-31 13:23:09-04:00,UncleMeat,,"Jumping in here to provide another perspective. In my field (security), conferences are way more prestigious than journals, which are not frequently read. S&P, the premier conference, had a 20 page limit on submissions this year so its not like you are only getting a small portion of the research. Sometimes conference papers later get expanded into a journal article but its unusual. Conferences are the unit of currency.

This is just a weird property of some subfields in CS. The more mathy people tend to use journals (in my experience) more frequently but outside of heavy theory its almost exclusively conferences.",4lrs4k,t1_d3psv70,sandwichsaregood,,Reply,2,0,2
d3qo28s,2016-05-31 14:08:57-04:00,worst,,"Even in theory you have stuff like STOC and POPL.

Things still move fast in CS, so I guess conferences make more sense (also, our conferences tend to be way more heavily peer reviewed from what I know of other fields).

I mean, there are conferences like SOSP and OSDI that are every two years because they are expecting really well developed work. There are even conferences that have moved towards a hybrid system with rolling submissions like VLDB, but even there, the best of the submissions still get presented at a conference. (Incidentally, this is why it's often hard for non CS people to judge, at all, a CS CV... they see a bunch of conferences and think the CV sucks :))",4lrs4k,t1_d3qlzbz,UncleMeat,,Reply,1,0,1
d3q7r1d,2016-05-31 06:10:42-04:00,ProfessorAlgorithm,,"Acceptance rate is often inversely proportional to impact factor.  It is the very act of turning away poorly written papers, or those containing mediocre ideas, that makes these journals so impactful.

While lower impact journals and conferences proceedings can have great ideas in them, they are less dense with these ideas.  High impact journals, then, make it easy for researchers to catch up on developments since almost of their ideas are worth reading about.

High impact journals often advertise their low acceptance rate, in fact.  The more articles are rejected, the more thorough their screening/peer review process, which should improve the average quality the papers accepted.

Why don't you post the topic area, and someone in that area may be able to make a few suggestions?

A suggested strategy, that will take a very long time, would be to submit the paper to progressively lower impact journals, eventually trying for conferences.  If you are a master's student or undergraduate, you may have a tough time.  If a doctoral candidate, your luck is better.  A better strategy is to take advantage the experience and expertise of your supervisor.  She/he probably has some intuition where the paper will get accepted.  Their name is probably on the paper, anyway, right?",4lrs4k,t3_4lrs4k,rakup_master,,Comment,5,0,5
d3qcof8,2016-05-31 09:38:56-04:00,worst,,">I want to publish my first research paper.

Cool! Publishing is incredibly rewarding :)

>But I am not sure if it is good enough to get selected in the top journals. I want to publish it in a good journal as it's an original idea.

Not to be mean, but, it's probably not good enough to get selected for a top journal. They are top venues for a variety of reasons, one of which is that they have a reputation for rejecting even really good work.

>What are some good journals paid or free that accept easily?

Two things.

1. There is basically no such thing as a good pay-to-play venue.
2. There is basically no such thing as a good venue that is easy to be accepted into. The selectivity of a venue is one of the defining characteristics that makes it good.

Related, but, have you had any experts give you comments on your manuscript? One of the biggest pitfalls I see people fall into is that what they think is a novel idea has already been explored (finding related work is an art!) The second biggest pitfall is that they just don't know how to write a paper yet. There are a lot of things that we expect to see in a top quality publication, many of which are not overly apparent unless you've written/read a bunch of papers already.

Usually an established professor helps a student gain experience in this matter. You should try to hunt someone down to help you out too :)
",4lrs4k,t3_4lrs4k,rakup_master,,Comment,3,0,3
d3prz7i,2016-05-30 20:36:49-04:00,None,,[deleted],4lrs4k,t3_4lrs4k,rakup_master,,Comment,-10,0,-10
d3q54fr,2016-05-31 03:30:06-04:00,mpdehnel,,"Yeah, that's cute. I could list the names of some journals but that wouldn't be remotely helpful: OP hasn't even specified the area of research so anything I suggested would be meaningless beyond saying it will probably end up in LNCS. Secondly I think it's more helpful to tell OP that if you want to present your CS research, doing so in a journal is not normally the way it's done: the usual route (within CS, for whatever reason) is to submit it to a conference (peer reviewed, many but by no means all are tough to get into), and then if there is more work that couldn't fit in the 8-16 page conference paper (depending on the venue), OP could then submit an extended version of it to a journal. It is my impression that journals are much more likely to accept work (and indeed the work is more likely to be ready for a journal) if a shorter version of it has already been accepted into a decent conference. 

So, that's why I didn't just give OP some arbitrary journal suggestions, and why I wasn't ""being a dick about it"". ",4lrs4k,t1_d3prz7i,None,,Reply,8,0,8
4lnldg,2016-05-29 22:56:13-04:00,MakeItSTYC,Are there circumstances in which old technology is harder to decrypt than new technology?,"More specifically, in the context that the U.S. government is using ""70's"" computer technology as the tool of communication by which the State Department operates.",,,,,Submission,17,0,17
d3oxmqa,2016-05-30 03:31:25-04:00,visvis,,"You mean cryptography using old machines? No, that should never be the case. The idea in cryptography is that the effort to encrypt/decrypt with the key must be much less than the effort to break the code without the key. The low computational power of old computers means that they have to work with smaller keys. As the number of possible keys is exponential in the key size, it becomes relatively much easier to break the code with the more powerful newer machines.

One exception is the one-time pad, which is fundamentally unbreakable, but it is unpractical because keys are huge and key distribution needs to be done out of band. In this case, old computers have no computational (dis)advantage, though limited storage could be a serious issue when needing to store lots of key data.",4lnldg,t3_4lnldg,MakeItSTYC,,Comment,5,0,5
d3oyhqh,2016-05-30 04:19:47-04:00,Arcosim,,"The [Voynich manuscript](https://en.wikipedia.org/wiki/Voynich_manuscript) is still a mystery. Deep inside I wish it's just gibberish, a centuries long ultimate act of trolling. ",4lnldg,t3_4lnldg,MakeItSTYC,,Comment,4,0,4
d3p3n1q,2016-05-30 09:07:28-04:00,Spoor,,"Five centuries ago, people probably had more important things to do with their time than trolling though. ",4lnldg,t1_d3oyhqh,Arcosim,,Reply,0,0,0
d3p5bh9,2016-05-30 10:08:14-04:00,shotdown_by_maria,,"On the US military using floppies to coordinate its nuclear operations, there was an [interesting thread](https://news.ycombinator.com/item?id=11772966) over at Hacker News a few days ago. From a security standpoint, the main arguments for continuing to use floppies is that unlike a network connection they can't be hacked remotely, and unlike usb drives many of them don't have firmware (so you aren't going to have [this](http://radxa.com/mw/images/e/e6/RK_Driver_Assistant_Windows_Security.jpg) pop up at the crucial moment). Furthermore because the older technology is much simpler, the engineers can understand much more thoroughly what's going on and therefore be sure there isn't going to be a problem.",4lnldg,t3_4lnldg,MakeItSTYC,,Comment,4,0,4
d3pka23,2016-05-30 16:58:49-04:00,DairTheBear,,"Well not usually. The technology and its derivatives would have to be extremely old and virtually unheard of. For example, the navajo code talkers used a very old, unknown language. ",4lnldg,t3_4lnldg,MakeItSTYC,,Comment,2,0,2
d3p22fq,2016-05-30 07:56:06-04:00,MadTux,,"Well one time pads are completely unbreakable and were used back in ww2, but they aren't particularly useful.",4lnldg,t3_4lnldg,MakeItSTYC,,Comment,1,0,1
4lmtw0,2016-05-29 19:31:26-04:00,earthceltic,"Should I convert SQL database website pages to static pages for a potentially popular site? How do I do that, if so?","A user fills in a web form and their database populates. The information provided will need to be viewed by MANY other users, surrounded by other code that makes it browser-friendly. Think like Wordpress, but with no comments or other little interactions that require the database afterward. Should I somehow generate static pages per post? I was thinking python might be able to do it just fine although I'd have to figure out how to trigger it from a database insert. 

The reason I'm researching this is because I understand that MySQL has issues with tons of queries on a very popular website, and I'd like to code this linux web app to be as efficient on hardware as humanly possible from the ground up. Keeps the costs down, keeps the headache minimal later on. I fully realize that I might be going down the wrong path and that MySQL may be just fine with good programming or settings, but I'd like to have some of you experts weight in. Thanks!",,,,,Submission,2,0,2
d3oqa1h,2016-05-29 22:49:00-04:00,Pseudofailure,,"If you're only planning on basic select queries, a SQL database should be fine. The performance hits are mostly noticeable if you have high traffic sites, and complex queries: based on your description, most of your queries would just be single index selects like `SELECT post, title FROM posts WHERE post_id=:pid` which are very simple and shouldn't be a significant bottleneck even on high traffic sites. 

On top of that, I'd use any kind of database over pure static files on disk. Management will be much easier over time. 

Finally, just a warning, I think you might be prematurely optimizing your project. It's great to plan out something and make good choices from the beginning, but trying to optimize for edge cases is just going to slow you down and introduce likely unneeded complexity. ",4lmtw0,t3_4lmtw0,earthceltic,,Comment,3,0,3
d3om196,2016-05-29 20:40:15-04:00,billdroman,,"MySQL has pretty good read throughput (for example, see [this site](https://www.mysql.com/why-mysql/benchmarks/), although they have multiple connected clients to get to those QPS numbers). You should take care of basic performance optimizations, like making sure your ""recent posts"" and ""post details"" queries use indices, but the further optimizations you're describing are unnecessary for now and may be so indefinitely.

Get some server latency monitoring up, too, so you can tell if/when latency becomes an issue and what the main contributors are.",4lmtw0,t3_4lmtw0,earthceltic,,Comment,2,0,2
d3p9l0d,2016-05-30 12:13:33-04:00,xiongchiamiov,,"It depends on your situation. We have no idea what ""lots"" means to you, or what resources you currently have available, or how your app uses those resources.

What I can tell you is that the web only works at all because of caching, and usually multiple layers of it. Pregeneration of full pages is one method, but there are plenty of others.

I also generally recommend that everyone have Varnish sitting at the front of their stack, because it can save you in a pinch.",4lmtw0,t3_4lmtw0,earthceltic,,Comment,1,0,1
d3pba06,2016-05-30 12:59:09-04:00,armpit_puppet,,"One thing I don't think previous comments have mentioned is that your web server can probably cache the rendered pages for you. If I've understood your post and your data is basically a user enters some data and the page never changes, you can set a reasonable TTL so that the DB is only hit when the page has expired. This could be set by a few lines of config and requires no change to the application code. 

Another option is something like Cloudflare which is geared towards this kind of thing. ",4lmtw0,t3_4lmtw0,earthceltic,,Comment,1,0,1
4llpki,2016-05-29 15:00:08-04:00,otarana,Help programming bot,"I want to make a bot to play a flash game. I once downloaded one that used to play Farmville. It would never have to open the actual flash game, it appeared to connect directly to the server and send commands. How would I go about starting to learn how to do this? How do I know what commands to send or where to send them to? ",,,,,Submission,2,0,2
d3ocjw2,2016-05-29 15:54:01-04:00,visvis,,"You'd need to reverse engineer the protocol between the flash game an its server. There are generally two ways to do this:

* Decompile the game and derive the protocol from the code
* Intercept the messages exchanged with the server using a tool such as Wireshark and derive the meaning from that

Unless the protocol is very simple or the things you're trying to do are an exact copy of something you can do yourself through the flash app, the first approach is probably easiest. However, it can be hindered by obfuscation.

Note that you may want to consider whether there are legal issues with this. In any case, doing this probably violates the TOS and could get you banned if they find out. In addition there could be trademark and/or copyright issues depending on what exactly you're doing.
",4llpki,t3_4llpki,otarana,,Comment,6,0,6
4lkt0m,2016-05-29 11:23:59-04:00,coaster367,"Is there a name/classification given to planar graphs that have the most number of edges? (i.e., |E| = 3*|V|-6)",,,,,,Submission,6,0,6
d3o4jgx,2016-05-29 12:04:45-04:00,PastyPilgrim,,I did two years of planar graph research and never encountered such a name. ,4lkt0m,t3_4lkt0m,coaster367,,Comment,2,0,2
d3ohy6q,2016-05-29 18:31:17-04:00,billdroman,,"Triangulations of the plane? Any such graph should be a triangulation, because if there were a region with 4 or more vertices, it would be possible to add another edge to the graph through that region.",4lkt0m,t3_4lkt0m,coaster367,,Comment,1,0,1
d3olp2d,2016-05-29 20:29:14-04:00,coaster367,,"Thanks, I knew that was true. I was wondering if there was a name to these graphs. ",4lkt0m,t1_d3ohy6q,billdroman,,Reply,1,0,1
d3u2xtg,2016-06-03 00:24:47-04:00,proofbycontradancing,,"I think I've heard the term ""maximally planar"" before.",4lkt0m,t3_4lkt0m,coaster367,,Comment,1,0,1
4lhi6o,2016-05-28 16:54:17-04:00,SteveTheGreat1,Closest sum in sorted list,"Hi guys, 

I'm not quite sure what algorithm to use to calculate a closest sum of floats in a sorted array to a target number. The number of used floats doesn't matter as long as its the closest and the number can be under or over the target number.

Example:  

    List = {0.4, 0.4, 0.4, 1.8, 1.8, 3.0, 3.0, 4.6, 4.6}
    Find closest to 7 would come out to {0.4, 0.4, 1.8, 4.6}.

Any ideas how this could be done with a fairly efficient algorithm?
",,,,,Submission,3,0,3
d3nc4lr,2016-05-28 17:10:51-04:00,davidthefat,,"I'd check if the number itself exists in the set. If not, get the biggest number under the target and go down the list and add the biggest number at or under the target minus the current sum. Keep doing till end of list or target reached",4lhi6o,t3_4lhi6o,SteveTheGreat1,,Comment,5,0,5
d3ng8sa,2016-05-28 19:16:58-04:00,wonger,,"What if you are trying to find the closest sum to 9 and have the list:

(3,3,3,8)

the result should be (3,3,3) but your algorithm would return (8)",4lhi6o,t1_d3nc4lr,davidthefat,,Reply,2,0,2
d3nggia,2016-05-28 19:24:03-04:00,SteveTheGreat1,,"Yeah exactly, that is also something that I need to consider...",4lhi6o,t1_d3ng8sa,wonger,,Reply,1,0,1
d3ngw1z,2016-05-28 19:37:54-04:00,wonger,,Have you tried just writing a brute force method to solve it or you haven't figured that out either,4lhi6o,t1_d3nggia,SteveTheGreat1,,Reply,1,0,1
d3nxy63,2016-05-29 07:56:46-04:00,SteveTheGreat1,,"No I haven't, not really sure how to tackle it. Could you point me in the right direction/give me some instructions? ",4lhi6o,t1_d3ngw1z,wonger,,Reply,1,0,1
d3o4ioz,2016-05-29 12:04:07-04:00,wonger,,"You have you find every possible sub array until you either run out of sub arrays to make or until the sum == the target. to do this, I would probably do a for loop for each index in the array and for each index, call a recursive function which will either concat the next element in the array or skip it and check the sum. ",4lhi6o,t1_d3nxy63,SteveTheGreat1,,Reply,1,0,1
d3nf4kk,2016-05-28 18:40:34-04:00,Gavinmaximus,,"if list is n vales, there are 2^n combinations of 1 and 0s under assumption all vales < target. You could simply take the product of all the vectors and sort.  Alternatively you cast the problem as an optimization and use something like branch and bound",4lhi6o,t3_4lhi6o,SteveTheGreat1,,Comment,1,0,1
d3oui8r,2016-05-30 01:10:37-04:00,Gavinmaximus,,No love,4lhi6o,t1_d3nf4kk,Gavinmaximus,,Reply,1,0,1
d3nodft,2016-05-28 23:35:20-04:00,shotdown_by_maria,,"Sounds like this could be adapted to the [maximum subarray problem](https://en.m.wikipedia.org/wiki/Maximum_subarray_problem), you'd just replace max with the least difference between the sum and the number.
",4lhi6o,t3_4lhi6o,SteveTheGreat1,,Comment,1,0,1
4lebdp,2016-05-27 23:49:16-04:00,Maddogmattdenn,Help me not suck at coding,"Ill be a junior in college for CS this fall but I just know I'm not where I need to be in terms of my coding ability. I've got a pretty decent IT background but I haven't really taken off when it comes to coding. I feel like I'm that guy who took Spanish in high school but is about as fluent as a Mexican kindergartner. I'm passing all my courses but I really want to spend the summer to really become really proficient. I've been coding in c++ but I want to branch out into a myriad of languages. What methods, drills, exercises, projects do you recommend to master coding? ",,,,,Submission,0,0,0
d3mmxwb,2016-05-28 00:04:23-04:00,ChaoticxSerenity,,/r/learnprogramming can help! :) ,4lebdp,t3_4lebdp,Maddogmattdenn,,Comment,3,0,3
d3mpy7o,2016-05-28 02:13:35-04:00,SHappens,,"You might also try one of the code challenge websites like Hackerrank, TopCoder, Codechef, etc. I've never personally tried them, but I know some pretty good coders who love these sites.",4lebdp,t3_4lebdp,Maddogmattdenn,,Comment,2,0,2
d3p7lrq,2016-05-30 11:18:24-04:00,jnevens,,"Python is a popular and easy language to learn and you can do pretty much everything with it. There are many online courses for it, e.g. on Coursera. If you are more into Machine Learning or Data Science, R is also a good language to know. Since you're already familar with C++, Java should look familiar too. ",4lebdp,t3_4lebdp,Maddogmattdenn,,Comment,2,0,2
d3ri1yn,2016-06-01 04:32:05-04:00,arphod,,"Firstly, are you talking about slinging daily 10-line toy programs assigned in class to teach (quite valuable) concepts, or do you want to write structured, complex, nontrivial programs that do something useful? These are very different things. I can't claim to help you ""master coding,"" but I'll take a swing at it for what it's worth.

As far as homework, I don't know what to say, other than FIGURE IT OUT, with enthusiasm. I mean that respectfully! We all have/had to. Shitty as this may sound, if you can't do university CS homework, you can't code. Figuring out how to do it never, ever, EVER stops with programming. I'm stumped regularly. Google that shit, buy a book, ask Dad, whatever it takes: figure it out. 

Now, past that, eventually you will be assigned a real project: for instance, a complex database program that stores real information that persists in files, info that may be retrieved in one way or another whenever the program is run. This is the sort of coding I'll touch on now. 

Learn first to see the problem (or project). Really break it down, digest it mentally. Imagine how you will simplify it into manageable parts (functions or different files containing groups of functions) that will work together and be each easily testable. See the program's structure when you lie down to sleep. See the flow. 

Now at this point, learn to recognize when you've gone down the wrong path. Is there a better way of doing X? Y? How will these decisions affect my outcome? If you have indeed fucked up, back out now, and do it right. It's always better to do it right first, but, shit, I don't always, and neither will you. Coding is about planning, first and foremost. 

When I think I understand my goal, then I choose a language that suits my needs. I will pick one I know, if at all possible. That said, a coder doesn't need to know all languages: a coder needs to know enough about his/her craft to learn any language that suits the need. And pretty damn fast. Now deal with the quirks and twists of your chosen programming language, and sort it out. 

Use your selected language to execute your already well-thought-out goal.

Test. Test. Test. Every. Single. Function. Bit by bit. Write little programs to verify the output of each is what is expected.

Use comments, a readable format, understandable variable and function names, then avoid global variables, double-check boundaries/overflow on arrays, look at memory leak possibilities (or use a language that cleans that shit up: I'm a C coder primarily :) ), and, uh, etc.


My $0.02. This is how I do it. I wish you well.

edit sp",4lebdp,t3_4lebdp,Maddogmattdenn,,Comment,1,0,1
4le8lr,2016-05-27 23:24:40-04:00,Aaronvan,Custom Data Files for Algorithm Testing?,"Is there a web site that generates random data as specified by the user? There are a number of sites that provide random words or random integers; however, I'm looking for something that will generate a combination of datatypes. For example, a file of 500 alternating random words/strings and integers (i.e, keys & values) or 2000 alternating random words, doubles, and special symbols. Downloadable as a text file, either csv or space delimited. Purpose would be to test various sorting & searching algorithms.

StackOverflow crushed me for even asking this question.",,,,,Submission,9,0,9
d3mmrjo,2016-05-27 23:57:54-04:00,zombarista,,Look for a Faker library in your language of choice. They generate great fake data and make your tests and demos really robust.,4le8lr,t3_4le8lr,Aaronvan,,Comment,3,0,3
d3n6o0z,2016-05-28 14:30:04-04:00,Aaronvan,,"Excellent , never heard of Faker libraries. Thanks. ",4le8lr,t1_d3mmrjo,zombarista,,Reply,2,0,2
d3mvi8d,2016-05-28 08:07:13-04:00,Gavinmaximus,,"Why not just generate your own? Just use a random number generator to index into a list of words, concatenate and repeat.",4le8lr,t3_4le8lr,Aaronvan,,Comment,2,0,2
d3n6par,2016-05-28 14:31:10-04:00,Aaronvan,,Not a bad idea.  Probably will go that route. ,4le8lr,t1_d3mvi8d,Gavinmaximus,,Reply,1,0,1
4lbrq4,2016-05-27 13:16:14-04:00,rabbiccu,WIFI Speaker - Raspberry Pi ?,"I want to make a WiFi speaker like Sonos or Wren. My original plans involved one ""home"" bluetooth tx and a few bluetooth ""satelitte"" rx, but after reading I dont think bluetooth has the range I am looking for. I am looking to put these speakers throughout my house, and play my audio from one central location. The audio broadcast from that location will be live audio on a 3.55 Jack.

So what I came up with is a speaker with Raspberry Pi + Wifi inside which will receive audio from the network and put it through to the speakers. There are two concepts I'm having trouble figuring out that I hoped someone here could shed light on!

1.  Is there a way that the interface can be just an on button, and that the device will boot up, join it's remembered network, and then pass audio?


2.  What would I need to get the Raspberry to boot up and begin passing audio to it's output from a designated source on the network?

",,,,,Submission,5,0,5
d3mjr8k,2016-05-27 22:19:14-04:00,justlikestoargue,,"Looks more like an askprogramming question than an askcomputerscience one. The computer science answer to your question, however, is yes, you can do this.",4lbrq4,t3_4lbrq4,rabbiccu,,Comment,3,0,3
4l8yqd,2016-05-26 23:28:30-04:00,underscore_frosty,Tips to bolster CompSci grad school application?,"So, this coming fall I'll be applying to various grad schools (MS and PhD), does anyone have any tips that would bolster my application, projects and whatnot?

I'll be working like I always do (IT Specialist), taking a class (retaking a databases class because I had a not so great fall quarter this year for a variety of reasons), and hopefully working on some research with a professor. That being said though, I'll still have a lot of free time and I'd like to fill this up with something productive that will help my application.

Thanks!",,,,,Submission,3,0,3
d3lhi4v,2016-05-27 02:49:59-04:00,visvis,,Best thing you can do is publish a paper at a good conference. You might want to discuss with your professor whether there's any project you could help out with.,4l8yqd,t3_4l8yqd,underscore_frosty,,Comment,2,0,2
d3uwcti,2016-06-03 15:54:28-04:00,assface,,"Research is #1 for a PhD program. You want your rec letter to say that you are capable of doing research and writing a paper on your own.

Solid grades, relevant experience, and a diverse background are what you want for an MS program.

Note that you are going to need to specialize when applying to grad school. You are not applying just a CS program. You are applying to focus in on a particular area. Your background should show that you are prepared for this.",4l8yqd,t3_4l8yqd,underscore_frosty,,Comment,1,0,1
4l7jwe,2016-05-26 17:15:47-04:00,iesfr115,"I'm not too good with computers or technology besides usage, so I figure this is the place to go get my question answered.","Eventually everything gets old and breaks down. What her it's living or an appliance or whatever. But, does a file ever get ""old""? For example, is it possible that a music file on iTunes doesn't work anymore. After a music file or something is transferred from hard drive to hard drive will it ever sort of expire? ",,,,,Submission,0,0,0
d3kzym0,2016-05-26 17:42:10-04:00,thatsavageghost,,"A file does not get old or go stale.  What can happen is that the environment that a file is used in can change.  In that event, the expected behavior can change or it can be unusable in some cases.  I don't use iTunes, or anything Apple for that matter, so I can't say much to that specifically.  However, my impression of Apple is that if you are using their products, you should have no issues with their proprietary files.",4l7jwe,t3_4l7jwe,iesfr115,,Comment,5,0,5
d3l1gqq,2016-05-26 18:18:35-04:00,iesfr115,,Understood. The iTunes file was just an example that first came to my mind. So essentially any file downloaded will last forever?,4l7jwe,t1_d3kzym0,thatsavageghost,,Reply,1,0,1
d3l1rym,2016-05-26 18:26:17-04:00,localhorse,,"It will last as long as the storage medium is intact. You may be thinking about an analog signal, which would degrade eventually (think of a cassette tape getting recorded from tape to tape, eventually a lot of noise would be introduced). Whereas a signal that is stored digitally will never change, as long as the storage medium is not damaged. But if that happened, you likely wouldn't be able to recover any of the file.",4l7jwe,t1_d3l1gqq,iesfr115,,Reply,3,0,3
d3l2213,2016-05-26 18:33:23-04:00,thatsavageghost,,"Well, I would argue nothing lasts, literally, forever.  If you have a file on a physical disk, eventually (probably hundreds of years) the disk will decompose.  So in that event, no.  If one were to continue to transfer the file to a compatible disk, it could last as long as there is a place to put it.",4l7jwe,t1_d3l1gqq,iesfr115,,Reply,2,0,2
d3l24aw,2016-05-26 18:35:02-04:00,iesfr115,,"Right, I suppose my question was does a file ever just go bad. But it has been answered. Thank you. ",4l7jwe,t1_d3l2213,thatsavageghost,,Reply,1,0,1
d3l8xte,2016-05-26 21:43:02-04:00,arrenlex,,"Here's an example of what thatsavageghost is talking about when he refers to environment changes:
Back in the early 2000's, WordPerfect was the most popular word processing suite. It had its own file format, .wpd. If you had a .wpd file today, it would be exactly the same as it was in 2000, but you'd have a hard time using it because you'd have to find a copy of an outdated program or a converter to open it and convert it into something modern.

So the files themselves don't expire, but they do become outdated when the software they are intended to be used with becomes outdated.",4l7jwe,t1_d3l1gqq,iesfr115,,Reply,2,0,2
d3len36,2016-05-27 00:46:50-04:00,dxk3355,,"Early 2000?  WordPerfect hasn't been popular since the 90s.  Even Wikipedia say ""by the early 1990s WordPerfect was no longer the de facto standard."" In the WordPerfect entry",4l7jwe,t1_d3l8xte,arrenlex,,Reply,1,0,1
d3les89,2016-05-27 00:52:06-04:00,dxk3355,,In my CS classes in my first year of school they made the point that bit and bytes don't wear out and copying them is trivial.  The computers they run on may break but given good hardware the programs can run indefinitely.  ,4l7jwe,t3_4l7jwe,iesfr115,,Comment,2,0,2
4l5cle,2016-05-26 09:42:09-04:00,SPFSanuk,A question on code styling.,"Hey AskCompSci,

I'm currently revising for my finals and i'm going through a past exam paper and have been asked in one of the questions to format some code correctly. 
I've done the obvious with variables initializing line by line not all on one line and lining up the parenthesis at the end of the program and loops, however...

There is an if statement in the code that reads:
    if(b[i+2]>240&&b[i+1]<10&&b[i]<10)
How would one go about correctly formatting this? I was thinking a new line after each &&? Would I be correct in this or is there another way of implementing this statement nicely.

Many thanks",,,,,Submission,1,0,1
d3kf85o,2016-05-26 10:09:07-04:00,IgnorantPlatypus,,"One of the more common styles has the following properties:

 * spaces around binary operators
 * split lines only when they are too long for 80 (or 100 or 120) columns

So it would look like:

    if (b[i + 2] > 240 && b[i + 1] < 10 && b[i] < 10)
",4l5cle,t3_4l5cle,SPFSanuk,,Comment,3,0,3
d3khpbl,2016-05-26 11:06:45-04:00,SPFSanuk,,"Thank you very much, I think this looks easier than line by line breaking the if.",4l5cle,t1_d3kf85o,IgnorantPlatypus,,Reply,1,0,1
d4jmgat,2016-06-22 13:26:13-04:00,andybmcc,,"> if (b[i + 2] > 240 && b[i + 1] < 10 && b[i] < 10)

They may also be looking for adding parenthesis to clarify order of operations (even though this example is pretty straight forward).

I would do the following in general just to make it absolutely clear:

    if ((b[i + 2] > 240) && (b[i + 1] < 10) && (b[i] < 10))

Then again, I would separate each condition into a method that described what it does.  I hate code like this with magic numbers and no context.",4l5cle,t1_d3khpbl,SPFSanuk,,Reply,1,0,1
d3kglwf,2016-05-26 10:42:05-04:00,ZealZen,,Depends on what the format standards will be set by your company/boss.,4l5cle,t3_4l5cle,SPFSanuk,,Comment,2,0,2
d3khq4c,2016-05-26 11:07:18-04:00,SPFSanuk,,"The University does not specify, just asking us to follow ""good programming practice""",4l5cle,t1_d3kglwf,ZealZen,,Reply,1,0,1
d3kqon8,2016-05-26 14:18:50-04:00,anamorphism,,"as /u/ZealZen alludes to, code style is purely subjective. the most important thing is that you're consistent across your code base.

that being said, as /u/IgnorantPlatypus points out, there are some rules that seem to be followed more often than not:

- spaces between keywords and opening parentheses. `if ()` instead of `if()`
- spaces around all binary operators. `b[i + 2] > 240 &&` instead of `b[i+2]>240&&`
- limiting the number of columns to some reasonable number. 80 was common for a while because of command line output.
- spaces after commas. if you were using a multi-dimensional array, you'd use `[1, 2]` instead of `[1,2]`",4l5cle,t3_4l5cle,SPFSanuk,,Comment,2,0,2
d3l9jwc,2016-05-26 21:59:07-04:00,SPFSanuk,,Thank you for helping out :) means a lot,4l5cle,t1_d3kqon8,anamorphism,,Reply,1,0,1
d3l93ps,2016-05-26 21:47:14-04:00,darthandroid,,"2 things matter when formatting code:

1. It must be clean and easy to read what is happening
2. It must be consistent

Tabs or spaces? Doesn't matter, just be consistent.  
Brackets on same or next line? Doesn't matter, just be consistent.

If you're working in a language that has particular coding conventions, follow them (see #2).

    if(b[i+2]>240&&b[i+1]<10&&b[i]<10)
violates #1 - It's very difficult to see what is going on here. Letters/Numbers/`&` all blur together, so it's difficult to see where they begin and end. Adding spaces improves this dramatically:

    if(b[i+2]>240 && b[i+1]<10 && b[i]<10)

The rest is subjective. I personally would style the line like this, adding spaces around the operators and keywords:

    if (b[i + 2] > 240 && b[i + 1] < 10 && b[i] < 10)",4l5cle,t3_4l5cle,SPFSanuk,,Comment,1,0,1
d3l9jp3,2016-05-26 21:58:57-04:00,SPFSanuk,,Thank you very much for this man,4l5cle,t1_d3l93ps,darthandroid,,Reply,1,0,1
4l536r,2016-05-26 08:38:25-04:00,zookeeper_zeke,X-Post: Linearity of Expectation,"Hi folks,

I'm taking a course on Coursera about approximation algorithms for fun and one of the modules uses probability to analyze the cost of a linear program with randomized rounding solution to the set cover problem.

One of the problem set questions is a basic probability question and is as follows:

You ask ten of your friends to give you one dollar (coin) each so that you can buy your favourite pizza.

When you reach the pizza place you realize that it's closed. So you return the coins to your friends by giving each of them a random coin.

1) How likely is that each of them gets the coin they put back?

2) How many of them will get in expectation the coin they put back? (Try to prove this elegantly using techniques discussed in the course)Define the random variables you use.

3) Now suppose that you take one coin to buy a coffee somewhere else. You redistribute the others dollar coins giving each of your friends one coin chosen uniformly at random. How many of your friends will get in expectation the coin they put back?

I've solved the first two parts and I'm thinking for the third part that I should define a random variable Xi that represents whether or not persion i get the coin back that she put in and E[Xi] is the probability that she gets it back.

The probability that she gets it back is 9 / 10 * 1 / 10 = 9 / 100. There's a 9 / 10 chance that her coin was not the coin that I bought coffee with and a 1 / 10 chance that she picks her coin or is left out as there are 9 coins for 10 remaining people.

Taking X as the number of folks that get their coins back I have

E[X] = sum(E[Xi]) over all persons E[X] = 90 / 100 = .9 people are expected to get their coin back.

Is my reasoning sound here? If not, can somebody show me the error of my ways?

Thanks so much,

Zeke
",,,,,Submission,2,0,2
4l51nu,2016-05-26 08:27:12-04:00,zookeeper_zeke,X-Post: Wald's equation for dependent decrements,"Hi folks,

I'm taking a course on Coursera about approximation algorithms for fun and one of the modules uses probability to analyze the cost of a linear program with randomized rounding solution to the set cover problem.

One of the techniques employed is Wald's equation for dependent decrements described here:

http://algnotes.info/on/background/stopping-times/walds-dependent/

For Alice's coin-flipping example, can somebody please describe to me how δ is increasing if it's chosen to be δ(nt) =1 / 6 * ceil(nt)? As the number of coins left decreases doesn't δ(nt) also decrease?

Clearly there's something missing in my understanding of how this works.

Thanks very much,

Zeke
",,,,,Submission,4,0,4
d3kcfdx,2016-05-26 08:50:38-04:00,james41235,,"Delta is the probability this is the last round.  So, as more coins go away, the probability that all will come up tails is higher.",4l51nu,t3_4l51nu,zookeeper_zeke,,Comment,1,0,1
4l45ju,2016-05-26 03:10:39-04:00,samz_manu,Projects an aspiring programmer can work on?,"I'm a junior in college and by now have a pretty good handle on Java, Matlab and C. What are some suggestions for projects/apps I can work on during the free time? I want to build something cool but am not a creative enough thinker to think of something to do. Not looking for a unique idea that hasn't been done before. Just inspiration to get started. ",,,,,Submission,11,0,11
d3kd3y7,2016-05-26 09:12:01-04:00,ObeselyMorbid,,Obviously you should make a text-based adventure... ,4l45ju,t3_4l45ju,samz_manu,,Comment,6,0,6
d3l2vod,2016-05-26 18:54:47-04:00,YourFavoriteBandSux,,You have moved into a dark place. You are likely to be eaten by a grue.,4l45ju,t1_d3kd3y7,ObeselyMorbid,,Reply,2,0,2
d3k81mw,2016-05-26 05:22:08-04:00,None,,You can use java and android studio to make an android app.,4l45ju,t3_4l45ju,samz_manu,,Comment,6,0,6
d3kbb65,2016-05-26 08:11:41-04:00,ANil1729,,Computer vision projects are cool.You can try using opencv directly or javacv a wrapper of opncv,4l45ju,t3_4l45ju,samz_manu,,Comment,1,0,1
d3l7jkp,2016-05-26 21:06:18-04:00,TheDreamerofWorlds,,"You have a few options. You could make scripts and tools that people would find quite useful, things like web crawlers (for example, a script that scans a website and downloads all files with a certain name or extension), or a youtube downloader application. 

You could also practice with a raspberry pi and just make cool/fun projects.  That would also be a great way to get out of your comfort zone and learn/try new things. 

You could even work on some larger open source projects like video game console emulators, various nth dimensional modelers (Blender for example), or even tronscript (see: /r/tronscript )

Just a few different paths you could potentially take. Best of luck, cheers!",4l45ju,t3_4l45ju,samz_manu,,Comment,1,0,1
d3qcocy,2016-05-31 09:38:53-04:00,Am0s,,"As a fellow college junior, I've found open source projects to be pretty fantastically useful. You end up learning a lot of things that you normally wouldn't until you plop into industry (mostly about tools, frameworks, best practices etc.) without the pressure of getting paid for it. 


There used to be some sites that would help you find projects to contribute to, but I don't recall them. 


Check out (and maybe post question to) /r/opensource /r/coolgithubprojects /r/progether",4l45ju,t3_4l45ju,samz_manu,,Comment,1,0,1
d3qjqd3,2016-05-31 12:32:30-04:00,samz_manu,,Thanks. Those are exactly the kinds of subreddits I was looking for. I have never done open source stuff before but is the expectation to add some new functionality to every project? Seems like most projects are already done. What's the expectation on open source projects? ,4l45ju,t1_d3qcocy,Am0s,,Reply,2,0,2
d3qn015,2016-05-31 13:45:29-04:00,Am0s,,"Every project has an enormous backlog of issues to work on. Bugs, features, redesigns, etc. 


Some of these are tracked on github, some track their issues on something like jira or mantis bug tracker. ",4l45ju,t1_d3qjqd3,samz_manu,,Reply,1,0,1
d3kisv0,2016-05-26 11:30:56-04:00,pahosler,,browse github and look for projects you might be interested in contributing to.,4l45ju,t3_4l45ju,samz_manu,,Comment,1,0,1
4l02o5,2016-05-25 11:03:47-04:00,GeeJo,Sanity check for definition of secondary storage.,"Hey all. 

I'm in the process of writing schemes of work for high-school IT teachers. It's for the new and upcoming British curriculum, so no exam papers to crib off of, only the official exam board specification. I'm *pretty sure*, though, that one of the specification points is just plain wrong, and I'd like a sanity check before I make a decision on whether to blindly go along with it or not.

The British exam board AQA asks students to:

> ""understand the difference between main memory and secondary storage. [...] **Secondary storage is considered to be any nonvolatile storage mechanism external to the CPU.**""

The first thing that occurs to me here, though, is ROM. ROM:

* is a storage mechanism (e.g. for the BIOS)
* is non-volatile 
* is located outside the CPU. 

By AQAs definition, though, this would make it secondary storage rather than main memory (which is what I'd always understood it to be).

________

Anyone have a better definition to separate main memory from secondary storage, though?",,,,,Submission,3,0,3
d3j8uy8,2016-05-25 12:34:08-04:00,heywire84,,"Main memory in today's computers is called RAM (random access memory).  It is volatile since once the power is turned off, the contents of the RAM is lost.

Everything else like disks, solid state drives, punch cards, tape, even network attached storage is all secondary storage.  ROMs and CD/DVD-ROM is also secondary storage mostly because it isn't primary storage, e.g. RAM.  Also, secondary storage can totally be volatile.  RAM disks, which are RAM chips on an expansion card or some other container, are one such technology.

The point of distinguishing between primary and secondary storage isn't so much the medium, but the way it is used.  **CPUs can *directly access* their primary memory through addressing.**  They cannot do the same thing with their secondary memories.  Accessing a secondary memory like a hard disk requires some extra software generally provided by the BIOS or the operating system.

The BIOS or UEFI system on modern computers is sort of a corner case.  There are special memory addresses that are mapped to memory chips which aren't on sticks of RAM.  These special memory addresses let the CPU directly access and read/write from the separate BIOS/UEFI chips.  However, once the CPU is finished booting and loads the first instruction from the operating system, it typically is never accessed again.  

Bottom line, the whole primary/secondary memory concept of computer (Von Neumann) architecture is sort of an older concept and the lines are considerably blurred with modern computers.  Virtual memory allows a CPU to treat secondary storage as addressable primary storage for example.

I would ask the AQA to change their definition to, ""Secondary storage is considered to be any storage mechanism not directly addressable to the CPU.""",4l02o5,t3_4l02o5,GeeJo,,Comment,5,0,5
d3jamyz,2016-05-25 13:12:30-04:00,GeeJo,,"Thanks. This fits with [the answer I got](https://www.reddit.com/r/computing/comments/4kzyof/sanity_check_for_definition_of_secondary_storage/d3j5b4y) over on the mirror thread on /r/Computing from /u/n64_chalmers, which was that

> a better definition of main memory is **any memory that is directly accessed by the CPU.**

I'll still have to provide the AQA definition as it's going to be the stock phrase that is demanded of students on exams, but I'll structure the mini-website, starters, and plenaries with the addressing/direct access definition and just footnote the ""official"" word on the issue.

You've been very helpful!",4l02o5,t1_d3j8uy8,heywire84,,Reply,4,0,4
d3jj3a3,2016-05-25 16:15:04-04:00,bekroogle,,"Sounds like you got your answer, but I thought I'd add that it almost sounds like they're hedging their bets with ""is considered to be"" instead of ""is defined as"". Makes me wonder if there was already some debate while drafting this document.",4l02o5,t3_4l02o5,GeeJo,,Comment,1,0,1
4ky8u0,2016-05-25 01:59:35-04:00,startrak209,Grammar in BNF?,"Can someone give me a ELI5 for grammar and BNF? I've tried youtube videos and it just does not click for me. [These](http://imgur.com/83SsNuU) are the problems I'm currently working on. Any help would be greatly appreciated, thank you in advance!",,,,,Submission,5,0,5
d3j1nyt,2016-05-25 09:48:21-04:00,crookedkr,,"A grammer is a set of rules that can be followed to generate a language, or confirm if an input is part of a language. BNF is a syntax for writing these rules.

For the first question, you are asked to say in words what the grammar produces. One technique for doing this is to try to make the shortest strung in the language. Then make a few others and see what you notice about them.

For the second part, if the string is accepted its easy, just write out the rules that generate it. If it is not part of the language it's a little harder but the strategy is to start with all the terminals that you would need and show that there is no way to work backwards to the start symbol. For example, if one string was 'oops' you could say that it can't be in the language because there is no production that outputs  an 'o'",4ky8u0,t3_4ky8u0,startrak209,,Comment,2,0,2
d3ja5ip,2016-05-25 13:01:47-04:00,startrak209,,Thank you!,4ky8u0,t1_d3j1nyt,crookedkr,,Reply,1,0,1
d3j20e7,2016-05-25 09:57:29-04:00,Madsy9,,"In your picture when non-terminals are on the right side, you can think of them as ""go to the rule with this name"". When they are on the left side, it is the name of a rule for the non-terminal.

You start parsing from the start symbol, which is usually labeled S. In your example it's called <S>. Now, the rule for <S> is ""<A> | <B>"" which means either <A> or <B>. To figure out which one you have, you have to look up the rules for <A> and <B> to see which one is legal given the input.

* The rule for <A> is: <A> ::= a<A>a | a<B>a | C
* The rule for <B> is: <B> ::= b<A>b | b<B>b | C
* The rule for <C> is: a | b | c

Small a,b or c here means a terminal. It's literally the letter a, b and c respectively in your input. A terminal is a symbol describing itself. Hence, there are never production rules for terminals. A non-terminal is the opposite: it is a symbol which describes a production rule, where the production rule can contain both terminals and other non-terminals and even refer to itself (known as recursion).

Another important point to remember is that no production rules can recurse forever: they have to terminate at some point to be useful. As you can see from the rules in your book, the production rule for C is not recursive. As such, both the rules for A and B will terminate with C, since C is the only option in the A and B production rules which terminates.

Some examples of strings that are in your AB language:

 * ""aaa"" - This matches A ::= a<A>a, where the <A> in the middle takes the ""or C"" branch, and C is also ""a"". Similarly, the same rule would match ""aba"" and ""aca""
 * ""aacaa"" Same as example 1, except that it takes the A production rule twice. It matches a<A>a, where the middle A is ""aca"", and the middle ""c"" matches <C>
 * ""baacaab"" Same as example 2, but it first matches <B> ::= b<A>b where the <A> is ""aacaa""",4ky8u0,t3_4ky8u0,startrak209,,Comment,2,0,2
d3ja56f,2016-05-25 13:01:35-04:00,startrak209,,Thank you! This helped me understand it much better.,4ky8u0,t1_d3j20e7,Madsy9,,Reply,1,0,1
4kxqoi,2016-05-24 23:31:51-04:00,flyodllyod,Need explanation on REST API,Could someone give me a list a materials where I can learn how to use API's in general and learn about REST? (Really appreciate if you can explain it here in your own words as well!) I have to know this soon and after looking up numerous sites I am still very lost. I'd appreciate any help in learning about this. Thanks! ,,,,,Submission,1,0,1
d3j51yg,2016-05-25 11:10:23-04:00,En0ch_Root,,"I can't help but sigh when I see future developers come here and ask reddit to tell them what they need to know and only what they need to know to pass a test or write a block of code.  

If these kids don't understand that a large percentage of this type of work is self study, research, and application of concepts, then someone isn't teaching them properly. Or they are too lazy to listen.  ",4kxqoi,t3_4kxqoi,flyodllyod,,Comment,2,0,2
d3pllxj,2016-05-30 17:35:25-04:00,DairTheBear,,"If you actually read the post, you'd know he's just wants some resources to learn about them. He's not asking you to teach or spoon feed him. Oh and it's pretty rude to call him a kid when you don't even know him. ",4kxqoi,t1_d3j51yg,En0ch_Root,,Reply,1,0,1
d3uu8f6,2016-06-03 15:07:47-04:00,En0ch_Root,,"Does the OP not know about Google?

You miss my point. As developers, we run across new things daily, and we need to understand how to research solutions BESIDES getting on Reddit and asking people to do our Googling for us. 

",4kxqoi,t1_d3pllxj,DairTheBear,,Reply,1,0,1
d3ioyxw,2016-05-25 00:20:33-04:00,thepobv,,"Uummm do you understand HTTP protocol? or different protocols like TCP, UDP? that might be a good thing to learn about first.

Basically APIs is the documentation on how software can communicate to one another. REST consists of some conventional method like GET PUT POST PATCH DELETE. each one have its own function...",4kxqoi,t3_4kxqoi,flyodllyod,,Comment,1,0,1
d3j4juv,2016-05-25 10:58:55-04:00,worldDev,,"[Teach a dog to rest](https://vimeo.com/17785736). A bit old, but still relevant.",4kxqoi,t3_4kxqoi,flyodllyod,,Comment,1,0,1
4kvpmv,2016-05-24 15:57:23-04:00,Martaway,Convert Dec to Bin: What is this symbol in my CompSci book?,,,,,,Submission,15,0,15
d3i5ysr,2016-05-24 16:23:16-04:00,gumbo_rogers,,"It's supposed to be a division symbol (although I've never seen this kind of notation).

    53 divided by 2 equals 26, 1 remainder.  
    26 divided by 2 equals 13, 0 remainder.
    13 divided by 2 equals  6, 1 remainder.
     6 divided by 2 equals  3, 0 remainder.
     3 divided by 2 equals  1, 1 remainder.
     1 divided by 2 equals  0, 1 remainder.

Now take all the remainders ""backwards"", i.e. 110101, and you have 53 in base 2.

See: https://en.wikipedia.org/wiki/Binary_number#Conversion_to_and_from_other_numeral_systems",4kvpmv,t3_4kvpmv,Martaway,,Comment,7,0,7
d3i7eyc,2016-05-24 16:54:55-04:00,Martaway,,"agh that makes sense,  thank you so much!",4kvpmv,t1_d3i5ysr,gumbo_rogers,,Reply,6,0,6
d3ijh7n,2016-05-24 21:55:43-04:00,FaithForHumans,,"I've seen it used occasionally. It's primarily used in instances where you're doing numerous divisions one after another (for example, base conversion).    
It's supposed to help when you don't know how many divisions you're go into preform so you don't have to guess how far down to start your paper. Also useful as it follows the English tendency to read from top to bottom. 

Granted it's against common convention... ",4kvpmv,t1_d3i5ysr,gumbo_rogers,,Reply,3,0,3
d3ihk9t,2016-05-24 21:08:29-04:00,bacondev,,Looks like multiple upside-down long divisions.,4kvpmv,t3_4kvpmv,Martaway,,Comment,3,0,3
d3i4v2t,2016-05-24 15:59:10-04:00,Martaway,,"It says 
>Convert 53 (base 10) to binary

Then starts using the square rootish type symbol to solve it.. what the heck is that? Its not explained anywhere in the book... I assume its something from Algebra but Ive been out of school so long I dont know and a reverse image search did nothing

Im on page 10 so Im guessing its something I should know already",4kvpmv,t3_4kvpmv,Martaway,,Comment,1,0,1
d3i62h3,2016-05-24 16:25:28-04:00,Mukhasim,,"Judging from the example, it looks like it's just division.

See the notation shown here:

https://en.wikipedia.org/wiki/Short_division",4kvpmv,t1_d3i4v2t,Martaway,,Reply,3,0,3
d3i72rk,2016-05-24 16:47:22-04:00,thepatman,,"I think the confusion here is based on the typesetting.  The / of the division and the _ where you're supposed to put the answer run together, making it look like one big symbol.  I think they're supposed to be more distinct.",4kvpmv,t1_d3i62h3,Mukhasim,,Reply,3,0,3
d3i7gui,2016-05-24 16:56:05-04:00,Martaway,,"that makes sense , thank you",4kvpmv,t1_d3i72rk,thepatman,,Reply,2,0,2
d3i7fur,2016-05-24 16:55:28-04:00,Mukhasim,,"I think it's deliberate. Look at the photo of the pencil-and-paper example in the ""Prime factoring"" section of the Wikipedia article that I linked to.",4kvpmv,t1_d3i72rk,thepatman,,Reply,1,0,1
d3i7fum,2016-05-24 16:55:28-04:00,Martaway,,Thanks man! appreciate the help,4kvpmv,t1_d3i62h3,Mukhasim,,Reply,2,0,2
d3i54xr,2016-05-24 16:05:10-04:00,CoolAndCursed,,"Just guessing, maybe something to do with polar notation of complex numbers? http://www.allaboutcircuits.com/textbook/alternating-current/chpt-2/polar-rectangular-notation/",4kvpmv,t3_4kvpmv,Martaway,,Comment,1,0,1
d3imvfn,2016-05-24 23:20:55-04:00,WatchMeStart,,"In certain countries, this is how division was represented. I vaguely remember my parents questioning how I wrote the division symbol (American). Turns out they used the upside down one.",4kvpmv,t3_4kvpmv,Martaway,,Comment,1,0,1
d3iqq6f,2016-05-25 01:22:08-04:00,Martaway,,"Makes sense the cover of the book indicates I have the ""International Student"" edition",4kvpmv,t1_d3imvfn,WatchMeStart,,Reply,2,0,2
4kvfnf,2016-05-24 15:03:48-04:00,MeLlamoJason,"I know nothing about computers, where can I learn the very basics?","I know how to press the power button, how to google, microsoft office and reddit.  That's it.  Where can I find some useful material that someone with absolutely no experience can learn?  What is programming? The internet? The physical parts of a computer?  How do they all work together?  I'm sure this is a stupid question to some of you but I've never needed to (or desired to) understand this stuff until now.",,,,,Submission,3,0,3
d3i65xs,2016-05-24 16:27:31-04:00,ishealright,,"I'm sorry but this is not at all a computer science question.

All the questions you listed can easly be googled.

- What is programming
- What is the internet
- What are the physical parts of a computer

",4kvfnf,t3_4kvfnf,MeLlamoJason,,Comment,6,0,6
d3ickud,2016-05-24 19:00:57-04:00,SmokingTurkey,,"*That* is his knowledge of computers, that he didn't know this isn't quite the sub to be asking this.",4kvfnf,t1_d3i65xs,ishealright,,Reply,2,0,2
d3i9god,2016-05-24 17:42:29-04:00,gerbil-ear,,I highly recommend How Computers Work by Ron White. It's illustrated too.,4kvfnf,t3_4kvfnf,MeLlamoJason,,Comment,5,0,5
d3i9yua,2016-05-24 17:54:41-04:00,pballer2oo7,,ahhh...where it all started ^to ^pick ^up ^speed,4kvfnf,t1_d3i9god,gerbil-ear,,Reply,1,0,1
d3i4bb4,2016-05-24 15:47:12-04:00,crookedkr,,Wikipedia would be a great place2 to start. Many of the articles on CS are pretty good and when they do get something wrong it's usually pretty technical and not something someone just learning about how things work would be bothered by. ,4kvfnf,t3_4kvfnf,MeLlamoJason,,Comment,2,0,2
d3j6xap,2016-05-25 11:51:47-04:00,YangsLove,,"To briefly answer your question to the best of my ability, because those areas are dedicated areas of study in itself, 

Programming, essentially is the steps that an individual take to produce an application/software. People often just view this as writing code, but it may include other things, like creating use cases, flow charts, etc, that might fall into the lines of a software engineer. 

Physical parts: core components to start looking into are the: Central Processing Unit (CPU), Hard Drives and then onto different drives such as solid state drives (SSD), Random Access Memory (RAM), Graphics Card, Motherboard, and Power Supply. This should get you started on a good foundation to build on top of. 

The internet: is basically just a huge network (which is a connection of many many computers connected to one and another to exchange information) 

Google is your friend, there's also many books out there that are simple enough for beginners to learn. Good luck!",4kvfnf,t3_4kvfnf,MeLlamoJason,,Comment,1,0,1
d3ibh92,2016-05-24 18:32:32-04:00,Britches,,"http://pages.cs.wisc.edu/~remzi/OSTEP/ 

Free operating systems textbook. Might be helpful along with some of the other stuff. I'm not sure if you can just jump into it, but with some baseline programming knowledge it's pretty helpful.",4kvfnf,t3_4kvfnf,MeLlamoJason,,Comment,0,0,0
d3i80yh,2016-05-24 17:08:40-04:00,Mukhasim,,"Try this book, which is available here for free as a PDF: http://download-mirror.savannah.gnu.org/releases/pgubook/ProgrammingGroundUp-1-0-booksize.pdf

The same book, as a website instead of a PDF, is here: http://programminggroundup.blogspot.com/

You'll need a Linux computer to follow through the entire book, but if you even read just the first couple of chapters then you'll know a lot more than you did.
",4kvfnf,t3_4kvfnf,MeLlamoJason,,Comment,-1,0,-1
4kulhm,2016-05-24 12:23:10-04:00,Hugh_Schmefner,Having trouble with MIPS Machine Language Representation... Help Appreciated!,"Hey guys, I'm a first year CompSci student. I have an exam in a couple of weeks, and whilst I understand the majority of this specific module, I have no idea about this question from last years paper.

http://imgur.com/Qjbhil5 

How would I even begin to answer this?",,,,,Submission,4,0,4
d3hx2kg,2016-05-24 13:07:56-04:00,pgoodjohn,,"Hey there, fellow CSer!
First of all, let's analyze the instruction: it's a OR immediate, comparing bit by bit a value stored in a register with a 16 bit immediate number returning one where at least one bit is a 1, 0 otherwise.
In order to build the machine language of the instruction we have to go from the blocks that we are given:
The first 6 blocks are the OP code, they are unique for each instruction and they are language defined. In this case the OP code is given in Hexadecimal so you just have to convert 0xD from hex to binary. Easy stuff, the first 6 bits are 0011 01.
Let's keep to the easy stuff: the last 16 bits represent the immediate that you are ""ORing"" your value with, in this case 4, so you just have to express 4 in 16bit binary, 0000 0000 0000 0100.  
For now we have 0011 01xx xxxy yyyy 0000 0000 0000 0010.  
The xs and ys reperesent the values of the register where you want to save your value (xs, in your case $t2) and the register where the value you are comparing is stored (ys, in your case $zero).  
From the table you get that $zero is associated with the number 0 and $t2 is associated with number 10. Thus in binary $zero will be 00000 (remember, 5bit resolution) and $t2 will be 01010.  
Inputting $zero in the ys and $ts in the xs you get:  
 0011 0100 0000 1010 0000 0000 0000 0010  
which is the machine code for your instruction.  
Hope everything is clear, if it's not I'm sorry but it's one a.m. here and I just finished my homework and I'm quite tired :P  
If you need anymore informations about how conversion works, this page explains it really well:   https://en.wikibooks.org/wiki/MIPS_Assembly/MIPS_Details   
Keep having fun with CS :D   
  
EDIT: Formatting, evidently majoring in CS and Engineering doesn't teach you how to correctly format reddit comments.",4kulhm,t3_4kulhm,Hugh_Schmefner,,Comment,2,0,2
d3hxvxv,2016-05-24 13:26:00-04:00,Hugh_Schmefner,,"You're a genius. It's like i've just read an excerpt from 'MIPS For Dummies'! 

Thanks a bunch for your help dude. Keep on keeping on.",4kulhm,t1_d3hx2kg,pgoodjohn,,Reply,2,0,2
d3hxyjp,2016-05-24 13:27:36-04:00,pgoodjohn,,"Ahahah thanks mate!  
If you ever find yourself stuck, MIPS reference is an amazing resource, I forgot to link it before http://www.mrc.uidaho.edu/mrc/people/jff/digital/MIPSir.html :)",4kulhm,t1_d3hxvxv,Hugh_Schmefner,,Reply,1,0,1
4kujun,2016-05-24 12:14:26-04:00,fosted3,Reference-less traversal of perfect n-ary trees,"I am looking for methods of traversing a perfect *n*-ary tree (all non-leaf nodes have *n* children and all leaf nodes are at the same depth *h*), where the tree is stored linearly in depth first order.

Specifically - 
Is it possible to determine, using only the node index *i*, the total depth of the tree *h*, and the number of children per node *n*,

* The index of the parent node
* Is the node a leaf?
* The indices of the child nodes (if not a leaf)

If not, what other metadata is required?

I know:

The total size of the tree is (n^h+1 -1)/(n-1).

For non-leaf node *i*, the first child will always be *i+1*.

Leaf nodes are consecutive.

I do not have a CS degree and I have been having difficulty in finding a solution - if anyone has answers or a starting point it would be greatly appreciated!
",,,,,Submission,2,0,2
d3i0dhd,2016-05-24 14:20:38-04:00,lneutral,,"Depth-first order complicates the index math a little bit, but yes, you can determine those.

If you stored the values in breadth-first order, it might be simpler to understand:

* The children of the value at i would be at n*i+j for j = 1 to n.
* The parent node would be at floor((i - 1) / n).
* You'd know the node is a leaf if n*i is greater than the length of the array. The length of the array is, of course, the same whether it's depth first or breadth first.

By doing this depth first, you've permuted those values, but similar computation can get them for you.

Effectively, you're storing them in the same order as a preorder traversal would visit. One way to tell a node's parents, children, and whether it's a leaf is to traverse the tree from node 0 up to the index you want to know about, and record the parent, level, and children on the way there.

What constraints do you have? Can you allocate a second array of the same length as your tree? Is it possible to permute it to level-order?",4kujun,t3_4kujun,fosted3,,Comment,2,0,2
d3i4b1u,2016-05-24 15:47:03-04:00,fosted3,,"I'd like to store depth first, as I can copy a subset of the tree and have it still be in the proper order.

This code is targeted to run on CUDA systems - which is why I'm interested in copying a specific subset of the tree (to device memory) without significant computation.

My data structure is most likely going to be an array of structs - containing 5 (32b) floats. I'd like to align it on a power of 2 boundary, so 32B per struct would be nice. 64B is do-able.

The tree will contain over 1 billion elements so memory usage is somewhat of an issue. I have 128GB to play around with but the compute card only has 6GB - I'd like to maximize the size of the subsets I'm copying.",4kujun,t1_d3i0dhd,lneutral,,Reply,2,0,2
d3i5eeb,2016-05-24 16:10:58-04:00,lneutral,,"I see. That's a very sensible reason for using depth order.

In a way, depth and level order are opposites - with depth order, the array distance between siblings decreases as you go down the tree, whereas with level order, it increases.

If I'm understanding correctly, then, you want a purely numerical operation if possible, to cut down on overhead.",4kujun,t1_d3i4b1u,fosted3,,Reply,1,0,1
d3igdmm,2016-05-24 20:38:47-04:00,fosted3,,"Exactly. I am bound by memory latency in this case - pointer chasing is extremely slow, especially on the GPU. ",4kujun,t1_d3i5eeb,lneutral,,Reply,1,0,1
d3ih8p4,2016-05-24 21:00:34-04:00,lneutral,,"Well, a ""simulated"" traversal would be pretty bad, then. It's still in linear time - to figure out parents and children of index k would mean scanning through k items.

Retaining the relative index of parents and children might be an option - those wouldn't need to change if you're copying entire subtrees. It's one more integer per node, though.

If your n is large, though, you don't want the node data to be huge. If you know the current depth of the node, you can determine where its children will be based on the total height of the tree. 

What values of n are typical?",4kujun,t1_d3igdmm,fosted3,,Reply,1,0,1
d3i0vyo,2016-05-24 14:32:02-04:00,zefyear,,"Yes.

With the exception of the specific ordinals you've used to index the node, this concept is known as a heap, size can either be stored as ""bucket metadata"" or computed from the parent references.",4kujun,t3_4kujun,fosted3,,Comment,1,0,1
4ktczo,2016-05-24 07:47:07-04:00,puketronic,What happens during a system call?,"Disclaimer: I don't have a CS background. I'm reading  on this topic to better understand how my OS (linux) works.

From reading notes online, this is what I understand: 

An application runs in *user mode*, makes a system call, then switches to *kernel mode*, providing full access to privileged executions. This switch is called a *mode switch*. If the system call *blocks* (takes a long time), then the kernel suspends the process, replaces it with another thread or process, then picks up the suspended thread/process at a later time.  A *context switch* is the switching from one process or thread to another.

Is my summary generally correct? If so, then does the user-mode to kernel-mode *mode-switch* occur in the same thread? Regardless of if the application is single threaded or multi-threaded? That is, there is no switching from user-level threads to kernel-level threads, right? OR is there? Switching processes or threads only occurs during a context switch, I believe.

This topic alongside the topic of user-level threads and kernel-level threads mixes me up.",,,,,Submission,5,0,5
d3hpcjn,2016-05-24 10:09:31-04:00,visvis,,"> Is my summary generally correct?

Yes.

> If so, then does the user-mode to kernel-mode mode-switch occur in the same thread?

The kernel part doesn't really occur in any thread. It's an interrupt handler or something similar. It interrupts execution (of the user mode process) and jumps to a handler. This handler has its own stack (sharing it with the application could be unsafe) so you could say it's kind of a new thread but it all depends on your definition of a thread.

> Regardless of if the application is single threaded or multi-threaded?

That doesn't matter at all here. For the kernel, threads are just like processes except they share an address space and some state (assuming the kernel knows about them at all).

> That is, there is no switching from user-level threads to kernel-level threads, right? OR is there?

Whether the kernel itself is multithreaded is a separate matter entirely. One could write a kernel that only runs initialization code and interrupt handlers, with a single stack and no distinct threads at all.

Note that the term kernel thread can also be used to refer to a user-mode thread that is supported by the kernel (as a light-weight process) as opposed to one where the context switch is implemented entirely in a library in user mode.

> Switching processes or threads only occurs during a context switch, I believe.

No, in fact threads can be implemented entirely in user space. Switching processes on the other hand requires entering the krnel to be able to replace the page tables.",4ktczo,t3_4ktczo,puketronic,,Comment,4,0,4
d3i6anx,2016-05-24 16:30:15-04:00,_KetzA,,"> The kernel part doesn't really occur in any thread. It's an interrupt handler or something similar. It interrupts execution (of the user mode process) and jumps to a handler. This handler has its own stack (sharing it with the application could be unsafe) so you could say it's kind of a new thread but it all depends on your definition of a thread.

It depends on the OS actually, AFAIK linux is using a one-to-one mapping between userland threads and kernel thread. When a syscall occurs the control is given to the corresponding kernel thread which has its own stack for security purpose. By doing so, the user cannot modify the stack of the kernel thread when running a syscall.

",4ktczo,t1_d3hpcjn,visvis,,Reply,1,0,1
4ksfod,2016-05-24 02:10:52-04:00,sadelbrid,Quick question on this runtime complexity,"I have an algorithm with a runtime that looks like c*(n*(n-5)*(n-10) ... (1))

Would this runtime be of the order (n!) ? I'm uncertain because there are only n/5 terms in the sequence. Thoughts? (I'm not great at math as you might infer.) Thanks in advance.",,,,,Submission,4,0,4
d3hhuna,2016-05-24 04:49:39-04:00,gyroda,,"For n/5 terms it looks to be n^n/5 . I can't remember my arithmetic well enough (it's been a while and I'm half asleep) to see if I can pull a constant out of that or something. Wolfram alpha isn't giving me anything useful. 

What is c()?",4ksfod,t3_4ksfod,sadelbrid,,Comment,1,0,1
d3hmi60,2016-05-24 08:45:51-04:00,sadelbrid,,c is just a constant factor,4ksfod,t1_d3hhuna,gyroda,,Reply,1,0,1
d3ho37b,2016-05-24 09:35:13-04:00,gyroda,,Unless it's too do with the input you ignore it. The way you had typed it made it look like a function. ,4ksfod,t1_d3hmi60,sadelbrid,,Reply,2,0,2
4krawg,2016-05-23 21:03:53-04:00,pyroSeven,Calculating transfer rate of data for magnetic hard disk.,"http://imgur.com/CFJmmpr

Hey guys, I'm stuck on question 2biii. From my understanding, you'll need the data size that is to be transferred to calculate the transfer rate but I don't see it anywhere in the question. Anybody can help?

Thanks.

PS: Sorry if this is the wrong sub to ask this, I wasn't sure.",,,,,Submission,1,0,1
4kpdc6,2016-05-23 14:23:06-04:00,Reddang,"School doesn't offer CS, just CIS","So going into college I was undecided. I felt pressure from family and friends to study something in business, so I started accounting. I'm only 40 credits in which makes me a sophomore technically. I want to study CS but my school only offers CIS (Computer Information Systems) or Business/CIS. What's the difference between the two? I would like to get into programming and coding. Should I think about transferring to a school that offers CS instead? ",,,,,Submission,13,0,13
d3gx8px,2016-05-23 17:46:37-04:00,zefyear,,"Unfortunately this seems a common trend among all sorts of both public and private institutions.

My suggestion would be to major in Mathematics if CIS is the only program offered. A sweeping generalization held in industry is that CIS programs do not endow their recipient with anything close to the required skillset necessary to be an engineer.",4kpdc6,t3_4kpdc6,Reddang,,Comment,6,0,6
d3gsi61,2016-05-23 15:58:33-04:00,eliselaplace,,"Could you link to the program? In general it's difficult to know what is taught going by the name.

What makes you want to study computer science?",4kpdc6,t3_4kpdc6,Reddang,,Comment,3,0,3
d3gt1l5,2016-05-23 16:10:36-04:00,Reddang,,"http://www.edgewood.edu/Portals/WWW/pdf/Catalog/Edgewood-College-2016-17-Catalog.pdf#page=122
edit:id like to get into software engineering ",4kpdc6,t1_d3gsi61,eliselaplace,,Reply,1,0,1
d3gv7ds,2016-05-23 16:59:01-04:00,eliselaplace,,"This program might be good if you'd like to help out a small business with IT and small programming tasks. If you're interested in computer science fundamentals at all or would like to have a solid software engineering background I would recommend other programs. Keep in mind however that you can self-teach a lot, with the right motivation.",4kpdc6,t1_d3gt1l5,Reddang,,Reply,2,0,2
d3gx7aa,2016-05-23 17:45:37-04:00,wafflestealer654,,"Do you like working on low level programs and hardware? Look at the program info for Computer Engineering, and see if you'd like that.

As the other guy said, CIS is for managing and using computer systems for a business. Totally different than Computer Science.

Computer Engineering teaches you how to work with, program, and build microcomputers.

Computer Science is for more general purpose programs such as cloud applications and websites.",4kpdc6,t3_4kpdc6,Reddang,,Comment,2,0,2
d3gywmq,2016-05-23 18:27:40-04:00,None,,Ah well,4kpdc6,t3_4kpdc6,Reddang,,Comment,1,0,1
4ko49q,2016-05-23 10:13:02-04:00,freezingbum,Understanding IO models (kernel and user threads),"This is cross posted with /r/programming because my questions are more computer science questions, than a programming ones...

I'm trying to understand how the different IO models work and I find that this article summarizes it nicely:
http://www.ibm.com/developerworks/library/l-async/
but what don't understand is what is actually represented in ""blue"" and the ""context switch"" that occurs.

1) If we take the synchronous blocking IO model (as it is the simplest), then the blue represents CPU time, the application is running in the user thread, makes a system call, runs in the kernel thread, then responds to the application (another context switch)? And this occurs in one process. Is what I understand generally correct? the the blue represents CPU time, two threads involved (user level and kernel), and this is all one process?

2) Also, if an application is single threaded, does that mean that there is one user thread and one kernel thread? Figure 2 applies to single threaded applications right? If so, then how would figure 2 look in a multi threaded application?

And perhaps someone can link me to another article that explains what I am misunderstanding?
Thanks!",,,,,Submission,7,0,7
d3gq17m,2016-05-23 15:03:36-04:00,lordvadr,,"A context switch occurs every time a thread gets taken off of the CPU and another one put on the CPU.  Every process is at least one thread.  Most processes are exactly one thread.  The kernel is a process with many threads (at least one for each CPU).

So, while running normally, your application is being taken on and off the CPU at regular intervals so that other programs can run and/or the kernel can do what it needs to do.

But sometimes you ask to be taken off of the CPU by asking the kernel to do something on your behalf.  I/O is one of those instances.

So, to answer your first question, the context switch happens when you initiate the I/O, and another one when the I/O completes (and hundreds or thousands of others in between).  No, you don't have a ""kernel thread"" or anything, you're basically asking another process (the kernel) to do something on your behalf that you can't do (write directly to the disk, for example).

The blue represents time you're on the CPU, the kernel is on the CPU on your behalf, and the other parts are where other things are going on on the system.

If an application is single threaded, that means it only has one thread that can be put on a CPU at any given time.  You're still talking to the multi-threaded kernel.",4ko49q,t3_4ko49q,freezingbum,,Comment,3,0,3
4kiaay,2016-05-22 08:01:25-04:00,alazi,Goal of processing 1k SNMP points per ms. What will be the weakest link in my architecture?,"As stated, I'm curious where my architecture will break down with the goal of 1k points-per-millisecond. 

Here 'processing' is defined as a the moment my snmp protocol implementation realizes/retrieves the value of a point (input), mutated/processed in some minor way, and ends when it is sent it back out through a separate protocol implementation (output). 

There will be a network both before the input and after the output and I, of course, will not be factoring-in or measuring any network latency since even the xmission of a point over SNMP can take many ms. 

My target OS is centOS, and I'm not opposed to the idea of using an cluster of individual computers to achieve this goal. 

My language of choice would be java, and I'm sadly not proficient enough in C to use this as a solution. 

Many Thanks. ",,,,,Submission,0,0,0
d3f4iyd,2016-05-22 08:47:58-04:00,TheTarquin,,A.) You haven't provided a proposed architecture. B.) Most bottlenecks can only be discovered by empirical testing.,4kiaay,t3_4kiaay,alazi,,Comment,7,0,7
d3g94gs,2016-05-23 07:38:29-04:00,lgastako,,"Also, what the actual processing comprises matters.",4kiaay,t1_d3f4iyd,TheTarquin,,Reply,1,0,1
d3gcdqf,2016-05-23 09:41:03-04:00,alazi,,"Well, let's start with the proposal of one system running Centos using a standard x86 processor and a bus capable of 64bit. 

To me this approach would not be capable of passing that many bytes over the physical hardware inherit in a single system (even if I were to carve the hardware up into many virtual machines).. but I'd like input on that if anyone feels differently. 

another architecture would be something like a cluster of simple hardware. Again, thoughts? 

Edit: [a Dell server is a the typical goto hardware that I would propose as a single system] (http://www.dell.com/us/business/p/poweredge-r530/pd)",4kiaay,t1_d3f4iyd,TheTarquin,,Reply,1,0,1
4kfh87,2016-05-21 17:07:52-04:00,BAAAARRFFF,Does there exists an actual feasible solution to the problem without having to explicitly code the exceptions with all the '9's and pulling out ones hair ?,"Given a certain number, number of digits in it, and another integer k..I gotta return the maximum possible palindrome of the given number by changing no more than k digits and leading zeroes permitted, -1 if not possible..

For instance..
4 1
3943 should return 3993

6 3
092282, this'll return 992299

I'll appreciate all help.",,,,,Submission,4,0,4
d3epejj,2016-05-21 20:55:12-04:00,wafflestealer654,,"Treat the input number as an array of digits. Ex: [3, 9, 4, 3].

And then start at the biggest digit, and if you need to change it, then change it to match the digit on the other side. Keep doing that until you get to the middle.",4kfh87,t3_4kfh87,BAAAARRFFF,,Comment,3,0,3
d3emcuh,2016-05-21 19:16:06-04:00,magikker,,"You could break the problem in half and brute force it. 

Treat the number as a string with a limited character set and compute all possible palindromes. Then cast them to ints to test for the biggest.",4kfh87,t3_4kfh87,BAAAARRFFF,,Comment,2,0,2
d3erof3,2016-05-21 22:06:30-04:00,wafflestealer654,,"Try this JsFiddle. You may use the code anyway you want.
[jsfiddle.net/Lotx3o8x/1](https://jsfiddle.net/Lotx3o8x/1/)

It treats the ""certain number"" as an array of digits. It could be optimized to not use arrays, but for the sake of learning, it's going to be an array.

Then, it goes through the digits, and ensures that each digit is the same on both sides (hence making it closer to a palindrome). After doing that, if we're allowed to make more changes, it will start replacing the highest-place digits with 9's.",4kfh87,t3_4kfh87,BAAAARRFFF,,Comment,2,0,2
d3euz7g,2016-05-21 23:57:41-04:00,BAAAARRFFF,,"That does it..Thank you.

And now that I see it..I mean..I figured the solution in two totally separate halves which the stupid me initally though shouldn't correlate 'cause i never accounted for the 9's in the first case and almost a third of a way to the second..I rage quit cause the stuff got so so tangled and I couldn't make a way out

http://pastebin.com/ga1fKfq0

http://pastebin.com/mPfFiAtg

",4kfh87,t1_d3erof3,wafflestealer654,,Reply,1,0,1
4kf3ip,2016-05-21 15:38:23-04:00,BenRayfield,"Is reverse-computing npcomplete, for every possible finite size of memory and cycles?","https://en.wikipedia.org/wiki/Reverse_computation

https://en.wikipedia.org/wiki/NP-completeness",,,,,Submission,3,0,3
d3eplp4,2016-05-21 21:01:36-04:00,avaxzat,,"Reverse computation is not a decision problem and therefore not NP-complete. In fact, it is not even in NP at all, since NP contains only decision problems.",4kf3ip,t3_4kf3ip,BenRayfield,,Comment,1,0,1
d3guty3,2016-05-23 16:50:41-04:00,BenRayfield,,"As a decision problem, given this output, does any solution exist where the 50117th input bit is a 1?",4kf3ip,t1_d3eplp4,avaxzat,,Reply,1,0,1
d3ihqgy,2016-05-24 21:12:44-04:00,lneutral,,"Ah, /u/BenRayfield. I was just wondering whatever happened to you.",4kf3ip,t3_4kf3ip,BenRayfield,,Comment,1,0,1
4kd0ek,2016-05-21 06:17:13-04:00,open90,Platforms for Computing & Human Dimensions of Computing,"hello, This is my first day on Reddit so i hope it is ok to post this question on this community. Next September I begin studying computer science. I'm quite the slow learner and I have had a hard time in previous schools because of dyslexia and dyscalcula. I'm really nervous about beginning and i want to study up as much as i can before the school start. Is there some one who can give me something to get me started on Platforms for Computing & Human Dimensions of Computing. Thank you. Excuse me if my English is off.",,,,,Submission,2,0,2
4kaucm,2016-05-20 18:42:18-04:00,LukeyTheKid,"Are there alternatives to typical computer theory? Basically, is it possible to make a computer that doesn't rely on logic gates, switches, etc?","Based on my (very narrow) understanding of how computers work, the whole thing relies on the thousands and millions of individual logic gates, AND/OR/NOT/XOR etc. that are packed in there. So when computers operate, there's nothing complex going on at the individual level - we're effectively getting our answers and operations done by tons of microdecisions, almost brute forcing the answer (by ""brute force"" in this context, I just mean utilizing the power of sheer numbers).
 
I was watching a video on the limitations of current computers, the challenges once we approach the quantum level, and how quantum computing will help us continue to see Moore's Law in action. However, it is possible to imagine a (distant) future in which even quantum computing falls short, and I was wondering, what would come next?   
  
Is it theoretically possible for, or are there people working on, computers that completely eschew this method of computing, and instead try to achieve the same results with a different method? So no Boolean algebra, no gates, no switches, but the ""same"" functions.   
  
Obviously this would be unfathomably complex, and I personally cannot even offer a potential idea of what an alternative might look like. But I was just imagining a science fiction-like future, which requires computing that the physics of our current computing style does not allow for, and even quantum computing cannot handle. ",,,,,Submission,13,0,13
d3dj850,2016-05-20 19:02:08-04:00,sandwichsaregood,,"There are such a thing as analog computers, though designing and building a serious one has become a bit of a lost art. See this old Navy video on how mechanical [fire control computers work](https://www.youtube.com/watch?v=s1i-dnAH9Y4). Analog computers were in fire control for ships for many years into the modern era of digital computers  (into the late 80's IIRC) simply because they did a better job. The video I linked is fascinating, it goes from very simple gear adders up to intricate mechanical integrators that solve complex differential equations.",4kaucm,t3_4kaucm,LukeyTheKid,,Comment,16,0,16
d3diqx6,2016-05-20 18:48:30-04:00,w1282,,"I can't tell if this is what you're looking for, but you can look into biocomputers that use organic/biological material to do computations.",4kaucm,t3_4kaucm,LukeyTheKid,,Comment,6,0,6
d3dq2tb,2016-05-20 22:32:28-04:00,SakishimaHabu,,The spice must flow,4kaucm,t1_d3diqx6,w1282,,Reply,5,0,5
d3dj485,2016-05-20 18:59:05-04:00,splenetic,,"There are analogue computers, both electronic, fully mechanical and mixed electro-mechanical. They were used to calculate complex mathematical functions for jobs such as autopilots in missiles and gun aiming systems on ships.",4kaucm,t3_4kaucm,LukeyTheKid,,Comment,5,0,5
d3dnakg,2016-05-20 21:05:01-04:00,An_Ugly_Pigeon,,"That's an interesting question.

Our brain is a pattern recognition engine and operates with biological neural nets, and we have built some hardware artificial neural net systems. I should also mention that the science of computation is a broader, more theoretical field than the implementation of digital Von Neumann-like computers (1s and 0s, separate(ish) program and data stores/lines, logic gates, et. al). We don't have to build computers this way, but we've invested time into revising this design and it currently works very well.

There are also DNA-based data storage and I think computation systems currently being experimented with.

> So when computers operate, there's nothing complex going on at the individual level - we're effectively getting our answers and operations done by tons of microdecisions, almost brute forcing the answer (by ""brute force"" in this context, I just mean utilizing the power of sheer numbers).

I'm not really sure what you mean by this. Modern computers consist of layers upon layers of abstraction and complexity. The same applies for a radically different computational platform such as our brain. Our minds operate on top of layers of neural networks, which operate on top of individual neurones, which operate by receiving excitatory and inhibitory electrochemical impulses from the synapses on their dendrites then if the impulses reach a certain threshold, the neurone initiates an electrochemical impulse down its axon to the other neurones it's connected to, which in turn [the neurone] operates on microscopic biological processes to keep the neurone cell functioning, which in turn operates on the rules of physics.

> almost brute forcing the answer (by ""brute force"" in this context, I just mean utilizing the power of sheer numbers).

All computation is ultimately mathematical in nature. It is the theoretical foundation of computation.",4kaucm,t3_4kaucm,LukeyTheKid,,Comment,3,0,3
d3dvdi4,2016-05-21 01:55:48-04:00,Croned,,"There are many things that can accomplish what a ""normal"" computer is able to do, and there's a actually a term for it: [Turing Completeness](https://en.wikipedia.org/wiki/Turing_completeness). Most of the examples of things that are Turing complete are virtual, but there certainly others that exist in the real world.

Most notably, the brain is (arguably) a Turing machine, or at least a damn powerful computer. At certain tasks, the human brain can outperform every other type of computer that we know of: it is not, however, anywhere near as powerful as a binary computer at mathematical computations, which is what I assume you are looking for when you say that you want to continue Moore's Law.

See, unlike other other natural computers (protein creation from DNA is another biological form of computation) classical computers were made with a specific goal in mind: mathematical computation. Their origins stem from performing scientific calculations, so discovering a method of computation that doesn't rely on the fundamental mathematical properties of logic gates yet can perform math operations exceedingly well is unlikely. Even quantum computers are only useful in a limited range of computational tasks and will likely never fully replace classical computers.",4kaucm,t3_4kaucm,LukeyTheKid,,Comment,1,0,1
d3dwpyv,2016-05-21 03:08:07-04:00,Joat35,,I'm picturing something housed in an immense structure.,4kaucm,t3_4kaucm,LukeyTheKid,,Comment,1,0,1
d3dkj1u,2016-05-20 19:40:29-04:00,flopperr999,,All of the comments here are conjecture and here is mine: yes. 1's and 0's are just implementation detail. Compilers that output something other than assembly instructions that are executed on some other computing model is completely feasible. ,4kaucm,t3_4kaucm,LukeyTheKid,,Comment,-2,0,-2
d3dk7xb,2016-05-20 19:31:01-04:00,edrudathec,,"I think it would be possible, since brains would probably fit that description, but such computers would probably not be an improvement.",4kaucm,t3_4kaucm,LukeyTheKid,,Comment,0,0,0
4k790q,2016-05-20 04:09:17-04:00,nhjk,Resources for building a language autocompleter?,"I want to build an autocompleter like intellisense for javascript. I'm self taught so I'm not sure what topics to start looking into. What field of compsci does this fall under? Also, I would appreciate if anyone can point me to some good books and/or courses on the topics.",,,,,Submission,4,0,4
d3cr1yc,2016-05-20 06:23:35-04:00,metthal,,"You want to focus on parsing and how to build your own parser. In fact, it all falls under the field of formal languages, so if you really want to know what you are doing when writing parser, lookup something about regular and context-free languages and their equivalent models (finite and pushdown automata). From there, you can move on to writing LL/LR/LALR parsers. You can even start learning about parsing without knowing anything about formal languages, but you may run into problems. 

There are also tools for building your own lexers and parsers using declarative description of the language. Try to google flex and bison. 

If you are interested in parsing C/C++ then I really recommend you libclang. It already contains the parsers, you just get the abstract syntax tree of your code and you can do with it whatever you want. 

Parser helps you understand the code the user is writing. Without parser it is just a buch of characters, but with parser, everything has a meaning (semantics). Then, you just need to distinguish what the user is typing. For example, user declared object x of type A. Your parser tells you this. User then typed '.' to access the object interface. You just look at the left side of the '.', you find x, you search for your x in symbol table and find out it is of type A, so you look at this type and just show this interface to the user. ",4k790q,t3_4k790q,nhjk,,Comment,3,0,3
d3ct20w,2016-05-20 08:01:32-04:00,nhjk,,"Thanks, I already have a book on this I was planning on reading: [Introduction to Automata Theory, Languages, and Computation](http://www.amazon.com/Introduction-Automata-Theory-Languages-Computation/dp/0321455363/ref=sr_1_1?s=books&ie=UTF8&qid=1463745583&sr=1-1&keywords=introduction+automata). I just started reading CLRS though, do you think it would be helpful to finish it or are the two mostly unrelated?",4k790q,t1_d3cr1yc,metthal,,Reply,2,0,2
d3cy5a9,2016-05-20 10:35:38-04:00,metthal,,"Well, I haven't read CLRS but according to table of contents, it seems like it is common knowledge of all programmers around the world, and I would recommend you reading it if you are not familiar with basic data structures or algorithms like sorting, graph (state space) search etc. You may find hash tables and trees interesting if you want to make autocompleter.

The book you have linked is more specialized towards one field and that is automata theory. You will probably understand it even without programming knowledge, but you won't find any use of the things you read. You should read it after CLRS.",4k790q,t1_d3ct20w,nhjk,,Reply,2,0,2
d3d776n,2016-05-20 14:02:58-04:00,east_lisp_junk,,"> Without parser it is just a buch of characters, but with parser, everything has a meaning (semantics).

A parser does not discover meaning, only syntactic structure.

Parsing is also far from being the real meat of the problem in autocompleting JavaScript code. It takes actual semantic analysis to identify what's a sensible suggestion. Since JS doesn't have static types to tell you what methods an object has, what is legal to pass as an argument, etc., you're sort of on your own to devise an appropriate static analysis. Just looking for what's currently in scope is going to bring lots false positives and still won't find the methods available for calling on an object. Depending on how ambitious you are about suggestion quality, you may be looking at rather complicated data flow analysis (e.g., ""This object `k` was returned from the `foo` method of another object `q` that I was handed as an argument, and I need to know if `k` has a `bar` method whose result makes sense to pass to this other callback function `baz`."").

A lot of compiler textbooks include some material on static analysis, but they generally don't describe algorithms that can handle objects or first-class functions. Nielson, Nielson, and Hankin's ""Principles of Program Analysis"" will cover these issues but may be more complicated than necessary. Maybe also try Might and Van Horn's ""Abstracting Abstract Machines"" papers. Either way, you may find that JS itself is too complicated a language (see the LambdaJS and LambdaS5 work for a ""small"" description of JS's semantics). The good news is that this makes autocompletion for JS a really interesting project. The bad news is that it's a bit more than can really fit into a summer project by a solo beginner.",4k790q,t1_d3cr1yc,metthal,,Reply,2,0,2
d3dayhe,2016-05-20 15:29:29-04:00,metthal,,"Yeah, I agree that strictly parsing does not mean semantic analysis, but if you are using attribute grammars, you can assign semantic actions to your grammar rules and for example bison works that way and most compilers (we can consider it as compiler, because they basically perform similar actions) often do the same.

You pointed out thing I forgot about and that is dynamic language. You would probably need some kind of data flow analysis as you have mentioned to track the types. If I recommend some book, then it would be Data Flow Analysis: Theory and Practice. We have used this book in our Formal Analysis and Verification course at uni and it was great study material.",4k790q,t1_d3d776n,east_lisp_junk,,Reply,1,0,1
4k63rz,2016-05-19 22:24:22-04:00,KatsTakeState,What's a good structure for a server socket game in Java?,"It has to be multi-threaded too. I really only need advice on server side.

I was thinking using the outer class to handle to state of the game.(Start game, Round 1, Game over etc) and use a inner class that extends Thread to handle user input.

What are your suggestions? This is my first multi-threaded program and it will be in Java.",,,,,Submission,1,0,1
d3clt1l,2016-05-20 01:34:34-04:00,rocketbunny77,,Try /r/learnprogramming. You'll get a better response there I think,4k63rz,t3_4k63rz,KatsTakeState,,Comment,3,0,3
d3efwpe,2016-05-21 16:02:54-04:00,KatsTakeState,,Yeah hah I sorta knew this comment was going to show. ,4k63rz,t1_d3clt1l,rocketbunny77,,Reply,2,0,2
d3efm4n,2016-05-21 15:54:31-04:00,BenRayfield,,"Outer vs inner class are not relevant. I recommend including the source code of Apache HttpCore and export both that and your code into an executable jar file (doubleclick to run server instantly). HttpCore gives you the ability to write a threadable function that takes a byte array parameter and returns a byte array (or hook in data formats such as defined in http content-type), and use that as your server.",4k63rz,t3_4k63rz,KatsTakeState,,Comment,2,0,2
4k2f19,2016-05-19 09:20:26-04:00,Onlineshrekjr,DFA (Deterministic Finite Automaton) ending with 11 or 10,,,,,,Submission,3,0,3
d3bjeyz,2016-05-19 09:21:11-04:00,Onlineshrekjr,,Can anyone help? Thank you,4k2f19,t3_4k2f19,Onlineshrekjr,,Comment,1,0,1
d3bjtub,2016-05-19 09:32:58-04:00,bhrgunatha,,Are you asking [how ow to convert an NFA into a DFA](https://lmddgtfy.net/?q=How%20to%20convert%20an%20NFA%20into%20a%20DFA)?,4k2f19,t3_4k2f19,Onlineshrekjr,,Comment,1,0,1
d3bk4nh,2016-05-19 09:41:16-04:00,Onlineshrekjr,,"Actually im not quite sure. I was given the task of determining a DFA that ends with 11 or 10. Do i necessarily need to form a NFA first and then transform it to a DFA? Or is there a more direct way? 

If it is the former, i should have just googled how to transform NFAs to DFAs. Sorry.",4k2f19,t1_d3bjtub,bhrgunatha,,Reply,1,0,1
4k21v7,2016-05-19 07:45:43-04:00,llll1111,Help with ll1,"Hey guys,

i got :

S → A
A → BC
B → DE
C → -A | ε
D → (A) | num
E → *B | ε

and want to know:

Does it accept num*

I have no idea how to proof it...
Please help

Thanks

",,,,,Submission,0,0,0
4k1ucw,2016-05-19 06:30:51-04:00,guardianhelm,[Book Recommendation] What are some books that can help a Computer Scientist switch/expand to Medicine?,"Disclaimer: This is a repost from /r/suggestmeabook as I feel that the two audiences are heterogeneous enough to give different feedback.

A friend that has a background in electrical/software engineering and currently works on machine learning is interested in switching/expanding to Medicine and she's going to apply for a MSc in a relevant postgraduate department.

Her birthday is coming up soon and I'd like to help her get prepared for the transition by getting her a book. However I have no idea where to start and it doesn't help that she's being vague. She's told me that she's not sure what field she prefers but I know that she finds Medical Robotics and biology interesting. Her motivation is to ""be able to help people more directly"".

I'm looking either for something generic enough to cover a large field of applied computer science and be suitable for giving some direction to a newbie, or a book that's really exceptional, so it's worth it even if it omits important parts. Do you have any recommendations? If not, maybe you could direct me to a more suitable subreddit/source of information?

I'm sorry if this isn't the right place to ask this question but I feel really lost here.

I'm open to alternative resources as well, a book is just the first idea that came to mind.

Thanks a lot, I really appreciate your time.",,,,,Submission,6,0,6
4jxqr6,2016-05-18 13:11:59-04:00,CARGLE,Loops repeating 3 times.,"I'm currently working through the Java's Beginner's Guide (Sixth Edition) by Herbert Schildt and some of the example programs I have run that involve loops are doing something unexpected. When I have a loop execute with an incorrect condition it appears to repeat the check 3 times (without iteration) and then move on to the next expected place in the code. 

I was wondering if anyone could provide insight on this. Is there sum preset instruction I don't know about for failed loop statements? I'm using atom text editor and windows command prompt to run the .java files. ",,,,,Submission,2,0,2
d3ajbkk,2016-05-18 14:25:23-04:00,filljfry,,could you provide the code you are using? and the output? then it would be easier to tell what is going on.,4jxqr6,t3_4jxqr6,CARGLE,,Comment,5,0,5
d3ammg0,2016-05-18 15:34:35-04:00,CARGLE,,Just posted,4jxqr6,t1_d3ajbkk,filljfry,,Reply,1,0,1
d3alxj2,2016-05-18 15:20:35-04:00,GreyMX,,"The problem that you think you have is probably not the problem you really have. Provide the problematic code snippet, and then people here might be able to help you.",4jxqr6,t3_4jxqr6,CARGLE,,Comment,1,0,1
d3ammn4,2016-05-18 15:34:42-04:00,CARGLE,,Just posted,4jxqr6,t1_d3alxj2,GreyMX,,Reply,1,0,1
d3aml7b,2016-05-18 15:33:54-04:00,CARGLE,,"    //(Here is the code snippet) Loop until an S is typed.
    class ForTest {
      public static void main(String args[])
        throws java.io.IOException {
          int i:

      System.out.println(""Press S to stop.""):

      for (i = 0: (char) System.in.read() != 'S': i++)
        System.out.println(""Pass #"" + i):
    }
}
",4jxqr6,t3_4jxqr6,CARGLE,,Comment,1,0,1
d3ao78k,2016-05-18 16:06:13-04:00,panderingPenguin,,"Assuming you're talking about in the debugger, there are actually three separate statements in the for loop that can be executed:

    i = 0:
    (char) System.in.read() != 'S':
And  

    i++:

The first statement in a for loop only gets executed the first time you hit the loop, immediately before you enter the loop. The second executed before each iteration, and the iteration only proceeds if the second statement returns a value that evaluated as true. The third statement executed after each iteration.  I'm guessing you're seeing multiple statements executing on one line because of this.",4jxqr6,t1_d3aml7b,CARGLE,,Reply,1,0,1
d3aopwx,2016-05-18 16:16:14-04:00,CARGLE,,"Here is a picture of what I'm getting after running it with the ""incorrect"" input.

http://imgur.com/KcULUjT

Edit: Please ignore the text before the ForTest.java file is ran. ",4jxqr6,t1_d3ao78k,panderingPenguin,,Reply,1,0,1
d3ap7qq,2016-05-18 16:26:21-04:00,panderingPenguin,,"Ah, I see. Ignore my prior comment.

I'm going to take a wild guess and say that this has to do with the control characters inserted by the terminal when you hit enter, specifically linebreak (\n), and maybe a carriage return (\r) because I think the windows terminal inserts both.",4jxqr6,t1_d3aopwx,CARGLE,,Reply,2,0,2
d3apbwh,2016-05-18 16:28:45-04:00,CARGLE,,"Ahh. I just read up on that in a later chapter but didn't know it could have that sort of effect. That's awesome, thanks!",4jxqr6,t1_d3ap7qq,panderingPenguin,,Reply,1,0,1
d3atskj,2016-05-18 18:09:07-04:00,Coolisbetter,,"I would bet that this is it.

Using ReadLine I believe prevents this behavior. ",4jxqr6,t1_d3ap7qq,panderingPenguin,,Reply,1,0,1
4jx0qw,2016-05-18 10:49:54-04:00,HiggeldyPiggeldyPop,"Aside from court orders and legislation, how does a government SHUT DOWN the internet? Is there a switch to flip?",,,,,,Submission,2,0,2
d3akf7q,2016-05-18 14:48:40-04:00,lordvadr,,"The ""internet"" by itself, is a (essentially) a bunch of agreements between different ISP's and how/where they connect to each other.  There's a protocol (it's called BGP) by which every ISP tells every other ISP in the world what IP addresses they each have.  What I'm getting at is that if you told, say, AT&T to ""shut off"" the internet, there's nothing that would stop other ISP's from routing traffic to each other through the links they have to each other.  Any decent ISP has many ""transit circuits"" (what they're called).

If a government passed a law saying (for example) all US ISP's had to stop routing, nothing would stop (for example) Germany <-> England traffic.  Most (or, much) of the internet would hold itself together. 

It's not like consumer internet where you have one path to one provider.  Providers have many paths to and through many other providers.

With that said, there's a big vulnerability within the DNS system, by which most of the internet would quit ""working"" so to speak (at least from an end user perspective) by disabling the root dns servers (but it's hundreds of them).

Source: Work for an ISP/ITSP/MSP.",4jx0qw,t3_4jx0qw,HiggeldyPiggeldyPop,,Comment,3,0,3
d3akuas,2016-05-18 14:57:31-04:00,HiggeldyPiggeldyPop,,"OK, so when Iran, say, kills the internet within their borders, they're really just strong-arming the ISPs...

There's no direct action the government itself is taking...",4jx0qw,t1_d3akf7q,lordvadr,,Reply,3,0,3
d3alhgr,2016-05-18 15:11:17-04:00,lordvadr,,"Yeah, kind of.  In many countries (especially authoritarian ones), the main ISP is owned and operated by the state because it's a nationally operated telco in the first place...because the state controls all information...and/or owns all the physical fiber into/out of the country.  So when Iran shuts off the internet, they may be strong-arming ISP's, or shutting down these ISP's connections from the state-owned ISP, or physically (or virtually) disconnecting the path(s) to the outside world.

The rest of the world just see's Iran's IP addresses fall out of the global routing table.",4jx0qw,t1_d3akuas,HiggeldyPiggeldyPop,,Reply,2,0,2
d3amabv,2016-05-18 15:27:32-04:00,HiggeldyPiggeldyPop,,"So, if they own and operate the pipe coming into their country, they can kill that connection. 

In this scenario, could their still be Iran <-> Iran traffic? In order to kill that, they'd have to go to each ISP, right? Or are they a sort of central hub?",4jx0qw,t1_d3alhgr,lordvadr,,Reply,2,0,2
d3an50x,2016-05-18 15:44:51-04:00,lordvadr,,"Well, that depends.  The internet is a complicated beast.  

If the state ISP *is* some kind of central hub (e.g., by law, you have to buy your bandwidth from the state ISP and you can't negotiate circuits to other ISP's), they could cut-off all other ISP's from each other, but now nothing stops traffic that originates and terminates all inside one ISP's network.  While there's probably not a whole lot of interesting stuff in there, it's not out of the question that the same ISP that hosts, say, www.sexygoats.ir doesn't also have a bunch of residential subscribers or other business customers.  Assuming things like DNS worked like they should, all that would continue to work.

But simple answer, yes, if the other ISP's have connections to each other, (assuming there's enough connections and capacity), the whole of Iranian internet could stay together.",4jx0qw,t1_d3amabv,HiggeldyPiggeldyPop,,Reply,1,0,1
d3ancjm,2016-05-18 15:49:02-04:00,HiggeldyPiggeldyPop,,"...and, of course, Google Loon could always float overhead. Accidentally...

Thanks :-)",4jx0qw,t1_d3an50x,lordvadr,,Reply,2,0,2
d3aibb7,2016-05-18 14:03:30-04:00,Total_InZAINity,,"To get internet data across the ocean and other large areas, they use fiber optic cables. A country could cut those cables severing connections between countries. However, it would probably piss off everyone and result in anarchy and war. To block internet within a country though, it would unrealistic to attempt to cut every cable in order to prevent all internet access.",4jx0qw,t3_4jx0qw,HiggeldyPiggeldyPop,,Comment,1,0,1
d3adbf6,2016-05-18 12:16:26-04:00,w1282,,They tell the ISPs to stop routing traffic.,4jx0qw,t3_4jx0qw,HiggeldyPiggeldyPop,,Comment,1,0,1
4jw991,2016-05-18 07:47:24-04:00,javaCl,How would I use Data Mining techniques to get information out of this water measurements data set? [x-post r/learnprogramming],,,,,,Submission,1,0,1
4jvx2u,2016-05-18 05:57:47-04:00,alphaaa55,CPU or GPRU,"When a program is running, is it the OS or the program itself that decides what computations are done by the CPU and which by the GPU, and how?",,,,,Submission,11,0,11
d3a3l5i,2016-05-18 08:06:08-04:00,TiarnaNaTuaithe,,"It would be the program itself. There are languages for GPU programming (e.g. CUDA, OpenCL) that allow the programmer to send data out to the GPU, and then have some functions execute there. This is how we are able to interact with the GPU in code. You can also write regular code to run in the same program, with GPU code being differentiated from it by certain keywords and/or by different extensions on the header file.",4jvx2u,t3_4jvx2u,alphaaa55,,Comment,6,0,6
d3a8016,2016-05-18 10:17:27-04:00,alphaaa55,,Do operating systems delegate the user interface part to the GPU using these languages?,4jvx2u,t1_d3a3l5i,TiarnaNaTuaithe,,Reply,3,0,3
d3a8djh,2016-05-18 10:26:31-04:00,TiarnaNaTuaithe,,"Yes, the graphics driver is what actually mediates the transfer of data etc. But which data is sent, and what is done to it, is decided by the program.",4jvx2u,t1_d3a8016,alphaaa55,,Reply,2,0,2
d3anyh1,2016-05-18 16:01:15-04:00,panderingPenguin,,"Only if they have something called hardware-accelerated rendering.  If not, they perform the same functionalities with software on the CPU.  Most modern rendering software has hardware acceleration capabilities but will have a fallback mode if no compatible GPU is available.",4jvx2u,t1_d3a8016,alphaaa55,,Reply,2,0,2
d3a6j8i,2016-05-18 09:39:47-04:00,Delwin,,"It's the program.

How boils down to the program telling the graphics driver that it's got some data to send to the GPU and some code to execute on the GPU.  If you want exact details I'd go look at CUDA, OpenCL, OpenACC or OpenMP 4.5.",4jvx2u,t3_4jvx2u,alphaaa55,,Comment,2,0,2
4jutfv,2016-05-17 23:46:27-04:00,H-a-r-o,Should I feel behind if I haven't done much with programming prior to taking CS classes?,"I'm currently at a community college and am looking to transfer to a four-year school in around a year. My current plan is to pursue CS as a major. The problem is that I haven't had much personal experience and honestly don't feel like I know that much about programming itself. The most experience I've had is with some basic coding while making a few terrible video games as a hobby. The thing is that it often seems like people going for CS majors already have a lot of knowledge that they've just picked up by themselves before even taking classes. I'm taking some intro level CS classes very soon, but I still can't help feeling behind. Is this a legitimate concern? If so, any tips on how to gain some experience? Thanks very much and have a great day. ",,,,,Submission,11,0,11
d39wtob,2016-05-18 02:08:34-04:00,achNichtSoWichtig,,"Short answer: no

Just scroll this sub, you'll find tons of questions like this. So don't worry.",4jutfv,t3_4jutfv,H-a-r-o,,Comment,3,0,3
d3bcyv3,2016-05-19 04:12:08-04:00,H-a-r-o,,"Yeah, I probably should've done some more looking around the sub first. Thanks though!
",4jutfv,t1_d39wtob,achNichtSoWichtig,,Reply,1,0,1
d3bmvxk,2016-05-19 10:50:16-04:00,achNichtSoWichtig,,"Np, I get why it can make you feel uneasy. Before CS I studied business crap, finished after 5 years, then started CS with no prior knowledge... and it went well for me... programming is not super hard to get into, if you are willing to learn new stuff and read quite a bit.",4jutfv,t1_d3bcyv3,H-a-r-o,,Reply,1,0,1
d39yhw5,2016-05-18 03:28:40-04:00,thegeicogecko,,"Not at fuckin' all. You sound like you're ahead of the curve honestly. I'm doing a computer systems minor, and am thus taking quite a few cs classes. I had 0 previous coding experience, but still do within the top 10% of my classes. Experience might help a bit, but it isn't a huge factor at all. ",4jutfv,t3_4jutfv,H-a-r-o,,Comment,3,0,3
d39uxui,2016-05-18 00:53:57-04:00,gbay,,"When I did my undergrad there were students with no experience at all (university of Waterloo, Ontario). However, go learn some things as a huge part of computer science and more specifically software development is being a self starter. You can start on sites like code academy.",4jutfv,t3_4jutfv,H-a-r-o,,Comment,2,0,2
d3ab1dx,2016-05-18 11:26:51-04:00,IAmNotNathaniel,,"> The most experience I've had is with some basic coding while making a few terrible video games as a hobby.

Sounds like you have more experience than most, already.

The important part is, did you enjoy making your games? Of course they were ""terrible"" if you compare them to anything commercially - they were your first.

But if you had fun poking around, seeing how changing this line or that line affected the end result, then you are in good shape.

Also remember that computer science != programming, although they go hand in hand.",4jutfv,t3_4jutfv,H-a-r-o,,Comment,2,0,2
d3bcyhz,2016-05-19 04:11:33-04:00,H-a-r-o,,"Hey, thanks for the responses everyone. This made me feel quite a bit better!",4jutfv,t3_4jutfv,H-a-r-o,,Comment,1,0,1
d3e8am9,2016-05-21 12:27:14-04:00,Tollwen,,I'm in the same boat. Good to know there are others out there pretty much starting from a clean slate too!,4jutfv,t3_4jutfv,H-a-r-o,,Comment,1,0,1
d3fuh8g,2016-05-22 21:37:44-04:00,CheekiBreekiIvDamke,,"Not even slightly. The kids who have been studying C since highschool will be ahead of the curve but they have their own challenges to face in life. If you put in good work you will achieve similar grades. You will start at the bottom learning the absolute fundamentals in almost every CS course. This is a good thing because a lot of people who are ""good with computers"" turn out to not know much about the actual workings. ",4jutfv,t3_4jutfv,H-a-r-o,,Comment,1,0,1
4ju5ym,2016-05-17 21:03:34-04:00,mr-sweet-and-awful,A couple questions regarding nslookup in the Windows command prompt,"Hello,

I'm pretty new to the world of computer science and ran into something that I'm hoping one of you kind folks can shine light on for me:

I have this homework assignment asking me to list the IP address produced by using the nslookup command for certain domains. The actual assignment is super easy, but while I was doing it, I realized that when I type ""nslookup www.ibm.com"" I get a different IP address that when I type ""nslookup ibm.com""

When I navigated to those IP addresses in Chrome, the ""ibm.com"" took me straight to the IBM website while the ""www.ibm.com"" IP address told me it was an invalid URL. 

According to whois, the IP address I got with ""nslookup ww.ibm.com"" is a Polish server belonging to Akamai. (Wikipedia says they provide cloud services and IBM is lasted as a customer, so this isn't exactly a startling discovery.)

What I'm trying to understand is why ""nslookup www.ibm.com"" and ""nslookup ibm.com"" produce different IP addresses. 

Thanks for reading and even more thanks if you feel like taking the time to answer. ",,,,,Submission,3,0,3
d3axdaj,2016-05-18 19:42:33-04:00,zefcfd,,"www is a subdomain of ibm. the top level domain ibm.com is resolved because people at IBM created a DNS A record that will resolve that host ""ibm.com"" to a specific address. i believe people refer to this as a ""naked domain"". however people often redirect people using the www. subdomain to just ibm.com using a CNAME record. a cname record allows you to effectively resolve a host name by providing another host name to resolve. so their dns config might look like


A RECORD | ibm.com | 192.168.99.100
CNAME      | www.ibm.com      | ibm.com



using the ""dig"" tool from commandline

    :: QUESTION SECTION:
      
     :ibm.com.			IN	A

    :: ANSWER SECTION:
   
     ibm.com.		683	IN	A	129.42.38.1


      :: QUESTION SECTION:
      :www.ibm.com.			IN	A

      :: ANSWER SECTION:
      www.ibm.com.		1928	IN	CNAME	www.ibm.com.cs186.net.
      www.ibm.com.cs186.net.	60	IN	CNAME	www.ibm.com.edgekey.net.
      www.ibm.com.edgekey.net. 7200	IN	CNAME	www.ibm.com.edgekey.net.globalredir.akadns.net.
      www.ibm.com.edgekey.net.globalredir.akadns.net.	825 IN CNAME e2874.x.akamaiedge.net.
      e2874.x.akamaiedge.net.	20	IN	A	23.79.0.131

the thing to understand here is that a host name is the only thing you can count on being consistent. ip addresses are always changing (e.g. when you resolve ibm.com from france, you may get an ip address for a server living closer to france where ibm hosts their site - which is called geographical distribution and is used frequently with international sites to improve performance). also your ip address may change in cloud environments (like amazon aws). 

that's plenty of info for now, but i recomend using dig and traceroute  to understand things better ",4ju5ym,t3_4ju5ym,mr-sweet-and-awful,,Comment,2,0,2
d39newb,2016-05-17 21:35:03-04:00,YetAnotherUserName0,,"From a DNS perspective, there is nothing special about the 'www' host name.  It is just a common convention to make them the same.  I suggest you look up setting up DNS in named.  A shortcut is this question/answer:  http://serverfault.com/questions/145158/how-do-you-add-an-a-record-for-a-root-domain",4ju5ym,t3_4ju5ym,mr-sweet-and-awful,,Comment,1,0,1
d39q2m5,2016-05-17 22:35:09-04:00,mr-sweet-and-awful,,Thanks for the lead! I appreciate you taking the time to help me out. ,4ju5ym,t1_d39newb,YetAnotherUserName0,,Reply,1,0,1
4jr0fe,2016-05-17 09:54:46-04:00,floating_on_a_wave,Systems engineering practice,"Dear all,

I have an interview (which will consist of a practical activity) in a few weeks with a tech company for a role in systems engineering. I know the practical activity will involve centos linux commands and Puppet/ Ansible. Does anyone recommend any resources to help me prepare? Or any basic tutorial showing how to deploy manifests in Puppet or anything to do with Ansible?  

Thanks!",,,,,Submission,4,0,4
d39qgbh,2016-05-17 22:44:09-04:00,_ntnn,,"Not really a /r/AskComputerScience question, but anyway:

The introductions on their websites are pretty good. For practice you
can use [Vagrant](https://vagrantup.com) for reproducible virtual
machines - also keeps your whole training configuration in one version
controlled directory, since vagrant automatically downloads the boxes
and can provision the boxes with your configuration.

In place of Vagrant any configurable vm hypervisor will do, however
Vagrant can use multiple backends, thanks to plugins (their default
vmware (iirc) is also just a plugin).

If you don't have experience with the command line yet set up at least
a vm or better as dual boot on another partition and get comfortable.

There was the Linux 101 from the Linux Foundation on edx available for
some time and I think it was recently opened completely (usually in edx
courses you have to sign up during a specific period,
sometimes it requires you to do monthly work etc.pp.), so you may be
able to do it at any time.

However - it is a very basic course that will only help you to get
started and without daily practice you probably won't get through.
",4jr0fe,t3_4jr0fe,floating_on_a_wave,,Comment,1,0,1
4jnwbu,2016-05-16 19:12:01-04:00,Eilai,DFA Union turned into a NFA? Is this correct?,"I have a question that took the form of:

Construct the DFA that accepts: {ba} ∪ {b^n: n ≥ 1}.

So as I understand it, first I construct the DFA for the LHS, and then the DFA for the RHS, and then find their union.

For the LHS I have states: {q_0, q_1, q_2} where on b q_0 -> q_1 and on a q_1 -> q_2; q_2 is final.

For the RHS I have just two states {r_0, r_1}, where since we need at least one b, r_0 -> r_1 on b and then loops with r_1 being the final state.

I construct the cross product of states so: Q = {{q_0,r_0}, {q_1,r_0}, {q_2,r_0}, {q_0,r_1}, {q_1,r_1}, {q_2,r_1}}.

Now, as I understand it; since it's union, all transitions for M(q) stay, as do all transitions for M(r), and all final states also stay.

{q_0,r_0} b-> {q_1,r_0},
{q_0,r_0} b-> {q_0,r_1}, ...

Here is where I get stuck, as it seems like I end up with an NFA as a b transition happens from the same state into two divergent directions.

What did I do wrong here?",,,,,Submission,1,0,1
d385umu,2016-05-16 20:03:28-04:00,metthal,,"Union of two DFAs does not necessarily results in DFA, the guaranteed is only that union of two regular languages is regular language. Since it is regular languague for sure, it can be represented by some DFA that accepts this language. That means, if you really want DFA, you can still determinize it afterwards. ",4jnwbu,t3_4jnwbu,Eilai,,Comment,1,0,1
d3860gp,2016-05-16 20:07:34-04:00,Eilai,,"Actually according to my professor my problem is that I did a cross product: that is apparently only for Intersection: for Union I create an e-NFA and then use subset construction stuff to turn it into a DFA.

Does this sound correct to you?",4jnwbu,t1_d385umu,metthal,,Reply,1,0,1
d386fx9,2016-05-16 20:18:26-04:00,metthal,,"I guess it depends on the study materials or professor whether you do it through cross product or introduction of epsilon transitions from the new start state. You can even create your own algorithm where you remove those epsilon transitions and that should result in DFA.

If your professor does not like the cross product, you should probably look into study texts or ask him what algorithm is expected from you. However, I think he should accept any solution that works. And cross product does. ",4jnwbu,t1_d3860gp,Eilai,,Reply,1,0,1
4jnnor,2016-05-16 18:16:57-04:00,jjrr123456,Do phones passively generate and store accelerometer data that could be viewed at a later date?,"I saw a news story about a man who was found severely injured, though police are uncertain if he was hit by a car, fell from a building, or something else. This got me thinking about car accidents, hit and runs, and other situations where someone likely had their phone with them, something traumatic happend, but then there was a lot of uncertainty surrounding what/when the traumatic event happened. Do phones already capture some of this data? If not, would it be possible? Are there good reasons to *not* capture some of this?",,,,,Submission,10,0,10
d383rr8,2016-05-16 19:09:08-04:00,lneutral,,"No, in general they do not. 

One good reason is that storing all the readings for any significant length of time requires a lot of space.

Another is that IMU data is notoriously noisy. Without any other data, calculating the trajectory of the device is very unreliable - you might look up dead reckoning if you're interested in hearing more.",4jnnor,t3_4jnnor,jjrr123456,,Comment,4,0,4
d38gw9y,2016-05-17 01:14:50-04:00,apendleton,,"The extent varies from sensor to sensor, but all phone sensors are battery-hungry to at least some extent, so phones don't typically keep them powered. Apps have to request data from them, which causes the phone to power them. The apps can then do whatever they want with it (including save it someplace), and if they apps also get permission to run in the background they might be able to do it continuously. Foursquare, for instance, captures your location frequently to notice when you're in a business it knows about, and uploads information about your location to its servers. One could do the same with the accelerometer, conceivably. But it's not an inherent function of the phone.",4jnnor,t3_4jnnor,jjrr123456,,Comment,2,0,2
4jjec4,2016-05-15 23:20:49-04:00,lilred181,Should I study Number Theory or Discrete Math first?,"I was recently complaining to a professor during a casual conversation that I feel as if I got very little substantial knowledge from my Discrete Math course and that the Number Theory sections of the course were my favorite part. As a graduation present he gave a [book](http://www.amazon.com/Classical-Introduction-Modern-Graduate-Mathematics/dp/038797329X?ie=UTF8&keywords=number%20theory&qid=1463368304&ref_=sr_1_5&sr=8-5) on Number theory and a [book](http://www.amazon.com/Logical-Approach-Discrete-Monographs-Computer/dp/0387941150) Discrete Math. I would like to get more into the theory side of Computer Science because that is what I am interested in as a hobby. So would it potentially be more beneficial to work through one of these books first or does it not really matter?

TL;DR: would it be more beneficial to study one subject before another?",,,,,Submission,14,0,14
d3769ur,2016-05-16 01:14:01-04:00,docares,,"I feel discrete math is more beneficial because all of the topics covered are bridges to more advanced topics.

* Logic analysis
* Combinatorics
* Elements of proofing
* Trees
* Integer mathematics

Number theory is beneficial to further pure mathematics skills and is extremely interesting. Yet, I find that I use discrete math topics on a daily basis.

Edit: Have you taken a course and/or are comfortable proofing?",4jjec4,t3_4jjec4,lilred181,,Comment,10,0,10
d37fksg,2016-05-16 09:17:25-04:00,Darkflux,,"I'm not saying I use Number Theory every day, but if you're interested in cryptography, Number Theory is the mathematical foundation for how it works. 

Edit: eg. http://crypto.stackexchange.com/a/16486",4jjec4,t1_d3769ur,docares,,Reply,5,0,5
d4jmru9,2016-06-22 13:32:48-04:00,andybmcc,,"As well as coding, integrity checks, etc.",4jjec4,t1_d37fksg,Darkflux,,Reply,1,0,1
d37zcsy,2016-05-16 17:17:44-04:00,lilred181,,Thanks for the advice. I have taken a course on proofs but I am not great at them.,4jjec4,t1_d3769ur,docares,,Reply,1,0,1
d376fvl,2016-05-16 01:20:52-04:00,mbizzle88,,"Discrete math helps with understanding algorithms and data-structures, so obviously that lines-up well with computer science. Number theory helps with further study of abstract algebra (group theory, ring theory, field theory, etc). Abstract algebra is really cool, but isn't especially useful for computer science.",4jjec4,t3_4jjec4,lilred181,,Comment,4,0,4
d381kq3,2016-05-16 18:11:56-04:00,Spanone1,,What's a good way to learn discrete math?,4jjec4,t3_4jjec4,lilred181,,Comment,1,0,1
4jhboa,2016-05-15 14:35:11-04:00,idontlistenwell,What kind of electrical engineering background is needed to read computer architecture books?,"Reading computer architecture books, say Hennessy's, is like reading a biology book for me. A lot of long paragraphs describing terms and concepts. It's very boring, and I don't learn much, just new jargon and facts. Like cache has tags and so on. 

I wonder if I am missing some pre-requisite EE knowledge. If you can recommend me books that can fill in the gaps how these stuff such as caches are implemented hardware-wise, that would be great. Like real textbooks, not *Code*. My background is maths. It's not like I am looking to work with Intel, I just noticed c++ conference talks on youtube mention more and more on how hardware affects your code performance/optimization. Seems they are worth knowing if you are writing code.

This post may sound really ignorant, I apologized for that.
",,,,,Submission,4,0,4
d36lfqv,2016-05-15 15:03:45-04:00,kooppoop,,"Know basic design of logical circuits, that's a start. A good benchmark I think is, you know enough if you can understand the hardware that composes a basic ALU cell (logic gates, muxes, etc.).

That said, you're not gonna get away from high level concepts, and you're gonna need to have a bit of a programmer's perspective to understand why bits mean certain things in certain contexts and how they are transformed to create meaningful action. Sure, maybe you just want to know hardware implementation of caches, but the same cache can be implemented any number of different ways in hardware, and what is more salient in the study of hardware is the different types of caches and how they work. This is very much a ""give a man a fish v. teach a man to fish"" problem.

If you want to learn hardware, learn to think high level and be able to translate that understanding to a finer granularity when you need to. Nobody is thinking about individual logic gates when they're trying to understand hyperthreading. Learn to think of large parts as black boxes that talk to each other, and learn how those boxes should behave.",4jhboa,t3_4jhboa,idontlistenwell,,Comment,4,0,4
d36odvf,2016-05-15 16:25:26-04:00,idontlistenwell,,"Thank you. Googling 'digital circuit design' and similar terms led me to this book *Bebop to the Boolean Boogie*. Seems to fit my purpose. Not too technical,  and has a bottom-up explanation.

Believe it or not, I actually have passed a computer architecture class. But as I said, it's just biology for me. Yes, we wrote some benchmark code to show the hardware effects, but they are just collection of facts for me. Because of this hardware feature, this or that happens and so on. ",4jhboa,t1_d36lfqv,kooppoop,,Reply,1,0,1
d36pdv1,2016-05-15 16:51:29-04:00,kooppoop,,I would also look into learning a hardware description language like vhdl. The moment I knew I had a good understanding of basic architecture was when I was able to write a datapath in vhdl and actually run code on it. That gives you a really good understanding of functional units and how they interact,4jhboa,t1_d36odvf,idontlistenwell,,Reply,1,0,1
d36tv4r,2016-05-15 18:49:53-04:00,Merad,,"Try *Computer Systems: A Programmers Perspective* by Bryant and O'Hallaron. It's an excellent text and a much more gentle introduction to the subject matter. I haven't cracked open Hennessy and Patterson since my grad level architecture class, but I recall it being much more in depth. ",4jhboa,t3_4jhboa,idontlistenwell,,Comment,1,0,1
d36y1p7,2016-05-15 20:54:12-04:00,NearSightedGiraffe,,"Would also recomend starting with computer organisation/ design text books, I cannot remember the authors of what I used in school, but they wrote one book on comp org which was an easier read and then the nore chellenging computer architecture.

Beyond that, as others have said- enough knowledge of logic gates to understand an ALU. Within this, make sure you understand the purpose of the clock and rising v falling edge. Otherwise pipelining is going to be rather harder. And pippining was already hard to start  with.",4jhboa,t3_4jhboa,idontlistenwell,,Comment,1,0,1
d37vldi,2016-05-16 15:51:05-04:00,idontlistenwell,,"Thank you. I tried *Structured Computer Organization* when I took my computer architecture class. But that has an even higher level discussion. And longer prose. Good book though, easy to read.

It seems because I am not a CS student, and most CS students who take architecture have done logic design and circuits courses. So they can connect the conceptual discussion on computer architecture book to physical implementation. While for me, caches, pipelining, SIMD and so on are just abstract conceptual stuff that have real performance consequences.",4jhboa,t1_d36y1p7,NearSightedGiraffe,,Reply,1,0,1
4jfro3,2016-05-15 07:46:02-04:00,NeedToRegisterQuick,What are some advice you can give to a Compsci student trying to gain a love for maths?,"Enough to at least motivate me to learn properly for my mathematical subjects.

[Reposted via reply to a comment]

I used to love maths when I was younger around grades 3-10 but only because I could easily do the calculations in my head. 

Since then, I've had a tough time with it since the effort required increases massively and I was lazy. Having gaps in my foundational knowledge made it harder for me to enjoy learning new concepts. 

I want to play around with data, machine learning and better my programming skills with maths. I want to get to the point where I really do enjoy learning about new concepts. 

I'd love to see the practical element in maths, I think that'd motivate me more. 

What do you guys suggest?

",,,,,Submission,12,0,12
d36ibn8,2016-05-15 13:37:17-04:00,umib0zu,,"Are you in college yet? It's definitely hard to see how ""math"", which I'm assuming you mean calculus, is useful in programming at that level. I actually program for a living, but I came from the other side, physics and math. I had plenty of calculus/linear algebra courses, so on my end, my question was, ""What's all this computer science stuff about and how is it related to math?"". Maybe I can maybe offer some help in the other direction by giving examples of what motivated me to learn more and use my math skills in computer science. One thing that really stood out to me was how in upper level, [complex analysis is directly used in the general study of algorithm analysis](https://www.youtube.com/watch?v=qap2MyBTSZk). Another is how there are interesting [programming languages that can many fields in mathematics are embedded in](http://math.mit.edu/~dspivak/teaching/sp13/CT4S--static.pdf), almost making this language [a ""hardware"", while a field like Algebraic Topology, a software](https://www.youtube.com/watch?v=OupcXmLER7I). What's interesting is even [logic can be seen as embedded in this programming language](https://www.cs.kent.ac.uk/people/staff/sjt/TTFP/), but [it might not be too obvious how it is for a while](http://arxiv.org/pdf/0903.0340v3.pdf).

What's important to know is that, computer science programs always include some amount of calculus study. There's a reason for this. I think what you're missing is that even a bachelors does not mean that's all their is to a subject. Computer science is vast, and you've likely only seen a small portion. Know that you should take all the knowledge you can get, because you may never know when you might need it.",4jfro3,t3_4jfro3,NeedToRegisterQuick,,Comment,3,0,3
d36a1o5,2016-05-15 09:27:14-04:00,CS027,,Why are you studying Computer Science? What do you hope to achieve with it? What is your mathematical background? Why are you currently not inspired by math? ,4jfro3,t3_4jfro3,NeedToRegisterQuick,,Comment,2,0,2
d36ajg3,2016-05-15 09:46:22-04:00,NeedToRegisterQuick,,"I used to love maths when I was younger around grades 3-10 but only because I could easily do the calculations in my head. 

Since then, I've had a tough time with it since the effort required increases massively and I was lazy. Having gaps in my foundational knowledge made it harder for me to enjoy learning new concepts. 

I want to play around with data, machine learning and better my programming skills with maths. I want to get to the point where I really do enjoy learning about new concepts. 

I'd love to see the practical element in maths, I think that'd motivate me more. 

What do you suggest?

",4jfro3,t1_d36a1o5,CS027,,Reply,1,0,1
d36bzn4,2016-05-15 10:35:06-04:00,CS027,,"Computer Science *is* math. I always find it interesting when people are interested in CS but say they don't like mathematics because with some exceptions the former is a subset of the latter. 

Figuring out the runtime of an algorithm is math. You want to deal with big data? Pretty much all analysis you'd be doing on it would be with statistics. Machine Learning is all math. Clustering is multi dimensional math. Programming languages are category theory. Graphics linear algebra. It goes on and on. 

Is it possible to work as a programmer without really getting math? Sure. You can build boring CRUD applications. But if you want to do the cool things? Really understand how things work? That doesn't just require math. It flat out IS math. ",4jfro3,t1_d36ajg3,NeedToRegisterQuick,,Reply,11,0,11
4jefb5,2016-05-14 23:12:43-04:00,EnterSadman,Physical examples of a PDA,"When abstract machines are taught, they are typically stratified into finite state machines (FSM), push down automata (PDA), and Turing machines. 

It's easy for me to conceptualize a FSM because there are tons of examples like thermostats, electric doors, etc.

It's also easy enough to see Turing machines as our own computers (with the obvious limitations that the von Neumann model imposes).

What I cannot conceptualize is a physical PDA. I found a wiki article that listed things like systems to oversee a CPU, but nothing more tangible. 

Do you know of an example of such machine?",,,,,Submission,10,0,10
d36fo7l,2016-05-15 12:21:57-04:00,veeberz,,The only concrete example I can think of is a parser. PDAs accept any strings belonging to a language described by a CFG. So compilers and interpreters can use PDAs to detect syntax errors (i.e. source code is rejected by PDA),4jefb5,t3_4jefb5,EnterSadman,,Comment,2,0,2
d36guz5,2016-05-15 12:55:47-04:00,EnterSadman,,Ah yes! I recall now the example of parenthesis matching as an introduction to PDAs.,4jefb5,t1_d36fo7l,veeberz,,Reply,1,0,1
4jaasg,2016-05-14 02:02:32-04:00,Koyhaku,How can I limit my video streaming bandwidth?,"I've made an album showing a GlassWire graph of three different YouTube videos at 1080p: http://imgur.com/a/Jqrfc

The problem comes in at the end when the third YouTube video maxes out my bandwidth at 75 mbps. I'm wondering if there's a way to limit capping like that, there's 4 people in my home which pretty much constantly watch clips of anything on various sites, if one of those people are capping out the network the rest of us are buffering needlessly. It only takes a few seconds to download a short video, but I've noticed if I click on a link while the network speed is already capped, when the network isn't capped anymore, the link I clicked on doesn't load until I refresh the page which is annoying to say the least, on a Hulu video that's another 30-120 seconds of ads (I don't use Adblocker for websites I like).

If more info is needed to answer the question I'll provide.",,,,,Submission,6,0,6
d35g7y1,2016-05-14 14:01:19-04:00,dkaarvand,,"Well, you can't control the traffic in any way from your personal computer. It has to be a through the router, and if it does not support QoS or Bandwidth limiter, you're out of luck.

What router do you have?",4jaasg,t3_4jaasg,Koyhaku,,Comment,3,0,3
d35j5z4,2016-05-14 15:29:48-04:00,Koyhaku,,"I have the Verizon Quantum router. It doesn't look like it supports QoS or Bandwidth limiting. I guess I'm going to need to purchase a new router. I'm looking at this one: http://www.newegg.com/Product/Product.aspx?Item=N82E16833704039
TP-LINK TL-WR841N Wireless N300

The reviews seem like it holds out alright, but I'm not sure about anything beyond that. I welcome any input you or someone has.",4jaasg,t1_d35g7y1,dkaarvand,,Reply,1,0,1
d35qt3c,2016-05-14 19:19:33-04:00,dkaarvand,,"Do not under any circumstances buy a router that does not have an open source firmware. There are many respected community members creating custom firmware for any open source routers being sold out there. I currently have the Asus RT-AC66U, which is an OpenSource router that is based on Linux. Usually when buying a router, they either lack a lot of features, or they got a lot of issues with the official firmware. After around 12 months after the release, the company stops updating the firmware.

&nbsp:

If you buy an OpenSource router, the community members add tons of features and fix a lot of the bugs from the official one. There's a few respectable ones around, and they are:  
1. [DD-WRT](http://www.dd-wrt.com/site/index)  
2. [Merlin AsusWRT](https://asuswrt.lostrealm.ca/)  
3. [OpenWRT](https://openwrt.org/)  
4. [TomatoWRT](http://www.polarcloud.com/tomato)  

There's a lot of difference between the developers' firmware. Merlin takes the official firmware from Asus, and fixes alot of the issues as well as developing extra features. He focuses more on delivering performance rather than features. DD-WRT offers tons of features, but lacks in stability and performance and Tomato offers a bit of both. So you got a lot to choose from. It can transform an old Linksys WRT54GL (old router, but OpenSource) to a complete beast of a router with tons of features and stability if you install Tomato or DD-WRT.

So get an OpenSource router. Any other routers should be ignored.",4jaasg,t1_d35j5z4,Koyhaku,,Reply,3,0,3
d352l9e,2016-05-14 04:27:21-04:00,danltn,,Do you mean QoS settings on your router?,4jaasg,t3_4jaasg,Koyhaku,,Comment,1,0,1
d352sgd,2016-05-14 04:40:12-04:00,Koyhaku,,"Yes, something similar. My particular router doesn't have QoS settings. I've googled for software but the only thing that pops up is Net Limiter. 
I'm guessing my only option is to buy a new router. That's a real hassle for me though because I'd need to switch the coax cable to an Ethernet cable through a few walls cause my current is from Verizon >.>. I was hoping there was another option.",4jaasg,t1_d352l9e,danltn,,Reply,1,0,1
d35j85p,2016-05-14 15:31:38-04:00,Pseudofailure,,"If you own the router, and it's a compatible model, you could flash it with a different firmware like [OpenWRT](https://openwrt.org/), [DD-WRT](http://dd-wrt.com/site/index) or [Tomato](http://www.polarcloud.com/tomato), among others. ",4jaasg,t1_d352sgd,Koyhaku,,Reply,2,0,2
d3531go,2016-05-14 04:57:02-04:00,danltn,,Verizon router ethernet cable to new router WAN port :),4jaasg,t1_d352sgd,Koyhaku,,Reply,1,0,1
4j7ub0,2016-05-13 15:01:03-04:00,thatonequestion,I recently acquired an old server (HP Proliant DL380 G5). What are some unique CS related things I can do with it?,"to keep it short a guy was selling it on craigslist for $40 without the RAM or Hard drives. I've already ordered both and I still feel like I'm coming out on top here. 

Currently I'm probably just going to set it up as a personal web server, but I was wondering if there was anything particularly unique I could do with it? It's still a pretty powerful machine despite its age.",,,,,Submission,10,0,10
d34kgt6,2016-05-13 17:33:24-04:00,nighthawk648,,"You can set up a python server on there to accept incoming connections. Then you can write network protocols, or if your into web development you can host your own website. Or if you like software development you can host your own database and then make an app that interacts with the data being stored. ",4j7ub0,t3_4j7ub0,thatonequestion,,Comment,3,0,3
4j6fmx,2016-05-13 09:58:41-04:00,Zondartul,Can Automated Theorem Provers do more than find proofs?,"Heya, hope this is the right place for this. I have no formal education in AI, logic, or machine learning, but I consider myself a decent programmer. Currently I'm trying to wrap my head around Automated Reasoning. I've been reading about what Automated Theorem Provers are and how to use them, but some things are still not clear to me:

1) Can they do more than answer yes/no questions (i.e. proving a theory or failing to prove it)? Is there a way for ATP to solve a problem like ""find some prime number greater than 10 and print it""?

2) If ATPs are not the best for "" questions"" and ""logic puzzles"", then what is? I would like to find a program that can read a description of a solution, and then generate a solution that fits that description (where ""solution"" can be an arbitrary collection of variables, an elaborate plan of action, or some other kind of structured answer)

3) Could you recommend any books on either ATP or automated reasoning (particularily the symbolic kind)?",,,,,Submission,1,0,1
d36h4w0,2016-05-15 13:03:42-04:00,EnterSadman,,">  Can they do more than answer yes/no questions (i.e. proving a theory or failing to prove it)?

No. A theorem prover says ""yes this is logical"", or ""no this is not logical"".

>  If ATPs are not the best for "" questions"" and ""logic puzzles"", then what is?

A Turing machine. Good luck though, you'll run into the halting problem pretty quickly.

> Could you recommend any books on either ATP or automated reasoning

It's rather foolish to jump right in to theorem provers without understanding logic (which is evident from your questions). I would recommend a book by van Dalen called ""Logic and Structure"". If you want to actually play around with a theorem prover (and you don't want to get into type theory) I would suggest the system Coq.

EDIT --
I also want to point out that you used numbers rather flippantly in your question. Any time we involve proofs that include Peano axioms (like your prime number question), you're going to run into Godel's incompleteness.
",4j6fmx,t3_4j6fmx,Zondartul,,Comment,1,0,1
4j4489,2016-05-12 21:54:25-04:00,TheLastKantian,Does Computer Science take memes seriously or is it more of a psychology thing?,"Is their any texts that try to dismantle memes and their respective ""information"" nature from a computer science perspective? I guess memes could fit into the data science part of it. Thanks!",,,,,Submission,0,0,0
d35b70x,2016-05-14 11:30:18-04:00,StephenSwat,,"Assuming you're talking about *memes* as we know them on reddit (viral videos and funny image captions), I am not aware of any research done on the ""information nature"" as you put it, although I am not entirely sure what you mean by that. I think you will find much more interesting research regarding memes within the field of, as you mentioned, psychology. [This](http://www.aaai.org/ocs/index.php/%20ICWSM/ICWSM11/paper/viewFile/2757/3304) article gives a general introduction to internet memes and [this](http://dl.acm.org/citation.cfm?id=2433489) details some ways to tell and predict how viral a meme is or will become.",4j4489,t3_4j4489,TheLastKantian,,Comment,1,0,1
d3fuwrx,2016-05-22 21:48:56-04:00,TheLastKantian,,"Yeah, this is the stuff I was looking for.",4j4489,t1_d35b70x,StephenSwat,,Reply,1,0,1
d3funkg,2016-05-22 21:42:24-04:00,CheekiBreekiIvDamke,,Unrelated to CS study although you'll find the IT faculty of most unis is filled with post-meta-ironic memer students. ,4j4489,t3_4j4489,TheLastKantian,,Comment,1,0,1
d3fuwbj,2016-05-22 21:48:37-04:00,TheLastKantian,,"Absolute cancer, they must be purged.  /u/ StephenSwat gave me what I was looking for though.",4j4489,t1_d3funkg,CheekiBreekiIvDamke,,Reply,1,0,1
4j3vpq,2016-05-12 20:55:23-04:00,DangerZone23,Any tips to unlock restrictions on a time-based PDF?,"I purchased and downloaded a PDF from a 3rd party that handles my undergraduate university transcripts. [http://www.studentclearinghouse.org/](http://www.studentclearinghouse.org/)

The main PDF is only able to be opened with Adobe Reader. Then inside that PDF, is another PDF. The actual transcript. I've tried to open it, but it's refusing because it's time based(past 30 days). I've tried the following methods to get it to unlock:

* Changing the date and time on my computer - did nothing.
* Trying an online PDF hacker service - there's no password so there's nothing to crack.

Am I shit out of luck? Any other ideas you can think of to unlock it? The official error message is:

This document is locked. Your permission to view it has expired. If you want to open the document, contact the person who sent it to you and ask them to extend the expiration date.

If I did that, I would have to pay for another damn transcript! Any ideas? Thanks!",,,,,Submission,5,0,5
d33i2bg,2016-05-12 21:47:21-04:00,crookedkr,,Open it with a text editor and you may be able to just delete the commands that limit viewing. I've seen this with other restrictions.,4j3vpq,t3_4j3vpq,DangerZone23,,Comment,3,0,3
d33jgyp,2016-05-12 22:24:08-04:00,DangerZone23,,"Excellent idea! However, I'm not sure where to find and what to delete. Tried playing around with a few things, but it kept corrupted the PDF. Perhaps changing the dates around. I'll try that.

BAH! No luck.",4j3vpq,t1_d33i2bg,crookedkr,,Reply,2,0,2
d34apob,2016-05-13 13:45:13-04:00,lordvadr,,I'll get it open for you.  PM me for contact details.,4j3vpq,t3_4j3vpq,DangerZone23,,Comment,1,0,1
4j0fyd,2016-05-12 08:33:07-04:00,iamahugederpderp,University of Amsterdam MSc in Artificial Intelligence vs. University of Copenhagen MSc in Computer Science.,"Hey,

I was just accepted to the University of Amsterdam for an MSc in Artifical Intelligence and to the University of Copenhagen for an MSc in Computer Science. 

From what I can tell from rankings (and taking them with a grain of salt...) is that both universities are very good schools. So reputation-wise, I don't think I can go wrong with either. Finances look similar (I'm a dual US/EU-citizen, so I'll get negligible/free tuition and probably tack on US or European loans to cover living expenses). 

Now, the thing is I'm not really sure what I want to do in CS. My undergrad is in physics and I took a decent amount of CS/CE but not a full degree's worth or anything and there are whole areas I haven't touched (programming language theory, for example). I took an intro to ML class along with some Udacity/Coursera ML stuff and thought ML/AI/Computer Vision was super neat, hence my application to Amsterdam's AI program. 

I'm planning to go for a PhD. But I'm not certain. So here are my questions:

1. Will choosing Amsterdam over Copenhagen significantly improve my ability to do a PhD in an AI-area? (Copenhagen offers AI courses, and I could do my Master's thesis in an AI-area if I wanted). 

2. I'm guessing that if I choose Amsterdam, my career and PhD options would be pretty much limited to AI, right?

3. If I choose Copenhagen and choose to not get a PhD, would it make life more difficult if I wanted to do AI work in the industry?

4. Given that my degree is in physics, would you say it makes more sense to just do Copenhagen to get a good, comprehensive CS foundation? And then afterwards, either in my second year of the MS or for a PhD, I can concentrate on a specific area? Or if I know I'm interested in an area (AI), should I just go for that at the risk of not discovering some other area I might be interested in?

5. It is likely I will struggle more in Copenhagen's program due to my limited CS background. Is taking this risk prudent?

I know these questions may feel sort of ""obvious"", but I'd like to see what you guys think. Maybe I'm missing something and it's a significant life decision on my part.

Really the contention is going into a department I know I have interest in (AI at Amsterdam) at the risk of closing opportunities if I choose Amsterdam over Copenhagen. I'm also giving some preference to Copenhagen since I'd rather live there over Amsterdam. 

Thanks so much.




",,,,,Submission,7,0,7
d32uess,2016-05-12 12:26:17-04:00,fosterbuster,,"Can't really help you with your questions.. But you might want to take living costs and housing availability into account. 

I'm Danish, and even Danes have a really hard time finding a place to live which can be rented or bought with human money.",4j0fyd,t3_4j0fyd,iamahugederpderp,,Comment,2,0,2
d33113f,2016-05-12 14:50:17-04:00,iamahugederpderp,,The situation is only a little better in Amsterdam from what I can tell. ,4j0fyd,t1_d32uess,fosterbuster,,Reply,1,0,1
d32upmu,2016-05-12 12:32:47-04:00,ukkoylijumala,,"Maybe /r/cscareerquestions might have been more fitting for this, but anyway.

>  I took an intro to ML class along with some Udacity/Coursera ML stuff and thought ML/AI/Computer Vision was super neat, hence my application to Amsterdam's AI program. 

Without knowing the reason why you specifically mention Amsterdam here: UCPH has courses in ML and Computer Vision, and you are eligible to take courses from other universities in the greater Copenhagen area (namely: Danish Technical University and IT-University).

Here are the course databases:

* UCPH: http://kurser.ku.dk/
* DTU: http://www.kurser.dtu.dk/search.aspx?menulanguage=en-gb

> It is likely I will struggle more in Copenhagen's program due to my limited CS background. Is taking this risk prudent?

That depends on how limited it is. The mandatory courses are:

* Advanced Programming (learning functional programming paradigms), 
* Advanced Algorithms and Data Structures (exactly what the name says, uses the [standard literature from Cormen, Leiserson, Rivest, Stein (CLRS)](https://mitpress.mit.edu/books/introduction-algorithms))
* Advanced Computer Systems (how concurrent and reliable systems are designed/built, this one may be difficult for you)
* Machine Learning (good introduction, was cranked up a little this year regarding difficulty)

These are done in the first semester (!) and you should know how to program solutions to more or less theoretical problems. I assume you do.

Except these, you can choose courses (almost) freely as you like.

I want to add that there is a very good study environment at UCPH.

Since am not really familiar with your specific wish-field (just a tiny bit), I may not be able to help you much, but if you have questions about the university in general, shoot me a PM. I also know of at least one other redditor that is studying here (although also not in AI/ML), if you want a second opinion :D",4j0fyd,t3_4j0fyd,iamahugederpderp,,Comment,1,0,1
d331qft,2016-05-12 15:05:44-04:00,iamahugederpderp,,"Hi there,

>Without knowing the reason why you specifically mention Amsterdam here: UCPH has courses in ML and Computer Vision, and you are eligible to take courses from other universities in the greater Copenhagen area (namely: Danish Technical University and IT-University).

I'm aware of that! But since Amsterdam's program is specifically AI their offerings are a little more extensive from what I can tell. You're also a little more focused on AI since--even though you can direct your study program at UCPH to that effect--the entire program there is AI and all your fellow students are doing it too, etc. 

>That depends on how limited it is. The mandatory courses are:

I saw the mandatory courses, but didn't look up their descriptions until now. I already know Haskell fairly well and have done significant concurrent programming, so I think I'd be fine for Advanced Programming. Algorithms is a bit scary since it's been five years since I took an algorithms class! And I'm actually not too worried about Computer Systems--my operating systems class spent a long time on remote systems (reliably sending commands, remote procedure calls, etc), file systems and networking and talked about these things in the context of distributed systems. So I think I'll be alright. :D
And I've already taken an introductory ML class, so that last one should be fine!

So I guess upon further inspection, I think I'll be fine as long as I spend some time before the program starts reviewing algorithms. 


>I want to add that there is a very good study environment at UCPH.

I'm surmising from your post that you're doing or did an MSc in CS at UCPH. Have you been very satisfied with the program? What's your general impression?

Thanks! :)








",4j0fyd,t1_d32upmu,ukkoylijumala,,Reply,1,0,1
d33581x,2016-05-12 16:21:31-04:00,ukkoylijumala,,"> Amsterdam's program is specifically AI 

Oh, that makes sense :D

> I'm surmising from your post that you're doing or did an MSc in CS at UCPH. Have you been very satisfied with the program? What's your general impression?

Yep, international student as well, just writing my thesis at the moment.

I am very satisfied indeed. As mentioned before, the environment is awesome. There's no other way to describe it. Plenty of social activities, if you're into that, but it's also fine if not. As an international student, you would probably take part in the welcome program, where you meet, most likely, very cool people. You will make friends and you will sit with them in the student-run canteen (Kantinen) and work on assignments, sometimes until late at night, and it will be exhausting, but so gratifying at the same time.

It is difficult to know what to write, what is satisfactory to me, may not be sufficient for you. So, if you have more specific questions, go ahead :D

The teachers are very eager to actually teach you and they like discussion and criticism when they have the time, so mostly during the lectures or in ""labs"". Most of them have also a good sense of humor. The system follows some flat hierarchy, but that is probably similar in the Netherlands.

The only real negative thing I can say is that they had a very badly executed study reform last summer, due to a political shift, and on top of that some extensive funding cuts. Luckily the student body negotiated some mitigations of that bad reform which will take effect this autumn, so you wouldn't notice much of that anymore :) The funding cuts also killed the annual spring festival, which is now held under ""emergency lighting"" in a much smaller venue thanks to some sponsors.

Just another thought: You could, in theory, do the first year at DIKU (that's what the CompSci department at UCPH is called), and the second year including thesis in Amsterdam. You could take AI specific courses in the first semester and write your thesis in the second with an expert in that field.

But I'm not trying to advocate that much or change your mind, just trying to help you to make an informed decision :)",4j0fyd,t1_d331qft,iamahugederpderp,,Reply,1,0,1
d33769b,2016-05-12 17:04:30-04:00,iamahugederpderp,,"Thank you for all the information!

So I have decided on Copenhagen over Amsterdam. But then I got an email from the University of Edinburgh saying I've also been admitted to their program. So now I don't know again, haha.

Really, the quality of life seems better to me in Copenhagen. And I prefer the more flexible program (Edinburgh is also Artificial Intelligence). I am, however, fairly concerned about housing availability and cost of living. 

What has been your experience with regard to housing? If I do end up going Copenhagen (and I'm leaning towards it still), I'll arrange a trip in early June to go to the city and find housing as soon as possible. Is that realistic? I've read it can take months and months. Maybe I'd do an Airbnb or something and stay until I found a place. ",4j0fyd,t1_d33581x,ukkoylijumala,,Reply,1,0,1
d33avmn,2016-05-12 18:34:09-04:00,ukkoylijumala,,"> What has been your experience with regard to housing? If I do end up going Copenhagen (and I'm leaning towards it still), I'll arrange a trip in early June to go to the city and find housing as soon as possible. Is that realistic? I've read it can take months and months. Maybe I'd do an Airbnb or something and stay until I found a place. 

Unfortunately the housing situation is dire, but if you come in June, you might have a chance to get places where students just left, or will leave soon. The university has a housing foundation, but they are rather expensive and the places are gone quickly, but they also have a list of private lessors (you have to request that by e-mail) who would like to rent out to students: this is how I found my place. There are also websites that aim at finding groups of student that would rent a shared flat together, and the default apartment finding websites, of course. Be aware of deposit scammers. But since you'd come personally, this is less of a problem. The prices are also really high, ~400 EUR or ca. 3000 DKK a month is almost the minimum.",4j0fyd,t1_d33769b,iamahugederpderp,,Reply,1,0,1
d34etmh,2016-05-13 15:17:37-04:00,iamahugederpderp,,"I was expecting much worse than 400 EUR. (I.E. 1000 or so a month). That's not so bad. But it probably heavily depends on roommates/a room vs. apartment, etc. I'm reluctant to get roommates at first since I don't know anyone yet :( .

Anyway, I'm visiting cph on the 25th and will start looking then and will likely then come back for the whole summer and do my best to find housing before the semester starts. :)

Also, I was looking at the block/timetaple groups at cph (http://www.science.ku.dk/english/student-life/studying-at-the-faculty/academic-calendar/). Is it true that on Wednesdays and Thursdays you just have 8 hours of one class :O?",4j0fyd,t1_d33avmn,ukkoylijumala,,Reply,1,0,1
d3550f8,2016-05-14 07:09:29-04:00,ukkoylijumala,,"> I was expecting much worse than 400 EUR. (I.E. 1000 or so a month).

If you can afford 700 EUR a month, you're golden... probably. To me it was also more important to find a quiet and calm study environment, without being dragged in annoyingly frequent social activities with flatmates. Luckily I found exactly what I was looking for, but this is far from guaranteed.

> Is it true that on Wednesdays and Thursdays you just have 8 hours of one class :O?

I'm not sure what you're referring to here, maybe there is 3 hours lecture and some labs afterwards, but 8 hours seems a bit excessive.

In general you should think of courses with the same schedule class (i.e. A, B, C, D) to have a strong likelihood of collisions. So you might wanna avoid this, if you can.

In general there are not *that* many actual lectures, but a lot of time where you are expected to work on assignments and self study. The approximate distribution of how the course responsible expects you to divide your time is also written on the courses page. Often you have some practice/exercise/lab classes, which are optional, but highly recommended. These are often longer/more than the lecture for that class in order to ensure that everybody can get a fair share of the teachers (or TAs) time.",4j0fyd,t1_d34etmh,iamahugederpderp,,Reply,1,0,1
d35o8pn,2016-05-14 18:00:09-04:00,iamahugederpderp,,">To me it was also more important to find a quiet and calm study environment, without being dragged in annoyingly frequent social activities with flatmates.

Yeah, that's very important to me. I'm somewhat introverted/a bit of a loner, so I always look for a quiet/simple place to live. Hopefully I'll manage to find something to similar to what you have found.


>I'm not sure what you're referring to here, maybe there is 3 hours lecture and some labs afterwards, but 8 hours seems a bit excessive.
In general you should think of courses with the same schedule class (i.e. A, B, C, D) to have a strong likelihood of collisions. So you might wanna avoid this, if you can.

I think I misunderstood the A,B,C,D system. On CU's site it says things like Wednesday: 8:00-12:00 C1, 13:00-17:00 C2. I understood that to mean that a C class goes the entire day during those hours.

Anyway, thanks a lot for all your detailed and helpful responses. I really appreciate it.",4j0fyd,t1_d3550f8,ukkoylijumala,,Reply,1,0,1
d33ucof,2016-05-13 05:45:29-04:00,har777,,Congrats for the admits ! Edinburgh has a stellar reputation in AI according to what I've heard. You should really consider it. Also Amsterdam has Max Welling :),4j0fyd,t1_d33769b,iamahugederpderp,,Reply,1,0,1
d33ucz9,2016-05-13 05:46:01-04:00,har777,,How much does tution + housing cost at Copenhagen for you ?,4j0fyd,t1_d33581x,ukkoylijumala,,Reply,1,0,1
d33upe8,2016-05-13 06:06:45-04:00,ukkoylijumala,,"There is no tuition for EU, CH and Nordic citizens (perhaps others as well).

Housing for me is about 430 EUR a month, including internet, heating and water. Although the internet connection is terrible :D",4j0fyd,t1_d33ucz9,har777,,Reply,2,0,2
d33uy0h,2016-05-13 06:20:18-04:00,har777,,I kinda assumed that you were from outside the EU :D,4j0fyd,t1_d33upe8,ukkoylijumala,,Reply,1,0,1
d34epb7,2016-05-13 15:14:57-04:00,iamahugederpderp,,International just means not Danish. :) ,4j0fyd,t1_d33uy0h,har777,,Reply,2,0,2
4iwi1i,2016-05-11 14:59:57-04:00,ramalama-ding-dong,Is combining multiple inputs into a single transition valid on DFA?,"For example, if the grammar was

S -> abA

A -> baB

B -> aA|bb

From starting state S in the FA, can we draw the transition as ""ab"" for the input and go directly to state A?  Or do we need input from start state S as ""a"" which goes to state A1, then from A1 we use input ""b"" to reach state A.

I ask this because a deterministic FA means that we need to cover all inputs from and to every possible state.  

There's the interpretation that the inputs are on {a, b}, but you could convince me that the inputs of this language are actually {a, ab, ba, bb}.  The problem doesn't specify the alphabet or inputs though.

I know I've seen in textbooks that sometimes inputs/transitions are combined like in the example I just gave, but I never paid attention to whether those were DFA or just NFA.",,,,,Submission,6,0,6
d31qxkx,2016-05-11 15:59:19-04:00,crookedkr,,"I might be wrong but it looks like a simple DFA to me. Though it is written as a grammar, which in general don't have to be DFAs, it looks like this one is. Any DFA can be written as a regex and vice versa. The language here is generated by the following regex (unless i missed something):

    abba(aba)*bb

That is you start at S so must write 'ab' and have the nonterminal A left over.

    abA

but then you are forced to write 'ba'

    abbaB

There is only one terminal so the end has to be 'bb' at some point. So the 'bb' on the end can either be immediate or you can go back to A generating 'a' along the way but then you only have one choice, you have to generate a 'ba' and now you are back at the deciding point. You can pump 'aba' 0 or more times then ebd with 'bb'. Now that I write it i'm pretty sure my regex is correct.

The state machine  would be easy to draw from the regex but i don't have an easy way to add it to this comment. To answer your question on whether you can add more than one symbol on a transition...i don't see why not, it's a forced output so why add two transitions, but what ever your TAs say is what you should do. The alphabet is very clearly {a,b} i'm not sure what you mean by inputs. If you have a DFA/regex you should be able to take a string of the alphabet of any length and determine if it is part of the language or not.",4iwi1i,t3_4iwi1i,ramalama-ding-dong,,Comment,1,0,1
d31rpcc,2016-05-11 16:15:07-04:00,ramalama-ding-dong,,"The question actually asks to construct a DFA out of the grammar, which is why I ask about DFA representation of this grammar.

The question about more than one symbol on a transition is because the answer should be a DFA, not NFA.  That means we have to represent every transition with every input and to every state.  If combining symbols is acceptable, then the combination of possible inputs on any transition would be limitless.  I'd have to show a, ab, ba, aba, abb, aaa, bbb, etc... and that's only from the start state.  If we can only have a single symbol per transition, then we can clearly say symbol a takes us to this state, and b takes us to this other state.

I've found the answer in the textbook though.  Simplifying transitions and combining symbols is for convenience only and is no longer a proper DFA.",4iwi1i,t1_d31qxkx,crookedkr,,Reply,1,0,1
d31v2w5,2016-05-11 17:26:41-04:00,_ActionBastard_,,"Sounds to me like you want epsilon transitions (allowing the empty string as an input symbol so that you can change state without really consuming anything).  You get an NFA if you add those, which you can always translate to an equivalent DFA, but I wouldn't bother if you've not encountered them in your coursework yet.",4iwi1i,t1_d31rpcc,ramalama-ding-dong,,Reply,1,0,1
d31z25r,2016-05-11 19:02:15-04:00,crookedkr,,what text book are you using?,4iwi1i,t1_d31rpcc,ramalama-ding-dong,,Reply,1,0,1
d32850f,2016-05-11 22:54:26-04:00,ramalama-ding-dong,,Formal language and automata theory by Peter Linz. It's terrible. ,4iwi1i,t1_d31z25r,crookedkr,,Reply,1,0,1
d36hd1u,2016-05-15 13:10:09-04:00,EnterSadman,,"That's one of the better resources on the subject, and it's the one I'll open if I have a quick question. ",4iwi1i,t1_d32850f,ramalama-ding-dong,,Reply,1,0,1
4iwe3u,2016-05-11 14:37:27-04:00,PyroFox123,"Kiddie likes computers alot, kiddie wants to learn. I need some help. [Questions]","I want to become ""Experienced"" in computers. [software, hardware, etc] Tell me what and how to do it
1. What should I start learning at first?
2. What sites do you reccomend for this?
3. Is Math really so crucial?
There probably will be more as my curiosity grows. Whoever helps me now, check this post a few times occasionaly, it might make all the difference for me",,,,,Submission,3,0,3
d31seu8,2016-05-11 16:29:40-04:00,None,,[deleted],4iwe3u,t3_4iwe3u,PyroFox123,,Comment,2,0,2
d31wdrk,2016-05-11 17:56:41-04:00,PyroFox123,,"Hey, Can I ask the major a thing? What grades, contests, etc I need on my backround to get a GOOD job in software development/""White hat hacking""",4iwe3u,t1_d31seu8,None,,Reply,1,0,1
d320psk,2016-05-11 19:44:24-04:00,darthandroid,,"Honestly, keep above a 2.0 and as many internships as you can.

Work experience and your ability to actually design a program matters more than grades or contests.",4iwe3u,t1_d31wdrk,PyroFox123,,Reply,1,0,1
4iv4qe,2016-05-11 10:21:57-04:00,greg0ry_,Automata: Constructing DFA'S Question,"For instance, if I was asked to construct a DFA which accepts strings from the alphabet {a,b,c} where the sum of a's and b's is odd and the sum of a's and c's is divisible by 3.

I have the answer in front of me, but it does not explain how it got ther. For instance, the answer explains that due to the DFA's requierments, there is consequently six states. How did they get that number? Is there a specific process I should follow to get the answer? 

The answer is this: http://imgur.com/aBzupzk

I understand 10 is the final state because 1 remainder would make the number odd, along with 0 remainder from the modulo 3 would make the number divisible by 3, but that's about it. Could someone walk me through the process so if I was asked a similar question I could create the DFA?",,,,,Submission,6,0,6
d31ilu4,2016-05-11 13:04:54-04:00,avaxzat,,"The basic idea behind these constructions is very simple. First, you analyze all the cases that can present themselves: these will form the set of states of the DFA. Then, you take each pair of states and think about how the automaton should go from one state to the other and vice versa. This gives you your transitions.

For example, in your situation, the following cases need to be considered if the automaton encounters the symbol a:

* if the sum of a's and b's is odd => the sum of a's and b's becomes even:
* if the sum of a's and b's is even => the sum of a's and b's becomes odd:
* if the sum of a's and c's is x mod 3 => the sum of a's and c's becomes x+1 mod 3.

The cases for b and c are analogous. Thus, your states need to encode the current parity of a+b as well as a+c mod 3. You will need one state for each possible situation. Then you simply connect them together according to the laws of arithmetic.",4iv4qe,t3_4iv4qe,greg0ry_,,Comment,1,0,1
d31klee,2016-05-11 13:46:33-04:00,CS027,,"In addition to the straightforward analyze what's happening method, there's also a sledgehammer that you can use. Recall that the intersection of two DFAs are also a DFA. You can compute this intersection by taking the cartesian product of all possible pairs of states and then drawing out the transitions. This isn't likely to be minimal.  

[Here](http://math.stackexchange.com/a/147500) is one explanation of this.",4iv4qe,t3_4iv4qe,greg0ry_,,Comment,1,0,1
d31nt4a,2016-05-11 14:54:32-04:00,ramalama-ding-dong,,"Additionally, the resulting DFA would be correct, but it may not be clear exactly what the transitions are to each state, or to tell what input has been given to reach any given state. Typically the transitions and state labels of a DFA constructed by intersecting two DFA is arbitrary, and conserving the spirit of the original DFA's can be quite difficult depending on the complexity of the requirements on the original DFAs.

For the given problem though, it would be faster for me to use this algorithm than to build it from scratch.  I'm not a quick thinker, but can follow instructions quite well :)",4iv4qe,t1_d31klee,CS027,,Reply,1,0,1
4itfof,2016-05-11 01:18:00-04:00,ramalama-ding-dong,"Help on some Formal Language/Automata questions (prove language regular/not regular, show string not accepted by a grammar)","I'm a grader for a professor at my university for formal language and automata.  I'm having trouble solving a problem and I must have the correct answers/understand the answer since I need to grade fairly and accurately.

1. Is L = {a^m b^n : |n-m| = 2} regular? Prove.

I'm pretty sure the answer is not regular (can't build an FA for this).  Is this disproven to be regular by the pumping lemma?  When I took this course, we weren't taught how to prove/disprove regular, just identify.



Thanks for any help.",,,,,Submission,3,0,3
d31367y,2016-05-11 04:57:35-04:00,Fairbr0,,"I've had a go, and found it was non-regular. 

http://imgur.com/cLDdC8s",4itfof,t3_4itfof,ramalama-ding-dong,,Comment,5,0,5
d31hgkg,2016-05-11 12:40:47-04:00,ramalama-ding-dong,,Thanks!  I practiced some other pumping lemma usage and it's not nearly as complicated as it seems at first.  ,4itfof,t1_d31367y,Fairbr0,,Reply,1,0,1
d31vhts,2016-05-11 17:36:07-04:00,_ActionBastard_,,"Good to hear.  Something that always helped direct my intuition with pumping lemma shit is this:  FA's cannot count to n.  They can count to 9, or 69, or 420 squillion.  But they cannot count to n.",4itfof,t1_d31hgkg,ramalama-ding-dong,,Reply,1,0,1
4it4pl,2016-05-10 23:47:37-04:00,just_learning_,Drivers versus Application on Baremetal,If I am writing code baremetal on a processor (no small kernel or full fledged OS) is application code and driver code on the same layer?,,,,,Submission,1,0,1
4irode,2016-05-10 17:56:17-04:00,Occams_Blades,"My engineering class is learning Arduino. Yeah, I have some questions?","First of all, I'm sorry if this isn't the right place for this sub, but I have a few questions. Thank you.


A. We keep putting ""Serial.begin(9600);"" in our start up. I know we need it, but what does it do?

B. Where can I learn extra cool stuff about Arduino or Java in general?

C.  What cool things do you suggest I try to make?

D. Why do I put ""void"" in ""void Setup"" and ""void loop?""",,,,,Submission,3,0,3
d30jf7m,2016-05-10 18:15:07-04:00,tyggerjai,,"A) Serial.begin() is an abstraction that covers a multitude of sins, but basically it enables the hardware UART (serial transceiver) on the Arduino, allowing it to use pins 0 and 1 for serial transmission. This is what lets it talk to the Serial Monitor in the IDE, or to serial peripherals. 

B) Arduino.cc is the home of Arduino (legal issues notwithstanding). Adafruit are one of my favourite suppliers of related stuff, and they have extensive tutorials and project ideas. The Arduino is actually programmed in C++. There's also /r/Arduino

C) whatever you're into. Instructables, Adafruit and hackaday have plenty of ideas and resources.

D) C++ functions explicitly declare a return type so the compiler knows what to expect. If you want to return nothing ( often referred to as a procedure ), the return type is ""void"". The other common one you will see is ""int"", for functions that return an integer.  ",4irode,t3_4irode,Occams_Blades,,Comment,10,0,10
d30sa5e,2016-05-10 22:03:03-04:00,filthywabbit,,"To further clarify /u/tyggerjai's answer to (A), the 9600 in Serial.begin(9600) is the data rate in bits per second (also called 'baud'). You can use lots of different bit rates, depending on what you're doing, but for most purposes the default 9600 baud is fine.",4irode,t1_d30jf7m,tyggerjai,,Reply,6,0,6
d30lsib,2016-05-10 19:16:17-04:00,Occams_Blades,,"Wow, I don't think I could have gotten a better answer. Thank you.",4irode,t1_d30jf7m,tyggerjai,,Reply,2,0,2
d31vwby,2016-05-11 17:45:23-04:00,_ActionBastard_,,C++ ?! harrumph filthy casual i optimize my assembly by hand come down to my level,4irode,t1_d30jf7m,tyggerjai,,Reply,1,0,1
4iqxs2,2016-05-10 15:26:26-04:00,TissueReligion,Recommended way of self-teaching systems programming?,"I've found myself with more free time recently, and I wanted to work through a systems programming class. The problem I've been experiencing... is that almost every class I can find online requires university authentication to access some part of the course materials, and there don't really seem to be any textbooks with exercises that would cover an undergraduate course amount of material.

Any recommendations?",,,,,Submission,12,0,12
d31bcen,2016-05-11 10:26:13-04:00,RazerWolf,,"I'd suggest building an actual system from the ground up: it'll give you a lot of context and understanding of the big picture. 

The Elements of Computing Systems: From NAND to Tetris
http://www.nand2tetris.org/

They have a coursera course as well, links are all on that page.  ",4iqxs2,t3_4iqxs2,TissueReligion,,Comment,3,0,3
d30s2tb,2016-05-10 21:57:54-04:00,PastyPilgrim,,Do you already know computer architecture and how hardware works?,4iqxs2,t3_4iqxs2,TissueReligion,,Comment,2,0,2
d30t0l8,2016-05-10 22:22:04-04:00,TissueReligion,,I don't -- I'm trying to put together a CS curriculum for me to work through.,4iqxs2,t1_d30s2tb,PastyPilgrim,,Reply,2,0,2
d30td70,2016-05-10 22:30:42-04:00,PastyPilgrim,,"Ah. I would start with computer architecture before getting to systems programming then. [Hennessy and Patterson](http://www.amazon.com/Computer-Architecture-Fifth-Quantitative-Approach/dp/012383872X) is the book that (I think) most Computer Architecture classes use to teach the material. Before that, however, you may want to learn circuit basics and introductory computer organization though.

It's going to be pretty difficult material to teach yourself unfortunately. Computer Architecture is hard and requires a lot of foundational knowledge (circuits, organization, logic, etc.), but once you understand how hardware works, then you should be able to move into systems programming with more ease.",4iqxs2,t1_d30t0l8,TissueReligion,,Reply,4,0,4
d31bgn6,2016-05-11 10:29:03-04:00,None,,"Definitely read this book. It's huge, so maybe just sightsee what interests you in the later chapters, but the early chapters will give you a really solid understanding of how modern processors work.",4iqxs2,t1_d30td70,PastyPilgrim,,Reply,2,0,2
d30td4m,2016-05-10 22:30:40-04:00,kunteper,,For computer arch udacitys vid series seem to be nice. Im taking an architcture class and i watched a couple of those vids when studying.,4iqxs2,t1_d30t0l8,TissueReligion,,Reply,1,0,1
d3111py,2016-05-11 02:54:35-04:00,SUsudo,,Itunes U has a class from Pepperdine with lectures and slides on computer architecture. ,4iqxs2,t1_d30t0l8,TissueReligion,,Reply,0,0,0
4iq00n,2016-05-10 12:20:41-04:00,papermountain,How should I word this?,"I've been asked by a hiring manager to change the wording on my resume to reflect not job-based experience of programming language but an educational one.

I have a computer engineering degree, which got me employment as a sysadmin (which I actually enjoy more than coding) so I have no work experience as a programmer directly.

I have ""working knowledge"" of programming languages on my resume. My thinking was that it meant that while I'm a bit rusty, if I have a goal I can sort of bust out a working bit of code to reach that goal but it won't be efficient or the most beautiful. But he thinks it's misleading that I haven't ""worked"" as an engineer.

What's a better word that I can use?

Sorry if this is the wrong place.",,,,,Submission,3,0,3
d309ztx,2016-05-10 14:53:37-04:00,CoolGuyJay,,How about Familiar or Fluent In? ,4iq00n,t3_4iq00n,papermountain,,Comment,1,0,1
4ippd3,2016-05-10 11:20:08-04:00,maddogliar,How to get past censorship that replaces SSL certificates?,"My company wifi uses its own certificates to replace all https websites. Any idea how to get around this from the client side? VPN connections are also blocked.
I work in a repressive country where companies are allowed to bond staff for long periods (many years). I am stuck with this company and quitting is not an option.",,,,,Submission,4,0,4
d302xw4,2016-05-10 12:23:05-04:00,visvis,,"How about using Remote Desktop, VNC or something similar to browse the internet from a computer outside the company's network?

An alternative would be to set up tethering, where your computer gets to the internet through your phone's internet connection.

In any case, keep in mind that you will likely be fired if they find out.",4ippd3,t3_4ippd3,maddogliar,,Comment,3,0,3
d307c6r,2016-05-10 13:57:15-04:00,5225225,,"> In any case, keep in mind that you will likely be fired if they find out.

I wonder how well that plays with the ""you can't quit"" part.

",4ippd3,t1_d302xw4,visvis,,Reply,3,0,3
d307spx,2016-05-10 14:07:05-04:00,visvis,,"Modern slavery is rarely in the form where people are forced to work with a whip. More likely, quitting/being fired would have unreasonable financial consequences.",4ippd3,t1_d307c6r,5225225,,Reply,4,0,4
d30h93x,2016-05-10 17:24:37-04:00,nindustries,,"Tunnel everything through a SSH SOCKSv5 (NOT v4) proxy on your machine. You will need an external server (scaleway,..) with an ssh daemon listening on 443 or 80. Or try tunneling SSH over SSL, see what is required.",4ippd3,t3_4ippd3,maddogliar,,Comment,2,0,2
d30dbwh,2016-05-10 16:02:41-04:00,zombarista,,"You might be interested to know that sites using EV certificates will lose their ""Green Bar"" status due to the forged certificates.  EV certification authorities are pinned into evergreen browsers and cannot be forged without losing the green bar.",4ippd3,t3_4ippd3,maddogliar,,Comment,0,0,0
4ikgv8,2016-05-09 12:53:35-04:00,questionthrowaway887,Do you use Perl or Python for your job? Would you be willing to answer a few simple questions?,"*This is for a school project. We need to interview someone who uses Perl or Python for their job. The questions are simple and you don't have to be specific (I don't need your name or details that might make your employer uncomfortable. Something vague and general is fine.) Thank you to anyone who responds.*

- What company do you work for?

- Does your company make use of Perl or Python?

- For what type of job or project is the language being used?

- Why is that language being used? Does it do something better than other languages that makes it more suitable to the project?",,,,,Submission,3,0,3
d2yste6,2016-05-09 13:13:49-04:00,EndingPop,,"BD 

Python in my group. 

We do CAE, and Python is used for automation of our simulations as well as for analysis of the results after the fact. 

Python is the language our simulation software supports in their API, so it's basically required for that reason. That said, the ease of use is a huge reason we use it for other work too, not to mention the large number of scientific computing libraries available (numpy , scipy, etc). Our group is made up of mechanical and biomedical engineers, not computer scientists, so those last two are critical. 

I hope that helps! ",4ikgv8,t3_4ikgv8,questionthrowaway887,,Comment,3,0,3
d2yv0b6,2016-05-09 14:03:12-04:00,james4765,,"A legal software company

Perl for just about everything

It's used for our main application - we create corporate legal software, the application has been in active development for 20 years and we have customers that have been using it for that long.

The biggest reason we still use it is we have millions of lines of business logic, that have decades of experience built into it, and we'd rather keep customers on a mature, reliable tech platform than chase flavor of the week dev environments.",4ikgv8,t3_4ikgv8,questionthrowaway887,,Comment,3,0,3
d2zb1d0,2016-05-09 20:30:44-04:00,okiyama,,Do you guys have trouble finding perl developers? I'd guess it's probably really tough to find people with experience in it nowadays ,4ikgv8,t1_d2yv0b6,james4765,,Reply,1,0,1
d2zccoh,2016-05-09 21:04:49-04:00,KronktheKronk,,Developers use languages as tools. I'd never turn down a great software engineer for something as stupid as not being intimately familiar with my particular wrench.,4ikgv8,t1_d2zb1d0,okiyama,,Reply,1,0,1
d2zcdy2,2016-05-09 21:05:44-04:00,james4765,,"They recruited me away from another Perl shop. Honestly, we work very hard to not use the screwball parts of Perl - the Perl Best Practices book is required reading, and we have a LOT of code analysis tools (some of which we've written into git pre-commit hooks) and code review to make sure that the bad stuff doesn't come in.

As a result, it's not too bad to bring a junior developer up to speed. All of the senior devs have been working in Perl for a long time (and no we're not all greybeards) and have worked in other languages as well.

Perl has this reputation for being hard to deal with, and it'll definitely let you write unmaintainable crap, but much like PHP, if you're careful you can write maintainable software.",4ikgv8,t1_d2zb1d0,okiyama,,Reply,1,0,1
d2zczfa,2016-05-09 21:21:07-04:00,okiyama,,"Hah you may have just scared me off of ever taking a gig at a perl shop  =P thanks for the insight though, that's interesting. My company has been struggling to find C++ talent so I figured something like perl would be a nightmare. ",4ikgv8,t1_d2zcdy2,james4765,,Reply,1,0,1
d2zdkst,2016-05-09 21:36:03-04:00,james4765,,"Honestly, up to a point, talent is talent. It's easier to take a good programmer and teach a language than to find a programmer that has your specific skill set.",4ikgv8,t1_d2zczfa,okiyama,,Reply,2,0,2
4iiy2n,2016-05-09 06:15:45-04:00,noobeeee,Cheapest way to record five FM radio channels,"Would you be the cheapest way - in device and electricity cost to record five FM radio channels live?

I have bought FM record dongle from eBay for 5 bucks each. However, running it on the desktop will required me to set up a separate desktops and then run the computer 24/7 which will eat up my electricity bill.

I will need to do processing to the recorded audio at the same time so i will need to use computer.

Please help",,,,,Submission,2,0,2
d2yh4wy,2016-05-09 07:49:14-04:00,gstuartj,,Grab a Raspberry Pi 2/3. I think it could handle a few audio streams and they hardly use any power. You won't notice it on your bill.,4iiy2n,t3_4iiy2n,noobeeee,,Comment,3,0,3
d2ykitd,2016-05-09 09:51:36-04:00,ericGraves,,"There is only one time domain signal for all FM channels. So you would not need to worry about different audio streams, you would just need to run it through a filter.

So rasp pi + GNUradio would work. ",4iiy2n,t1_d2yh4wy,gstuartj,,Reply,4,0,4
d2yle8m,2016-05-09 10:16:09-04:00,atxweirdo,,"A multiband filter is precisely what user is looking for
",4iiy2n,t1_d2ykitd,ericGraves,,Reply,3,0,3
d2ynkb5,2016-05-09 11:11:50-04:00,UtterlyDisposable,,"And if the FM radio dongle won't work, a $10 rtlsdr probably will.",4iiy2n,t1_d2ykitd,ericGraves,,Reply,1,0,1
d2yodj2,2016-05-09 11:31:14-04:00,noobeeee,,Can you link me to eBay page to buy?,4iiy2n,t1_d2ynkb5,UtterlyDisposable,,Reply,1,0,1
d2yoe97,2016-05-09 11:31:42-04:00,noobeeee,,Do you think I can use USB extender since one rasp only have two USB ports?,4iiy2n,t1_d2yh4wy,gstuartj,,Reply,1,0,1
d2ys7we,2016-05-09 13:00:08-04:00,gstuartj,,"Using a USB hub shouldn't be a problem as long as you get a good 1.5-2A power supply. But from what other posters are saying it sounds like you might only need one tuner, anyway.",4iiy2n,t1_d2yoe97,noobeeee,,Reply,1,0,1
d2yinv7,2016-05-09 08:51:23-04:00,magikker,,"This might not be the best subreddit for your question. You might want to look at a more audio engineering focused sub. 

With that said, If you want to minimize the number of computers you are running you could get a multi-channel DAC and run all the signals to a single machine as separate tracks in a recording.",4iiy2n,t3_4iiy2n,noobeeee,,Comment,1,0,1
4iite2,2016-05-09 05:22:33-04:00,coder4lyf,Difference between height-balanced tree and weight-balanced tree?,"And an example of trees which are height balanced but not weight balanced, and one that is weight balanced but not height balanced? so I can understand the difference better please.",,,,,Submission,5,0,5
4ifo7d,2016-05-08 14:23:19-04:00,sj-f,How would you find the Gödel number of a constant?,"I'm trying to wrap my head around Gödel numbers and I think I understand it for the most part but I'm not really sure how I would represent constants. I know that you could represent a constant as the product of its prime factors, but how do you get the primes? I can see that you could do successor of 1 all the way up to the prime you're interested in but that seems overly complicated. I'd appreciate your input.",,,,,Submission,6,0,6
d2yesim,2016-05-09 05:30:59-04:00,avaxzat,,"Gödel encodings (and hence Gödel numbers) are not unique, so it really depends on the application. The trick I'm familiar with is to encode a sequence of 1+1+1+...+1 until you have your constant.",4ifo7d,t3_4ifo7d,sj-f,,Comment,1,0,1
4ieaml,2016-05-08 07:58:46-04:00,CraftyBarbarianKingd,would it be a good idea to dedicate a processor solely for the OS?,"So I am currently learning about OS and it seems to me that a lot of processing power would be dedicated for the OS alone for stuff like the different kinds of scheduling, handling the differnet discs, handling I/O, and all that. Now I've only studied OS systems in single processor machines, but would it be a good idea to just dedicate in a multi-processor computer system to just dedicate one of the processors for the OS or how else is it implemented?",,,,,Submission,16,0,16
d2xeqj8,2016-05-08 10:02:23-04:00,james41235,,"You typically don't need to.  There are a few specific cases where it can help, but for normal OS usage it's not necessary.",4ieaml,t3_4ieaml,CraftyBarbarianKingd,,Comment,8,0,8
d2xf6q2,2016-05-08 10:19:01-04:00,CraftyBarbarianKingd,,"Oh thank you. 

Could you give me a small hint as to what these cases might be?",4ieaml,t1_d2xeqj8,james41235,,Reply,5,0,5
d2xma7z,2016-05-08 13:44:13-04:00,james41235,,"As /u/ostracize says, typically you'll make things worse by trying to do this yourself.  I've seen it successfully for things like:

Balancing interrupts.  Some hardware only interrupts core 0 and the IRQ can't be reassigned.  For really busy hardware it's useful to pin the OS to that core and keep applications off it.

Separating busy user applications from OS.  If I know that a user application *can* take all of the CPU if allowed, it's useful to pin the OS to a certain core (usually core 0 because of above) and the pin the applications on other cores.  This way the OS applications won't interfere with the OS, and vice-versa.

RTOSs can benefit from this.  It's a useful way to ""guarantee"" minimum latency within the OS.

All that being said, don't do it unless you know what you're doing AND have the time to measure under load many many times.  And don't measure under what you ""think"" load will be.  Measure under real load and verify what you see.",4ieaml,t1_d2xf6q2,CraftyBarbarianKingd,,Reply,4,0,4
d2xjm1p,2016-05-08 12:31:43-04:00,high_side,,"It's not a bad concept, it's sort of what smartphones did in kicking rf processing, music playing, and other task out to separate chips. It's what a graphics card does.

But smartphones manufacturers don't have to worry about compatibility, pc makers do. Such a large architectural change would not go over well.

Conceptually, you are also trading the performance cost of context switching for the performance cost of a bus to off-chip/off-core processing. Caches would be affected variously as well.

And there are a million other ways to look at this, e.g. from the power consumption standpoint, or security, or application compatibility.",4ieaml,t3_4ieaml,CraftyBarbarianKingd,,Comment,8,0,8
d2xh6dc,2016-05-08 11:22:49-04:00,BonzoESC,,"It's not desirable to do that: much of how programs interact with the OS is through system calls, which depend on sharing registers and regular memory with the kernel.

More info about syscalls: http://blog.packagecloud.io/eng/2016/04/05/the-definitive-guide-to-linux-system-calls/ http://zinascii.com/2016/the-illumos-syscall-handler.html",4ieaml,t3_4ieaml,CraftyBarbarianKingd,,Comment,3,0,3
d2xkp56,2016-05-08 13:00:57-04:00,visvis,,"Generally not. Ideally, the operating system itself does not run code most of the time. Usually it only runs in response to system calls and interrupts, in which case it either updates some data structures or issues an IO operation and then immediately runs either the original or a newly scheduled process. As such, under normal circumstances having an entire core for the kernel would be wasteful.

One benefit would be being able to get rid of locks as the kernel would no longer be multithreaded. However, locking overhead is largest when contention is high, which means many application simultaneously request operations from the kernel. With the kernel on just one core, these operations cannot run concurrently and some processes will have to wait while others are serviced. Even worse, IO devices may have their results ready and not get a new job until processes are done using the kernel. Effectively, you get a big kernel lock design. As a consequence performance turns out worse.

",4ieaml,t3_4ieaml,CraftyBarbarianKingd,,Comment,3,0,3
d2xih29,2016-05-08 12:00:24-04:00,ostracize,,"You will find that, in general, you should not try to outsmart the lowest levels of computing (assemblers, compilers, OSes, etc.)

These are built on decades of development and the best, most efficient, options have already been chosen for you. You are more likely to make things worse rather than better.

If the OS wants to share your system and application processing across all cores, I would trust it.",4ieaml,t3_4ieaml,CraftyBarbarianKingd,,Comment,2,0,2
d2xj2oe,2016-05-08 12:17:14-04:00,high_side,,"Welcome to computer science where creative thinking is not allowed.

We definitely never stick with things because they have market share. Never ever.",4ieaml,t1_d2xih29,ostracize,,Reply,7,0,7
d2xl8oy,2016-05-08 13:15:38-04:00,ProfessorAlgorithm,,"This post doesn't say that, though.  It says that you aren't typically going to outsmart decades of research with 5 minutes of thought.",4ieaml,t1_d2xj2oe,high_side,,Reply,5,0,5
d2xmi7g,2016-05-08 13:50:18-04:00,101C8AAE,,That wasn't their point. Not everything is the way it is because of extensive research.,4ieaml,t1_d2xl8oy,ProfessorAlgorithm,,Reply,6,0,6
d2xs62y,2016-05-08 16:19:15-04:00,high_side,,"> you should not try to outsmart

Like, don't even attempt it. Because existing systems are always well considered, particularly in the case where design choices may have been cost/technology based.  And in the many, many cases where the legacy design was produced by very smart people, the exercise of finding out why it was done that way is not worth it.",4ieaml,t1_d2xl8oy,ProfessorAlgorithm,,Reply,3,0,3
d2xncv9,2016-05-08 14:13:20-04:00,Nocsaron,,"Just to add some clarity to those who have said in general no, but instances exist where yes. In a project (currently in prototype stage) we are lacking the OS to one processor while we run our specific programs with interrupts disabled on other processors. We're doing this because we need to run hard real time and being able to choose which processor we run processes on gives us a little bit extra speed which can be the difference between failure and success ",4ieaml,t3_4ieaml,CraftyBarbarianKingd,,Comment,1,0,1
4i5u82,2016-05-06 12:22:49-04:00,sj-f,"""Any two-variable boolean function can be written with AND and NOT"" question","Hi, I am reading The New Turing Omnibus and I came across this in the third chapter about systems of logic. I'm not sure I understand this - how would you represent p OR q in terms of AND and NOT?

[Here is the page where it appears.](http://m.imgur.com/mpmd7i6)

I appreciate your help.

EDIT: I may have figured it out - you have to go at it from the negative direction, i.e. (p' AND q')' - is that right?",,,,,Submission,16,0,16
d2vckqf,2016-05-06 13:07:43-04:00,atyeo,,"Have a look at [De Morgan's Law](https://en.wikipedia.org/wiki/De_Morgan's_laws) and [NAND logic](https://en.wikipedia.org/wiki/NAND_logic).

In summary, all of the logic functions (AND, OR, etc) can be created using a combination of AND and NOT.
",4i5u82,t3_4i5u82,sj-f,,Comment,11,0,11
d2vlvx3,2016-05-06 16:40:02-04:00,sj-f,,"Very interesting, thank you",4i5u82,t1_d2vckqf,atyeo,,Reply,1,0,1
d31vnqs,2016-05-11 17:39:55-04:00,_ActionBastard_,,Interesting Hardware Fact:  it takes fewer circuit components to make NAND gates than AND gates.  AND gates are implemented as a NAND followed by a NOT.  ,4i5u82,t1_d2vlvx3,sj-f,,Reply,1,0,1
d2vjkom,2016-05-06 15:45:57-04:00,ukkoylijumala,,"> EDIT: I may have figured it out - you have to go at it from the negative direction, i.e. (p' AND q')' - is that right?

You can check this using a truth table, since you only have two input ~~values~~ variables:

p| q | p ∨ q | ¬(¬p ∧ ¬q)
---|---|----|----
0| 0| 0| 0
0 | 1| 1| 1
1 | 0 | 1| 1
1| 1| 1| 1

So yes, your solution works :)
",4i5u82,t3_4i5u82,sj-f,,Comment,4,0,4
d2vlw2v,2016-05-06 16:40:08-04:00,sj-f,,Thanks!,4i5u82,t1_d2vjkom,ukkoylijumala,,Reply,1,0,1
d2vv5ou,2016-05-06 20:55:12-04:00,shirro,,"You can implement anything with NAND or NOR gates. They are universal. You can tie the inputs together to turn either into a NOT. You can negate outputs to turn them into AND or OR. And a NAND with negated inputs becomes an OR and a NOR with negated inputs becomes an AND. 

When I was a kid I only bought TTL nand gates chips to play with because I could build anything with them. These days my kids play Minecraft where a block plus redstone torch forms a NOR gate. You can build flip flops and latches and all sorts of complicated circuits with them. The Apollo guidance computer consisted entirely of NOR gates.",4i5u82,t3_4i5u82,sj-f,,Comment,1,0,1
4i1kq3,2016-05-05 15:38:24-04:00,Kendama_Llama,"Deadlock in the field, but not the lab?","My professor asked us the following question and I'm having trouble coming up with an answer. If the the lab conditions are sufficiently thorough, why wouldn't deadlock be detected? What is he getting at here?


A system has two processes P and Q and two locks _a_ and _b_. Locks are acquired using the call request() and release() call.

a) A lock that has been acquired by one process is held exclusively until it is released by that process.

b) A process holding one lock can request another.

c) No preemption is possible.



This system has been tested extensively in the lab and deadlock never occurred. However, when taken into the field the deadlock occurs every time the system is started. How is this possible?",,,,,Submission,10,0,10
d2u9l2b,2016-05-05 16:24:40-04:00,MCPtz,,Timing. Race Condition. Different hardware may lead to this. Unknown use case. Bad power source. Hardware failure. So many real world things leaves this to be an open ended question. ,4i1kq3,t3_4i1kq3,Kendama_Llama,,Comment,7,0,7
d2v1tcy,2016-05-06 08:43:57-04:00,BarqsDew,,"It could even be as simple as running a debug build in the lab, but running a release build in the field. Higher optimization levels mess with timings that would otherwise be constant.

having other programs running in the background in the field, same hardware but different driver versions, etc...",4i1kq3,t1_d2u9l2b,MCPtz,,Reply,5,0,5
d2u8e3h,2016-05-05 15:58:24-04:00,visvis,,"Chances are the scheduling is different, for example due to a different OS or faster/slower CPU or I/O devices.",4i1kq3,t3_4i1kq3,Kendama_Llama,,Comment,2,0,2
d2uz7zo,2016-05-06 06:52:46-04:00,insolent_cur,,Even just changing the cooling can potentially influence this. So say moving from a frigid lab server to one operating in the backroom of a corporate office.,4i1kq3,t1_d2u8e3h,visvis,,Reply,2,0,2
d2u837g,2016-05-05 15:51:54-04:00,james41235,,"This is a very strangely worded question.  Two processes holding two locks each easily leads to race conditions, which causes the deadlock.  The best guess I have is that the ""Sufficiently thorough"" lab settings aren't the exact same as the field, so the processes go through different sequences when starting in the field which encounters the deadlock.

The other guess is that you were just unlucky and it could always deadlock in the lab, but never did.",4i1kq3,t3_4i1kq3,Kendama_Llama,,Comment,1,0,1
d2up6ij,2016-05-05 23:07:31-04:00,daV1980,,"This is actually a situation that comes up all the time in real programs. 

Even computers that are identical--same model CPU, motherboard, RAM, hard drives, GPU, etc can exhibit radically different behaviors. 

Programs that are multithreaded and have a race condition are obvious demonstrations of this phenomena, but it actually happens all the time. 

The reason is that fundamentally those devices are physical goods that were manufactured with physical processes that have tiny variations--well within acceptable tolerances--but that cause variations in low level behaviors. To be a bit more concrete, one example of this is thermal behavior at the lowest level. One CPU will have slightly different thermal behavior than a chip from the same batch (possibly even from the same wafer). As a result, those CPUs can choose different cycles to turn on power-saving functionality, which in turn will result in them completing operations at different wall-clock times, which in turn leads to the OS scheduler (which is also based on wall clocks, not cycle counts) to preempt at different instructions. 

Race conditions are extremely sensitive to when threads are swapped in and out, and above is just one component where micro differences can result in obvious macro behavioral differences. 

Hope this helps!

Source: I helped architect GPUs in use I hundreds of millions of PCs worldwide. ",4i1kq3,t3_4i1kq3,Kendama_Llama,,Comment,1,0,1
4i0pa6,2016-05-05 12:34:43-04:00,Chieve,Heap question?,"When i google this question i get something not related to comp sci

How many heaps can we make woth n values?",,,,,Submission,1,0,1
d2tzp92,2016-05-05 12:50:43-04:00,thewataru,,"Draw for first few small values of n and count by hand. Make a hypothesis, then try to prove it. Hint: draw your heap as a binary tree. Then top element if fixed - largest one in your set. Remaining n-1 values have to be split between two subtrees an each subtree is some valid heap. Therefore 

    F(n) = F(k) * F (n-1-k) * C_{n-1}^k

there F(n) is what you search for, k is the size of one subtree and C is combinations number.",4i0pa6,t3_4i0pa6,Chieve,,Comment,1,0,1
d2tzwp8,2016-05-05 12:55:10-04:00,thewataru,,"Actually, there's no easy answer. See [A056971](http://oeis.org/A056971).",4i0pa6,t3_4i0pa6,Chieve,,Comment,1,0,1
4hzw9i,2016-05-05 09:30:20-04:00,Silikone,Do different computers produce different results?,"Given two computers that have many differences, but nonetheless can execute the same instructions, should the resulting calculations always be identical? For example, say that a Windows program is designed to do many complex calculations and approximations for hours and dump the entire process into a file, would running this program on a brand new Intel laptop create the same file as an ancient Chinese Cyrix PC, assuming the program wasn't specifically made to discriminate between different hardware?",,,,,Submission,1,0,1
d2trr6x,2016-05-05 09:47:39-04:00,visvis,,"Assuming the computation depends only on things that are deterministic and does not trigger any hardware bugs in the system, they should produce the same results. If, however, the program does something nondeteministic like measuring the time or having race conditions (cases where what happens depends on thread scheduling) the results may be different. However, they would also potentially be different from run to run on the original machine. Hardware bugs are also a potential issue. One example that springs to mind is [Rowhammer](https://en.wikipedia.org/wiki/Row_hammer), where memory cells can be erased by writing to the neighboring cells many times. The effect of this issue would depend on system-specific things like the memory refresh rate.",4hzw9i,t3_4hzw9i,Silikone,,Comment,6,0,6
d2ts0gu,2016-05-05 09:54:38-04:00,cogman10,,"It depends.  If the processing is on purely integer based (no floating point arithmetic), and it doesn't rely on things like clock count, and there are no major errata in the way.  And no threads are involved. And assuming perfect operation of things like the hard drive and the same OS/software dependency environment.

Yes

But changing any of these statements and it will depend heavily on what the program is doing.  For example, floating math may be different between the two machines, but that difference is likely going to be far down the significant digit range.  So long as your app isn't saving float values and it doesn't require high floating point precision, it will probably produce the same results.",4hzw9i,t3_4hzw9i,Silikone,,Comment,4,0,4
d2ts7mx,2016-05-05 10:00:05-04:00,dxk3355,,"Just executing the same instructions would generally lead the same results since the individual instructions and outputs are very well defined.  List visvis mentioned when you start doing stuff outside of instructions like scheduling you can see different results in this case.

If you were to compared x86 to PowerPC for example I would say you could expect slightly different outputs in some cases because the actual instructions are different.  Floating point may be different between the two sets for example.",4hzw9i,t3_4hzw9i,Silikone,,Comment,1,0,1
d2ubahi,2016-05-05 17:01:29-04:00,cogman10,,"Not always, unfortunately.

Particularly with things like floating point math or processor clock count.

With floating point math, there is a lot of internal state in the processor which can change the results slightly.  Further, because it is already assumed to be an approximation, intel and others are more lax about what goes in the least significant digits.  Clock frequency can play into the result you get in your float and that can vary depending on OS and CPU.

And of course, errata can play a major role in what the processor does.  Because processors maintain their own set of internal state, it is always possible that you hit some corner case where one processor behaves differently from another.

I don't mean this to sound like a ""the sky is falling"" sort of thing.  These sorts of differences are generally really tiny and will almost never result in different behavior.  But they do exist and you should be aware of them if you are doing anything that relies on that part of the device (most aren't).",4hzw9i,t1_d2ts7mx,dxk3355,,Reply,1,0,1
4hx2v8,2016-05-04 18:44:17-04:00,git_her_done,"How much memory is stored in a word processor's cached ""deleted text?""","When I delete text in MS Word, for example, I have the option, up to a certain memory limit, to hit 'Undo' until that text reappears in my document. What's the cache limit?",,,,,Submission,12,0,12
d2t5rvp,2016-05-04 20:01:53-04:00,lneutral,,"There's a data structure called a stack at work, which stores edits in such a way that the last one recorded is the first one retrieved when undo is pressed. 

The same data structure is used in lots of other software: Photoshop, for example, which keeps an extremely long edit history. 

Given that an edit is a fairly small piece of data in word processing, it's hard to say if the stack has to be capped at all. It's just as easy to build them with space limits as without, but if there is a limit, it almost certainly differs between versions of Word, platform to platform, and so on. ",4hx2v8,t3_4hx2v8,git_her_done,,Comment,8,0,8
d2te2ok,2016-05-04 23:40:08-04:00,mastermindxs,,"Word does not have a set limit on undo levels. The only limit is the amount of RAM and virtual memory your machine has. Depending on encoding, each character is between 1 to 4 bytes. A typical word doc is only a few kilobytes to hundreds, they rarely even reach the limits of ram allocated to the whole of the running word app. You could fill a word doc with enough characters to fill gigabytes and still be able to delete and undo the whole thing without even touching virtual memory. 

http://wordribbon.tips.net/T011202_Changing_the_Maximum_Undo_Levels.html

http://stackoverflow.com/questions/4850241/how-many-bits-in-a-character",4hx2v8,t3_4hx2v8,git_her_done,,Comment,5,0,5
4hwtbd,2016-05-04 17:46:12-04:00,modallyseinlifeworld,"What are some erudite theory of computation PDF's and/or books on heterarchies in information ecology: and subsequently, their knowledge management?",,,,,,Submission,4,0,4
d2t5dj4,2016-05-04 19:51:28-04:00,lneutral,,"Are you a human being, or just a sentient Markov chain trained on the least-used synonyms pulled from a Soviet-published thesaurus of the English language? ",4hwtbd,t3_4hwtbd,modallyseinlifeworld,,Comment,10,0,10
d2temf5,2016-05-04 23:57:27-04:00,SakishimaHabu,,/r/iamverysmart,4hwtbd,t3_4hwtbd,modallyseinlifeworld,,Comment,1,0,1
4huwku,2016-05-04 11:11:32-04:00,Upularity,Question about Network Topology,"I don't know where else to post this :(  
https://imgur.com/k1uAwJW     

In case you can't figure out the 4th question, it is ""*Draw the complete network topology*"".

[Here](https://imgur.com/kivASFM) is my answer for the 4th question. Please let me know if I did anything wrong.

p.s. Unifi is an ISP.


",,,,,Submission,1,0,1
d2sn3rl,2016-05-04 13:05:21-04:00,Bottled_Void,,"I'd draw three seperate FTUs.


And I'm not to sure if the supervisor computer would be part of the ring or not. Either way, I would have had the connection to something other than the edge of the ring.


A few extra labels wouldn't hurt either.


Looks pretty good though.",4huwku,t3_4huwku,Upularity,,Comment,2,0,2
d2vmqy7,2016-05-06 17:00:20-04:00,mohajaf,,"I think the ring should be demonstrated as a token ring (all nodes connected the same way your server is connected, tapping into the ring).

I think the implication is that the three sites are geographically separated. In that case each will have a separate FTU.

Also, I don't understand  why each exec node needs to be on a different Layer3 interface. Why shouldn't they share the same forwarding domain?

The backup router design seems strange too because there remains a single point of failure where the main router is. ",4huwku,t3_4huwku,Upularity,,Comment,1,0,1
4hukpt,2016-05-04 09:57:45-04:00,HowIsntBabbyFormed,When re-saving a jpeg image is it possible to distort the new file less by using compression info from the original file?,"Let's say you have to edit a jpeg and you don't have access to the raw original. It's already gone through one round of lossly compression and if you save it again as a jpeg, it'll go through another, losing even more info from the original.

Now if you look at [this example of jpeg compressions/artifacts at wikipedia](https://en.wikipedia.org/wiki/JPEG#/media/File:Jpegvergroessert.jpg) you can easily make out the 8x8 blocks of pixels jpeg uses at its  base. Let's say you edited a section of the image no where near one of the blocks shown in that close-up. I assume when savinging, the program will send the whole image through jpeg compression, treating the (lossy) rendered image as the raw source for compression.

Would it be possible to instead scan the image, looking for sections which are obviously artifacts of previous jpeg compression, and use them in new newly saved file as-is, saving those sections from double lossy compression?

You'd probably have to be saving at the same compression level as the input image, and possibly with the same codec, but it seems like it should work.",,,,,Submission,2,0,2
d2xo5zd,2016-05-08 14:34:49-04:00,deftware,,update: I stumbled across this paper that might be of interest http://www.cl.cam.ac.uk/techreports/UCAM-CL-TR-813.pdf,4hukpt,t3_4hukpt,HowIsntBabbyFormed,,Comment,1,0,1
d2sf6t8,2016-05-04 10:07:10-04:00,deftware,,"sounds like something for deep learning to solve - reconstructing detail lost in compression and/or resolving compression artifacts.

There's no real way to create the missing information out of thin air that would resolve the artifacting of the 8x8 blocks neighboring eachother. You would have to introduce another layer of information pertaining to how the blocks *should* compare to their neighbors in a way that would make it trivial to 'smooth' out their junctions.",4hukpt,t3_4hukpt,HowIsntBabbyFormed,,Comment,-1,0,-1
d2sfjrz,2016-05-04 10:16:15-04:00,HowIsntBabbyFormed,,"I'm not saying ""reconstruct the missing information out of thin air"". I'm saying: Use the fact that it was previously saved as the same compression format that you're now trying to save it as to avoid double compression of at least some regions. Don't re-compress the same region again -- ignoring the fact that it was already once compressed.

If you look at one of the 8x8 pixel regions in the first picture I linked, you can easily see that it looks that way because of jpeg compression. I know it's impossible to figure out what the original raw image actually looked like from that (otherwise it wouldn't be called lossy compression). But maybe you could figure out what the first saved, compressed, version of that region was, and just use that when you save the second time, rather than re-doing the whole image and losing even more info.",4hukpt,t1_d2sf6t8,deftware,,Reply,1,0,1
d2tesag,2016-05-05 00:02:38-04:00,deftware,,"I understand entirely as I've had the same exact idea/thought myself in the past. To my understanding of jpeg encoding (as a programmer with first-hand jpeg encoding experience) saving a new jpeg of an existing source jpeg that has been decoded to raw 24-bit RGB pixel data *should* result in an identical jpeg provided that the same exact encoding algorithm is used, as they can all vary by their compression tables that are used, resulting in variances...

ie: if you *do* attempt this, you would want to use the same exact imaging program with the exact same settings, including the familiar compression 'percentage' value. One jpeg saved from mspaint might be compressed differently than a jpeg from say Photoshop, or GIMP.

This is because all the quantization that occurs with a given implementation should result in data that doesn't quantize differently when passed through the same encoding implementation.

This also depends on the decoding scheme, and whether or not it attempts to interpret the quantized data differently.

In my opinion there are just too many variables due to the complexity of jpeg encoding implementations and especially their number of varying optimizations.

    ""But maybe you could figure out what the first saved, compressed, version of that region was, and just use that when you save the second time""

I don't think you're correctly imagining what the ""figure out"" part would comprise, which is why I suggest something like an AI technique as something that would do such a thing. Otherwise you'd be doing the ""figuring out"" by hand with each macroblock and your own experience and understanding concerning what things should look like.",4hukpt,t1_d2sfjrz,HowIsntBabbyFormed,,Reply,1,0,1
d2tcqpk,2016-05-04 23:01:06-04:00,VainWyrm,,"Ooh, I like this! I might give this a shot!",4hukpt,t1_d2sf6t8,deftware,,Reply,1,0,1
4huhk2,2016-05-04 09:35:48-04:00,adriankoshcha,"Want to get a computer science degree, but intimated by calculus, should I go for it anyway?","I'm a junior in highschool, I've been looking at perspective universities/colleges on and off, not entirely sure if I even want to go. Though one consistency I see throughout most CS courses whether it be a bachelor degree, or a diploma, is the requirement of taking calculus. 

My problem with taking calculus is in my freshmen and softmore years, the switch from traditional based learning methods to the common core crap completely ruined the math skills I had. I **really** struggled with quadratic algebra, and I really struggled with geometry as well. From the very little of calculus I know, it's like algebra+trig+alot of other maths combined into one. This to me is scary as fuck. Even though I excel in most other areas of education, math has now become my weak-point. I saw how much others who excelled in math struggled with trigonometry, and this makes me thing calculus will only be worse, and is turning me off from CS.

TL;DR: my math skills suck, calculus looks scary, should I go for it anyway?",,,,,Submission,5,0,5
d2slcqo,2016-05-04 12:28:04-04:00,cdrootrmdashrfstar,,"You're only bad at math in the same way that someone who doesn't practice painting will say they're bad at painting.

If you want to get better at math, don't be passive about it -- open up KhanAcademy, a textbook, YouTube, or any of the thousands of free resources available on the internet and learn math.",4huhk2,t3_4huhk2,adriankoshcha,,Comment,13,0,13
d2sogfv,2016-05-04 13:33:41-04:00,adriankoshcha,,"You're right, it's my own fault.",4huhk2,t1_d2slcqo,cdrootrmdashrfstar,,Reply,10,0,10
d2sv1c9,2016-05-04 15:51:31-04:00,CARGLE,,Why did someone downvote you? Lol,4huhk2,t1_d2sogfv,adriankoshcha,,Reply,3,0,3
d2xrmre,2016-05-08 16:05:31-04:00,trenchgun,,Dont worry about it. Just start practicing. :),4huhk2,t1_d2sogfv,adriankoshcha,,Reply,1,0,1
d2se7a7,2016-05-04 09:41:35-04:00,greeniguana6,,"Yes, calculus will be something you'll probably struggle with but luckily a lot of skills you learn in calculus are entirely new. As long as you understand trig functions and the unit circle, and basic factoring, you'll probably be fine. If you take a pre-calculus class next year, definitely focus on that.

Most of your first semester of calculus will be understanding how to work with limits, derivatives, and integrals, and their special cases, which aren't really taught in any lower classes. I'm majoring in computer science right now and my calculus classes are definitely my hardest classes, the CS courses aren't bad at all if you enjoy them.",4huhk2,t3_4huhk2,adriankoshcha,,Comment,7,0,7
d2sjn4w,2016-05-04 11:51:01-04:00,sandwichsaregood,,"> precalc 

So as someone who continued on to study math and works as a mathematician, I thought precalc was much harder than BC calculus. Precalc is a lot of memorization and grunt work, whereas calculus is where you start to think for yourself in math and transition to learning to think like a mathematician. 

A lot of it is down to the quality of your teacher (and I had a terrible precalc teacher and great calculus teacher) but calculus in high school is pretty different from what precedes it (hence why you have precalc to help transition).

Amusingly though if you continue with math, you'll have to take real analysis, which is basically learning calculus all over again but in a much more formal/rigorous way.",4huhk2,t1_d2se7a7,greeniguana6,,Reply,4,0,4
d2sewv2,2016-05-04 10:00:07-04:00,adriankoshcha,,"> As long as you understand trig functions and the unit circle, and basic factoring, you'll probably be fine.

That's the thing...didn't take trig because I saw how many other people were struggling with it, ended up getting the math credit I needed elsewhere.",4huhk2,t1_d2se7a7,greeniguana6,,Reply,1,0,1
d2si20t,2016-05-04 11:15:21-04:00,greeniguana6,,"Just learn the unit circle and what sine, cosine, tangent, cotangent, secant, and cosecant do and you should be fine in terms of trig.",4huhk2,t1_d2sewv2,adriankoshcha,,Reply,5,0,5
d2sitvs,2016-05-04 11:32:57-04:00,794613825,,"For me, precalc was
a LOT harder than actual calc. There are really only three new things in calculus: the limit, the derivative, and the integral. 

The limit lets you take a function that would be undefined at a point, and say ""Fuck the rules, what would it be if it WAS defined?"" 

The derivative tells you the slope of the graph at every point. 

The integral tells you the area under any graph, from any point to any other point. 

That's really it. The hardest part of calculus is wrapping your head around what those things actually mean. The manipulation of the functions is actually quite easy once you understand these concepts. ",4huhk2,t3_4huhk2,adriankoshcha,,Comment,4,0,4
d2ss5dy,2016-05-04 14:50:50-04:00,totalanonymity,,"Do you have any particular advice for getting better at problems involving applications for derivatives and integrals? I seem to do the raw differentiation and integration just fine and once applications come up, I'm like a four-year-old in a Costco with no parents.",4huhk2,t1_d2sitvs,794613825,,Reply,2,0,2
d2stdp5,2016-05-04 15:16:45-04:00,KingomTrek,,"Not OP, but practice, practice, practice. Namely when you read a problem, visualize and figure out what you are trying to find and how you need to find it",4huhk2,t1_d2ss5dy,totalanonymity,,Reply,2,0,2
d2svpm5,2016-05-04 16:05:48-04:00,794613825,,"The main applications of derivatives and integrals are conversions between things like Position, Velocity and Acceleration. The derivative of an objects Position function is its Velocity function, and the integral of Velocity is Position. The derivative of Velocity, or the second derivative of Position is Acceleration, and the first and second integrals of Acceleration are Velocity and Position, respectfully. Derivatives past Acceleration are called Jerk and Jounce, the fifth derivative of Position. Past that I don't think there are official names, but you can still take derivatives as far as you want for that kind of problem. When you get to more advanced Calc, you can find the direction of the steepest slope on a surface by taking the derivative of the surface formula with respect to each variable that makes up the surface, or you could find a local max or min by finding where that slope is zero. There are all sorts of applications. 

As /u/KingomTrek said, the best way to think about it is to just visualize the problem, see what you're being asked about. ",4huhk2,t1_d2ss5dy,totalanonymity,,Reply,2,0,2
d2sli4d,2016-05-04 12:31:15-04:00,Bottled_Void,,"Calculus isn't really required for CS. Although, depending on the course you do, they might force you to do it. Or as you've said they might have expected you to do it already (if for no other reason as to weed out the students that aren't strong in math).


The main math you'll need for CS is simple algebra. But you do have to have a head for numbers. A lot of CS is essentially problem solving and math is a great tool to help you do that.


If you've spent your high school career avoiding doing math, then you are going to struggle. And yes, calculus is probably going to be extra hard if you have skipped out on doing trig.",4huhk2,t3_4huhk2,adriankoshcha,,Comment,5,0,5
d2sems5,2016-05-04 09:52:57-04:00,Silverlight42,,"I graduated from Computer Science, we did a ton of calculus, I hated it.

In high school I didn't have any calculus experience, and to top it all off all my math in school was learnt in french.

It wasn't that big a deal.

I wouldn't worry about it too much.  The fact that you are worrying, well you're probably ahead of most people.


also note I didn't care too much about my grades.  I had anything computer oriented down before I took the classes, I didn't really need to learn or study 90% of the material - just innate interest and computer knowledge.  I just wanted the piece of paper tbh.",4huhk2,t3_4huhk2,adriankoshcha,,Comment,2,0,2
d2seye2,2016-05-04 10:01:13-04:00,adriankoshcha,,"I'm debating whether to do it just for a piece of paper, because it's pretty pathetic that you ""need"" these pieces of paper...at least in my opinion.",4huhk2,t1_d2sems5,Silverlight42,,Reply,0,0,0
d2sffw2,2016-05-04 10:13:34-04:00,Silverlight42,,"Yeah, that was sort of my thinking.  If you like science, computers and want a nice well rounded education, a Comp Sci curriculum is nice.  There's always other opportunities too, but I never bothered.  I did enjoy some of the classes.


I say go for it.  I did, but I couldn't have seen myself do much else.  It was worth it.",4huhk2,t1_d2seye2,adriankoshcha,,Reply,1,0,1
d2sz9ml,2016-05-04 17:21:10-04:00,None,,[deleted],4huhk2,t1_d2seye2,adriankoshcha,,Reply,2,0,2
d2t367h,2016-05-04 18:54:23-04:00,adriankoshcha,,"> ... it isn't about a diploma and a handshake.

from what I'm told by most teachers, counselors, etc...seems to be that way. They seem to try and make it seems like your whole life hinges around getting **some** type of diploma and/or degree. This perspective kinda sickens me, that **everyone** should go to college/uni, not everyone is ""meant?"" for college/uni? Don't know if that's the right way to word that/


> Your in high school, and sorry to say but with that statement, it shows. 

Alot of my cyclical attitude towards college/uni comes from older peers(20's,30's,or older) who just tell me how much of a ""joke"" college/uni was (i.e. was too easy for them, it didn't feel like an improvement over highschool). They feel like it just wasn't worth the money they invested.

 >  Unfortunately most people lack the discipline to do that.

I can totally agree with that.",4huhk2,t1_d2sz9ml,None,,Reply,1,0,1
d2sqw78,2016-05-04 14:24:42-04:00,RatherPleasent,,"As long as you understand Calc. 1 well it'll be easy to transition into 2 and 3. If you don't ""get"" Calc. 1 you're going to be on a hell ride for the next two semesters. ",4huhk2,t3_4huhk2,adriankoshcha,,Comment,2,0,2
d2st39a,2016-05-04 15:10:40-04:00,None,,[deleted],4huhk2,t3_4huhk2,adriankoshcha,,Comment,2,0,2
d2t3491,2016-05-04 18:53:03-04:00,adriankoshcha,,"> CIS

Will look into that, thanks.",4huhk2,t1_d2st39a,None,,Reply,1,0,1
d2tizek,2016-05-05 02:52:39-04:00,NearSightedGiraffe,,"Depends on your university. Some universities will require higher levels of maths than others. From the programming side, I haven't found calculus that important. Statistics, linear algebra and similar are much more useful in the long run and IMO easier to do ",4huhk2,t3_4huhk2,adriankoshcha,,Comment,2,0,2
4hswaf,2016-05-04 00:22:16-04:00,dominohound,Finding the nyquist frequency?,"Hey guys, I'm fairly new to these sorts of problems, don't even know if I'm posting in the correct subreddit tbh. I've been asked to find the nyquist frequency for sampling the function:

 y = sin(5 * t * (2*pi));  

I know that the nyquist frequency is double the highest frequency in the signal, and that the signal produces a continuous frequency of 5Hz, so the nyquist frequency should by 10Hz right? But when I sample the function at 10Hz (I've been using Matlab to do these plots), the result is a straight line. Can anybody help me out here? All help will be appreciated. Thanks :)
",,,,,Submission,2,0,2
d2sawy0,2016-05-04 07:49:58-04:00,ericGraves,,"Yes it is 10 Hz. The reason you are encountering problems comes from what exactly is happening when you sample.

## Simple Explanation 

For a sine wave, if you sample at 10 Hz, what do you get? 

sin(pi * n) 

That is a slight problem since that function is 0 for all values of n. So yes, you get a straight line. But if you were to start your sampling at t = 1/20, you would instead get 

sin(pi * n - pi/2) 

which would alternate between 1 and -1, which you could then reconstruct. 

## Better explanation
[Whenever you sample a waveform, the new signal is comprised of an infinite summation of the signal times Ak * cos(2 * pi * fs *k* t)](http://images.books24x7.com/bookimages/id_7567/fig69_01.jpg). This is best understood in the frequency domain, where it can be viewed as replicating your original signal at all harmonics of the sampling frequency. So sampling at twice the highest frequency should result in reproducing the signal right? You would think this because the distance between each harmonic is greater than the frequency range of the signal. But the problem here is that you are right on the boundary point, or in other words the nyquist frequency is strictly greater than. In fact this is not a problem for all signals (such as cosine here would be fine), but in this specific case it is. The reason for this is that the coefficients for sin in the frequency domain are j/2 and - j /2. Thus the boundary points cancel themselves out. Leaving no signal behind. Once again, you can compare this with cosine, with coefficients 1/2 and 1/2, which do not cancel and thus can be sampled. 

So, the theory says you must do strictly greater than twice the highest frequency to guarantee recovery. Weird stuff occurs at equality, which is not always bad. I say strictly, because given this and that you can clearly recover signals by undersampling them as well, but you have to make sure they do not interfere with themselves. And in reality, you would take some large multiplier greater than the highest frequency so you would never really run into this exact problem. 
",4hswaf,t3_4hswaf,dominohound,,Comment,1,0,1
4hqzkb,2016-05-03 18:31:14-04:00,neS-,"I have a complex, theoretical question about digital pictures, and really any computer file.","This is something I've wondered for awhile, and of course I don't think there's any real way of this being done IRL. Basically lets take a photograph of myself for example. The computer views a picture (at least this is how I think it works, maybe its magic I have no real idea) as a series of numbers, and these numbers tell the computer what color to make each pixel, in order to get the end result which would be a picture of me.

Now lets say theoretically, I was able to recreate independently every single variable that makes the picture of me, look like me. Would I have the same picture? Could you theoretically, render a picture that hasn't even been taken/ doesn't exist? Of course this wouldn't be possible for any real complex picture. An image file of a pure color, I can imagine could be replicated in practice.



I just want to know theoretically if it would be possible if by some impossible chance, to completely independently ""create"" a photo. I assume the same could be said for any other type of computer file, whether it is a video, or a song. I just imagine it would take an infinite amount of time to actually successfully do it.

I don't know if what I said made any sense, I'm not a computer science guy, but ever since I was a little kid I always pondered this question. Again its all incredibly theoretical. It would be like saying If I sat on my computer and somehow independently wrote the entire windows 10 OS, exactly as it is. Would my end result be Windows 10?",,,,,Submission,0,0,0
d2ro7pm,2016-05-03 19:00:01-04:00,DoktorJeep,,"Yes, see ms paint",4hqzkb,t3_4hqzkb,neS-,,Comment,6,0,6
d2rseve,2016-05-03 20:39:09-04:00,Frore,,"So as the other posters are answering, bits are bits. You can create a file by hand that decodes into a coherent picture. You can indeed create a ""new"" picture of your family by just typing on your keyboard.  

Pretty interesting idea huh? Along the same lines, since pi is an infinite and random sequence of numbers, there are pictures of you encoded within pi. A picture of your future spouse exists within pi. Everything you ever say.. you get the idea. 

(This idea came from a youtube video somewhere but I can't find it anymore, vsauce maybe. If anybody knows please provide source)  

Related videos:

[Are Shakespeare's Plays Encoded within Pi?](https://www.youtube.com/watch?v=uXoh6vi6J5U)

[Messages For The Future](https://www.youtube.com/watch?v=GDrBIKOR01c)

[How Many Things Are There?](https://www.youtube.com/watch?v=C6eOcd06kdk)

[Will We Ever Run Out of New Music?](https://www.youtube.com/watch?v=DAcjV60RnRw)",4hqzkb,t3_4hqzkb,neS-,,Comment,5,0,5
d2rssng,2016-05-03 20:47:19-04:00,neS-,,"Thanks for the first post that seemed to take my question seriously, and explain it in a short, and understandable way.

I already had a rough idea it was possible, I just wanted to confirm my theory, and you explained it better than I ever could.

 Of course in reality you could spend the combined lifetimes of every human being, with the sole goal of trying to find your picture in pi/infinity, and never find it. Or you could find it on your first try :)",4hqzkb,t1_d2rseve,Frore,,Reply,2,0,2
d2rniqb,2016-05-03 18:42:58-04:00,njaard,,"A computer has no ""soul"". If two files have the same bits, then they're the same file.

I don't think you understand your question, and therefor, any answer will be unsatisfying.",4hqzkb,t3_4hqzkb,neS-,,Comment,2,0,2
d2rnrz6,2016-05-03 18:49:13-04:00,neS-,,"So basically If I was to randomly generate a picture with the same bits, as picture of you, that I have never seen/have no access to, I could theoretically have the same exact picture? Lets then say there is an alternate universe where I do the same thing, except the original picture doesn't exist, I could get a picture of you that was never taken, yet looks exactly like you, with a big fat cock in your mouth, and it would be completely indistinguishable from a real picture?",4hqzkb,t1_d2rniqb,njaard,,Reply,-1,0,-1
d2rnwo8,2016-05-03 18:52:27-04:00,njaard,,"Um. How old are you? To answer your question...

I would be distinguishable from the ""real picture"" because you have the knowledge that real picture is correct, but you don't have that information about the reproduction.

You're talking about information theory.",4hqzkb,t1_d2rnrz6,neS-,,Reply,0,0,0
d2rr0ej,2016-05-03 20:07:00-04:00,Sqeaky,,"No need to be rude, this person clearly needs help understanding. Maybe this is not the right sub for such basic question, but a more appropriate response would be to direct neS elsewhere.",4hqzkb,t1_d2rnwo8,njaard,,Reply,1,0,1
d2rob7x,2016-05-03 19:02:18-04:00,neS-,,"Lets say I didn't know which picture was which. I just had two identical pictures, one taken by a photograph, and another made by some monkey's slamming on a keyboard infinitely until you get the picture. Both as computer files, have the same exact variables. Again I'm smart enough to know that in reality, that if this was attempted, you couldn't replicate a picture without having the original. I was just curious about the possibility of basically the same ""code"" being created independently of each other, yet being the same. If I had a ""random picture generator"" that infinitely creates pictures, over an infinite course of time, could you theoretically basically have a picture of ""everything"". Basically the infinite monkey typewriter question, but instead with a picture. Or even music for example. Could I theoretically have a program that randomly generates mp3's at random lengths, with random sounds, and end up with a hit song that is identical to a song that's going to be made and released in 2020.",4hqzkb,t1_d2rnwo8,njaard,,Reply,-1,0,-1
d2rog1b,2016-05-03 19:05:36-04:00,njaard,,"Yes. A few trillion* years after the universe ends.

^(* a made up number for example)",4hqzkb,t1_d2rob7x,neS-,,Reply,2,0,2
d2rr817,2016-05-03 20:12:07-04:00,Sqeaky,,"I ask this in all seriousness.

Do you know High School level math? If so try to calculate how many pictures can exist that are 1920 pixels wide and 1080 pixels talls using only 2 colors, black and white. Then try again with 24 bit color, 256 shades of Red Green and Blue.

If you don't think you can do math try to describe as much as you can and I will try to help you get past that.

If you/we do this you can get a sense of how many pictures there and then we can start talking about how long it would take to search through that ""problem space"".

EDIT - I wrote a program to generate random images in high school, quite similar to how you describe it.
",4hqzkb,t1_d2rob7x,neS-,,Reply,1,0,1
d2rsdot,2016-05-03 20:38:28-04:00,neS-,,"I'm aware of how improbable it is, that's why I am asking in theory. I don't actually believe that you could have a random image generator that generates every possible picture. What I am asking is basically the infinite monkey theorem, but with a computer instead, and instead of trying to replicate Shakespeare, its goal is to generate a 1:1 replica of an image.

Again in reality there are text generators that replicate the infinite monkey theorem, and you can leave it up for days and days, and it will be nothing but gibberish, if the occasional word, or partial sentence. We don't have enough time to generate enough words, or even to go through it all to see if all of Shakespeare's works are recreated word for word. 


It's completely a theoretical question I'm asking. Even if I had a trillion years, and a random image generator, I completely understand that odds are the end results would most likely be completely random unrecognizable images of random shape and color, and would likely never generate a photo of a ""person"" yet alone a photo that already exists.

I just wanted to know if it was in the rarest most astronomically low probability, that it could be done. Or if there was something more to an image file that I didn't know. ",4hqzkb,t1_d2rr817,Sqeaky,,Reply,1,0,1
d2rv3m5,2016-05-03 21:39:00-04:00,Sqeaky,,"All image files, jpg, png, bmp are different ways to map the pixels on your to numbers on disk. You can even read up on how [Bitmaps are stored](http://paulbourke.net/dataformats/bitmaps/) ( [Wikipedia has more links too](https://en.wikipedia.org/wiki/Bitmap) )o r even the how the more sophisticated but still old [JPGs are stored](http://www.imaging.org/ist/resources/tutorials/inside_jpeg.cfm).

If your hypothetical program generated the pixels then you can ignore the underlying representation and just rely on some tool to covert your pixels to jpgs or whatever file format you care about.

If it generates the underlying representation then it either needs to be format aware or it will waste a huge portion of its time making invalid files.

Lets talk about just making a program that generates pixels, this is trivial to do. There are even a few online, Here is one geared toward for making pretty pictures rather than truly covering the whole space of all images: http://www.random-art.org/

> I just wanted to know if it was in the rarest most astronomically low probability

So yes it can be done, and those odds are easy to calculate we just need to know the size of the search space.

Everything from here down is me rambling about odds.

That is the reason I wanted to start with the math if because that is what your question really comes down to. The number of states an image can assume directly relates to the likelihood of duplication. The number of states is 2^24^(1920*1080) (2 states per bit 24 bits per color in and width, height, bits of color). The first pixel can be one of 2^24, then so can the second, and so on so the number of colors to power of the number of pixels is our formula. 

For 24 bit 1920x1080 gives us the preposterous number 3.130x10^13874. Even will a billion tries per second we couldn't exhaust the the search space in billions. So if we are looking for 1 in image in that it would be miraculous to find it without intelligently searching it out.

For a black and white 100x100 image the number is still huge: 1.995x10^3010 So even searching this seems preposterous.

Even a black and white 10x10 can be one of the huge 1267650600228229401496703205376 possible images. I wouldn't want to randomly traverse that space, though it seems possible with some engineering.",4hqzkb,t1_d2rsdot,neS-,,Reply,1,0,1
d2rqyib,2016-05-03 20:05:45-04:00,Sqeaky,,"What you are describing with: 

> Could you theoretically, render a picture that hasn't even been taken/doesn't exist?

Is all photo editing software. 

Yes, it can be done and works well... I mean look at Star Wars and Jurassic Park.

> I just want to know theoretically if it would be possible if by some impossible chance, to completely independently ""create"" a photo. I assume the same could be said for any other type of computer file, whether it is a video, or a song

It is possible to create multiples of a file. I do not think this is what you meant, but copying a file does this. I think you meant somehow randomly stumbling onto the same numbers/bits in a file this can happen too.

Lets look at smaller simpler files. Lets take ASCII or UTF8 text files, because they are simple and well specified, all computer pretty much agree on the same set of rules for making and describing them. If we stick to the english alphabet and common numbers and symbols the way each character is stored is as a single 8 bit binary number. Here is a chart showing which characters map onto what numbers: http://www.ascii-code.com/

If you put some stuff into a file, perhaps the text ""Text File Contents"" the computer will store this as the following series of numbers: 84, 101, 120, 116, 32, 70, 105, 108, 101, 32, 67, 111, 110, 116, 101, 110, 116, 115

If you and I each opened a text on machines with the same way of storing binary numbers and each typed ""Text File Contents"" into a file they would be identical in terms of numerical representation. Each of our files on the disks in our computer would contain the same numbers listed above, they would be the copies of same file in two places.

",4hqzkb,t3_4hqzkb,neS-,,Comment,1,0,1
d2s9dmj,2016-05-04 06:33:40-04:00,BBQspaceflight,,"To pitch in, this sounds like it has some machine learning elements in it. For example, software that reads handwriting is provided with a lot of handwritten letters, and finds patterns that are common for one letter (which sounds like the variables you refer to). I imagine you can use this information too to create a new picture of a letter that is not part of the training dataset.",4hqzkb,t3_4hqzkb,neS-,,Comment,1,0,1
d2rqj8i,2016-05-03 19:55:44-04:00,throwawaylolololoolo,,"this is easily the worst question i've ever seen on askCS.  You're asking if two files contained the same data, would they be the same? 

*Of course*.  Have you never copied a file?  ",4hqzkb,t3_4hqzkb,neS-,,Comment,1,0,1
d2rrnns,2016-05-03 20:22:12-04:00,neS-,,"Yeah I've copied files, that wasn't what I was asking.

 I was asking if you could in theory, have this happen, but by pure chance. Completely independently of each other. Hell even years before or years after. That's a much more complicated question, and answer than copying a file. Its easy to copy something when you have access to the original. It would be like if we both painted a painting that was identical in every single imaginable detail, yet we never met, or saw each other's work. Its possible in theory, but in reality there would always be a difference no matter how close the two paintings appear.",4hqzkb,t1_d2rqj8i,throwawaylolololoolo,,Reply,1,0,1
d2rv9rh,2016-05-03 21:42:57-04:00,okiyama,,"Let's get some conceptual basis for you then.

All files on a computer are, as you said, made up of 0s and 1s. So there may be a file that looks like this:

    001010011010010101

What does that mean? Well, nothing. In fact, on it's own any data has no meaning. All meaning is imposed upon the data by humans. So, let's say that you and I decide we are going to make our own cool file format. It's going to be a simple one, that contains either a 1 or a 0. If that file contains a 1, it means that I think you're a cool dude. If it contains a 0, it means I don't think you're a cool dude. So I send you that file over email and you open it up to find:

    1

I think you're a cool dude! Wow! What you and I have just invented is a new file format. It is a meaning that is imposed upon that file. Real file formats are of course much more complicated. The format for an image, like a BMP file would be something like:

    The first four 0s and 1s tell you how many pixels wide the image is. The next four 0s and 1s tell you how many pixels tall the image is. The rest of the file contains color data to describe each individual pixel.

So then if I send you an email of a really cool BMP file I made, and you knew the file format of a BMP file then you could meaningful display that BMP file I sent you! The importance is the standard that we have both agreed upon. If you don't know what to expect when you get that image, you can't write a program to read the data and make it show up on the screen because you've got no idea what the data means!

Now, if you're thinking ahead to randomly assigning bits you mean realize that taking a perfectly good BMP file and changing it could either break the format, or change the image. For example, in the BMP format I gave above, if you have an image that is:

    This image is 5 pixels wide and it is 2 pixels tall. Now, here's 10 pixels worth if color data.

If you changed one of the 10 pixels of color data to be a different color, you'd get a different image. One pixel would be a different color. Likely not something the naked eye would notice. If, however you change it to be:

    Here's a picture that's 5 pixels wide and 200 pixels tall. Now, here's 10 pixels worth of color data.

Suddenly, you've broken the image! The cool program you wrote to go make that picture appear on screen is going to look for 1,000 pixels worth of color data, only find 10 pixels worth and not know what to do. That's because you've now broken the file format we agreed upon.

So what the means for your question is that it is **entirely** dependent upon how we define our file format. Some are complicated, some are simple. Depending on the format, random data will look differently. To get super theoretical, you could define a file format such that any data you throw at it gives the same exact image everytime. That's because all meaning is imposed by the file format. So you're free to say, ""No matter what data is held in this file, it means an image of a banana"".

If you're looking for a more concrete answer to ""how long would it take for 2 random streams of data to be equal"", that depends on the size of the data. If the data is 2 megabytes, roughly image sized, it would take a modern processor somewhere in the neighborhood of 10^300,000 times the age of the universe to get those two streams of zeros and ones to be exactly the same.",4hqzkb,t1_d2rrnns,neS-,,Reply,1,0,1
4hlt62,2016-05-03 01:15:08-04:00,silentcon,Get decimal components from hex,"This is in assembly, so I don't have access to converter methods.

For example:

    0x4D2 = 1234 (dec)

I would like to know some operations from the hex value that gives me the decimal components (thousands, hundreds, tens, ones).

I'm only interested for decimal values < 10,000 or hex < 0x2710

I did some trial and error and I found that:

    0x4D2 / 0x3E8 = 0x1 (1 is the thousands place of 1234)
    0x4D2 % 0x64 = 0x2 (2 is the hundreds place of 1234)
    0x4D2 ? ???? = 0x3 (3 is the tens place of 1234)
    0x4D2 % A = 0x4 (4 is the ones place of 1234)

I think even better if I'm able to get a hexadecimal value that represents its equivalent decimal value.

    0x64 = 100 (dec)
    0x64 and (some unknown calculation) = 0x100 (visually similar to 100 dec)

    0x270F = 9999 (dec)
    0x270F and (some unknown calculation) = 0x9999

Hopefully you get what I want. I'm sure there is a relationship with hex and dec.
",,,,,Submission,5,0,5
d2qpt63,2016-05-03 01:59:11-04:00,HenryJonesJunior,,"This sounds suspiciously like a ""do my homework for me"" question.

What have you tried so far?  What thoughts do you have on how to approach this?",4hlt62,t3_4hlt62,silentcon,,Comment,3,0,3
d2qq0nm,2016-05-03 02:08:36-04:00,silentcon,,"It is a homework. Is it against the subreddit rules? If not, im happy with clues. I have a hunch there is a more efficient way. 

Ive tried the example i've given in the post. I was able to find the tens place. It works but it is not a good solution. ",4hlt62,t1_d2qpt63,HenryJonesJunior,,Reply,2,0,2
d2r56fg,2016-05-03 12:01:57-04:00,lordvadr,,"**Edit:** Some words

You have to understand number bases a little bit.  Humans work in ""base-ten"" (mostly because we have ten fingers), but what does that mean?  It means we have ten symbols in our numbering system (0-9).  They could be circles and squares and such, and they'd still be symbols.

(Excuse the x vs * in multiplication.  It keeps throwing me into italics and I don't care enough to figure out how to escape an asterisk.)

So what does `2315` mean?  Well, in any base, it's the symbol value times the base raised to the position's power... so `2315` means...2x10^3 + 3x10^2 +1x10^1 + 5x10^0. The first position is to the zero-power (and anything to the 0 power is 1, so it's a completeness thing to say 10^0 instead of 1, and likewise to say 10^1 instead of just 10).

Now, it just so happens that most number bases borrow our symbols from base 10. 

So, let's talk about binary.  It has two symbols, 0 and 1. `101101`, converted to decimal is (leaving out the zeros) 1x2^5 + 1x2^3 + 1x2^2 + 1x2^0 = 32+8+4+1 = 45.

What you have to do is stop thinking about ""tens position"" or ""hundreds position"" in other bases...those only exist in base 10.  In binary, it's the ""one's position, two's position, four's position, eitght's position, but nobody really uses that phraseology.

There's another numbering base called *Octal*, which, you guessed it, has 8 symbols in it (0-7).  You mostly only see this in Posix permissions, but it's useful to understand academically.

By the way, at least in C, you specify an octal number by prefixing it with a zero (0234) is 234 in octal, or 156 in decimal.  I'll do the conversion for you, 2x8^2 + 3x8^1 + 4x8^0 = 2x64 + 3x8 + 4x1 = 128 + 24 +4 = 156.

So, in octal, you have the 1's position, the 8's position, the 64's position, etc.

Now let's talk about hex.  Hex has 16 symbols, and since base-10 only has ten, there are an additional symbols barrowed from the roman alphabet (A-F), so the 16 symbols are (0123456789ABCDEF) with the once above 9 having, well, the base-10 values  above 9 (A=10, B=11, ..., F=15).  Same rules apply: 0x4D2 is 4x16^2 + 13x16^1 + 2x16^0 = 4x256 + 13x16 + 2x1 = 1234.

So, if you were to convert hex digits to binary, you'll find that F=1111, which is, by the way, why we use because two hex digits is exactly one byte.  One digit is called, I'm not kidding, a nibble (half a byte).  This is why it's useful in computation.

So, I realize that using base-10 math to convert between number bases isn't the easiest thing to do, but until you get your head around it, that's what you'll have to do.

Fun thing is binary to/from hex because

    0000 = 0  \
    0001 = 1   \
    0010 = 2    \
    0011 = 3     \ Octal two/from binary is just as easy because it's just 3 bits.
    0100 = 4     /
    0101 = 5    /
    0110 = 6   /
    0111 = 7  /
    1000 = 8
    1001 = 9
    1010 = A
    1011 = B
    1100 = C
    1101 = D
    1110 = E
    1111 = F

So, from there, if you see binary of 100101001010101010001011011101001, break it up into nibbles 0010,1001,0101,0101,0001,0110,1110,1001, and convert directly: 0x295516E9.

But, say you have a random decimal value that you want to convert to hex.  You do this like subtraction.  First you have to find the leading digit, which is easy, you just start taking *base*^*position* until you get to a number too big and you know your target starts one position lower.  Take 2197 decimal.

* The positions in hex are 1, 16, 256, 4096.  4096 is too big, so you know it's a 3 digit number.
* You start finding ""digits"" in your target base that is one digit too-small to be bigger than your starting number.  So 9x16^2 (2304) is too big, so first digit is 0x8.
* Subtract 8x16^2 (2048) from 2197 and get 149 left over.
* Do it again.
* Next digit, Ax16 is too big (160) but 9x16 is (144) is fine, so second digit is 9 and the remainder is 5.
* Last digit is always 5.
* Your answer is 2197 in decimal is 0x895.

Now, there are far more efficient ways to do this with integer division and modulus (%) and such, but if you're doing it on paper, you need to understand how it works.  Every digit is basically *origin* / *base*^*position*, then you take *origin* % *base*^*position* and work down to smaller digits.",4hlt62,t1_d2qq0nm,silentcon,,Reply,4,0,4
d2s6p14,2016-05-04 03:47:19-04:00,silentcon,,"Yes I did it with division and modulus since doing the algorithm takes a lot more instructions and logic into it.

Thank you for the explanation.",4hlt62,t1_d2r56fg,lordvadr,,Reply,2,0,2
d2qqu3q,2016-05-03 02:47:39-04:00,HenryJonesJunior,,"You say you don't have access to ""converter methods"" - presumably you mean methods to convert numbers between different bases.  Have you considered writing your own?

Consider, for example, converting a number into its string representation.  Could a similar technique be useful here?",4hlt62,t1_d2qq0nm,silentcon,,Reply,1,0,1
d2qvzgx,2016-05-03 07:49:20-04:00,Bottled_Void,,"How is the data stored? Is the 'hex' value already in memory?


Values in memory aren't in hex format, or binary, or decimal. They're just a series of bits which mean whatever you want them to mean.


Or are you reading a string of characters that you need to convert?",4hlt62,t3_4hlt62,silentcon,,Comment,1,0,1
d2s6n30,2016-05-04 03:44:16-04:00,silentcon,,"No. It is time, stored as bits on the memory, but needs to be represented as decimal because the display either decodes from hex or you have to manually turn on the different segments to form numbers. Then the represented decimal has to be divided into seconds and milliseconds.

Looks like division and modulus operations are the way to go. Thanks.",4hlt62,t1_d2qvzgx,Bottled_Void,,Reply,2,0,2
d2s8z8p,2016-05-04 06:09:47-04:00,Bottled_Void,,"Yes. Sounds about right. Just make sure to subtract the units before dividing to avoid any weird rounding errors.


But just forget about hex all together. As far as you're concerned, you've got a decimal number in memory that you just want to pick the digits out of.


I only say this because I've had 'Octal' digits in memory before and had people ask me how to store them as a hex number or a binary. Well, it already is, just look at it a different way.


But yeah, there isn't a simple easy to use way of getting decimal digits, which is a bit of a shame, but not that hard to work around.",4hlt62,t1_d2s6n30,silentcon,,Reply,1,0,1
d2qwr09,2016-05-03 08:20:25-04:00,BonzoESC,,"1. Figure out how to convert a single hexit between 0 and 9 to decimal 
2. Figure out how to convert a hexit above 9 to decimal
3. Figure out how to convert two hexits to decimal
4. Figure out how to convert any number of hexits to decimal

Learning the process of learning in this case will be more useful than begging for answers. ",4hlt62,t3_4hlt62,silentcon,,Comment,1,0,1
4hlmw3,2016-05-03 00:22:40-04:00,Ptch,[Java] What is the difference between these two classes?,"Just a simple question I am having a bit of trouble with. Another way to ask would be, what is the point of having the 

    public static void main(String[]args) 
Thanks all!


Segment 1:

    public class test{
        public test(){
            System.out.print(""Hello"");
        }
    }

Segment 2:

    public class tester{
        public static void main(String[]args){
            System.out.print(""Hello"");
        }
    }",,,,,Submission,1,0,1
d2qnhkw,2016-05-03 00:31:01-04:00,cheryllium,,"The test() is Segment 1 is a constructor. It will only run if some other code instantiates a test object. 

The main method is the entry point of a Java program. Your program will not run without a main method. When a Java program runs, it starts by calling that main method. ",4hlmw3,t3_4hlmw3,Ptch,,Comment,3,0,3
d2qnl9t,2016-05-03 00:34:18-04:00,Ptch,,But can't I run a program by calling methods from a constructor?,4hlmw3,t1_d2qnhkw,cheryllium,,Reply,1,0,1
d2qoaex,2016-05-03 00:58:25-04:00,cheryllium,,Something would have to call that constructor first. ,4hlmw3,t1_d2qnl9t,Ptch,,Reply,2,0,2
d2qpj0l,2016-05-03 01:46:55-04:00,Ptch,,Oh I see what you mean. Thanks!,4hlmw3,t1_d2qoaex,cheryllium,,Reply,1,0,1
d2qoldg,2016-05-03 01:09:32-04:00,dxk3355,,The point of main is so that every program has a defined entry point for the computer to start execution from.  Without it you just have a pile of code in who knows what order.,4hlmw3,t3_4hlmw3,Ptch,,Comment,3,0,3
d2qpjek,2016-05-03 01:47:24-04:00,Ptch,,This makes more sense now. Thank you for taking the time!,4hlmw3,t1_d2qoldg,dxk3355,,Reply,1,0,1
4hke1e,2016-05-02 19:13:17-04:00,GamesAreArt,Good framework for graphs.,"I would like to be able to load a graph from snap data, and then be able to compute the Centrality, Triangle Count,  as well as other similar measures without rolling my own package. I also need a language which I can create a snap graph as well as compute a limited bfs(within x distance only), and add and remove edges. These don't have to be the same program/framework. ",,,,,Submission,8,0,8
d2qe4t3,2016-05-02 20:32:27-04:00,incredulitor,,"I've had good luck with Neo4j and NetworkX.

Edit: don't know about how they might interact with snap data but maybe translating it into the native format could still be less work than writing your own framework.  Can you give an example of the kind of data you're looking at?",4hke1e,t3_4hke1e,GamesAreArt,,Comment,1,0,1
d2qgmwb,2016-05-02 21:31:55-04:00,shiftedabsurdity,,"igraph for C, R, python ",4hke1e,t3_4hke1e,GamesAreArt,,Comment,1,0,1
d2ww7hz,2016-05-07 20:04:59-04:00,GamesAreArt,,"THANK YOU SO MUCH, this I believe is exactly what I'm looking for. You are amazing. ",4hke1e,t1_d2qgmwb,shiftedabsurdity,,Reply,1,0,1
4hkath,2016-05-02 18:55:06-04:00,bridesign34,USB Web Keys - how reliable are these?,"I'm a web designer/developer and I had a client ask me about USB Web Keys - which are essentially USB sticks that when inserted into a USB port, will automatically launch a browser and take the user to a web page. Apparently this does NOT use the OS's Autorun feature, which current OSs disable for security purposes. The little bit I've been able to find, technically, say that it mimics a USB keyboard to essentially launch the browser and ""type"" in a URL. They are sold all over for marketing purposes. There are a number of sites/companies that sell them and claim they are perfect for direct marketing, handing out at trade shows, etc. Of course, these companies all vouch for their reliability, but I'd like to hear from real people whether or not these are reliable enough to use and send to potential clients? They claim to work with Windows and Mac, but they seem very hacky. Anyone have any insight or opinions on these devices?",,,,,Submission,1,0,1
d2qbep6,2016-05-02 19:24:04-04:00,maq0r,,"I can't speak for how ""awesome"" they are in the Web sense. But as a NetSec engineer I can tell you.

USB devices that plugin and launch a web browser is a hackers wet dream. I can put payloads on the USB or/and the website that pops up. In fact, plenty of malware spreads through USB. So much so that any company with a reputable infosec department would ban through policy these devices.",4hkath,t3_4hkath,bridesign34,,Comment,5,0,5
d2uhmzx,2016-05-05 19:42:44-04:00,bridesign34,,"Oh yeah, I definitely understand those implications. I'm just wondering how reliable they are from a technical standpoint. Just for context, my client is a summer camp who wants to use this to distribute some marketing material to prospective families. They would definitely just be used for ""auto linking"" to a web page that will play a promo video. But seeing as these would be going to prospectives, they want to ensure that the webkey's would work across all platforms on desktop or laptop computers. If there's even say, a 10% failure rate where these devices don't work correctly, then they would not go this route. Thanks for your reply and if you have any additional insight into that!",4hkath,t1_d2qbep6,maq0r,,Reply,1,0,1
d31mmjg,2016-05-11 14:29:25-04:00,lhamil64,,"Thinking of it from a user's perspective, if I got a ""flash drive"" from a company and it automatically launched my browser when I plugged it in, I would be very annoyed. It's basically like a hardware pop-up. ",4hkath,t1_d2uhmzx,bridesign34,,Reply,1,0,1
d2tsfa7,2016-05-05 10:05:43-04:00,dxk3355,,Whoever thought of these is either a complete idiot or a genius hacker.,4hkath,t3_4hkath,bridesign34,,Comment,1,0,1
d2uhnmp,2016-05-05 19:43:12-04:00,bridesign34,,"Right? Now that i'm aware of them, I'm certainly going to be wary of any that I receive.",4hkath,t1_d2tsfa7,dxk3355,,Reply,1,0,1
4hjl1p,2016-05-02 16:22:20-04:00,rocksamis,Which is better option blogging or web designing,,,,,,Submission,0,0,0
d2rqx9s,2016-05-03 20:04:55-04:00,throwawaylolololoolo,,lolololololololololololololololololololololololololololololololololololololololololololololololololololololololololololololololololololololo,4hjl1p,t3_4hjl1p,rocksamis,,Comment,1,0,1
d2sdz7f,2016-05-04 09:35:22-04:00,rocksamis,,what is this fucking?,4hjl1p,t1_d2rqx9s,throwawaylolololoolo,,Reply,1,0,1
4hfmck,2016-05-02 04:47:25-04:00,Itxi_atea,Does anybody know a messaging system with this characteristics?,"Hello everybody,

I'm looking for a messaging system. Currently I'm using Amazon SQS to provide the following functionality:

I declare a queue for each priority I've defined (normal, premium, admin) and then enqueue the messages according to that priority. The problem resides in the fact that now I have more than two types of messages, one for each development environment (one for Android, one for IOs, one for Windows and so on...). 

So to mantain this scheme I may delcare one queue for each environment and in that environment one for each priority. Althought this is factible, It doesn't seem like a good practice to me. (n priorities for m environments, duh)

In my opinion, the system could be better if you can simply ask for a message with this ""pseudo-code"" query:

""Give me the most priority message for Android""

This cannot be implemented as it is in Amazon SQS because there are no filters available in that system (it's a simple queue service).

This is the basic idea for the system I'm looking for, furthermore, the system must also have the following characteristics:

* It must be scalable.
* It must be a FIFO or pseudoFIFO system.
* It is capable of giving simple statistical information like the mean time a message is in the system.
* The system must guarantee that the message is processed before removing it from the system. (I.E.: A message is not lost if the receptor fails)
* I must be capable of knowing if a message is being processed.

Does anybody know something to achive this functionality?,
thank you very much!

P.S.: I've never asked in this subreddit so please, let me know if this is not the appropiate subreddit for this question.

",,,,,Submission,3,0,3
d2q792m,2016-05-02 17:48:32-04:00,Ready_Player_Two,,How about [IBM Message Queue](http://www-03.ibm.com/software/products/en/ibm-mq). Looking at its descriptions it sound like it should have the features you require. ,4hfmck,t3_4hfmck,Itxi_atea,,Comment,1,0,1
4hf0t1,2016-05-02 02:42:35-04:00,cwrutu,Masters in Computer Science (AI concentration) Question: How much should I compromise for a university with a better reputation and ranking?,"I just got admits for Fall 2016 for both UFL and UTD. I also had an admit for Northeastern but declined it because of cost/benefits. Now UTD and UFL cost almost same. But UFL has a better ranking and reputation that UTD. UTD, however, seems to have more courses in my field of interest - AI.

Is there a huge difference between faculties at UTD and UFL?

If I am fine with jobs anywhere round the world after my Masters, by how much will the job opportunities after Masters get affected by the University?

edit: I am not very interested in PhD. However I will love a challenging job in AI after graduation.
",,,,,Submission,2,0,2
d2q37mz,2016-05-02 16:26:14-04:00,KronktheKronk,,"The university you pick, if reasonably similar, will have negligible impact on your job prospects post degree.

What's far more important is the breadth and depth of real life experiences you get in the interim.",4hf0t1,t3_4hf0t1,cwrutu,,Comment,2,0,2
4heou3,2016-05-02 01:39:15-04:00,jbw976,what are your favorite videos on practical topics in computer science/engineering related to writing and releasing exceptional production code?,"I'm interested in topics such as scalability, reliability, availability, etc. for real production systems.  What are your favorite videos on that broad spectrum of topics that are non-trivial, interesting, and can help a senior level software engineer learn some patterns and designs for building solid and large scale code bases?

the first example that really stuck in my mind was a google engineer's story about being brought in to help fix the healthcare.gov disaster:
http://youtu.be/GLQyj-kBRdo

any suggestions for more great content like that would be appreciated!",,,,,Submission,12,0,12
4hdgg5,2016-05-01 22:07:48-04:00,CyberVin,Can somebody please explain the Navigation Drawer in the Google Play Store?,"Hoping you know Android based Java and XML (Android Studio)

So, I've been stressing over a project for a week, but nevertheless, I'll keep it short. So you know how the Play Store has a navigation drawer right? I that drawer, there's a tab for ""Apps"" and another tab for ""Movies"" for example. Are these fragments or activities? If they're activities, can you explain how it is able to switch activities ""behind"" the navigation drawer?",,,,,Submission,0,0,0
d2uqol5,2016-05-05 23:51:54-04:00,adarshurs,,"I think they are fragments, because it's not possible to use a single instance of navigation drawer on multiple activities.

I hope 'm not late :)",4hdgg5,t3_4hdgg5,CyberVin,,Comment,2,0,2
d2ygdnh,2016-05-09 07:11:00-04:00,CyberVin,,"Alright, thank you, and nope, not at all 😊",4hdgg5,t1_d2uqol5,adarshurs,,Reply,1,0,1
4hc0ob,2016-05-01 18:16:25-04:00,Lifelong_Throwaway,Botting in Computer Games,"In games like League of Legends,  Team Fortress 2, and Call of Duty, programmers writing scripts to play the game for the human player with the intention of cheating is a pretty big problem. My question is, what goes into writing software like that? Do programmers hook into the game to get information about where things are (terrain, enemies, etc.)? How is that done? Do they directly read the game state, or process just the image on the screen?",,,,,Submission,19,0,19
d2oxkoe,2016-05-01 18:45:03-04:00,E765,,"One way to start is sometimes looking through modding apis for certain games. There are usually experienced people with tutorials on forums. One good example is RuneScape bots.

I believe a lot of the initial work is through trial and error.",4hc0ob,t3_4hc0ob,Lifelong_Throwaway,,Comment,3,0,3
d2p90rk,2016-05-01 23:42:52-04:00,craiig,,This guy did a neat project that intercepts DirectX api calls to figure out where things are on screen: https://graphics.stanford.edu/~mdfisher/GameAIs.html,4hc0ob,t3_4hc0ob,Lifelong_Throwaway,,Comment,3,0,3
d2oyl1f,2016-05-01 19:14:12-04:00,oliver_kane,,"The requirements for Bots are very very widely varied. Some apps run on consoles, some run in the browser, and some are install apps on your phone. However, all box have a couple things in common.

All box have to have a way to represent the current game state. This state can come from many sources. If the hacker is lucky, they'll be able to hook directly into an API. If there are lucky, I'll have to resort to something dirty such as screen scraping, or other techniques where they directly read the screen in order to understand what's going on. Armed with this knowledge they can create a representation of what is going on in the game from the perspective of the bot.

Once the bot has an idea of what's going on in the game, so have to rely on some form of intelligence to make decisions on behalf of the player, sometimes this is very simple such as performing a repeated set of actions. Other times it's very complex all the way up to full-blown artificial intelligences. At the end of the day though they have to make some sort of decision on how to act.

Once the bot has chosen an action it needs to have a way to relay that into the system. Again there are numbers of ways to do this. The safest way to do this is to simulate actual user input such as keystrokes gestures or touching the screen. The reason I say say this as these usually fire up an actual instance of the game, making it much more difficult to detect. If an API is available, some developers will opt to go directly to the API making calls against their application infrastructure directly. After these are executed the game or the bot will need to refresh its internal state.

I'm sorry this is full of grammatical off on this but I I'm being a terrible person, and dictating while driving.",4hc0ob,t3_4hc0ob,Lifelong_Throwaway,,Comment,4,0,4
d2oyhvg,2016-05-01 19:11:36-04:00,Lifelong_Throwaway,,"It's difficult to talk about this without sounding like someone interested in cheating at games, but just out of curiosity I've done some stuff with memory scanning programs like [Cheat Engine](http://www.cheatengine.org/) and manually tested out what it would be like to do memory editing on a program. In my example, I'm looking at a fighting game similar to Smash, and it seems near impossible to find the memory locations where every little detail is stored, like values representing what move is currently being executed. In this case, I'm guessing it's necessary to actually reverse engineer the game and figure out how things work manually rather than memory scanning your way in?",4hc0ob,t3_4hc0ob,Lifelong_Throwaway,,Comment,1,0,1
d2po645,2016-05-02 10:42:41-04:00,None,,"They are just games, this knowledge is more important.

Yes, to cheat by memory modification will require reverse engineering the game. Writing software to do this (like a program to input a move and right away scan for memory changes, to narrow down that move state variable) may be necessary.

Its just software, if you can find the right angle (""games"" ""cheating"" is probably not it) to google then you can cross all the hurdles here with the open internet.",4hc0ob,t1_d2oyhvg,Lifelong_Throwaway,,Reply,2,0,2
d2pdlvl,2016-05-02 02:42:43-04:00,chasevasic,,"It completely depends on the type of game, type of automation, and platform. Some bots don't need to know the games state, they are basically a just a script. Some bots do directly read or modify the game state",4hc0ob,t3_4hc0ob,Lifelong_Throwaway,,Comment,1,0,1
d2pnvzn,2016-05-02 10:35:35-04:00,None,,"I always thought the https://en.wikipedia.org/wiki/GameShark was a good example of memory modification - the N64 console would take games in the form of cartridges (an electronics board contained in a plastic shell), the GameShark was like this but also had a cartridge plug on top so that normal games would electrically communicate through the GameShark to get to the console. This way the GameShark has total control over the game's memory and behavior and could make changes without the game ""knowing"".

The N64 version had a feature where you could find your own cheats by narrowing down memory changes (like find out how to get infinite ammo by shooting once and checking, shooting again, the GameShark code would then detect exactly which memory contents changed).

Given a way to modify memory (in this case by putting something between the game and hardware) you can implement anything. For something like LoL or WoW the game developers have time to care about this (the equivalent can be done entirely in software), so they may have checks in their code to be sure the game memory makes sense and hasn't been modified by something external - often bots will automate/emulate mouse and keyboard input instead of directly cheating the program, as that is much more difficult to detect (because it can be made to look human enough).",4hc0ob,t3_4hc0ob,Lifelong_Throwaway,,Comment,1,0,1
4h83bz,2016-05-01 02:42:00-04:00,casums18734,PHP + Angular JS,"I'm starting an internship in a few weeks and I was told I'd mainly be working on a PHP/AngularJS web app.
I'll probably get some training, but it wouldn't hurt to get a head start - especially since I only have experience with languages like C and Java. I've learned the basics of HTML and CSS but I have no PHP or Javascript experience.
I looked online for guides/tutorials and found some for PHP, and some for AngularJS, but none for both. None of them even mention how to set up an environment to use both. Can someone point me in the right direction? ",,,,,Submission,4,0,4
d2o97kc,2016-05-01 04:58:49-04:00,JimmmyGray,,"I really like 'thenewboston' YouTube channel. I've included two links to his Angular JS and PHP playlists. 

Angular JS: http://www.youtube.com/playlist?list=PL6gx4Cwl9DGBYxWxJtLi8c6PGjNKGYGZZ

PHP: http://www.youtube.com/playlist?list=PL442FA2C127377F07",4h83bz,t3_4h83bz,casums18734,,Comment,2,0,2
d2ou3yr,2016-05-01 17:10:04-04:00,agranam,,"AngularJS is not Angular2 though. It's too much that differ between the two. Angular2 doesn't even use $scope and the preferred language to use is TypeScript.

[This playlist](https://www.youtube.com/watch?v=zKkUN-mJtPQ&list=PL6n9fhu94yhWKHkcL7RJmmXyxkuFB3KSl) helped me a lot with AngularJS and made everything click, especially with filters and factories and stuff.",4h83bz,t1_d2o97kc,JimmmyGray,,Reply,2,0,2
4h7b1n,2016-04-30 22:05:40-04:00,KrustyKrab111,How to modify breadth first search code to get a path within a certain range?,"The following is the code for breath first search traversal, but Im unsure how to modify it to such that it can find the path between two specific start and end nodes? 

`public void bfs(Node root)
    {
        //Since queue is a interface
        Queue<Node> queue = new LinkedList<Node>();

        if(root == null) return;

        root.state = State.Visited;
         //Adds to end of queue
        queue.add(root);

        while(!queue.isEmpty())
        {
            //removes from front of queue
            Node r = queue.remove(); 
            System.out.print(r.getVertex() + ""\t"");

            //Visit child first before grandchild
            for(Node n: r.getChild())
            {
                if(n.state == State.Unvisited)
                {
                    queue.add(n);
                    n.state = State.Visited;
                }
            }
        }


    }
`",,,,,Submission,7,0,7
d2o850p,2016-05-01 03:48:48-04:00,zanidor,,"Sounds like a homework question? If you tell us what your thoughts are and where you're stuck, we can probably help you figure out what you're missing.

My generic advice would be to think about how a BFS visits nodes and how you might be able to remember some of those sequences.",4h7b1n,t3_4h7b1n,KrustyKrab111,,Comment,3,0,3
d2o8ece,2016-05-01 04:04:44-04:00,KrustyKrab111,,"My initial idea was to maintain a stack that maintains the path that we need. We check if any of the neighbors of the node we are currently are equal to the destination node, if it is , then we push the currrent node and the neighbor node to the stack, else we simply add the current node and continue doing BFS on the children. But this doesnt seem to work or i cant make full sense of it and probably need a few ideas on how to go about it.",4h7b1n,t1_d2o850p,zanidor,,Reply,2,0,2
d2on4cg,2016-05-01 14:08:31-04:00,zanidor,,"Imagine you have a tree that looks like:

          A
        / | \
       B  C  D
      /     / \
    E      F   G
              / \
             H   I

and you want to find a path from A to H. What order does the BFS visit the nodes? What will your stack look like on each visit? If I understand you correctly, by the time you get to G your stack will contain A, B, C, D, E, and F.

What would happen if, on each step, instead of knowing just the current node you're visiting, you also knew the path you took to get there? In other words, when you're visiting node F, instead of knowing just ""the current node is F"", you knew that ""the current path is A-D-F."" Would that help? How could you set that up?",4h7b1n,t1_d2o8ece,KrustyKrab111,,Reply,2,0,2
d2oj9b4,2016-05-01 12:25:51-04:00,PastyPilgrim,,"You want to get a path *after* performing the search. To do this, whenever you visit a node, you want to store the node that you traveled through to get to your current node. So, for example, if I travel from A -> B -> C, then I want C to store a reference to B and B to store a reference to A. Then, when I'm generating the path, I just follow those stored references from end to beginning.

[Here](http://i.imgur.com/MwY7qIJ.png) is a generic search function that performs a DFS, BFS, or A* search depending on the kind of storage structure that you give it. As you can see, my Vertex class includes a reference to a parent. When I find a new node along the s-t path, I store the node that I traveled through. Then, once I've found t and the path is complete, I just follow that chain of references back up to s.",4h7b1n,t3_4h7b1n,KrustyKrab111,,Comment,2,0,2
4h71rm,2016-04-30 20:52:40-04:00,His_little_pet,Parent child relationship in a database (having trouble understanding it),I just don't quite understand what it is.  Can anyone help me?,,,,,Submission,6,0,6
d2nyvqh,2016-04-30 21:36:56-04:00,Bottled_Void,,"It's just a one-to-many relationship.


So say you have a database which contains orders for goods. The parent would be the order and the individual items in the order would be the children.


Every order will have at least one item, but could have several. And for any given order there is a single order entry in the database.",4h71rm,t3_4h71rm,His_little_pet,,Comment,6,0,6
d2o237o,2016-04-30 23:18:18-04:00,His_little_pet,,Thank you for explaining it to me.  I thought it was more complicated than that.  ,4h71rm,t1_d2nyvqh,Bottled_Void,,Reply,2,0,2
4h4zx7,2016-04-30 12:05:20-04:00,OmegawOw,ChatBot features for a,"Hi guys,
I wanted your suggestions on making a Chat Bot for a friend of mine. Started out just testing out Chat Bot functionality but then I thought it would be a nice little gift for her.

Any suggestions on what basic features I could make that someone might enjoy ? I thought a zodiac option that sends links to those zodiac fact pages, maybe a song option that sends links to songs.

Was looking for some more ideas that would be fun to have in a basic chat bot. Thanks in advance !",,,,,Submission,0,0,0
d2nuryc,2016-04-30 19:36:15-04:00,GoldenDragonXIV,,"Another subreddit like /r/learnprogramming might be more help as /r/askcomputerscience is more oriented towards questions about theoretical compsci!

But to answer your question: It depends what the bot is going to be used for. This will be unique to whatever service you are wanting to run your bot on and what you want your bot to be. Will it be a cute little bot that reacts to what users say? Will it be a practical bot that serves some kind of purpose (e.g a personal assistant type bot?) 

A few features from my IRC/facebook chatbot that might help to get you started include:

* ignore/unignore function
* repeat function
* remind me function
* score/points for each user stored in a database
* various functions that just select a bunch of text from a file (insults, pickup lines, copypastas)
* functions that perform some action (/hug, /slap, etc)

Small things that give a personality to your bot are good in my opinion (something like giving your bot a favourite food/person/thing and making it react whenever anyone mentions it for example), and there are a bunch of apis out there for you to utilise if you want to do more complex things like search for images or whatever. The most important thing is to use your imagination!",4h4zx7,t3_4h4zx7,OmegawOw,,Comment,2,0,2
4h1k5a,2016-04-29 17:15:03-04:00,TheBeardofGilgamesh,"If the word ""add"", ""addition"", and ""adam"" are in a Trie, would inputting [a,d,a] return true?","I only ask this because I was looking at a C Trie struct: 

     typedef struct node {        
       bool end_word;
       struct node *children[27];
     } node;

from what I understand is all three point to the same node at Level 3, and addition follows the same path as add a->d->d->i. . 

and if ""add"" is a word the structs boolean would be **True**, so would that mean if I typed ada, it would return true?",,,,,Submission,7,0,7
d2mnq6n,2016-04-29 18:24:37-04:00,BonzoESC,,"I don't believe it would.

            > a > m*
    a -> d <
            > d* > d > i > t > i > o > n*

""ada"" would navigate you to the second ""a"" node on the third leve) on the way to ""adam"". Without ""ada"" being in the trie, it wouldn't be marked as an end node.",4h1k5a,t3_4h1k5a,TheBeardofGilgamesh,,Comment,4,0,4
d2mo4q4,2016-04-29 18:35:46-04:00,TheBeardofGilgamesh,,"so the second level d from a i.e. [a]->[d]-[26 pointers] aren't the same child array? I thought it was **{ end_word: true, [a,NULL, NULL, d, NULL, NULL ..]}** ",4h1k5a,t1_d2mnq6n,BonzoESC,,Reply,1,0,1
d2mr13w,2016-04-29 19:54:28-04:00,BonzoESC,,"Nope, each node has its own children. ",4h1k5a,t1_d2mo4q4,TheBeardofGilgamesh,,Reply,3,0,3
d2mtybk,2016-04-29 21:13:59-04:00,LasagnaAttack,,Does that mean it's a bad implementation?,4h1k5a,t1_d2mnq6n,BonzoESC,,Reply,1,0,1
d2mvtyk,2016-04-29 22:06:06-04:00,BonzoESC,,"In the absence of code, only looking at the struct, it looks fine.",4h1k5a,t1_d2mtybk,LasagnaAttack,,Reply,1,0,1
d2mool1,2016-04-29 18:51:15-04:00,Quintic,,"Each level can have multiple nodes, and each node can have end_word be either true or false.

In your case, A, D, D returns a different node than A, D, A.",4h1k5a,t3_4h1k5a,TheBeardofGilgamesh,,Comment,2,0,2
d2mrwxg,2016-04-29 20:18:29-04:00,TheBeardofGilgamesh,,"so does that mean every node only has one letter that is not NULL? Because couldn't node D[second letter] have both A and D in it's 26 children point out to a node? Or is it [a]->[d]->[d]->[true [empty children]], would it be the node the third D points towards that has the true? ",4h1k5a,t1_d2mool1,Quintic,,Reply,1,0,1
d2mvvft,2016-04-29 22:07:11-04:00,BonzoESC,,"No, because ""add"" and ""addition"" have the same prefix. You don't stop checking for word membership in the trie as soon as you hit a node with `end_word` set. You stop when you get to the end of your word, and see if you're on a node with `end_word` set.",4h1k5a,t1_d2mrwxg,TheBeardofGilgamesh,,Reply,3,0,3
d2nl8vf,2016-04-30 14:58:47-04:00,TheBeardofGilgamesh,,"yes, but let's say the word addition was not in the Trie, only the word add and adam would the last d of add a->d->d-> point to a node with 27 NULL children and a true boolean? or is the true boolean on the node the third D is on?",4h1k5a,t1_d2mvvft,BonzoESC,,Reply,1,0,1
d2no5hj,2016-04-30 16:20:00-04:00,nemec,,"The latter. `add` and `addition` share the same first three nodes. If you searched for `addition` the ""node chain"" would have two true `end_word`s in it: one on the `n` and one on the second `d`.",4h1k5a,t1_d2nl8vf,TheBeardofGilgamesh,,Reply,1,0,1
d2nqlh3,2016-04-30 17:31:01-04:00,TheBeardofGilgamesh,,"but the d before the second d as in ['a']->['ad']->['a', null,null,'d'] come from the same node? ada and add, wouldn't they both come from the parent prefix list that would of the 27 have a pointer at both a and d? 
",4h1k5a,t1_d2no5hj,nemec,,Reply,1,0,1
d2nqqo0,2016-04-30 17:35:10-04:00,TheBeardofGilgamesh,,I am assuming it must be that the end_word:true must be on the node that is referenced in the ['ad'] node. wait. . . . .  I see where my confusion came from. each of the 27 children potentially point to a node with it's own 27 children. ,4h1k5a,t1_d2nqlh3,TheBeardofGilgamesh,,Reply,1,0,1
d2nqxat,2016-04-30 17:40:31-04:00,nemec,,"/u/BonzoESC is right with his little diagram. Each node has its own unique children, including the children themselves.

            > a > m*
    a -> d |                      > o > n* 
            > d* > d > i > t > i |
                                  > v > e*

The above contains `adam`, `add`, `addition`, and `additive`",4h1k5a,t1_d2nqqo0,TheBeardofGilgamesh,,Reply,1,0,1
d2ntg10,2016-04-30 18:55:01-04:00,TheBeardofGilgamesh,,"so just to check n* and e* are on the same children array, and currently point to a node that has a boolean true, but all null children? ",4h1k5a,t1_d2nqxat,nemec,,Reply,1,0,1
d2nwfqj,2016-04-30 20:25:35-04:00,BonzoESC,,"Got bored, implemented a trie in elixir: https://github.com/bkerley/trietest#trietest

~~I didn't implement pretty inspect formatting though, that's next.~~

Nodes with bold lines are ending nodes: http://i.imgur.com/eh7vjDG.png

The root node has ""a"" and ""l"" children, ""a"" under root has ""d"" and ""y"" children, etc.",4h1k5a,t3_4h1k5a,TheBeardofGilgamesh,,Comment,1,0,1
d2o1nhw,2016-04-30 23:04:05-04:00,TheBeardofGilgamesh,,"Ok so I see in Elixir you initialize a new letter into an hashobject if it does not exists as in and that letter is a key to a new empty hash object. 

I get it now my confusion came about the strict confines of C as in relation to the struct above where the keys are pre defined in a struct pointer array of length 26. 

        typedef struct node {  
          bool end_word:
          struct node *children[26]:
        }

I think in this above example when the [a]->[d] when you create a new d then the children[3] = &node. And then the node at level 3 would(before anything else is added), be {end_word: true, [NULL * 26]}. But anyways, my confusion was misplaced because that node that says **end_word: true** is on a separate node, but the address is on a shared list with **a** on the depth 2 **d**. 

looking back I don't understand how I got confused, the end_word boolean refers to the letter that is the Key of the node. 

Thanks for the help, honestly looking back I can't understand how I got confused(as if my past logic got overwritten). But I seemed to imagine that when the struct for the second letter d had the array where both a had an address to **a** new node and so did **d** and that's where the boolean was set, rather than at the nodes a or d where pointing to",4h1k5a,t1_d2nwfqj,BonzoESC,,Reply,1,0,1
d2o244e,2016-04-30 23:19:11-04:00,BonzoESC,,"> But I seemed to imagine that when the struct for the second letter d had the array where both a had an address to **a** new node and so did **d** and that's where the boolean was set, rather than at the nodes a or d where pointing to

My first set of unit tests had to be rewritten after I made the same wrong assumption.",4h1k5a,t1_d2o1nhw,TheBeardofGilgamesh,,Reply,1,0,1
d2o1y80,2016-04-30 23:13:44-04:00,TheBeardofGilgamesh,,is elixer a JVM language? It looks like Ruby + Javascript[fp style],4h1k5a,t1_d2nwfqj,BonzoESC,,Reply,1,0,1
d2ohk1y,2016-05-01 11:36:26-04:00,BonzoESC,,"I think of it as ""what if Rails developers made a language for the Erlang VM.""",4h1k5a,t1_d2o1y80,TheBeardofGilgamesh,,Reply,1,0,1
d2ohk9c,2016-05-01 11:36:36-04:00,BonzoESC,,Web version: http://triedemo.herokuapp.com/,4h1k5a,t1_d2nwfqj,BonzoESC,,Reply,1,0,1
d2orar3,2016-05-01 15:56:56-04:00,TheBeardofGilgamesh,,"whoa, so you're rendering a image on the server side? How did you create the image? 

I was originally looking into using native D3 trees to make the Trie, much like this very slightly modified Trie I found on blocks(link to original is included): https://scredisapi-scgraph.rhcloud.com/fromblocks 

but I am actually trying to make a C visualization [Imgur](http://i.imgur.com/wRbCKis.png) this list will be collapsible, and grey just means NULL, but if it has a struct then it will be highlighted. But I am trying to figure out how to fit it all, and the reason why I wanted to double check is because nodes will be branching out from the same children array. What I think i will do is have only one struct expanded per level of depth at a time, non expanded would just be the ones that actually point to something. 

Have you ever used [openshift](https://www.openshift.com)?",4h1k5a,t1_d2ohk9c,BonzoESC,,Reply,1,0,1
d2p07yu,2016-05-01 20:00:16-04:00,BonzoESC,,"The tree rendering is done client-side with [Viz.js](http://mdaines.github.io/viz.js/), which is why the web page is 3mb.

Rendering it as a digraph is pretty nice. The array indexed by letter codes in your example is just a C-friendly way of doing the map indexed by codepoints: you get O(1) seek times at the expense of a bit more memory (27 pointers regardless of how many entries there are in the collection). Using maps in Elixir was easy because writing the code to index into one is easy, ""updating"" them is easy, and Elixir doesn't really have fixed-size and mutable arrays anyways. Choosing what _not_ to display is super-important, possibly more so than how to display it.

Never used open shift: I've been using Heroku since 2009, the price for a zillion joke apps and single-use type things that nobody uses is right, and I already know it :P",4h1k5a,t1_d2orar3,TheBeardofGilgamesh,,Reply,1,0,1
d2p1hp3,2016-05-01 20:33:19-04:00,TheBeardofGilgamesh,,"oh yeah, I am writing it as pretend C, since keys in JS just like elixir/ruby/python can be added dynamically, so trie['a']['d']['d'] will either be an object {} or undefined. But that's because the JS run time already has the background functions that allow new objects to be made dynamically. So I won't actually have the null array for every object, just create a function that outputs the SVG objects and colors the letters  in Object.keys blue. 


And yes the hiding, and making it controllable is the hardest part. 

Also regarding open shift, I think you can have 4 applications running for free, and then more than that[unless you create a new account] is pretty cheap. One thing I like about it is there is no ""sleep mode"" on free apps, so it's great for prototyping or launching a service for example what I sent you used to be an autocomplete service https://scredisapi-scgraph.rhcloud.com (which uses a Trie as well). Earlier, when I wanted to show you the block Trie I just added a new route **/fromblocks**, plopped in the local .HTML file i was playing around with  and **git pushed** it. ",4h1k5a,t1_d2p07yu,BonzoESC,,Reply,1,0,1
4h00gd,2016-04-29 11:50:39-04:00,Resolius,Determining Distance of Reflector,"I'm working on a project on FFT algorithms and one question involves determining how far away a primary reflector is, given pulse return signal measurements. It also provides the velocity of the sound and sampling rate as well as telling me that the receiver is turned off for .05 seconds after the pulse is transmitted.

I am not really sure where to begin on this problem if someone can point me in the right direction. I know I need to figure out the distance travelled by the signal and then cut that in half to find the primary reflector, but that's all. Thank you!",,,,,Submission,1,0,1
4gxbet,2016-04-28 21:58:00-04:00,KWtones,MacBook flashing question mark on startup and no keyboard commands are working,"I've tried holding command r, command t, option, command option r p...none of these are making it get past the flashing question mark.  I know my keyboard is working because when I do command option r p, it does the second restart as it is supposed to, but then just goes back to the flashing question mark.  Help!",,,,,Submission,0,0,0
d2ljswf,2016-04-28 22:02:01-04:00,Sir_not_sir,,/r/applehelp may be better. Have you tried resetting the PRAM?,4gxbet,t3_4gxbet,KWtones,,Comment,1,0,1
d2m242j,2016-04-29 10:00:58-04:00,artillery129,,"Try booting in safe mode, hold command+S on boot, then you can run diagnostics, also, this is not a computer science question",4gxbet,t3_4gxbet,KWtones,,Comment,1,0,1
d2m600l,2016-04-29 11:33:47-04:00,x_Zoyle_Love_Life_x,,Usually means your Mac can't find or boot into an operating system. Hard disk is dead or you have a corrupt OS,4gxbet,t3_4gxbet,KWtones,,Comment,1,0,1
4gw6la,2016-04-28 17:20:37-04:00,filljfry,How to learn to program actual functionality?,"I am a sophomore in a CS undergrad program and I have been coding and learning CS since early high school. All of the programming I've done revolves around using standard I/O, but that is not as realistic functionality-wise for the real world. I have learned Java and am in a Systems class learning C right now. I was wondering how I am able to transition from just coding assignments, to creating something that can run with a GUI that has actual functionality to it. I am looking for any tips/resources to help me understand how I can do this.",,,,,Submission,7,0,7
d2lipkg,2016-04-28 21:37:20-04:00,darthandroid,,"> using standard I/O, but that is not as realistic functionality-wise for the real world.

standard I/O is *huge* in the real-world, mainly because it is one of many different forms of *stream processing*. As big data becomes more and more important, the ability to work on streaming data is critical because you can't pull all of the data into memory at once.

Many programs don't have GUIs. Learn networking and sockets. webservices and RESTful APIs don't have any visual interface, and are full of functionality.

-----

Build something with functionality first, that operates as a command-line tool or networked service. Then, add an optional GUI to make it easier to use.",4gw6la,t3_4gw6la,filljfry,,Comment,6,0,6
d2m7v7u,2016-04-29 12:15:35-04:00,xiongchiamiov,,"Really, all web development is just string processing. Text comes in, text goes out.",4gw6la,t1_d2lipkg,darthandroid,,Reply,3,0,3
d2lkrqp,2016-04-28 22:24:23-04:00,whalt,,I think this just perpetuates the myth that the interface is just a skin you slap on at the end. It can be and often is but truly great user experiences are not an afterthought. How software is to be used should be an integral part of it's design.,4gw6la,t1_d2lipkg,darthandroid,,Reply,1,0,1
d2lqm2e,2016-04-29 01:05:48-04:00,darthandroid,,"A *GUI* and a *user experience* are two different things: the former is but a small part of the latter, and while the design of the GUI is very important to a successful product, the GUI itself *is* only skin deep. Interfaces should be thin and lightweight, and you should have as many of them as makes sense for your product. They should feel native to the platform upon which they are being used. Don't put too many functions into a single GUI/Interface, it makes it difficult to use.

User experience on the other hand affects the design of your entire product, from data model to geographic location of servers to interface.",4gw6la,t1_d2lkrqp,whalt,,Reply,2,0,2
d2ltj39,2016-04-29 03:19:52-04:00,MyKingdomForAShip,,"If you just want to build programs with GUIs you can do that in Java or you might find it easier to try something like QT. QT is based on C++ but knowing C and Java you should be able to figure it out. It has a nice GUI designer. (NewBoston on YouTube has some good tutorials)

As already stated, a GUI is just one aspect of a program. If you want your program to do something meaningful you have to focus on building useful functionality. 

Your assessments should be teaching you about the different aspects of each language so try to combine those aspects in interesting ways (like build your own calculator app or write a server in Java and then talk to it on a client written in C or something). Building little projects focusing on completing a realistic good will only make you a better coder. ",4gw6la,t3_4gw6la,filljfry,,Comment,2,0,2
d2lt8c5,2016-04-29 03:03:52-04:00,None,,"The visual studio ide makes it really easy with c++ or c#. C# is very similar to java so you could pick it up easily. [here](https://www.youtube.com/watch?v=njyrhf-wd70)

Java can also do windowed applications if you want. As for the functionality, that's up to you. Pick an idea and run with it. doing projects on your own is definitely important so keep it up. ",4gw6la,t3_4gw6la,filljfry,,Comment,1,0,1
d2rr6ei,2016-05-03 20:11:00-04:00,throwawaylolololoolo,,"javafx is the standard library within java for creating GUI'd applications.  I'd recommend watching some tutorials on javafx on youtube, and i think you'll be able to get the hang of it in 1-2 days.  Its really not that bad.",4gw6la,t3_4gw6la,filljfry,,Comment,1,0,1
4gvtht,2016-04-28 16:03:34-04:00,ghostofdilla,Project ideas for an undergrad,"I have a few courses to take in the summer and I figured I would take a computer science project course to fulfill one of my credits. One of my professors suggested that I could approach a professor in a particular research group with a project idea pertaining to their research (e.g. proposing a systems project to a professor doing systems research). I was thinking (maybe somewhat naively) I could approach a prof in ML/NN by using a challenge (kaggle or similar) as inspiration for an idea. I also wanted to approach a prof doing research in programming languages but have no idea what to propose. I've only taken an introductory programming languages course.

Does anyone have any ideas? Anywhere I can get inspiration?

*Also, are there any free **advanced** programming languages courses/resources I can use to learn more about the topic? A text you have to buy would be sufficient, my school library would probably have it.",,,,,Submission,8,0,8
d2ljr27,2016-04-28 22:00:49-04:00,Kendama_Llama,,Offer to build them a teaching tool that would be beneficial for their classes.,4gvtht,t3_4gvtht,ghostofdilla,,Comment,2,0,2
d2mg0c9,2016-04-29 15:15:09-04:00,ghostofdilla,,"Thank you, but I was thinking of doing a computer science project rather than a software engineering one. It would be a good cap to my undergrad.",4gvtht,t1_d2ljr27,Kendama_Llama,,Reply,1,0,1
4gub4o,2016-04-28 10:49:22-04:00,xeon1234,When the evaluation section in a paper is just too good.,"Sometimes, when I look to the evaluation section of the paper, I see nearly perfect results that makes me wonder if those tests are really that good, or the evaluation was faked. I don't mean that all the evaluation is fake, what I wonder is if the authors have chosen just the best results and just show them in order to the paper be accepted.

This situation makes me think about something. Imagine that I have a table with 100 results, and I decide to just show the best 90 results, removing the other 10. Is this a correct thing to do? How can I confront the authors when the results seems to be so perfect?

",,,,,Submission,10,0,10
d2l8u4b,2016-04-28 17:35:12-04:00,MCPtz,,"Repeat the experiment.

If it's not repeatable, it's not science.

If repeating is not practical, then you're going to need all the data they use to make their conclusions. If they don't make it available... not good.

If it's impractically large to make available... well CERN did it. If you're willing, there's a way to get their data. If they didn't keep their data at this point, then their results aren't worth revisiting.",4gub4o,t3_4gub4o,xeon1234,,Comment,3,0,3
4gr7nc,2016-04-27 19:20:48-04:00,Bloaf,"Can we make Inferno, but with the CLR?","1. [The Inferno operating system is super cool](https://en.wikipedia.org/wiki/Inferno_%28operating_system%29) and has a number of features that to me seem ""obviously right.""   
2. Microsoft has open sourced [all the things](https://github.com/dotnet/coreclr) in their CLR system.


Is it feasible to create a version of Inferno that has uses (C#, F#, etc) -> CIL -> CLR instead of Limbo -> Dis? ",,,,,Submission,7,0,7
d2kbx2x,2016-04-28 00:44:14-04:00,theobromus,,You might be interested to read about [Midori](https://en.m.wikipedia.org/wiki/Midori_(operating_system\)),4gr7nc,t3_4gr7nc,Bloaf,,Comment,2,0,2
d2kbz6m,2016-04-28 00:46:02-04:00,Bloaf,,"I've heard about it, and kinda wish I could try out Sing#",4gr7nc,t1_d2kbx2x,theobromus,,Reply,1,0,1
d2kcctr,2016-04-28 00:57:54-04:00,theobromus,,It's really a bummer to me that it got cancelled. But it was always kind a weird project - there's not really a lot of space for new OS's.,4gr7nc,t1_d2kbz6m,Bloaf,,Reply,1,0,1
d2kcxfu,2016-04-28 01:17:47-04:00,Bloaf,,"I don't know about that.  I feel like ""the internet of things"" will present an opportunity, like cellphones did for android (although my understanding of how different than linux it is is hazy.)",4gr7nc,t1_d2kcctr,theobromus,,Reply,1,0,1
d2mllso,2016-04-29 17:27:40-04:00,theobromus,,"Android basically is a Linux kernel plus a bunch of stuff to make it a phone OS (APIs, drivers for everything, all of the UI). In my opinion, Midori was always fighting an uphill battle since it was a new kernel and set of APIs, so it needed entirely new software, and doubly so since at Microsoft it would always be competing with Windows. In a lot of ways I think they treated it as a research project (basically asking what you could actually achieve with a model based on managed-code based process security instead of using virtual memory to do it).",4gr7nc,t1_d2kcxfu,Bloaf,,Reply,1,0,1
d2k2w9l,2016-04-27 20:41:32-04:00,jhartwell,,Yes-ish. This has been done with [CosmosOS](https://github.com/CosmosOS/Cosmos). They go from CIL->machine code using [IL2CPU](https://en.wikipedia.org/wiki/IL2CPU). It would be a TON of work to get the CLR to run on your operating system as you would need to modify the CLR in order to be able to interface with your OS,4gr7nc,t3_4gr7nc,Bloaf,,Comment,1,0,1
d2k4xwa,2016-04-27 21:34:02-04:00,Bloaf,,So what would stop us from using RyuJIT to make the machine code instead of IL2CPU?,4gr7nc,t1_d2k2w9l,jhartwell,,Reply,1,0,1
d2k5y0k,2016-04-27 21:59:39-04:00,jhartwell,,"IL2CPU is an AOT compiler, meaning ahead of time. This would compile down to machine code as a normal compile process. RyuJIT is a just-in-time compiler. It is embedded in the CLR in order to better generate machine code during process. RyuJIT requires the application to be running in order to generate the machine code.",4gr7nc,t1_d2k4xwa,Bloaf,,Reply,2,0,2
d2k7959,2016-04-27 22:32:42-04:00,Bloaf,,"Ah, I see.  I didn't mean that we had to write the whole OS in C#/F#.  Inferno is partly in C, then the rest in its VM language.  If I'm understanding things correctly, we would need to AOT the C/C++ bits that are OS/CLR, then we could JIT the rest?",4gr7nc,t1_d2k5y0k,jhartwell,,Reply,1,0,1
d2k8m89,2016-04-27 23:07:18-04:00,jhartwell,,"You would only need to run the AOT IL2CPU if you were writing c#. If you are writing c or c++ then you would use your normal compiler. I'm pretty sure, but I could be wrong,  that you can't JIT an operating system which means that ryujit would not be able to be used for the actual operating system.  You wouldn't be able to run the operating system on the CLR because the CLR needs an operating system to run. There would be a lot of modification needed to run it. ",4gr7nc,t1_d2k7959,Bloaf,,Reply,2,0,2
4gnvl3,2016-04-27 06:32:19-04:00,Biermoese,How does RAID4 (Redundant Array of Independent Disks) work?,"I read an article that explained RAID quite well, however I don't understand the workings of RAID4. What is ""parity information""? As illustrated in [this](http://www.acnc.com/_images/raid/raid4.jpg) picture, we have (in this case) four HDDs that store information and a fifth one that stores the ""parity"" information. But what is this exactly and how can I restore lost data from it?",,,,,Submission,4,0,4
d2j75w9,2016-04-27 07:59:56-04:00,dxk3355,,Parity is just a bit to check the other bits .  That sign above the disks is a XOR gate.  That is used to calculate the bit for the parity based on the data bits.http://www.ee.surrey.ac.uk/Projects/CAL/digital-logic/gatesfunc/,4gnvl3,t3_4gnvl3,Biermoese,,Comment,2,0,2
d2jelpz,2016-04-27 11:25:42-04:00,matt_bishop,,"So, as a bit of background, each bit can be either 1 or 0. An XOR gate outputs 1 when the two input bits are different, and it outputs 0 when the two input bits are the same. By setting up a small circuit of XOR gates, we can essentially set up a Boolean algebra equation. As long as only one disk fails, that equation has only one unknown value and we can solve to find that value using the values from the other four disks. 

A ⊕ B ⊕ C ⊕ D = P

The way the evaluation works, if an odd number of A to D are 1, then P will be 1. If a bit on disk B fails, P will tell you whether there are an odd or even number of 1 bits at that location on all disks, and you can simply count the 1 bits at that location on A, C, and D to see if B needs to be 1 or 0.",4gnvl3,t3_4gnvl3,Biermoese,,Comment,2,0,2
d2kn0ch,2016-04-28 09:22:45-04:00,Biermoese,,"Neat, thanks! :)",4gnvl3,t1_d2jelpz,matt_bishop,,Reply,1,0,1
4gnh8e,2016-04-27 03:51:54-04:00,moeseth,Are any undergraduates in the third year looking for a paid internship?,"Hi,

Our IT startup, www.melomap.com, from Myanmar is looking for some interns.

You will not be coming into Myanmar to work. You will work online from where you are. It will be a paid internship with minimum 3 months commitment.

You will be working as software engineers. You will have a change to work on interesting NLP projects. You will write software to automate some of the things we are doing here.

If you are interested, please email your CV to admin@melomap.com.",,,,,Submission,3,0,3
d2jcgk9,2016-04-27 10:36:35-04:00,TropicalAudio,,"""Paid"" is a very broad term. European interns would expect a payment that is far, far higher than the median wage in Myanmar, so you should probably clarify a ballpark of what your budget is here.",4gnh8e,t3_4gnh8e,moeseth,,Comment,4,0,4
d2jex8a,2016-04-27 11:32:52-04:00,moeseth,,We are willing to pay up to $700/month.,4gnh8e,t1_d2jcgk9,TropicalAudio,,Reply,1,0,1
d2jjkd3,2016-04-27 13:14:08-04:00,None,,[deleted],4gnh8e,t1_d2jex8a,moeseth,,Reply,2,0,2
d2k7hfn,2016-04-27 22:38:18-04:00,moeseth,,"I see. Thanks for the reply. However, $700/month is the maximum we could offer right now.
",4gnh8e,t1_d2jjkd3,None,,Reply,1,0,1
4gn130,2016-04-27 01:09:42-04:00,winter_mutant,What are the absolute minimum requirements for a computer to run Linux? Could you (in theory) run it on a Gameboy Color? How about a Gameboy Advance?,"I was inspired by [this total control hack of Pokemon Yellow](http://aurellem.org/vba-clojure/html/total-control.html). But I'm not just curious about Nintendo handhelds. I want to know exactly how flexible and tolerant Linux is of older (or weirder) hardware. There's that old joke that you can install Linux on a toaster, and I guess I want to know how much truth there is in that.",,,,,Submission,23,0,23
d2j2bk1,2016-04-27 03:20:38-04:00,visvis,,"One of the major issues would probably be the lack of an MMU. Linux [can be configured](http://unix.stackexchange.com/questions/190350/mmu-less-kernel) to run without an MMU but this restricts which programs can run. Many programs assume they can map memory with calls like mmap and this is not possible without an MMU. [uClinux](http://www.uclinux.org/) provides an MMU-less kernel with a set of programs that have been adapted to run in this situation.

Another potential issue is the fact that the GBC has an 8-bit CPU while most types in C are required to be at least 16 bits. The compiler can work around this by implementing 16-bit operations using 8-bit operations but it will likely be extremely slow as Linux is not written with an 8-bit CPU in mind.",4gn130,t3_4gn130,winter_mutant,,Comment,25,0,25
d2j2jwg,2016-04-27 03:33:41-04:00,winter_mutant,,Interesting! Thanks a lot for the answer.,4gn130,t1_d2j2bk1,visvis,,Reply,3,0,3
d2kcu6h,2016-04-28 01:14:33-04:00,None,,"Wait, so the answer is a yes?

Wow.",4gn130,t1_d2j2bk1,visvis,,Reply,1,0,1
d2kejik,2016-04-28 02:26:03-04:00,visvis,,"More like ""it's not completely unthinkable"". Of course there is also the issue of size so you'd have to trim down the system considerably, possibly to the point people would no longer recognize it as Linux.",4gn130,t1_d2kcu6h,None,,Reply,2,0,2
d2j79uq,2016-04-27 08:04:26-04:00,minimim,,"The problem people run into in practice is the lack of drivers.

What /u/visvis said is also true, but even [this can be worked around](http://dmitry.gr/index.php?r=05.Projects&proj=07.%20Linux%20on%208bit), if you put enough effort into it.",4gn130,t3_4gn130,winter_mutant,,Comment,7,0,7
d2ja3bc,2016-04-27 09:36:34-04:00,Aquifel,,"Unix at least can run on the GBA: http://www.kernelthread.com/publications/gbaunix/

Most of the later iterations of nintendo handhelds have some sort of linux/unix port as well.",4gn130,t3_4gn130,winter_mutant,,Comment,4,0,4
d2ka4cr,2016-04-27 23:49:09-04:00,Merad,,"Linux has been successfully [run on Atmel 8 bit microcontrollers](http://dmitry.gr/index.php?r=05.Projects&proj=07.%20Linux%20on%208bit).  However, it required the use of some external RAM chips, and the MCU had to run it via an ARM emulator because Linux doesn't support 8 bit CPUs directly.  The author reports that it works and is *usable*, if you don't mind 2-4 boot times and waiting 1 min+ for it to process a typical terminal command.",4gn130,t3_4gn130,winter_mutant,,Comment,2,0,2
4gjdez,2016-04-26 11:37:57-04:00,Czurch,Looking for an informational Data Structures desktop background,"Hey all,

I was wondering if anyone could share a background that includes data structures and their time complexity for put()/get(). I've found that setting my background to relevant information helps me learn it that much faster but I've been having trouble finding something like this. I would make something myself but I'm not much of a photoshop guru and I thought I saw something similar on this sub some time ago. Any help would be appreciated. Thanks!",,,,,Submission,9,0,9
d2i8txg,2016-04-26 14:24:21-04:00,Eracoy,,"Perhaps you could take some screenshots from http://bigocheatsheet.com/
Or for Java, which has some standard implementations of various data structures, you could look at https://datastructuresinjava.wordpress.com/order-of-complexity-of-data-structures-cheat-sheet/
",4gjdez,t3_4gjdez,Czurch,,Comment,3,0,3
d2io9dm,2016-04-26 20:13:01-04:00,Czurch,,This will work perfectly. Thanks so much!,4gjdez,t1_d2i8txg,Eracoy,,Reply,2,0,2
4ggxag,2016-04-25 22:59:01-04:00,Alpha_Ts,"Computer Science majors of Reddit, I have a few questions for you","Hello, I am a Freshman at Stephen F. Austin State University. I have recently been tasked with investigating and reporting my major for an English class. I have a few questions for those of you out in the workforce.

* Why did you decide to major in Computer Science?
* What job do you currently hold?
* Do you enjoy your job?
* What did you expect going in?
* Did your expectations match your experiences?
* What college did you attend?
* Do you believe that a major in Computer Science is versatile?

Feel free to answer any or all questions. If you want to add your own input, go right ahead. 

Edit: Formating",,,,,Submission,29,0,29
d2hhh9e,2016-04-25 23:24:15-04:00,IGetConfused,,">Why did you decide to major in Computer Science?
What job do you currently hold?
Do you enjoy your job?
What did you expect going in?
Did your expectations match your experiences?
What college did you attend?
Do you believe that a major in Computer Science is versatile?

Why? Because I've always enjoyed the problems. I love how it's always evolving. And I like how it is unique and similar to a book. (Every program you write I mean.)

I'm a software engineer. I work in a back end position. 

I love my job. 

I expected nothing. Previously I had only self developed applications and done personal projects. The field is nothing like academia or self practice. I went in as a blank slate and I'm glad. I've gotten the opportunity to learn so much. 

My experiences exceeded my expectations. 

I attended Texas Tech University and University of Texas Austin

I think computer science is insanely versatile. I think it's much like writing and it's all on your imagination. ",4ggxag,t3_4ggxag,Alpha_Ts,,Comment,4,0,4
d2hjc83,2016-04-26 00:17:48-04:00,cravenspoon,,">University of Texas Austin  

I've heard their CS is about par for the course, but that the math portion was rather difficult. (from people who went either to or from that school to/from another) Do you think that holds weight?",4ggxag,t1_d2hhh9e,IGetConfused,,Reply,1,0,1
d2hjp55,2016-04-26 00:29:19-04:00,IGetConfused,,"
>I've heard their CS is about par for the course, but that the math portion was rather difficult.

Math at UTA is one of the hardest subjects. The department is ridiculous and challenging.  So for the CS portion I took there was a huge influence of math. There were also a lot of math required classes. So I'd say math was for sure the hardest at UTA than anywhere else I've been. 

Their CS degree is above par. It's consistently one of the top in the nation. It isn't CM but it's up there for sure. Especially for a public school. 

>Do you think that holds weight?

Do I think the statement above holds weight? Yeah
Do I think the degree holds weight? Depends, but sure. ",4ggxag,t1_d2hjc83,cravenspoon,,Reply,1,0,1
d2hjrd3,2016-04-26 00:31:21-04:00,cravenspoon,,">Their CS degree is above par  

I'm glad to hear it. I'm grabbing a few associates and then heading to an area 4 year. The math part worries me, but my girl is a math major, and can explain it in C-A-T  

>Do I think the statement above holds weight? Yeah Do I think the degree holds weight? Depends, but sure.  

Oh, yeah, just the statement that they are hard on math. ",4ggxag,t1_d2hjp55,IGetConfused,,Reply,2,0,2
d2hk8bx,2016-04-26 00:47:20-04:00,IGetConfused,,"Be careful...
I've heard from many, many people that coming from a 2 year college to a school like UTA hurts a lot of people. Basically, the classes at UTA are much harder than any class at a 2 year college. It is very fast, very in depth, and very challenging. Also, math is an integral part of computer science. There is a lot of abstract information in computer science and it closely relates to mathematics. If you need help there are a ton of resources to help you succeed. But don't go in expecting to know everything and not wanting help. ",4ggxag,t1_d2hjrd3,cravenspoon,,Reply,0,0,0
d2hklel,2016-04-26 00:59:25-04:00,cravenspoon,,"I'm well aware of the challenges. I'm still debating on UT. At least with CS I have a whole company who will help me understand what I don't. I'm not bad at math, it just takes longer to click. Most CS topics click quick. For that reason I'm still considering it. But at this point in the industry I might just go Texas State. Of course, I still need to finish 3 AS degrees. ",4ggxag,t1_d2hk8bx,IGetConfused,,Reply,1,0,1
d2i1z9a,2016-04-26 12:03:22-04:00,None,,"> At least with CS I have a whole company who will help me understand what I don't.

You mean to learn in the workplace? This is good, but don't expect worker bees to share a degree of knowledge with you. And its not just the knowledge, its the practice, like lifting weights.",4ggxag,t1_d2hklel,cravenspoon,,Reply,1,0,1
d2i247d,2016-04-26 12:06:21-04:00,cravenspoon,,"Yeah, guys on our dev team. One of them helps push me in the right direction when I rabbithole too hard.  

Oh, I know you have to practice, but it's nice to have help when you get stuck ",4ggxag,t1_d2i1z9a,None,,Reply,2,0,2
d2qf487,2016-05-02 20:56:09-04:00,Dr_Geoff_Fairchild,,"I double-majored in CS and math at UT. It's definitely difficult, but not impossible at all. When you take data structures, algorithms, automata theory, and the other theory-heavy classes, it can become difficult. But it's well worth it, especially if you plan on going to grad school for CS, where things only get harder. ",4ggxag,t1_d2hjc83,cravenspoon,,Reply,1,0,1
d2hj43r,2016-04-26 00:10:35-04:00,flash89912,,"Why did you decide to major in Computer Science?
* It's what I've wanted to do since I got my first computer in elementary, programming languages have always been easy for me to pick up regardless of the language. 

What job do you currently hold?
*Backend Database Developer

Do you enjoy your job?
*Love it! I love solving problems.

What did you expect going in?
*I thought it would be so awesome working with a large corporation like the Microsoft and Googles of the world...

*I thought I would be writing a lot more of my own code

Did your expectations match your experiences?
*Startups and midsize companies are much more fun to work with than large corporations...though maybe not as stable. Im good at what I do though, so hasn't been an issue for me.

*You spend a lot of time working on code you inherit, which is not fun at times. 


What college did you attend?
*Texas A&M 

Do you believe that a major in Computer Science is versatile?
*It has been so far in my career. I have done everything from web development, software development, some upper management. Now after our team had a need for a database developer... I learned what I needed to know and moved up into that role. As long as you are willing to keep up with the new technologies which sometimes change very fast... It's very versatile. If you are the type of person that does not like learning new things on your own..Ie new programming languages etc. may not be as versatile for you..
",4ggxag,t3_4ggxag,Alpha_Ts,,Comment,3,0,3
d2hk6l1,2016-04-26 00:45:41-04:00,Im_usually_me,,I graduated from West Texas A&M. Probably what describes me best is a systems admin - I support servers to routers.  I loved every moment while obtaining my degree and love what I do. I chose it because I wanted to go anywhere in the world and not have any job limitations. And to support my family.,4ggxag,t3_4ggxag,Alpha_Ts,,Comment,3,0,3
d2hl10u,2016-04-26 01:14:58-04:00,Alpha_Ts,,My first year will be coming to an end soon. So far Computer Science has been pretty fun. It's nice to see that people are loving what they do. ,4ggxag,t1_d2hk6l1,Im_usually_me,,Reply,2,0,2
d8ml6pa,2016-10-10 19:39:56-04:00,spaceminions,,"I'm going to go to college station next fall but other than that I plan on doing pretty much the same thing. Any advice on courses or A&M stuff or whatever else you can think of? Also how rural an area can you live in and still find sysadmin positions? I'd like to see if it's really practical to live far enough from a city to be comfortable. (I'd love to live in the middle of nowhere with no neighbors and a sky full of stars, but that doesn't really mix with having internet and UPS service, so I'm having trouble deciding what to do.)",4ggxag,t1_d2hk6l1,Im_usually_me,,Reply,2,0,2
d8mp4ud,2016-10-10 21:13:28-04:00,Im_usually_me,,"I don't know what classes they offer now. It's been a few years since I was at this one. I'm sure they've changed with time. But my advice to you is to never drop a class. Even if it looks like you're failing. One of my upper level math classes had 7 grades and the final counted twice. We could retake our lowest grade test. So moral of that story is that I was failing until I took my final and the lowest graded test and I passed.  Take a variety of classes in the CS area. You might find that you love databases more than web development. And you will be able to tie all the areas together in your head.

As far as rural and Internet, I've supported my businesses with dsl (1.5mb) until this last year. I finally got fiber. The only hard part with being farther away is hardware failure. You will need to implement good warranties and have someone on the other end that can follow instructions and troubleshoot accurately. If you want to go rural, it's doable. And you might find it easier to run your own webserver, cloud services, etc. Every address is deliverable now. Since 911 was fully implemented, even Dell will support warrantied servers/pc's in Timbuktu.

Last but not least, LOVE your college days!!! I wish I could go back. Join the CS club, travel to competitions and conferences. I wish you success...",4ggxag,t1_d8ml6pa,spaceminions,,Reply,2,0,2
d8mr08r,2016-10-10 21:55:31-04:00,spaceminions,,"Thanks for the response! Good info. I'm not sure if I would do as well working from home: it always seems much harder to do schoolwork at home. Also, do you take a cut in pay / benefits / etc by telecommuting? ",4ggxag,t1_d8mp4ud,Im_usually_me,,Reply,2,0,2
d2hn0fg,2016-04-26 02:40:21-04:00,does_flips_and_shit,,">Why did you decide to major in Computer Science?

I really enjoy the tangibility of creating new things that have a purpose and do what you want them to do. Since then, I've realized that CS has a lot more to offer than just that. Computer Science is a lot about understanding, deconstructing, and creating large-scale systems. The development of systems thinking is both fascinating and beyond useful.
>What job do you currently hold

I'm a senior now but I'm an intended product implementation manager -- basically a mix between product management and software engineering
>Do you enjoy your job?

I thought software engineering internships were okay but am hoping to really enjoy the multidiciplinary approach to my new job
>What did you expect going in?

I expect to be required to learn a lot the minute I start working and never stop learning my entire time there. If I'm ever not learning, I'll quit and find a new job where I can learn more
>What college did you attend?

UC Berkeley
>Do you believe that a major in Computer Science is versatile?

Absolutely. The analytical skills that you develop through studying computer science can be applied well beyond software development. You learn to deconstruct problems into manageable subproblems and that spans well beyond the fields one tends to consider after majoring in CS",4ggxag,t3_4ggxag,Alpha_Ts,,Comment,3,0,3
d2hjfw7,2016-04-26 00:21:02-04:00,MALON,,"I graduated 7 years ago and now I run a small time repair shop. I coded websites for a while, grew to dislike web work, still do. I don't know what else to add, if any one has questions, fire away",4ggxag,t3_4ggxag,Alpha_Ts,,Comment,2,0,2
d2hny1u,2016-04-26 03:29:03-04:00,Axe_Loving_Icicle,,Why did you end up disliking web work? I'm currently thinking about getting into web development as a way to earn money and learn over the summer. Any advice?,4ggxag,t1_d2hjfw7,MALON,,Reply,1,0,1
d2hwqhi,2016-04-26 10:07:09-04:00,wontfixnow,,"* set up contracts
* don't do it under 500$-1000$
* set up contracts for maintenance and other things around it
* Get them to use a proper hoster and make clear that hosting is not in the budget mentioned above
",4ggxag,t1_d2hny1u,Axe_Loving_Icicle,,Reply,2,0,2
d2mgu9b,2016-04-29 15:34:08-04:00,MALON,,"I'm sorry for taking so long to respond!

For me, I didn't have a good environment mostly. My boss could not see the future, and still is trying to make a small newspaper survive. What I mean by that is the web company I worked for is funded by a newspaper business, so you can imagine how it's doing financially. Very poor computers (HPs literally purchased from walmart for around $300 a piece). The desks were very strange, too, very little space on them. They were Ikea or Ikea-knockoff type stuff. And oh god the windows. A nice view, but bossman refused to get any kind of shades and the sun would be shining in your eyes in the morning and off the screen in the evening. Oh yeah and did I mention that the other dev in the office (yes, just one other dev) only knew how to make wordpress sites? So most of my time was spent dealing with WP.

Besides the environment, there was a fair few of our clients who wanted really tacky things on the website and could make an otherwise nice looking website turn not-so-great. I really hate it when people (clients) who don't know what they are doing make my pretty website look bad. Granted, I did my best to still make it look okay and often times it was workable, but it was _tiresome_ to have to make their ugly wishes try and look nice.

Now, I am my own boss and I code for fun by owning game servers (always fun to code up plugins that change game behavior). I live in a pretty rural area, so there aren't many coding positions near me that I could do full-time, so instead I now do computer repair. It's a fun little business and although I'm not rich, I'm happy. At least, happier than I was!",4ggxag,t1_d2hny1u,Axe_Loving_Icicle,,Reply,2,0,2
d2i4ri3,2016-04-26 13:00:34-04:00,spongebue,,"* I've been using computers since I was 4, albeit just to play games.  I've always enjoyed building things (that came from my dad, no question) but couldn't afford raw materials.  Programming has a similar mindset of building and problem solving, but without the same need for consumable raw materials.  I started in high school, and with that being a very viable, sustainable career, I naturally started studying it in school.

* I'm a software developer for a major airline - that was another industry interest that I developed in college

* I do enjoy my job overall.  I work with a fairly specialized system (rule engine) and have become the go-to guy in the company for this.  Plus, being an employee of the airline, I get unlimited free standby flights for me, my wife, my parents, and my so-far-nonexistent kids.  What I do not enjoy is that an announcement was made that we will have to move to another state if we want to keep our jobs... I'm waiting on an offer to see what happens, but I may be changing jobs to stay in a city I love.

* When I first started, I make a lot of the rookie mistakes - forgotten semicolons or parentheses, sloppy code, trying to decide which loop is best for the job... all things that come naturally with experience.  But I underestimated how naturally it would come, and expected a lot more of the simple stuff to be a big factor in the professional world.  It's not.  If you do something like that, you'll catch it very quickly when you see red underlines in your code, and fix it right away.  By the time I was in college, I started to expect something a little closer to what I'm actually experiencing, mostly working with others.

* Expectations from college were mostly met, yes.  Like I said, those expectations varied a bit over time.

* I graduated from University of Wisconsin-Eau Claire.  A medium-sized school with what I consider to be a very good computer science program.  Technically my major was in software engineering, which is not comprehensive (needs a minor for a degree) and has less math requirement.

* I think that Computer Science is one of the most versatile fields there is, seriously.  If you study finance, economics, hospitality, restaurant management, geology (oil), or whatever, you're pretty much tied to that industry for life.  If it goes down, you better hope you've got a good enough skillset to be an indispensable employee.  If not, you'll be hunting for a job along with many others in your field.  With software development, it sure helps to have knowledge of the business you're working in, but that's secondary.  If I decide to leave my current job because of the move, I can look into programming positions for just about any of the industries I mentioned above.  There's always some booming industry, barring another great depression.",4ggxag,t3_4ggxag,Alpha_Ts,,Comment,2,0,2
d2hh28f,2016-04-25 23:14:28-04:00,SockPuppetDinosaur,,"* Why? Money. Fun challenges. Constant improvement resulting in job security
* I'm a software developer for an environmental company. We work on problems with water around the globe.
* I like it. I'd love to have a bigger team...
* I expected to be challenged and grow professionally. I have.
* Wright State
* I think it is incredibly versatile as long as you have some other passions. CS by itself seems fairly boring to me until you have something to apply it to. This is why I have lots of small app ideas when i walk around but near-zero interest in programming to learn outside of work.",4ggxag,t3_4ggxag,Alpha_Ts,,Comment,1,0,1
d2htzj0,2016-04-26 08:49:41-04:00,thepobv,,"From Ohio here, I assume you went to wright state because it's close to home. Did you find your employment within the area? How hard was it?

I'm about to move to a big city after grad, didn't look too much within the dayton area. ",4ggxag,t1_d2hh28f,SockPuppetDinosaur,,Reply,1,0,1
d2hwagi,2016-04-26 09:56:01-04:00,SockPuppetDinosaur,,"Definitely went to WSU because cheap and close to my parents but far enough that I didn't have to visit every weekend!

I had an internship my Junior year, got another one my senior year which was converted to a full-time job. 2.5 years later I got another, higher paying job for a company I cared about and I'm still there.  I get lots of offers from companies in West Chester all the way to Columbus. Never had to work too hard to find a new job. My first internship ended abruptly (needed a higher skill full-time person) and I got the second internship within a month.",4ggxag,t1_d2htzj0,thepobv,,Reply,1,0,1
d2hysqr,2016-04-26 10:55:07-04:00,thepobv,,"Yeah I had offers from West Chester and Columbus as well but not really from the 937... I know two of my friends who are seniors at WSU and I think they might have a hard time finding stuff.

But that might just be on them.
 ___
Glad everything worked out well for yah!",4ggxag,t1_d2hwagi,SockPuppetDinosaur,,Reply,1,0,1
d2i0cym,2016-04-26 11:29:07-04:00,SockPuppetDinosaur,,"Oh both of my jobs are in the 937 luckily! The problem is that a lot of the places around here need good programmers, not people faking it. I've seen a lot of fresh college grads come in, have no idea how to program, and then be let go within 4-6 months.",4ggxag,t1_d2hysqr,thepobv,,Reply,1,0,1
d2hojj0,2016-04-26 04:03:40-04:00,BrokeDiamond,,"> Why did you decide to major in Computer Science?

I had some exposure through APCS in high school, and I was pretty good at it. I had always wanted to be an engineer but hadn't really decided on a field, and it seemed like a good choice. Also, if you're good it pretty much guarantees a great career.

> What job do you currently hold?

I'm currently a second-year undergraduate at UCLA. I've interned doing web development at my local county government and The Aerospace Corporation, and I hopefully will be working with IBM this summer in the Watson Health group. I am also currently a software engineer for UCLA's CubeSat mission.

> Do you enjoy your job?

Web development was ehh, but I enjoy computer science as a whole. I like to learn how things work that so many others take for granted, and I get to work on a lot of cool things on the CubeSat mission.

> What did you expect going in?

I was told that CS would be some of the hardest years in my career, but it would be well worth it once I graduated. So far it seems pretty true.

> Did your expectations match your experiences?

Yes.

> What college did you attend?

UCLA. Go Bruins!

> Do you believe that a major in Computer Science is versatile?

Yes, right now I can walk into nearly any project-based engineering club on campus and find a job for myself. The CubeSat team needs simulation engineers, AIAA needs flight computer developers, Racing needs embedded software engineers, IEEE will flood you with everything, and every week more people seem to be looking for web devs and app developers.",4ggxag,t3_4ggxag,Alpha_Ts,,Comment,1,0,1
d2i6g0o,2016-04-26 13:34:41-04:00,IrishFeeney92,,"**Why did you decide to major in Computer Science?**
I was always a huge fans of games and computers in general

**What job do you currently hold?**
Cyber Security Consultant / Penetration Tester


**Do you enjoy your job?**
Hugely


**What did you expect going in?**
For it to be far more manual and technical, some of the tools available are crazy good and theres always personal tweaking that can or needs to be done

**Did your expectations match your experiences?**
Definitely, I really enjoy it still
 

**What college did you attend?**
Dublin Institute of Technology, Ireland


**Do you believe that a major in Computer Science is versatile?**
110%, I did my degree in Comp Science and it provided a general area for us to explore what parts of IT we enjoyed and what we hated, while also giving us enough experiences to challenge for roles we were interested in. I liked it so much I came back to college Part Time while working a full time job to do my MSc
",4ggxag,t3_4ggxag,Alpha_Ts,,Comment,1,0,1
d2icjef,2016-04-26 15:42:35-04:00,elykl33t,,"* Sometime in like 8th grade I was learning a bit of my first programming language (never really got anywhere), but I was like ""I'm gonna be a programmer."" Turned out I was good at it, so it stuck.
* Software Engineer
* I enjoy it well enough, and it enables me to do the things I really enjoy in my off time, so it works well.
* I expected more or less exactly what I got. My previous internship experiences and time in school prepared me well, I felt. Sitting down and working away at a project for a good part of each day doesn't feel to different than the same in college, just with more meetings and more people.
* My expectations have matched my experiences for the most part, outside of some bullshit corporate office shuffling.
* Virginia Tech (Go Hokies)
* I do believe it is versatile. It provides opportunities to get involved in lots of different work within the computing field (be it coding, consulting, a technical lead role, etc.), as well as general opportunities not unlike that of a Business major, since I feel that it provides general critical thinking and problem solving skills, as well as a work ethic, that puts you in a place to succeed even if you wind up working outside computing.",4ggxag,t3_4ggxag,Alpha_Ts,,Comment,1,0,1
d2ifwhp,2016-04-26 16:53:19-04:00,None,,"I majored in computer science because even though I didn't do programming before college, I always found using computers to be very intuitive. Also, my sister majored in computer science, so I guess I wanted to emulate her slash I knew she really enjoyed it.

I develop software like most of us here. I really love it. It would take a lot of time to go into what my expectations were, but on the whole I expected to solve real-world problems with abstract thinking and I got to do just that. I feel very blessed. 

Edit: yes, a CS major is versatile. There are lots of different kinds of programming jobs out there, and I would bet good money that there will be for the next century at least. Also, some CS Majors I know what went into testing and enjoy that.",4ggxag,t3_4ggxag,Alpha_Ts,,Comment,1,0,1
d2ioexn,2016-04-26 20:16:37-04:00,MCPtz,,"1. Why did you decide to major in Computer Science? Programmed computers since I was a kid. Fascinating stuff.
2. What job do you currently hold? Embedded software engineer in robotics.
3. Do you enjoy your job? Yes, emphatically.
4. What did you expect going in? No idea, other than I was going to sit in front of a computer.
5. Did your expectations match your experiences? I didn't expect the math. Many of my classes were spent purely writing math on pieces of paper, rather than on a computer, but I learned to enjoy it. As I was completing my degree, I foresaw that I'd be a ""code monkey"" unless I specialized. I ended up getting a Master in Robotics and Control. Not sure if that is a correct assessment.
6. What college did you attend? University of California Santa Cruz
7. Do you believe that a major in Computer Science is versatile? Yes, very. Departments in our university have made concerted efforts to complete research that involved multiple disciplines.",4ggxag,t3_4ggxag,Alpha_Ts,,Comment,1,0,1
4gduxx,2016-04-25 11:38:18-04:00,yehmum,IOS Developer Intern for startup. Help?,Hi I am a CS Student that got hired on as an unpaid intern for a startup and it is my duty to aid in the development of an IOS app for the company. I have 10 weeks and I have never worked with swift and I have never been through the whole process of developing an app so I was wondering if you guys could give me some tips or give me insight into the process of developing an app in general. Thanks!,,,,,Submission,6,0,6
d2gnroa,2016-04-25 11:57:43-04:00,KronktheKronk,,Tell them to go fuck themselves and find a paid internship.,4gduxx,t3_4gduxx,yehmum,,Comment,19,0,19
d2h8y15,2016-04-25 19:56:39-04:00,Duckkg5,,This is the right answer ,4gduxx,t1_d2gnroa,KronktheKronk,,Reply,3,0,3
d2gq5d8,2016-04-25 12:48:58-04:00,qoou,,The compensation for an unpaid internship mentoring and experience. If you're not getting mentoring then you can get the same experience releasing your own product. ,4gduxx,t3_4gduxx,yehmum,,Comment,10,0,10
d2gwsoo,2016-04-25 15:12:33-04:00,rocketbunny77,,Or through a paid internship. Or a junior level job.,4gduxx,t1_d2gq5d8,qoou,,Reply,5,0,5
d2gy2w3,2016-04-25 15:39:44-04:00,qoou,,Yes. Obviously. My assumption was that OP's options are limited.,4gduxx,t1_d2gwsoo,rocketbunny77,,Reply,3,0,3
d2gwrvc,2016-04-25 15:12:04-04:00,rocketbunny77,,I feel that if they are not giving you these tips then there's not really a point to this internship.,4gduxx,t3_4gduxx,yehmum,,Comment,3,0,3
d2h9xnc,2016-04-25 20:21:50-04:00,rfinger1337,,"Also, there are laws that prevent them from allowing you to do actual work for free.  If you are working on their app, they owe you at least minimum wage.

Also, you are a software developer, you will be responsible for writing the software that ultimately allows the computers to take over the world and destroy humanity.  

That's worth more than minimum wage. Fuck them, find someone willing to pay you. They are out there.



(... not to put too fine a point on it... :-D )

",4gduxx,t3_4gduxx,yehmum,,Comment,3,0,3
d2hbhk0,2016-04-25 21:00:13-04:00,None,,[deleted],4gduxx,t3_4gduxx,yehmum,,Comment,2,0,2
d2hbpz2,2016-04-25 21:06:01-04:00,yehmum,,it was either this or a help desk internship where I would spend the summer explaining to idiots that the reason their internet isnt working is because they flipped the wifi switch on the side of their laptop off. This startup is my best friends and everyone is unpaid because there is no funding. him and his partner won a case competition at our school that provided them with a ton of resources to help their business plan take off and they needed another programmer. ,4gduxx,t1_d2hbhk0,None,,Reply,0,0,0
d2huzo3,2016-04-26 09:20:43-04:00,east_lisp_junk,,"> This startup is my best friends and everyone is unpaid because there is no funding.

Then the position you're looking for is ""cofounder,"" not ""intern.""",4gduxx,t1_d2hbpz2,yehmum,,Reply,2,0,2
d2hbrpn,2016-04-25 21:07:12-04:00,None,,[deleted],4gduxx,t1_d2hbpz2,yehmum,,Reply,1,0,1
d2hbtqq,2016-04-25 21:08:39-04:00,yehmum,,im hoping so and assuming so. If it does take off I would at least like to get some stock in the company and maybe a position in the compony as well,4gduxx,t1_d2hbrpn,None,,Reply,0,0,0
d2hg60x,2016-04-25 22:52:21-04:00,ReginaldDouchely,,"If you're going to assume instead of getting it in writing, you might learn a lot from this internship.",4gduxx,t1_d2hbtqq,yehmum,,Reply,3,0,3
d2hhq6s,2016-04-25 23:30:48-04:00,yehmum,,i know...,4gduxx,t1_d2hg60x,ReginaldDouchely,,Reply,1,0,1
d2hi3e4,2016-04-25 23:40:38-04:00,KronktheKronk,,"Don't just resign yourself to whatever they are willing to pay out if this thing is a success.

Go in to the office tomorrow and talk seriously about your compensation in the event this business becomes profitable.",4gduxx,t1_d2hhq6s,yehmum,,Reply,3,0,3
d2hjq7l,2016-04-26 00:30:17-04:00,yehmum,,there is no office. there is no anything lol. it hasnt even started yet. jesus christ not a single person contributed anything reguarding what I was asking for with this post. ,4gduxx,t1_d2hi3e4,KronktheKronk,,Reply,0,0,0
d2hk8ed,2016-04-26 00:47:24-04:00,KronktheKronk,,"There's an idea, and you're working to make it a reality, for free.

How will you feel when they make ten million dollars off this thing and you walk out with debt?",4gduxx,t1_d2hjq7l,yehmum,,Reply,2,0,2
d2j34bc,2016-04-27 04:07:25-04:00,yehmum,,Should I ask for stock in the company or something?,4gduxx,t1_d2hk8ed,KronktheKronk,,Reply,1,0,1
d2o9utw,2016-05-01 05:44:44-04:00,ImmaculateDissection,,"First thing to do is choose a good modeling language and write and draw every possible interaction from the User with the app. You can do this with UML Use Case Diagrams for example. Find all possible functions/problems a User of the app will experience. Form there you should define clear goals and prioritize them. this is the Basis before you Start coding. From These models you can think about implementing these functions but before you do try to pick some well suited frameworks or patterns to work with. Try to think about the specific problems of programming an app. It is iOS so keep in mind screen sizes,  tablets,  iOS versions etc.... Sorry for the long post :>",4gduxx,t3_4gduxx,yehmum,,Comment,1,0,1
d2scihy,2016-05-04 08:50:40-04:00,yehmum,,no this is the kind of stuff I want to hear! Thank you!,4gduxx,t1_d2o9utw,ImmaculateDissection,,Reply,2,0,2
d2ud63k,2016-05-05 17:45:29-04:00,ImmaculateDissection,,"you're welcome. I have no experience with swift but the development process can and should be divided into small parts and tasks - workflows. If it is swift or C# or JS, thats just a small Part of the whole process.  Feel free to ask anything.  There is a lot to keep in mind when programming in a Business environment",4gduxx,t1_d2scihy,yehmum,,Reply,1,0,1
d2r4yvc,2016-05-03 11:57:22-04:00,Orange__Tang,,"My suggestion would be to sit down and define the goals of the project. From there start with use cases etc. Use the brain storming information to come up with a design hierarchy (how will the classes work, what do they need to do. Its a lot easier to implement stuff with a clear goal. My next suggestion would be to start working, top down, and implement what you can. For the things you don't know how to code, use google and stack overflow. I don't happen to know swift either, but this sort of process at least makes it more manageable. Also, consider a process model, for workflow purposes (scrum is what I used). Im sorry I don't have specific swift stuff, but hopefully this information will be useful.",4gduxx,t3_4gduxx,yehmum,,Comment,1,0,1
d2scmx7,2016-05-04 08:54:50-04:00,yehmum,,No this is all good stuff thank you!,4gduxx,t1_d2r4yvc,Orange__Tang,,Reply,1,0,1
d3dlnzl,2016-05-20 20:14:31-04:00,Spearmint66,,"Drop me a message if you have any specific questions, I have ~2 year experience working on iOS and Swift!

Main recommendations (over and above the fact you really should being paid):

- Join a Meetup group
- Get started by learning enough to do a personal project
- Find tutorials online (Ray Wenderlich, Code with Chris, Brian Advent ...)
- Check out the new Standford iOS 9 course, it's all in swift an has a more CS style approach than other 'learn from scratch' methods

Matt",4gduxx,t3_4gduxx,yehmum,,Comment,1,0,1
d2hbe52,2016-04-25 20:57:54-04:00,yehmum,,well this thread is a bomb,4gduxx,t3_4gduxx,yehmum,,Comment,1,0,1
d2i05a1,2016-04-26 11:24:29-04:00,rfinger1337,,"
Yeah, well you significantly misrepresented the situation too, didn't you?",4gduxx,t1_d2hbe52,yehmum,,Reply,2,0,2
d2schk6,2016-05-04 08:49:45-04:00,yehmum,,I did :p,4gduxx,t1_d2i05a1,rfinger1337,,Reply,1,0,1
4gde48,2016-04-25 09:51:10-04:00,dhumidifier,Question about AI,"Why is a heuristic that is closer to the true cost better than one that is not? In other words, if I have two consistent heuristics why will the larger one be more optimal?",,,,,Submission,3,0,3
d2h8pmb,2016-04-25 19:50:36-04:00,algorithmsWAttitude,,"I am assuming that by ""larger"", you mean in the context of a heuristic giving an estimate that is guaranteed to not be larger than the actual value.  Larger means more accurate:  if the true answer is 100, knowing the answer is at least 90 gives you more information than knowing it is at least 9.  If the algorithm makes use of the estimate, you would expect a more accurate estimate to be more useful.  An inaccurate estimate (the destination is at least 0 miles from me) is the same as no estimate at all. ",4gde48,t3_4gde48,dhumidifier,,Comment,1,0,1
d2hnjtx,2016-04-26 03:07:38-04:00,_ActionBastard_,,Reduces the size of the search space.  Which is good.,4gde48,t1_d2h8pmb,algorithmsWAttitude,,Reply,1,0,1
4gam4d,2016-04-24 19:25:22-04:00,alak77,Homework help - ANSI C,"9.1 Determine the appropriate bit settings for TCCR1A, TCCR1B, OCR1A, and TIMSK1 using the following 
specific details:
use NORMAL MODE disconnect both OC1 pins,
use a pre-scale division of AN APPROPRIATE VALUE
use the TIMER OVERFLOW FLAG interrupt,
use a TIMER COUNT INITIAL value such that you receive 1 interrupt per second.

9.2 Create a function that initializes timer 1 based on the values determined in problem 9.1.

9.3 Create an ISR for timer 1. (Look ahead at 9.7)

9.4 Determine the appropriate bit settings for PCICR and PCMSK2 using the following specific details:
use Port D4 as an input pin,
disable the internal pull-up resistor,
using Port D4 corresponds to Pin Change interrupt 20.

9.5 Create a function that initializes the Pin Change interrupt based on the values determined in problem 9.4.

9.6 Create an ISR for Pin Change interrupt 20. (Look ahead at 9.7)

9.7 Create a program that utilizes the timer interrupt and the pin change interrupt in order to measure the frequency 
of a periodic signal on Port Pin D4. You can do this by saving and then clearing a counter each time the timer 
interrupt occurs (at a rate of once per second). Additionally, you can increment the counter by the number of 
low-to-high and high-to-low transitions on pin D4 with the pin change interrupt. Your main loop should monitor 
when the timer interrupt goes off, and then write the frequency to the serial port for the user to see. Note: your setup
function should call your two initialization functions, and then enable global interrupts in the SREG register.",,,,,Submission,0,0,0
d2fxmt6,2016-04-24 20:02:36-04:00,thegreatunclean,,"You have to sit down with the datasheet for whatever microcontroller you're targeting and figure out what are the appropriate settings for the named registers.

Once you get that you can get help over at /r/learnprogramming.  Not really a CS question.",4gam4d,t3_4gam4d,alak77,,Comment,2,0,2
d2i4fgf,2016-04-26 12:53:50-04:00,gbtimmon,,">Homework help

Oh look a neophyte and fellow journeyman needs help. I shall gladly help another along the road to this field I so very love Perhaps he or she too will be inspired and find the love and passion I have for a wonderful and creative field. 

>copy and pastes whole assignment, no evidence of effort.

Fuck you.",4gam4d,t1_d2fxmt6,thegreatunclean,,Reply,1,0,1
4ga899,2016-04-24 17:54:04-04:00,TobyHJ,Question about Parity Bits,"Very new to Computer Science as I read a presentation on Data Transfer. I understand the purpose of Parity checking etc but what what unclear to me was when to add a 0 or a 1 to the code- are we free to choose or are there specific conditions for adding a 0 or a 1? The lecture slides were pretty confusing to me.

Thanks.",,,,,Submission,5,0,5
d2g53a0,2016-04-24 23:32:49-04:00,ldpreload,,"A little more background: The idea behind parity is that the recipient checks each byte to make sure that it fits whatever the parity rule is, and discards it if it's bad. Usually a higher-level protocol will then detect the missing byte, and request that part of the data be retransmitted. Parity is generally only used on protocols where entire bytes could go missing anyway&mdash:e.g., on a phone line, someone could pick up the phone and disrupt all data traffic briefly&mdash:so any higher-level protocol already needs to deal with missing bytes. Parity checks convert corrupt bytes into missing bytes.

The rule for the recipient is easy: make sure each byte has an even number of 1 bits, if using even parity (or an odd number of 1 bits, if using odd parity). So the sender sends seven data bits, containing actual data, and one parity bit, which is only used to make parity come out right. If the receiver sees that the parity is indeed correct, then the lower seven bits are valid. If not, discard this byte.

Protocols from the post-modem era tend not to use parity, but instead checksums on an entire packet of data. The idea is similar to parity, though: you have some checksum bits in the header, which are set to whatever value is needed so the checksum of the entire packet, including the checksum bits, comes out to zero. If it doesn't, discard the whole packet and re-request it. These days we expect much lower error rates, so it's fine to discard a whole packet, and we also don't expect entire bytes to go missing. We also have the same protocol that does the checksum be responsible for requesting re-delivery.

Fancier protocols use error-correction codes, which allow tiny errors (e.g., one bit flipped) to be not only detected but also corrected, by knowing in _which_ bit was the error. The math is much more involved but pretty cool.",4ga899,t3_4ga899,TobyHJ,,Comment,3,0,3
d2ge8b1,2016-04-25 07:10:22-04:00,ericGraves,,"I would disagree with that notion of lower error rates. Sure for hardwired connections that is possible, but for wireless connections the error rates are insane. A lot of protocols for wireless communication use extremely sophisticated error correction codes because a fraction of the values sent have been corrupted.

But the really cool thing? [Some of these modern error correction codes are literally just a lot of parity checks on subsections of the string.](https://en.wikipedia.org/wiki/Low-density_parity-check_code) And they can correct a fraction of the packet length. In fact, for every n bits of information n/(1-H(r)) bits of parity can protect against n*r errors, where H(r) is the binary entropy (assuming r < 1/2). As long as the errors are randomly distributed throughout the n( 1 + 1/(1- H(r) ) ) bits. ",4ga899,t1_d2g53a0,ldpreload,,Reply,1,0,1
d2ft4ps,2016-04-24 17:55:28-04:00,TobyHJ,,"Am I correct in thinking that we add a 0 when there are an even number of 1's and a 1 when there are an odd amount of 0's? If so, there is a mistake in the presentation I have been learning from.",4ga899,t3_4ga899,TobyHJ,,Comment,1,0,1
d2fu5pu,2016-04-24 18:22:55-04:00,tjwarren,,"When using even parity, you want to end up with an even number of 1s. When using odd parity, you want to end up with an odd number of 1s. The parity bit should be set accordingly to give you your odd or even number.

For example, `101` has two 1s. For even parity, I would add a 0 (so the value overall has two 1s). For odd parity, I would add a 1 (so the value overall has three 1s).

The choice between even and odd parity is essentially up to the designer of the system.",4ga899,t1_d2ft4ps,TobyHJ,,Reply,3,0,3
d2fx1zy,2016-04-24 19:45:54-04:00,TobyHJ,,"Great response, thank you.",4ga899,t1_d2fu5pu,tjwarren,,Reply,1,0,1
4g7x63,2016-04-24 08:11:09-04:00,NeedHelp4Bachelor,Need advice on which Bachelor program I should chooose,"So I'm currently deciding between two different courses:
Knowledge Engineering in the University of Maastricht and Software & Information Engineering in the University of Technology in Vienna.

To compare here is an overview of lectures:
http://www.maastrichtuniversity.nl/web/Schools/DKE/TargetGroup/ProspectiveStudents/BachelorsKnowledgeEngineering/KnowledgeEngineering/CourseDescriptions.htm

https://tiss.tuwien.ac.at/curriculum/public/curriculumSemester.xhtml?le=false&windowId=eff&key=46100&semester=YEAR

Now I know they are quite different. However I'm mostly concerned which one of them can be regarded as more competitive rather than which suits my interests more, since I can see myself enjoying both of this courses almost equally.",,,,,Submission,6,0,6
d2fdyr5,2016-04-24 11:03:22-04:00,Nattfrosten,,Lol germam,4g7x63,t3_4g7x63,NeedHelp4Bachelor,,Comment,2,0,2
d2ff736,2016-04-24 11:40:51-04:00,NeedHelp4Bachelor,,The link/course descriptions are in English. ,4g7x63,t1_d2fdyr5,Nattfrosten,,Reply,1,0,1
d2ffh0v,2016-04-24 11:48:53-04:00,Nattfrosten,,"I checked it out, I'd say that nr 2 is better in current market, but maybe nr 1 is better when you graduate",4g7x63,t1_d2ff736,NeedHelp4Bachelor,,Reply,2,0,2
d2ffl8h,2016-04-24 11:52:17-04:00,NeedHelp4Bachelor,,Thanks a bunch! Can you maybe give me some reasoning? My guess is that option 2 gives me a broader knowledge in the field of CS/Software Engineering while option 1 helps me to grow into a better problem solver especially considering Maastricht's approach in Project centred learning. I also find it really valuable how math heavy option 1 is as I think it's easier to pick up theory after I graduate than train the logical part of my brain. Please tell me if I have the wrong idea ,4g7x63,t1_d2ffh0v,Nattfrosten,,Reply,1,0,1
d2fiet9,2016-04-24 13:11:45-04:00,Nattfrosten,,"The second seemed more focused on practical things, but the first seemed to have better foundations for machine learning, which may grow in the following years.",4g7x63,t1_d2ffl8h,NeedHelp4Bachelor,,Reply,1,0,1
d2fk09f,2016-04-24 13:54:12-04:00,NeedHelp4Bachelor,,Would you suggest if I choose to become a Software Engineer option 1 is enough if I pick up additional knowledge/skills outside my University courses? ,4g7x63,t1_d2fiet9,Nattfrosten,,Reply,1,0,1
d2g6169,2016-04-25 00:03:01-04:00,Nattfrosten,,"You will have to train at home no matter what, and it's enough to just train at home (66% of devs are self taught) 

If I were in your seat I'd take nr 1, since ml is cool. ",4g7x63,t1_d2fk09f,NeedHelp4Bachelor,,Reply,1,0,1
4g2j3v,2016-04-23 03:10:05-04:00,planetkhaan,Want to take Design of Programming Languages but professor doesn't use a textbook.,"Next fall my school is offering *CMSC 305 DESIGN OF PROGRAMMING LANGUAGES*. I want to take it but want to start studying in the summer because those who have taken it told me it was extremely difficult. Also, the professor doesn't use a textbook so students have to rely on lecture notes and office hours basically. I'm a particularly bad lecture learner and usually won't do well in a class without a textbook. Do any of you have any recommendations for something I can get started with? Here's the course description:

This course will cover a selection of issues important to the design of programming languages including, but not limited to, type systems, procedure activation, parameter passing, data encapsulation, dynamic memory allocation, and concurrency.  In addition, the functional, logic, and object-oriented programming paradigms will be presented as well as a brief history of high-level programming languages. Students will be expected to complete a major programming project in Standard ML of New Jersey as well as other programming assignments in Java or Prolog. Prerequisite:  CMSC 201 (Data Structures).",,,,,Submission,6,0,6
d2e929z,2016-04-23 09:52:16-04:00,albasri,,Get the class notes from the people who have already taken it,4g2j3v,t3_4g2j3v,planetkhaan,,Comment,6,0,6
d2eafn9,2016-04-23 10:40:56-04:00,jzarob,,Would recommend Structures and Interpretation of Computer Programs. It's the Gold standard for programming language text books,4g2j3v,t3_4g2j3v,planetkhaan,,Comment,5,0,5
d2eak2b,2016-04-23 10:44:58-04:00,east_lisp_junk,,"[Essentials of Programming Languages](http://www.eopl3.com) and [Programming Languages: Application and Interpretation](https://cs.brown.edu/~sk/Publications/Books/ProgLangs/2007-04-26/) are both good for getting some basic exposure to a range of topics/ideas in PL. [Types and Programming Languages](https://www.cis.upenn.edu/~bcpierce/tapl/) is the canonical introduction to type systems in particular, though it covers a narrower portion of the language design space.",4g2j3v,t3_4g2j3v,planetkhaan,,Comment,2,0,2
d2ekzkx,2016-04-23 15:49:57-04:00,the_omega99,,"[Book recommendations related to the topic are here](https://www.reddit.com/r/compsci/comments/2q3rhg/good_introductory_books_for_programming_language/). Sure, your class doesn't use a book, but the topics will generally overlap well and it'll be useful to have another explanation of things.

And of course, you can take a load off your feet by learning SML before the class ends up needing it, if you don't already know it. Same for Prolog. I don't know much about SML, so can't say anything there, but Prolog is so radically different from likely any other language you've ever used. If you've never used it before, you'll really benefit from trying to learn it first. It's not really a complex or ""big"" language. Simply it's a very different paradigm that requires some mental gymnastics to get used to (I had a class that basically created a rudimentary Prolog parser in Haskell -- that was interesting).

If you haven't already, knowing a lot about the usage of functional programming is sure to be nifty. That's where I discovered how interesting language theory is. Pretty much the only thing I know about SML is that it's functional, so learning that would be a good starting point (my introduction was with Haskell, hence the earlier mention of an assignment).

You should probably know what the listed ideas are. At least have a general impression on that. This is knowledge that is quite applicable even outside the class, anyway. eg, knowing the basics of how type systems work would help you understand various compiler errors. It'll also help you in griping online about how bad Java's generics are. That's, like, a rite of passage.",4g2j3v,t3_4g2j3v,planetkhaan,,Comment,2,0,2
4g1a1s,2016-04-22 20:22:37-04:00,Erica993,Does anyone want to talk regularly over the internet about Computer Science topics?,"I'm 22 years old. I'm still a student in my final year of undergraduate studies. I don't have many people to talk to about topics related to computer science. Looking for anyone who feels the same or wants to socialize a bit over the internet, I would be very grateful.

We can talk about multiple topics and life in general.",,,,,Submission,15,0,15
d2dt0zn,2016-04-22 21:19:56-04:00,Adnotamentum,,I wish there was an irc for general computer science alongside the countless channels for programming.,4g1a1s,t3_4g1a1s,Erica993,,Comment,8,0,8
d2dy2gm,2016-04-22 23:56:19-04:00,amazing_username,,"I feel like a shill,  but, how about we start a Slack? We could share interesting discoveries, code, and whatnot there. ",4g1a1s,t1_d2dt0zn,Adnotamentum,,Reply,6,0,6
d2e69v4,2016-04-23 07:39:10-04:00,monster_bait,,I would like to get involved with that too :),4g1a1s,t1_d2dy2gm,amazing_username,,Reply,2,0,2
d2e8t6f,2016-04-23 09:42:33-04:00,EgoistHedonist,,"I was going to suggest slack as soon as I saw the title. It's a great idea!
Seems like compsci and computerscience are taken (not very surprising). Any ideas for a good name?",4g1a1s,t1_d2dy2gm,amazing_username,,Reply,2,0,2
d2ea9si,2016-04-23 10:35:37-04:00,amazing_username,,"ItsMoreFunToCompute

In case you enjoy Kraftwerk",4g1a1s,t1_d2e8t6f,EgoistHedonist,,Reply,2,0,2
d2drjzu,2016-04-22 20:36:15-04:00,alreadyheard,,What do you wanna talk about? I'm in a similar situation where I am pretty shy in my classes and don't really know any friends that wanna talk about this stuff. ,4g1a1s,t3_4g1a1s,Erica993,,Comment,3,0,3
d2dxten,2016-04-22 23:47:44-04:00,_--__,,"Feel free to come over to /r/compsci and raise some topics for discussion - we could certainly do with more commentary.  Even if you want a slightly less formal arrangement, we have a weekly ""Anything goes"" thread which usually gets a few responses.",4g1a1s,t3_4g1a1s,Erica993,,Comment,3,0,3
d2dry8w,2016-04-22 20:47:49-04:00,An_Ugly_Pigeon,,Sure. Send me a message if you want.,4g1a1s,t3_4g1a1s,Erica993,,Comment,1,0,1
d2duj1h,2016-04-22 22:03:41-04:00,MachNineR,,"I've been thinking a bit about this lately, I'll give it a try.
The discussion part that is, I think about CS all the time.",4g1a1s,t3_4g1a1s,Erica993,,Comment,1,0,1
d2dycyc,2016-04-23 00:06:32-04:00,slimjim_belushi,,I would. I'm just finishing my freshman year of college though. Can we all start an irc channel or something?,4g1a1s,t3_4g1a1s,Erica993,,Comment,1,0,1
d2e1g55,2016-04-23 02:16:23-04:00,EnterprisePaulaBeans,,\##compsci on Freenode?,4g1a1s,t1_d2dycyc,slimjim_belushi,,Reply,1,0,1
d2e1qha,2016-04-23 02:31:38-04:00,slimjim_belushi,,Just joined. No one's there.,4g1a1s,t1_d2e1g55,EnterprisePaulaBeans,,Reply,1,0,1
d2e3hc8,2016-04-23 04:20:52-04:00,DarkX9109,,I did the same.,4g1a1s,t1_d2e1qha,slimjim_belushi,,Reply,1,0,1
4fxu9m,2016-04-22 06:17:09-04:00,iCHAIT,What are some awesome programming and computer science subreddits that a computer science student should be following?,,,,,,Submission,24,0,24
d2cwu4j,2016-04-22 07:11:57-04:00,None,,"What I did was browse [https://www.reddit.com/subreddits/](https://www.reddit.com/subreddits/) and put anything programming related into a multireddit. Now when I want only computer related stuff I just browse that. After you get the main ones you can use the search to browse specific subjects. Just pick technologies you are interested in. 

/r/AskComputerScience
/r/AskProgramming
/r/coding
/r/compsci
/r/ConTalks
/r/cscareerquestions
/r/html5
/r/java
/r/javascript
/r/laravel
/r/learnprogramming
/r/mysql
/r/PHP
/r/PHPhelp
/r/programming
/r/reviewmycode
/r/SEO
/r/web_design
/r/webdev
/r/webhosting

",4fxu9m,t3_4fxu9m,iCHAIT,,Comment,6,0,6
d2d2hu9,2016-04-22 10:21:51-04:00,luisbg,,</thread>,4fxu9m,t1_d2cwu4j,None,,Reply,10,0,10
d2dp298,2016-04-22 19:24:02-04:00,BrokeDiamond,,No love for /r/python?,4fxu9m,t1_d2cwu4j,None,,Reply,2,0,2
d2e1h1p,2016-04-23 02:17:44-04:00,None,,I've never used it. There are subs for most languages and even technologies like node or git. You can customize a multireddit to whatever interests you. If I ever learn python then I'll drop it in there but for now it wouldn't help me.,4fxu9m,t1_d2dp298,BrokeDiamond,,Reply,1,0,1
d2dgjvg,2016-04-22 15:42:00-04:00,anamorphism,,"here are some i enjoy that /u/PrivilegedGlimpse didn't post.

/r/badcode - sometimes humorous, often times informative about what people consider to be bad practice.

/r/tinycode - check out some often impossible to read code snippets and see if you can decipher what's going on.

/r/dailyprogrammer - useful if you want to practice and don't have any ideas.

/r/ProgrammerHumor - terrible programming jokes and such.

and of course there's a sub for every language you're interested in.",4fxu9m,t3_4fxu9m,iCHAIT,,Comment,3,0,3
d2dcfc6,2016-04-22 14:07:29-04:00,cogman10,,"Here is what I have

https://www.reddit.com/user/cogman10/m/compsci",4fxu9m,t3_4fxu9m,iCHAIT,,Comment,2,0,2
d2doop9,2016-04-22 19:12:59-04:00,killerhawk25,,Not curated by me but this is a Computer Science multireddit I browse: https://www.reddit.com/user/Nezteb/m/computer_science,4fxu9m,t3_4fxu9m,iCHAIT,,Comment,2,0,2
d2dprzk,2016-04-22 19:44:49-04:00,IAmNotMyName,,Top post missed r/haskell and r/scala ,4fxu9m,t3_4fxu9m,iCHAIT,,Comment,2,0,2
d2e0ktj,2016-04-23 01:33:37-04:00,okmkz,,And /r/prorammingcirclejerk,4fxu9m,t1_d2dprzk,IAmNotMyName,,Reply,0,0,0
d2dd3n8,2016-04-22 14:22:41-04:00,tanenbaum,,Completely depends on your goals. What are your goals?,4fxu9m,t3_4fxu9m,iCHAIT,,Comment,1,0,1
d2ddwpg,2016-04-22 14:41:01-04:00,iCHAIT,,"My aim is to be updated on the advancement on at least the languages I am interested in, learn about what's new and interesting happening in the field of computer science. And like any other student it would be great to have some tips on how to prepare about technical interviews.",4fxu9m,t1_d2dd3n8,tanenbaum,,Reply,1,0,1
4fwcki,2016-04-21 21:51:16-04:00,Pandapoppin,Where to find information on Hyper - resolution and Demodulation in Automated Reasoning,"Hi guys, 

I have one last paper to write for this semseter, but I am having trouble finding information on a certain topic. I was hoping you guys could point me in the right direction for internet resources, or even e-books that I could purchase within a few days to do some reading and get a genral understanding of these issues. It is a short paper, so I don't need deep knowledge, just some generalization.

Here is what I have found so far that seems to be concrete. 

https://www.youtube.com/watch?v=VtSegM8tYlo

Thank you for your help. ",,,,,Submission,2,0,2
4fw8ls,2016-04-21 21:23:10-04:00,AverageOyster,"What ""shape"" is a 5-variable Karnaugh Map?","[Some background](https://en.wikipedia.org/wiki/Karnaugh_map#Karnaugh_map)

So a 3-variable map can really be thought of as printed on a cylinder and a 4-variable map can really be though of as printed on a torus, then what is a 5-variable map printed on?

You'll often see pictures like [this](https://i.ytimg.com/vi/1_cxX2pxf10/hqdefault.jpg) to show how one sub-map connects to the other, but each sub-map also behaves like a 4-variable map on a torus so this image isn't the full picture.",,,,,Submission,2,0,2
d2ci7ra,2016-04-21 21:27:37-04:00,AverageOyster,,"My thought is that since in the 3 and 4-variable cases the flat, 2D representations have to be folded into three dimensions to reveal their structure, then in the 5-variable case the ""flat"", 3D representation has to be folded into four dimensions to reveal its true structure.",4fw8ls,t3_4fw8ls,AverageOyster,,Comment,1,0,1
d2j3gua,2016-04-27 04:29:06-04:00,DubioserKerl,,"You would need to use a Hypertorus (https://en.wikipedia.org/wiki/Torus#n-dimensional_torus). But afaik you cannot display a Hypertorus in 2D without any major distortion. Thats why you stop using KV-diagrams with more than 4 variables and utilize Quine-McCluskey (https://en.wikipedia.org/wiki/Quine%E2%80%93McCluskey_algorithm) instead.
",4fw8ls,t3_4fw8ls,AverageOyster,,Comment,1,0,1
4fuk77,2016-04-21 15:07:02-04:00,IronZygote,"How do we generate a public key and a private key (as separate files), and use it to either encrypt or sign documents?",,,,,,Submission,16,0,16
d2c4uh2,2016-04-21 15:59:46-04:00,Steve132,,"The toolchain?  We use OpenSSL mostly.

The name of the format?  We use PEM format mostly, using RSA.

The algorithm?  Diffie-Helman Key Exchange algorithm mostly.

How does it work?  Watch this video https://www.youtube.com/watch?v=3QnD2c4Xovk",4fuk77,t3_4fuk77,IronZygote,,Comment,21,0,21
d2c5jo0,2016-04-21 16:14:51-04:00,IronZygote,,Thanks,4fuk77,t1_d2c4uh2,Steve132,,Reply,2,0,2
d2c6ivp,2016-04-21 16:35:49-04:00,besirk,,Simple and clear answer. You got my upvote.,4fuk77,t1_d2c4uh2,Steve132,,Reply,2,0,2
d2cl67h,2016-04-21 22:41:41-04:00,Sir_Qqqwxs,,"Super interesting video, thanks.",4fuk77,t1_d2c4uh2,Steve132,,Reply,1,0,1
d2cuai0,2016-04-22 04:35:59-04:00,IronZygote,,Can you tell how to do it using software? I was hoping someone would answer that,4fuk77,t1_d2c4uh2,Steve132,,Reply,1,0,1
d2id09a,2016-04-26 15:52:29-04:00,Steve132,,https://rietta.com/blog/2012/01/27/openssl-generating-rsa-key-from-command/,4fuk77,t1_d2cuai0,IronZygote,,Reply,2,0,2
4ftzhm,2016-04-21 13:12:42-04:00,sixtine,What part of mathematics (starting from algebra...) should I absolutely know in order to learn and master algorithms?,,,,,,Submission,6,0,6
d2by832,2016-04-21 13:37:59-04:00,Twerk4Werk,,"At least for my algorithm and data structure class, things weren't really math intensive. It was more conceptual. If I had to pick, I'd say discrete math helps most with algorithms, though. ",4ftzhm,t3_4ftzhm,sixtine,,Comment,6,0,6
d2c05uk,2016-04-21 14:19:01-04:00,SpaceButtrfly,,"Yeah, Discrete for sure. Maybe Linear Algebra?",4ftzhm,t1_d2by832,Twerk4Werk,,Reply,1,0,1
d2mly0y,2016-04-29 17:36:23-04:00,TheBeardofGilgamesh,,"Probability, and number theory. But the only reason for that is so you can pass coding interview questions. ",4ftzhm,t3_4ftzhm,sixtine,,Comment,2,0,2
4fpq9d,2016-04-20 17:42:06-04:00,pikapikaapikachu,Noob question about development,"Is Java considered a development language/skill??
Also what are some other examples of development skills? I know .NET is one. What are some others? I'd like to look into them. I'm just starting. Thank you!! :)
-possibly future female in STEM",,,,,Submission,2,0,2
d2axhhd,2016-04-20 18:02:51-04:00,CastigatRidendoMores,,"Yes, Java is a development language, and being able to program in it would be a development skill. .NET is a bit more complex, but think of it as ""programming using Microsoft languages"". If you're in C# or Visual Basic, that's .NET.

There are lots of languages and skills. Javascript, SQL, and CSS would be several other examples.

If you'd like a more thorough list of skills and technologies used in programming, Google has put out a [good list](https://www.google.com/about/careers/students/guide-to-technical-development.html). It might look overwhelming, but keep in mind that many or most developers don't have all the skills on this list.",4fpq9d,t3_4fpq9d,pikapikaapikachu,,Comment,4,0,4
d2bgi0y,2016-04-21 04:00:36-04:00,watsreddit,,"Java is a big one for sure. That said, which language you learn isn't really a big deal imo. Focusing on learning core CS concepts (data structures, algorithms, etc.), in any language, is what really matters, especially when it comes to actually successfully interviewing for jobs. Once you are experienced in CS, picking up a new language is actually pretty easy. Much easier than picking up a new spoken language. 

If you are/are going to be a CS major, you very well might find yourself needing to use completely foreign languages in your upper level classes, which you'll be able to pick up without too much trouble, at least compared to first starting out with a language. Last semester I did just that actually, did all homework assignments in C++ despite never having used it before.",4fpq9d,t3_4fpq9d,pikapikaapikachu,,Comment,3,0,3
d2axouw,2016-04-20 18:07:52-04:00,self_raising,,"Java is an object-oriented language (and more. Read up on the JVM - it's what makes Java so special).

C#/.net, is also an object-oriented language.

Other OO examples include c++, smalltalk, Delphi and many more.",4fpq9d,t3_4fpq9d,pikapikaapikachu,,Comment,2,0,2
d2az9mr,2016-04-20 18:47:46-04:00,xxkid123,,"Java is a common intro language in university level CS. Another one is Python. Both are easy and fast to get started with. 

I highly recommend either if you're just interested in getting started. Also, don't get easy mixed up with useless, both are very powerful and many programs are written in them. All Android apps use java, and new Python programs pop up all the time. 

Keep in mind Java is different from JavaScript, both are programming languages, but they're completely different. JavaScript is used primarily on websites and servers only.",4fpq9d,t3_4fpq9d,pikapikaapikachu,,Comment,2,0,2
4fkf2a,2016-04-19 19:24:39-04:00,rrrmr,Why is typed lambda calculus with polymorphism called second-order?,"In the [lambda cube](https://en.wikipedia.org/wiki/Lambda_cube), the axis λ2 is for ""polymorphism"" or ""second-order"" typed lambda calculus.

Why isn't that name used for the axis λw, typed lambda calculus with type operators? Don't you need type operators to get 'higher-kinded' types?

Also, there seems to be a level of difference between the above wiki article, and this page http://www.rbjones.com/rbjpub/logic/cl/tlc001.htm

Where the wiki has 'terms', the second link has 'types' and where wiki has 'types', the second link has 'kinds'.",,,,,Submission,13,0,13
d2a4af7,2016-04-20 05:48:25-04:00,rrrmr,,"Actually the wiki article on [dependent types](https://en.wikipedia.org/wiki/Dependent_type#Systems_of_the_lambda_cube) has some more info on the lambda cube. I found looking at the dependent type picture clarified things (and now it makes sense that at the object level we are dealing with terms and types, and at the type level we are dealing with types and kinds):

[:f : \Pi_{(x:A)} B:] -- simply typed

[:f : \Pi_{(A:Type)} C:] -- polymorphic

[:f : \Pi_{(A:Type)} Type:] -- type operator

[:f : \Pi_{(x:A)} B(x):] -- dependent product type

I think the dependent type x:A |- B(x) can also be defined as a polymorphic type:

[: B : A \rightarrow Type :]

or as a Pi type:

[: B : \Pi_{(X:Type)} X \rightarrow A :]

which I think is equivalent to:

[: B : \Pi_{(X:Type)} \Pi_{(x:X)} A :]",4fkf2a,t3_4fkf2a,rrrmr,,Comment,1,0,1
d2abki4,2016-04-20 10:21:17-04:00,umib0zu,,I think the best human readable explanation comes from the [haskell wikibook.](https://en.wikibooks.org/wiki/Haskell/Polymorphism).,4fkf2a,t3_4fkf2a,rrrmr,,Comment,1,0,1
4fk99v,2016-04-19 18:50:09-04:00,AtheistMessiah,"Why don't standard character-sets such, as UTF-8 or ANSI have an official character reserved to be a delimiter for ETL's?",,,,,,Submission,1,0,1
d29j128,2016-04-19 18:54:24-04:00,TheSageMage,,"Can you elaborate? ASCII has cell, row, and page delimiters that are meant for what I think you're getting at...",4fk99v,t3_4fk99v,AtheistMessiah,,Comment,4,0,4
d3lbe27,2016-05-26 23:03:47-04:00,AtheistMessiah,,"Sorry for the super late response.  If ASCII has the ability to indicate delimiter characters from one cell to the next, then why are commas so prevalently used as cell delimiters when they are so common in standard text. ",4fk99v,t1_d29j128,TheSageMage,,Reply,1,0,1
d3ldeln,2016-05-27 00:03:39-04:00,TheSageMage,,"I don't know the real history, but if I had to guess, it's most likely because people know what a comma looks like, but most won't see or know what the separator looks like. Most text editors would most likely render them as spaces or blank characters.",4fk99v,t1_d3lbe27,AtheistMessiah,,Reply,1,0,1
4fjdor,2016-04-19 15:45:05-04:00,P0tatoFTW,Quick question on algorithmic complexity,"If I have an algorithm which has a cubic complexity, if I multiply the number of steps by 5, do I multiply the time taken by 5 cubed by 125 as it is cubic?",,,,,Submission,3,0,3
d29dkvp,2016-04-19 16:48:48-04:00,_--__,,"No.  The time complexity is measured by the **input size**, that is, it is a function from the ""input size"" to the ""time taken"".  Note that ""time taken"" and ""number of steps"" are treated as the same thing.  However, for your cubic algorithm, if you multiply the *input size by 5*, then you would multiply the time taken (or number of steps) by 125.  ",4fjdor,t3_4fjdor,P0tatoFTW,,Comment,5,0,5
4fb49j,2016-04-18 05:53:29-04:00,Drunken_Consent,What is the reference point of a R-Tree?,"Okay, so recently I began to make an app that would require location-based matching.  Someone recommended that I looked into R-Trees and sure enough it seems to fit the bill.  I found an implementation that I can use in JavaScript of an R-Tree, and it accepts a Rectangle of 4 size dimensions.

When I am populating this tree, with what reference point does the R-Tree utilize?  I will have a Person's location in longitude / latitude.  They will then search for lets say within a 5-mile range.  

So their bounding rectangle box would simply be 5-miles above / below their latitude / longitude?  

Or would the first person submitted to the Tree become a 'root' from which everything else must be relative to them?

Another thing I was trying to wrap my head around is this scenario:

Person A and Person B are 6 miles apart.  

Person A is currently searching 5 miles around him.

Person B is currently searching 10 miles around him.

If my understanding is correct, the R-Tree would tell me A-B is a valid pair as there is an overlap, but really A shouldn't match with B until A's search is expanded to 10.  If that makes sense.  

Please help with any flaws in my understanding, trying to wrap my head around the concept well before implementing the code of my project haha
",,,,,Submission,3,0,3
d27cinv,2016-04-18 06:11:11-04:00,Drunken_Consent,,"After typing this out, and drawing the world's shittiest 3D graph, I think I may have figured it out... 

Assuming I can convert X miles to a certain degrees Long / Lat, then the Rectangle would just be:  Long + X, Long - X, Lat + X, Lat - X and then it would be the correct bounding box?  I would still like confirmation, but I think just thinking about it in my head, I assumed a longitude of 70 would 'overlap' anything lower than it, but kinda forgot about the 3D aspect.

Any confirmation or other insight would be helpful, thanks :D",4fb49j,t3_4fb49j,Drunken_Consent,,Comment,2,0,2
d294tgx,2016-04-19 13:43:19-04:00,Noctune,,You will need to compensate for the distortion you get from the [equirectangular projection](https://en.wikipedia.org/wiki/Equirectangular_projection) of lat/lon. Basically you need a wider search area the farther you get from the equator.,4fb49j,t1_d27cinv,Drunken_Consent,,Reply,1,0,1
4fasju,2016-04-18 03:47:24-04:00,soulslicer0,Searching/Cutting a bidirectional graph to find Boxes?,"http://imgur.com/AJr48Xf

I have the following bidirectional graph as seen on the left. I wish to analyze it to find boxes, meaning they have a cyclic/closed quad/90 degree shape. There are many ways to approach this, but I want to know if there are existing algorithms to do this
",,,,,Submission,2,0,2
4faqbw,2016-04-18 03:25:03-04:00,SixOnTheBeach,Why isn't PPI used as the main measurement of how good a monitor is over resolution?,,,,,,Submission,15,0,15
d27ad6z,2016-04-18 03:49:23-04:00,necuz,,/r/AskMarketing/,4faqbw,t3_4faqbw,SixOnTheBeach,,Comment,17,0,17
d27gpxd,2016-04-18 09:22:14-04:00,Syde80,,The same reason we don't measure/buy light bulbs based on their lumen output.,4faqbw,t3_4faqbw,SixOnTheBeach,,Comment,9,0,9
d280da3,2016-04-18 16:56:17-04:00,danltn,,"Good news, we do in the UK now!  All the lightbulbs have lumens printed on the box.

[or at least in Wilko, where I buy all my lightbulbs.]

",4faqbw,t1_d27gpxd,Syde80,,Reply,3,0,3
d281v31,2016-04-18 17:30:36-04:00,Syde80,,"In Canada its becoming common to see lumen count on the box, at least for LED bulbs... Though it's normally in smaller print on the back of the box.  The front will have like ""12W equivalent to 100W"" or something dumb.",4faqbw,t1_d280da3,danltn,,Reply,2,0,2
d27ao74,2016-04-18 04:08:35-04:00,groumpf,,"You want both PPI and size information. I guess resolution started becoming the standard measurement when PPI counts started stabilizing across makers: for a fixed PPI count, resolution gives you all the information you need.",4faqbw,t3_4faqbw,SixOnTheBeach,,Comment,4,0,4
d27bfy4,2016-04-18 04:58:45-04:00,SixOnTheBeach,,"Assuming equal sized screens, yes, but not otherwise.",4faqbw,t1_d27ao74,groumpf,,Reply,1,0,1
d27bhxt,2016-04-18 05:02:32-04:00,gilgoomesh,,"I think you misunderstand parent comment's point: PPI was fixed at about 100dpi for so long on the desktop that people became accustomed to focusing on other metrics.

Even now, desktop PPI is generally close to 100 or close to 200 for double density displays. Other densities tend to be nonstandard.

While laptops have traditionally been all over the place (often aiming for simply: as high as possible), Apple's laptops tend to be 115 or 230 dpi (non-retina and retina) making PPI a part of the product name and ensuring fairly consistent UI sizing, assuming monitor settings are left at defaults.",4faqbw,t1_d27bfy4,SixOnTheBeach,,Reply,6,0,6
d27ybon,2016-04-18 16:11:59-04:00,ImperialWalker12,,"We are lucky, at least, that they are proper inches!

As to lumens for light, it made sense to use wattage when tungsten filaments were all there were, as this was a consistent figure.

How is watt equivalent any better or worse than lumens?

There is a similar level of pedanticism with calories (Calories) and joules.  What practical benefit is there in using the latter over the former?",4faqbw,t3_4faqbw,SixOnTheBeach,,Comment,1,0,1
d28gsjl,2016-04-18 23:44:21-04:00,Syde80,,"> As to lumens for light, it made sense to use wattage when tungsten filaments were all there were, as this was a consistent figure.

No, it never made sense.  It was poor thinking however the trend started.  Marketing regarding what a product does (ie. emit light) should NEVER be based on its INPUTS (Watts).  You are buying it because of what it does, ie. its OUTPUTS (lumens).   The only reason you should base marketing on inputs is when you want to highlight efficiencies. 

> How is watt equivalent any better or worse than lumens?

Because there are LED bulbs that say they are ""10W"" and some that say they are ""12W"", yet their lumen output is (basically) identical.  You naturally believe that the 10W bulb does less than the 12W bulb, when infact the 10W bulb is just more efficient.

How mad would you be if I sold you a bulb that said ""1500W!"" yet it output light equivalent to a 40W incandescent?",4faqbw,t1_d27ybon,ImperialWalker12,,Reply,2,0,2
d29gwsn,2016-04-19 18:03:10-04:00,ImperialWalker12,,"It certainly does make sense if these are watts and the only form of illumination is tungsten filament.

In any case, the number of watts in tungsten equivalent is a finite amount of lumens.

So watt equivalent [tungsten] is neither better nor worse than lumens.",4faqbw,t1_d28gsjl,Syde80,,Reply,1,0,1
d29ina0,2016-04-19 18:45:11-04:00,Syde80,,">It certainly does make sense if these are watts and the only form of illumination is tungsten filament.

When I said it didn't make sense ever it was because it was short-sighted thinking to believe the only form of illumination for all ot time would be tungsten filament and we should market ""what the product does"" primarily based on ""what the product consumes"".  Not that it wasn't understandable to know what a product does based on the wattage rating.  When I say does not make sense, I mean it in the form of sensible, not understandable.

>In any case, the number of watts in tungsten equivalent is a finite amount of lumens.

>So watt equivalent [tungsten] is neither better nor worse than lumens.

It is worse in the way that it is not direct.  It would be like if I said my car has a top speed of 0.326 equivalent Ferrari 458.  This may not be technically incorrect, but it is an absolutely stupid way of expressing my cars top speed.  It requires knowledge of a product you may not have even seen before to know what the actual answer is.

Do your buy your computer's monitor primarily based on its power consumption?  Probably not except under rare edge cases..  I'm guessing you care more about size, PPI, connectors, dynamic range, refresh rate etc. Alot more than you do about its power requirements.


",4faqbw,t1_d29gwsn,ImperialWalker12,,Reply,2,0,2
d29jaxb,2016-04-19 19:01:09-04:00,ImperialWalker12,,"Are you in favour, then, of tearing M.P.H. road signs out and forcing K.P.H, in their place?

I happen to enjoy different ways of expressing measure, and view it as a loss that we have lost so many through forced metrication.

Was the lumen even a unit of measurement in use in the 19th century?

If you look at the definitions, they still derive from standard candles, so you may wish to keep this in mind when speaking of 'short sighted thinking' in this drive towards modernity, that candles, and not electricity were the basis for measure of illumination and its measure.",4faqbw,t1_d29ina0,Syde80,,Reply,1,0,1
d29kxbc,2016-04-19 19:39:44-04:00,Syde80,,">Are you in favour, then, of tearing M.P.H. road signs out and forcing K.P.H, in their place?

This comparison does not go along the same path as what I have been saying the whole time.  MPH and KPH are both measurements of outputs of a product.  I have no issue with using either ti measure the same thing, because they both as direct as possible measurements for something like speed.  What we were talking about before was measuring a product based on its inputs.  Again, what it consumes vs what it does.  The primary measurement should almost  always be what it does, what it consumes should be part of specifications.  Watts is what light bulb consumes and without additional context it tells you NOTHING about what the product does.  You need additional info such as bulb type.  That makes it a poor choice for unit of measure.

However, to answer your question, yes I am absolutely in favor of throwing away everything with mph or any other imperial system measurement.  Then again, I don't live in one of the 3 remaining countries that still use that system.

>I happen to enjoy different ways of expressing measure, and view it as a loss that we have lost so many through forced metrication.

There is nothing wrong with having multiple measurement systems.  However I hope your seeing by now this is not the point I'm trying to make.

>Was the lumen even a unit of measurement in use in the 19th century?

No idea of the history, of the lumen measurement, but certainly candela or candlepower would have been historical equivalents as a standard unit of measure.

>If you look at the definitions, they still derive from standard candles, so you may wish to keep this in mind when speaking of 'short sighted thinking' in this drive towards modernity, that candles, and not electricity were the basis for measure of illumination and its measure.

Don't have a problem with those units of measure as they are baseline units.  Every unit of measure has a base reference and that is unavoidable.  Just like the Litre was originally defined as bring 1kg of water.

",4faqbw,t1_d29jaxb,ImperialWalker12,,Reply,2,0,2
d29lmru,2016-04-19 19:56:36-04:00,ImperialWalker12,,"Tell me, please, how the United Kingdom, a nation of over 60 MILLIONS does not fall under '3 remaining countries' when all of our roadways are clearly marked in miles, yards, feet, and inches?

The Imperial gallon is also originally defined as 10 lb of water.",4faqbw,t1_d29kxbc,Syde80,,Reply,1,0,1
d29o8zc,2016-04-19 20:56:21-04:00,Syde80,,">Tell me, please, how the United Kingdom, a nation of over 60 MILLIONS does not fall under '3 remaining countries' when all of our roadways are clearly marked in miles, yards, feet, and inches?

Funny how I did not even mention the UK but you are asking me to explain this?  

At any rate, ill let those far more knowledgeable than me do that for you:   https://en.m.wikipedia.org/wiki/Metrication

> In 2006, three countries did not mainly use the metric system: the United States, Burma, and Liberia.

Yes their are exceptions, the UK has some as you already know.

I don't even have an issue with you using any unit of measure in the imperial system.  It's not one I would use, but I still consider them completely valid.  Seriously go reread my last post.  I'm done restating the same thing over and over that you just don't seem to be getting.

>The Imperial gallon is also originally defined as 10 lb of water.

I'm just not even sure what point you are trying to make here?  Perhaps you are just giving another example about how I said every standard unit of measure has a base reference?  

At any rate, I'm done, I've said the same thing in as many ways as I care to. Go use imperial system or anything else you want, I do not care what system you or anybody else uses.  Just please don't measure a product primarily by what it consumes and not by what it does or outputs.  That's it.  Watts is not something that a light bulb outputs.
",4faqbw,t1_d29lmru,ImperialWalker12,,Reply,1,0,1
d2axmmu,2016-04-20 18:06:22-04:00,ImperialWalker12,,"To be clear:  You are not a native speaker of English, then, and yet you wish to debate the finer points of English, a language which you are not a native speaker of, then?

I feel no need to refute an unverified article written by amateurs on the internet, but I assure you that a nation who measures her roads in M.P.H. is not 'officially metric' any more than Canada or Bermuda.  How does a country that measures every yard of roadway in Imperial 'metric'???

I suppose this makes your frequent, persistent, misunderstandings of my meaning and point more clear.


How is it, then, that you are so adamant, insistent, confident, when you are on such unsure footing?  Very continental, certainly, but not conducive to honest discourse and discussion, is it?

I would remind you, that it really is not your place what Britain chooses to classify herself as, metric or not!  But it is hardly the United States and two 3rd world nations, 'the only ones not having metricated' then, is it?

And 'watt equivalence' is just as valid a point, as it is not 'input' but rather the tungsten watt equivalent.  You have offered nothing to refute that, and were apparently not informed as to the origins of wattage for illumination output ratings.

How is that any less valid than candlepower, then?",4faqbw,t1_d29o8zc,Syde80,,Reply,1,0,1
d2b9ntz,2016-04-20 23:20:18-04:00,Syde80,,"laugh out fucking loud.

Told you I was done with you, not taking your bait.",4faqbw,t1_d2axmmu,ImperialWalker12,,Reply,1,0,1
d2858pt,2016-04-18 18:55:32-04:00,cogman10,,"DPI used to be a thing.  But then it got sort of confounded because companies like samsung decided to cheat.  Where DPI and PPI were pretty much the same thing, samsung decided to screw things up by sharing pixel colors.",4faqbw,t3_4faqbw,SixOnTheBeach,,Comment,1,0,1
d27xp4o,2016-04-18 15:58:16-04:00,Anders4000,,"I really don't care about PPI on my desktop monitor. I care about size, resolution and frame rate.",4faqbw,t3_4faqbw,SixOnTheBeach,,Comment,-1,0,-1
d285qim,2016-04-18 19:08:25-04:00,MonsieurBanana,,... ,4faqbw,t1_d27xp4o,Anders4000,,Reply,1,0,1
d285tod,2016-04-18 19:10:43-04:00,Anders4000,,...,4faqbw,t1_d285qim,MonsieurBanana,,Reply,1,0,1
4f9m9n,2016-04-17 21:48:43-04:00,tobyisthebest,Java Binary Representation for Integer.MIN,"Why is it that in Java, the binary representation of Integer.MIN is ""10000000000000000000000000000000"" instead of ""10000000000000000000000000000001""? Wouldn't the latter be larger than the former?

I would think that adding one to the two's complement of ""10000000000000000000000000000001"" would make it larger by exactly one.",,,,,Submission,3,0,3
d273ha4,2016-04-17 23:09:10-04:00,ProfessorAlgorithm,," ""10000000000000000000000000000000"" IS smaller than ""10000000000000000000000000000001"", which is why it is the minimum value.

",4f9m9n,t3_4f9m9n,tobyisthebest,,Comment,3,0,3
d27thui,2016-04-18 14:26:50-04:00,lneutral,,"To add to /u/ProfessorAlgorithm's answer: let's do a quick experiment to validate this:

Take a signed byte with value 0x80. In binary, this is 1000 0000.

The two's complement is formed by flipping the bits (0111 1111) then adding 1 (1000 0000).

So 1000 0000 represents negative 1000 0000, which is -128. Yes, it's weird that the two's complement ends up at the same binary value as you started with, but this will only happen for two values.

Determining the other value is an exercise left to the reader.",4f9m9n,t3_4f9m9n,tobyisthebest,,Comment,2,0,2
4f6pam,2016-04-17 10:06:44-04:00,bliskator,Making sense of quantum computing.,"The best that I can understand is that QCs can be massively parallel due to uncertainty of state.
So how does one have more than two states into a ""bit"" and sort the output states into information related to the input?",,,,,Submission,12,0,12
d26do5t,2016-04-17 11:27:30-04:00,manlycoffee,,"*I'm in no way an expert, so take what I say with a grain of salt.*

In most Von Neumann computers, we have bits, and these bits are assigned either 0 or 1. So let's say we assigned a bit with the value 1, the next time we read this same bit, we will get the value 1: if we are to reassign it to the value 0, the next time we read that bit again, we will get back the value 0.

Unlike bits, Qbits in quantum computers are instead assigned probability distributions of reading a 1 or 0. So you don't necessarily assign a qbit a value 0 or 1, but rather the probability of getting 0 or 1, e.g. you assign the probability of 20% that you will get 0 and 80% that you will get 1, for the next time we are to read from a particular qbit (if you *do* insist that a qbit should hold the fixed value of either 0 or 1, you can just assign the probability distribution of 100% for 1 and 0% for 0, and vice versa).

The above fact alone is what makes quantum computers ""parallel"" in a sense. That is, when you assign a probability distribution, you essentially get a ""density"" of what values you will get for every time you read from a particular bit. It's almost like tossing a coin, except with bias introduced to it on purpose. This fact is more ""abstract logic"" than anything technical.

In fact, assuming your desktop computer does indeed have a decent random number generator (perhaps [implemented somehow with hardware](http://cstheory.stackexchange.com/a/16381)), you can pretty much apply some quantum computing principles. That is, you can create a list of probability distributions, randomly generate either a 0 or 1 based on said distribution, and generate a list of 0s and 1s. But generating that list will have to be done sequentially with the traditional desktop computer, where as a quantum computer will simply give you your list of 0s and 1s just by looking at it, assuming you have generated a *list* of probability distributions, and assigned it to a *list* of qbits.",4f6pam,t3_4f6pam,bliskator,,Comment,8,0,8
d26he0z,2016-04-17 13:14:26-04:00,Majiir,,"Eh, so I am *also* in no way an expert, so I might be off on this... but *as I understand* it's a little more complex.

When you ""read"" the values from a quantum computer, you're really just reading binary values (0 or 1) and in order to get a sense of the probability distribution, you need to sample the output many times. This is how certain counterintuitive algorithms can actually work. For example, you can ""sort"" N items in O(sqrt(N)) steps in a quantum computer, but *loading* and *retrieving* that data takes at minimum O(N) steps.

The quantum states are also not just simple percentages, and quantum states can interact with each other without being ""reduced"" to a classical bit first. Some part of the quantum state can be ""hidden"" from classical observation, and then ""revealed"" again by a quantum gate.

Hoping a real expert comes by and clarifies!",4f6pam,t1_d26do5t,manlycoffee,,Reply,5,0,5
d26k885,2016-04-17 14:32:27-04:00,UncleMeat,,"> parallel

Be very very careful when using this word to describe QC. There is this pervasive myth that QC functions by ""trying everything at once"" and is somehow comparable to parallel computation when this just isn't the case.",4f6pam,t1_d26do5t,manlycoffee,,Reply,4,0,4
d26nt3b,2016-04-17 16:03:31-04:00,manlycoffee,,"Yep. Like what /u/Majiir said, you'd have to ""sample the output"" many times, which doesn't make it all that parallel, other than it being just a concept.

Perhaps I should have used the term ""parallel"" in the final paragraph to my comment: in quantum computers, a collection of qbits do not need to be sequentially read in order to get back data, where was I can't envision how that will be possible with classical computers.",4f6pam,t1_d26k885,UncleMeat,,Reply,1,0,1
d26ylar,2016-04-17 20:53:03-04:00,Fordrus,,"https://www.youtube.com/watch?v=JhHMJCUmq28&ab_channel=Kurzgesagt%E2%80%93InaNutshell

When you describe that pervasive myth, kurzgesagt appears to be propagating *exactly* that pervasive myth.  I'd be interested in hearing your thoughts on this one- kurzgesagt says in the above linked video, specifically, 
>""A normal logic gate gets a simple set of inputs and produces one definite output, a quantum gate manipulates an input of superpositions, rotates probabilities, and produces another superposition as its output.  So a quantum computer set up some qubits, applies quantum gates to entangle them, and manipulate probabilities, then finally measures the outcome, collapsing superpositions to an actual sequence of 0s and 1s.""

Then, here's the part where kurszgesagt appears to be contradicting you specifically: 
>""What this means is you get the entire lot of calculations that are possible with your setup all done at the same time.  Ultimately you can only measure one of the results, and it'll only probably be the one you want, so you may have to double check and try again.  But by cleverly exploiting superposition and entanglement, this can be exponentially more efficient than would ever be possible on a normal computer.""

Is it in the final sampling that you're disputing the performance of all calculations at once, that is, in the end, you can only sample one of the outcomes, and therefore describing the process as ""trying everything at once"" is false?  It is functionally a question of semantics- that we are computing all possibilities in parallel, but that because we must collapse it in the end, we're therefore not enjoying the fruits of all those calculations at once?

I hope I'm not being annoying, this is just something I very much want to understand- and when you mentioned that as a pervasive myth- well- if it is, I've been very much a victim of it, and I'm eager to clarify so that I can have some accuracy in my conception! :)",4f6pam,t1_d26k885,UncleMeat,,Reply,1,0,1
d26zffz,2016-04-17 21:15:16-04:00,UncleMeat,,"> ""A normal logic gate gets a simple set of inputs and produces one definite output, a quantum gate manipulates an input of superpositions, rotates probabilities, and produces another superposition as its output. So a quantum computer set up some qubits, applies quantum gates to entangle them, and manipulate probabilities, then finally measures the outcome, collapsing superpositions to an actual sequence of 0s and 1s.""

This is about as correct as you can expect a youtube video to be.

> ""What this means is you get the entire lot of calculations that are possible with your setup all done at the same time. Ultimately you can only measure one of the results, and it'll only probably be the one you want, so you may have to double check and try again. But by cleverly exploiting superposition and entanglement, this can be exponentially more efficient than would ever be possible on a normal computer.""

I think this is misleading at best. Because this myth is so pervasive I think its very important to choose words that cannot propagate this myth in people's minds. It doesn't accurately convey the limitations of QC (e.g., how much stuff can I do in one operation?) and tricks people into believing that QC is equivalent to non-determinism and therefore BPP = NP. If we really could do ""the entire lot of calculations... at the same time"" then why is Grover's Algorithm sqrt(n) rather than constant time? ",4f6pam,t1_d26ylar,Fordrus,,Reply,1,0,1
d270n5a,2016-04-17 21:48:31-04:00,Fordrus,,"> If we really could do ""the entire lot of calculations... at the same time"" then why is Grover's Algorithm sqrt(n) rather than constant time?

heh heh, I don't know, myself, I was rather hoping you could elucidate, I wasn't strictly familiar with BPP = NP as notation for that concept, though I *think* I'm familiar with the intuitive idea, and Grover's Algorithm- I think I'd read about that once, but I'm reading the wikipedia article now for a re-primer. :)

I guess it's my understanding that technically, you have a systemic quantum superposition that represents ""the entire lot of calculations"" completed at once, but that you can't actually **observe** the answer without collapsing the system, causing yourself to retroactively only have followed one path in the gated system- rendering the initial observation that you've 'performed' the entire lot of calculation effectively meaningless- but I'm afraid I'm a bit stumped as to how you would even make use of the observed outcome, if you couldn't be certain which path you had followed- or is the allure in that you're able to get an answer and trace back the logic gate path to understand how said answer was reached?

Aye aye aye- there is so much to learn in this sphere, but I'm so curious, and like I say, eager to understand it **correctly**.  Must simply read more on the topic, I suppose. :)

Thanks for your time, UncleMeat, I may message you in the future to vet any future insights I want to vet, if that's alright. :)",4f6pam,t1_d26zffz,UncleMeat,,Reply,1,0,1
d2760bo,2016-04-18 00:30:18-04:00,Steve132,,">  QC is equivalent to non-determinism

It is a kind of non-deterministic computer

> and therefore BPP = NP. 

This is the part that's incorrect.  You can't 'read back' all possible non-deterministic results so BQP != NP

> If we really could do ""the entire lot of calculations... at the same time"" then why is Grover's Algorithm sqrt(n) rather than constant time

Because each iteration of grover's algorithm does the entire lot of calculations at the same time, then multiplies the state in such a way as to get the 'desired' output within 1/sqrt(N) liklihood of coming out of the measurement....therefore you need to do k=sqrt(N) iterations to make the liklihood of the 'desired' output 100%.",4f6pam,t1_d26zffz,UncleMeat,,Reply,1,0,1
d27646b,2016-04-18 00:33:55-04:00,Steve132,,"> you can pretty much apply some quantum computing principles. That is, you can create a list of probability distributions, randomly generate either a 0 or 1 based on said distribution, and generate a list of 0s and 1s. But generating that list will have to be done sequentially with the traditional desktop computer, where as a quantum computer will simply give you your list of 0s and 1s just by looking at it, assuming you have generated a list of probability distributions, and assigned it to a list of qbits.

This part is incorrect.  The underlying ""list"" you're referring to is a list of 2^N probability values...the main reason why it's incorrect is that the probability distribution can be manipulated in it's entirety by the quantum comptuter, and different parts of the distribution can effect other parts without collapsing the distribution.",4f6pam,t1_d26do5t,manlycoffee,,Reply,2,0,2
d26c2vv,2016-04-17 10:36:53-04:00,artillery129,,Just watch that circle jerk of a video going around with the Canadian Prime Minister and apparently that's all you'll ever need to know,4f6pam,t3_4f6pam,bliskator,,Comment,5,0,5
d26jzpe,2016-04-17 14:25:54-04:00,lneutral,,Ugh. Tell me about it. I presented even a sliver of doubt that he actually understood quantum computing and got to argue with a bunch of people about how I was completely misrepresenting him. ,4f6pam,t1_d26c2vv,artillery129,,Reply,1,0,1
4f4mcp,2016-04-16 21:14:26-04:00,tobyisthebest,Super simple Regex problem,"s = ""65x"";
String a = Arrays.toString(s.split(""\\w?"",-1));

In the previous java code, the output is [, , , , ] and I am thoroughly confused. I'm not sure how this output occurs and I'm also not sure how the greedy quantifier works in Java's String.split.

Could someone explain how Regex quantifiers work in splitting Strings? (Please pardon my newbieness)",,,,,Submission,1,0,1
d296hgk,2016-04-19 14:18:55-04:00,_--__,,"Your regex ""\w?"" matches 0 or 1 word characters, so the following strings will act as delimiters: """", ""6"", ""5"", ""x"", ...  Note especially that first one.  If you try s.split("""",-1) you get ["""",""6"",""5"",""x"",""""] i.e. that delimiter gives you the empty string at the ""start"" and ""end"" of s (s.substring(0,0) and s.substring(3,3) respectively).  Since ""6"", ""5"" and ""x"" are also delimiters (and are preferred over """" by the use of the greedy quantifier), the result of s.split(""\w?"",-1) will be ["""","""","""","""",""""] as you obtained.  It is worth comparing this to s.split(""\w??"",-1) - i.e. the reluctant 0 or 1 match.  This prefers using the delimiter """" over ""6"",""5"",""x"" so it should give the same output as s.split("""",-1) [which it does].",4f4mcp,t3_4f4mcp,tobyisthebest,,Comment,1,0,1
4f48l2,2016-04-16 19:22:24-04:00,Beggar-So,Is there a use for multithreading on a single CPU?,"If there's a single CPU, then that processor can't run parallel instructions, right? So, if everything is serial, then is there a point to multithreading on a single CPU?",,,,,Submission,13,0,13
d25ssha,2016-04-16 20:26:21-04:00,rewardiflost,,"Sure.   ... examples may be a bit dated from my old architecture class. 
If it takes more than one or two clock cycles to fetch/write on a drive, other instructions that don't depend on the current data in transit can be executed by the CPU while the bus manager handles the data transfer.   

If the CPU is open to staging or parallel operations,  separate processes can be using the floating point unit simultaneously while a process uses the arithmetic logic unit.   In this case, a single cpu can be treated like separate smaller components.

It's basically finding ways to use all the non operational clock cycles that the CPU has while waiting for data or instruction updates.      Every cycle that goes from noop to something constructive is a performance boost.  ",4f48l2,t3_4f48l2,Beggar-So,,Comment,6,0,6
d25t0kn,2016-04-16 20:33:24-04:00,minesasecret,,"It's been a while since my OS class but I can think of a few reasons off the top of my head why writing a multithreaded program still makes sense for a single CPU machine. Hopefully others with more knowledge can chime in too.

One reason is that if you have workloads which do a lot of IO you can switch to another thread as you're waiting for the IO to finish. Nowadays I think you have asynchronous IO which you use too but I imagine using multiple threads is just easier, especially when your workloads are very independent.

If you have things which need to run in the background it'd be a bit annoying to run them all in the same thread because you'd need to write your own code to ""schedule"" them. In a way by making a new thread you're just offloading that work to the OS which is nice.

Also doing something all on a single thread vs with multiple threads on a single CPU processor isn't equivalent. Presumably your program isn't going to be the only thing running on the machine so the OS scheduler will likely schedule your program differently depending on how many threads it has. I imagine this may make it so you have more CPU time but don't quote me on that.",4f48l2,t3_4f48l2,Beggar-So,,Comment,3,0,3
d25wane,2016-04-16 22:15:37-04:00,cogman10,,"Couple of reasons.

First off, threading (even minimal threading) is convenient whenever you have some task which needs to wait for an off system thing to complete.  Think, reading the hard drive, talking over the network, waiting for a mouse to respond, or even just waiting for the next frame of the monitor to refresh.  You can do useful work while those things are happening which is why threading has existed since long before multi-cpu systems were available.

Another reason for it is simply because being able to dedicate a thread to some task is pretty useful in some cases.  For example, think of the JVM.  It is really convenient to run GC in a separate thread from the thread(s) that run the main application.  The GC thread doesn't have to have anything to do with the rest of the application.

Other sorts of things are things like monitors, resource managers, loggers, etc.  Just side tasks or processes which may need to share memory with the main application.

Threads themselves are a natural consequence of multi process operating systems.  If you want to run 2 applications at the same time, you need to be able to context switch between them.  This is part of the reason why threads have existed for so long.",4f48l2,t3_4f48l2,Beggar-So,,Comment,3,0,3
d25t9gl,2016-04-16 20:41:01-04:00,None,,[deleted],4f48l2,t3_4f48l2,Beggar-So,,Comment,2,0,2
d25yec8,2016-04-16 23:24:32-04:00,taricorp,,"Not just pipelining, but most modern microarchitectures are superscalar, so they can have multiple active instructions in a single pipeline stage and even complete instructions out-of-order. x86 has been doing these things since the Pentium, and most ARM Cortex-A chips (basically anything found in a reasonably new smartphone) are superscalar and out-of-order as well.",4f48l2,t1_d25t9gl,None,,Reply,1,0,1
d25z2xm,2016-04-16 23:48:01-04:00,ldpreload,,"Threads give you two things apart from parallel execution:
 * different stacks
 * the ability for one thread to be in a system call when another isn't

The first is basically just a convenience, but is a very nice one. (It can be emulated without any OS thread support, though.)

The second is a good way to work around most OSes having no good support for asynchronous I/O. If you make a system call `write(1, ""Hello world"", 11)`, it will wait until all 11 bytes are written before returning. On many OSes you can configure it not to wait but instead return a not-completed-yet code, _if it's a network socket_. But maybe you're writing to disk, and your disk is slow enough that you can do useful work while you're waiting. Or maybe you're doing an operation like `open(""/some/file/in/a/long/path""...)`, and the process of reading each directory from disk is a bit slow. Or maybe you're doing something else, like `sync()` or `rename(""file1"", ""file2"")`, or anything else that could maybe possibly be slow. Or maybe it's a driver-specific interaction with an external device. In theory, an OS could offer every single one of these via an asynchronous interface. In practice, those that try won't get every last call.

So a simple solution is to allow threads, and tell the program it can make a regular system call on one thread while continuing to do work on another. (You could maybe design a simpler interface that isn't quite threads, but since threads are definitely useful on multi-processor systems, it's usually not worth the effort/complexity to build not-quite-threads if you have to build threads anyway.)",4f48l2,t3_4f48l2,Beggar-So,,Comment,2,0,2
d25swoi,2016-04-16 20:30:04-04:00,anon_smithsonian,,"Sort of. 

[Hyperthreading](https://en.wikipedia.org/wiki/Hyper-threading), which is Intel's own implementation of something called *simultaneous multithreading*, is probably what you're thinking of. It was introduced in consumer-level CPUs with the Pentium 4 line of processors (back when single cores were the standard). 

There are some performance benefits that can be gained in certain situations by doing this, but the benefits/improvements aren't across the board. 

&nbsp:

**Edit**: clarified that Hyper Threading is what Intel calls their implementation of simultaneous multithreading.",4f48l2,t3_4f48l2,Beggar-So,,Comment,2,0,2
d25yiv1,2016-04-16 23:28:48-04:00,taricorp,,"> Hyperthreading, also known as simultaneous multithreading

I'd turn that around. Hyperthreading is Intel's marketing name for their SMT implementation. Other (not desktop-oriented) CPUs tend to go further with SMT, such as SPARC or IBM's POWER chips which may do as many as 8 threads per core.",4f48l2,t1_d25swoi,anon_smithsonian,,Reply,1,0,1
d25yubr,2016-04-16 23:39:40-04:00,anon_smithsonian,,"True: I'll clarify that Hyper Threading is Intel's implementation of SMT. 

The reason I named Hyper Threading first, even though the correct term is SMT, was because it's something I think the majority of people would recognize and understand the concept behind it vs. ""simultaneous multithreading"" (because, really, the term *multithreading* kind of already implies *simultaneous*... at least to me).",4f48l2,t1_d25yiv1,taricorp,,Reply,1,0,1
d25z738,2016-04-16 23:52:05-04:00,Merad,,"You know that the typical consumer PC only had a single core CPU up until 2006-2007, right?  Operating systems and programs still had the same fundamental functionality, it was just perhaps a bit less reliable (a single frozen program could easily freeze the whole system). None of this would have been possible without single-CPU multithreading. 

As it turns out, CPUs are *really* fast, and can switch between dozens of tasks per second without breaking a sweat. It also turns out that a great many programs, especially typical GUI programs in general, don't really use that much CPU time - they spend the overwhelming majority of their time sitting idle waiting for user input. ",4f48l2,t3_4f48l2,Beggar-So,,Comment,1,0,1
d2678kh,2016-04-17 06:54:46-04:00,cozos,,Well.. what if your bottleneck is network IO or something (i.e. http requests)?,4f48l2,t3_4f48l2,Beggar-So,,Comment,1,0,1
4f2144,2016-04-16 10:09:16-04:00,keypiksentrik,HELP! I want to self-study Computer Science.,"Good day! It's been a while since I posted something here on Reddit. It feels good to do something like this again.

Without further ado:

The title says it all. I want to teach myself Computer Science and I need your help in doing this. I have several questions prior to doing this because I don't want to mess it up. I want to take a systematic approach to this rather than just pick up crumbs of things and then bind them together until I feel I've learned enough. I am from the Philippines by the way and the environment I am in doesn't allow me to continue learning through educational institutions. To put it bluntly, I don't have the money to pay for tuition fees and the likes. I live within my means and I need to support 4 younger siblings until they can stand on their own. I was discouraged at first because of my situation but then I always think I can achieve more. I finished high school and I only completed a semester's worth of college education. I studied Computer Engineering back then. I'm 23 and I'm starting to feel like I need to do something now or I wouldn't achieve anything at all. I suck at math but I want to re-learn it and maybe I can understand it better. I hope these things help you guys in helping me in my endeavour. I'm a dreamer and an advocate of learning!

Here's what I want to know:

1. On a scale of 1-10 how hard do you think it is to learn Computer Science?
2. Where can I find resources/curriculum to follow?
3. What tools do I need for this?
4. Can you suggest any methods/study habits that helped you in learning Computer Science?
5. Should I do a general study of this or follow a path that I'm interested in? [I.E Focusing only on programming/design, etc.]

These are all I need to know for now. I'll post updates from time to time to update you guys about my progreess. Thank you guys so much in advance!

~Keypi",,,,,Submission,14,0,14
d25aeai,2016-04-16 11:33:43-04:00,IGetConfused,,">1. On a scale of 1-10 how hard do you think it is to learn Computer Science?
2. Where can I find resources/curriculum to follow?
3. What tools do I need for this?
4. Can you suggest any methods/study habits that helped you in learning Computer Science?
5. Should I do a general study of this or follow a path that I'm interested in? [I.E Focusing only on programming/design, etc.]


1) Depends on how in depth you're looking to go, and what you're wanting to do. 
2)edx has some good courses. Plural sight is also good, but costs money. There are a ton of resources. There are also lots of blogs which can help. 
3)A computer is all you have to have. Again it depends on what you're trying to do. I want to learn computer science is a huge thing. There are a ton of things to learn and study. 
4)Best way to learn is to do. Pick a language to start, pick a project and do it. If you want studying practices then the best one is to read. Not half ass read, but really read. 
5)Focus is better when looking for a job. There are basics to all of computer science(i.e. data structures) but it is up to you. 

Message me if you want to know more. I'd be happy to help do something. ",4f2144,t3_4f2144,keypiksentrik,,Comment,3,0,3
d25dxuv,2016-04-16 13:16:22-04:00,keypiksentrik,,Thank you for this! I'd definitely send you a message if I have some further questions! For now I think I'll start with the basics like the one I found in Coursera which is a short course offered for free by Stanford University. The course lets student grasp the basics first then focus on something afterwards.,4f2144,t1_d25aeai,IGetConfused,,Reply,2,0,2
d25nsmb,2016-04-16 17:54:04-04:00,rnda,,2) [Path to a free self-taught education in Computer Science](https://github.com/open-source-society/computer-science),4f2144,t3_4f2144,keypiksentrik,,Comment,3,0,3
d25rx37,2016-04-16 19:58:48-04:00,keypiksentrik,,This is gold! This seems to be perfect for a starter. Thank you very much!,4f2144,t1_d25nsmb,rnda,,Reply,1,0,1
d25pnqm,2016-04-16 18:48:53-04:00,Bottled_Void,,https://www.khanacademy.org/computing/computer-science,4f2144,t3_4f2144,keypiksentrik,,Comment,2,0,2
d25sjbn,2016-04-16 20:18:15-04:00,keypiksentrik,,This is a really good source. Thank you for this!,4f2144,t1_d25pnqm,Bottled_Void,,Reply,2,0,2
d25chdb,2016-04-16 12:34:45-04:00,visvis,,"I would say it all depends on what you want to do with it. Given that you're not going to get a degree for this anyways and you're probably not going into research either, I would start off with the skills that are most important in employment.

As such, I would definitely start off by learning programming. Plenty of programming courses can be found online and which one to choose should depend mostly on the language. For example if you want to go into web development Javascript might be a good start, for applications Java or C# and for systems programming C or C++. However, there are plenty of other interesting languages around.

Once you know the basic of programming, I think practice is most important. Experience makes you a better programmer and building open source applications may help you get jobs later. While doing so you will often encounter cases where you need to choose data structures and algorithms, which you can then look up and learn on the go.

I would say this covers the basics that you always need from computer science for it to be useful. Depending on which direction you want to go in you probably want to cover more but that would be very different between, say, artificial intelligence, computer security, theoretical computer science or software engineering.",4f2144,t3_4f2144,keypiksentrik,,Comment,1,0,1
d25duq1,2016-04-16 13:13:52-04:00,keypiksentrik,,"Thank you! This is very helpful! Now I can set my sight on programming and set one goal at a time from then on. Again, thank you very much! :)",4f2144,t1_d25chdb,visvis,,Reply,1,0,1
d25d5wc,2016-04-16 12:54:20-04:00,videoj,,"Here's someone who did it, you should read his results:

https://www.scotthyoung.com/blog/myprojects/mit-challenge-2/",4f2144,t3_4f2144,keypiksentrik,,Comment,1,0,1
d25dt05,2016-04-16 13:12:31-04:00,keypiksentrik,,"I saw this yesterday and yes this is quite good but he doesn't really give out information as to what particular curriculum did he follow. I mean, which lectures, exercises, etc. did he take on the MIT OCW for me to be able to gauge if I could follow the pattern in which he finished the course. But still this is evry helpful. Thank you very much!

Edit: Oh! I just found the link to the curriculum he followed. Thank you!",4f2144,t1_d25d5wc,videoj,,Reply,1,0,1
d25e637,2016-04-16 13:22:53-04:00,None,,"I would learn a programing language, because it's a lot of bang for your buck-time. Python is a nice bet. It's a modern programming language used in the industry and science world. 

But programming is not computer science, it's the cherry on the cake. 

1: It's easy because you can apply it in the real world and there is plenty of available resources. That being said, it's sometimes very cryptic and terse, but that the beauty of it. I would say 3.

2 : Da internet. Coursera offer good comprehensive curriculum, but plenty of others too

3 : A computer, a decent internet connection

4 : Hacking stuff together, tearing stuff appart, being hang to the concepts and not the tools. 

5 : You can go to the ground up from the theorie to the application ( math and algorithms to programming) Or the contrary. Both are fine and it's ultimately your choice 
",4f2144,t3_4f2144,keypiksentrik,,Comment,1,0,1
d25exym,2016-04-16 13:45:06-04:00,keypiksentrik,,Once I grasp the basics I'd definitely give Python a try. Your answer on number 4 is spot on! Thank you for this. You've been very helpful!,4f2144,t1_d25e637,None,,Reply,1,0,1
d25xy66,2016-04-16 23:09:13-04:00,zSilverFox,,"Use python to learn the basics. Jump in, a lot of beginner courses use it.",4f2144,t1_d25exym,keypiksentrik,,Reply,1,0,1
d263cxo,2016-04-17 02:44:46-04:00,keypiksentrik,,I'd definitely give it a go!,4f2144,t1_d25xy66,zSilverFox,,Reply,1,0,1
d25kth9,2016-04-16 16:29:50-04:00,ChadtheWad,,"I can suggest resources, but what you want to study depends a lot on what part of CS you're interested in. For example, are you interested in learning how to write programs or Web applications? Or are you more interested in setting up servers to host a website or manage multiple users? Or, would you be interested in a specific area in CS, such as designing secure systems, making games, or artificial intelligence?

CS is as challenging as you want it to be. As far as business goes, I don't think a general education like one you get in college is entirely useful, but you will probably want to understand the basic programming concepts no matter what field you go into.",4f2144,t3_4f2144,keypiksentrik,,Comment,1,0,1
d25rvz4,2016-04-16 19:57:52-04:00,keypiksentrik,,I have not decided as of yet on where to focus on. I want to grasp the basics first then I might try and focus something once I figure out my strengths and weaknesses during my study. Your suggestion of resources will be valuable to me anyhow so please send it! Thank you very much!,4f2144,t1_d25kth9,ChadtheWad,,Reply,1,0,1
d29byo0,2016-04-19 16:14:58-04:00,ChadtheWad,,"No problem. Probably the best way to start would be to learn how to write programs in some language and learn Object-Oriented (OO) techniques for writing programs. I would recommend using [Java](https://en.wikipedia.org/wiki/Java_\(programming_language\)), because it has strict conventions which should help you understand writing in an OO style. If you want to try using the programming language Java, you should check out the [notes and assignments from OpenMIT](http://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-092-introduction-to-programming-in-java-january-iap-2010/index.htm). There are a lot of general tutorials for Java out there as well.

After that, you will probably start looking at some algorithms and data structures courses. [Here](http://citc.ui.ac.ir/zamani/clrs.pdf) is a book that I like, but it gets a little math-heavy at times. You can mostly ignore the math as long as you understand the important conclusions -- the most important part of learning algorithms is understanding the implementation, and knowing how fast the algorithm runs based on the input. For example, there are multiple algorithms for sorting a list of numbers from least to greatest. Based on how the list is ordered originally, though, one algorithm may take more time to sort that list compared to another. This book also includes a ""Special Topics"" section which you can skip if you would prefer that.

When you start writing programs, I would recommend you use Google thoroughly. I've been programming for nearly 10 years now, and have rarely had an issue that wasn't asked by someone before. If you can't find anything related to your question, you can post it here or on StackOverflow and you will probably get an answer quickly.
",4f2144,t1_d25rvz4,keypiksentrik,,Reply,1,0,1
d29dbx9,2016-04-19 16:43:31-04:00,keypiksentrik,,"I am currently taking CS50 at Edx. I always thought the experience could be fun but when I got through the first lecture, It's very engaging and it makes learning very fun in a whole new level. Maybe it's just me and because I haven't experienced school life in the U.S. but still, It got me so motivated into completing the course. It's very easy to understand even for students who haven't got any prior experience in CS. After this course I'm gonna go and try to focus on C and Java. I'll keep you posted. Thank you very much!",4f2144,t1_d29byo0,ChadtheWad,,Reply,1,0,1
d25pl27,2016-04-16 18:46:38-04:00,maximuszen,,How are you supporting your siblings? And how old are they? Do you have any help?,4f2144,t3_4f2144,keypiksentrik,,Comment,1,0,1
d25shtg,2016-04-16 20:16:59-04:00,keypiksentrik,,"I work at night. 16, 12, 8 and 4 respectively. Well, there's my mom who takes care of them at home but I shoulder almost all the cost aside from the pension money my mom gets every month.",4f2144,t1_d25pl27,maximuszen,,Reply,1,0,1
d262w35,2016-04-17 02:21:11-04:00,maximuszen,,What kind of work do you do?,4f2144,t1_d25shtg,keypiksentrik,,Reply,1,0,1
d263496,2016-04-17 02:32:18-04:00,keypiksentrik,,I work in a call center as a sales representative. Not the kind of work I really enjoy but it's one of those jobs that pays really good for people without solid educational background like me.,4f2144,t1_d262w35,maximuszen,,Reply,1,0,1
d26ccv1,2016-04-17 10:45:59-04:00,maximuszen,,"Try to get a job in the field. There must be also tech related concerns at your work if you don't know how to solve it then hang around the ""expert"" guy in your office and absorb everything you can learn. Otherwise, Google it. You can find most solutions there.


I speak to Filipino and Indian customer service once in a while, you all are too nice. Don't get to the solution of the problem, instead you have to say all these niceties and affirm and confirm everything that is said. I'm a techie. That is not want I want. I don't care if you are a bit edgy, but this is not what your customer service supervisor wants. They want that fake saccharin nicety otherwise you'll get fired. I want solutions and fast not you to go through the bland solutionless algorithm on your computer screen.

You have a lot on your plate. Its best to work human hours.

Why are you interested in Computer Science?

",4f2144,t1_d263496,keypiksentrik,,Reply,1,0,1
d26ih1s,2016-04-17 13:44:28-04:00,keypiksentrik,,"That's spot on! I'm a computer enthusiast since I was a little kid. It started when counter strike became popular here. That was about 15 years ago I believe. Yes I loved the game but then the thing that runs the game just got my attention one day and I got hooked to it. When I was in High School, I always do extra-curriculars related to computers even though I know little about it. A bit of tinkering here and there gave me bits and pieces of knowledge that proved to be useful in my life later on. Looking back, Friendster was the social media giant that time and now that I think about it Friendster allows user to edit their profiles using CSS which I wasn't even aware is a programming language until this very moment. lol. But that's about it. Straightforwardly, I'm interested in Computer Science because I know it fits my skillset almost perfectly and it's something which I can't be demotivated in learning/doing.",4f2144,t1_d26ccv1,maximuszen,,Reply,1,0,1
d28wk0a,2016-04-19 10:43:14-04:00,maximuszen,,"You can write programs but financial return must be considered. I would work on phone apps, Android. Your idea's gotta be good and you gotta do the research",4f2144,t1_d26ih1s,keypiksentrik,,Reply,1,0,1
d28x47x,2016-04-19 10:56:10-04:00,keypiksentrik,,"I already started with Harvard's CS50 on Edx. It's hard but it's tons of fun and is very informative. Once I get a good understanding of the fundamentals, I'm gonna go and focus on programming and yeah, lots of research too. Your suggestion is very helpful. Thanks!",4f2144,t1_d28wk0a,maximuszen,,Reply,1,0,1
d27ebxo,2016-04-18 07:51:05-04:00,Hatewrecked,,">On a scale of 1-10 how hard do you think it is to learn Computer Science?

I think anyone can learn to code proficiently. CS is different from coding, though. Learning programming, resources are everywhere. Learning computer architecture, there are only a couple of good books and they can be very tough to follow. So I'm sure the rating depends on what areas you focus on. 

>Where can I find resources/curriculum to follow?

You can find programming books in the wiki for /r/learnprogramming. Because the field is so broad, there's no one book which can teach you everything from software engineering to cybersecurity... there are probably 30 books which could give you a good understanding of a particular area, though. You'll have to find what most interests you and look in that direction. When you need example problems, you can try to find a syllabus from some university, or perhaps try /r/dailyprogrammer.

Be careful about YouTube tutorials. They teach you how to do something, but they don't teach you how it works. That's not to suggest to avoid them, but don't use them as your primary source of information, as you'll end up with an incomplete understanding of programming.

>What tools do I need for this?

The books, a computer, plenty of interaction with other people (whether you're reading code, getting help, working on stuff together, etc.), and consistent motivation. It's tricky and long. This isn't a field you're going to master over the summer, but neither is any other field worth learning which could get you a job.

>Can you suggest any methods/study habits that helped you in learning Computer Science?

Don't become bummed or discouraged when coding. Seriously, this is 100% of everything involved in computer science. There's actually a thread on the top of /r/learnprogramming which I completely agree with: the hardest part is finding the motivation to learn every day. You're already sitting at your computer where you are seconds away from Steam, YouTube, reddit, etc., and you've got to tell yourself everyday that you're going to work on stuff instead. And when you get stuck, don't quit! Keep pushing forward, the answer is never as cryptic as you think it is. Take breaks but never lose sight of your short-term and long-term goals. 

Also, I've heard horror stories about people who have read entire programming books before ever even attempting to code -- they just said they read each chapter and ""got it."" There's no way that strategy will ever work. It would be like trying to become a professional painter by reading books and never picking up a paint brush.

>Should I do a general study of this or follow a path that I'm interested in? [I.E Focusing only on programming/design, etc.]

I would focus on programming first, to be honest. All CS students need to learn to program anyway. You will get a better grasp of computers and how they work. I learned first to program in Java... then I learned C and C++. I've done a little in Python and I want to learn Javascript over the summer. If I were you, I would learn C. Another guy suggested Python, and while Python gets things done, I don't think it's a proper first language. A good write-up on why is [here](https://www.reddit.com/r/changemyview/comments/1cyk99/i_think_python_is_the_best_first_programming/c9laccj). That's not to suggest that there is any ""bad language"" to start off on, or that there's an empirical best, but it's easy to get lost in the syntax of Java, and it's easy to get misled by the way Python is set up. C isn't perfect, but it will teach you a lot about how things work the way they work, and there's a billion C books and tutorials out there. C is also nice because you can later transition to C++, which is a little higher level and more used today. It's up to you though! I'm one voice of millions who will disagree.",4f2144,t3_4f2144,keypiksentrik,,Comment,1,0,1
d29dr2h,2016-04-19 16:52:24-04:00,keypiksentrik,,I am currently taking CS50 at Edx. It's so much fun! I'm gonna try and focus on programming and learn C and Java as my first two languages after I complete the course.. I'll keep you posted. Thank you very much for your help!,4f2144,t1_d27ebxo,Hatewrecked,,Reply,1,0,1
d2aj1ay,2016-04-20 13:02:23-04:00,keypiksentrik,,Anyone knows where I can download C software for free?,4f2144,t3_4f2144,keypiksentrik,,Comment,1,0,1
4f0p7j,2016-04-16 01:46:21-04:00,Eadpeard,Big theta complexity for nested each with log complexity.,"for (i = n; i > 0; i = i/2) { 
       for (j = i; j > 0; j = j/2) { 

Would this be O(lognlogn)? Still, whats the exact formula for the amount of iterations in the inner loop?",,,,,Submission,0,0,0
d251s29,2016-04-16 04:22:54-04:00,_ActionBastard_,,This is logn + (logn - 1) + (logn - 2) +.....   which is O(log^2 (n)),4f0p7j,t3_4f0p7j,Eadpeard,,Comment,2,0,2
d2507h0,2016-04-16 02:49:41-04:00,Eadpeard,,I found a formula (logn+1)(logn+2)..not sure why.,4f0p7j,t3_4f0p7j,Eadpeard,,Comment,1,0,1
d257uk4,2016-04-16 10:07:47-04:00,not_the_irrelevant,,"the outer loop will run for logn time and for each value of i,  inner loop will run for logn time.  therefore time complexity of this code is O(log²(n)).

also i would suggest you to not by-heart formulas,  it is difficult to remember such things.  better is to understand the why and learn how to solve the generic questions.  you can never be sure of what type of question you will have to face. ",4f0p7j,t3_4f0p7j,Eadpeard,,Comment,1,0,1
4exxgx,2016-04-15 13:56:39-04:00,joshiposhi96,"Concerned about electrics, wiring problem?","My computer used to randomly power-off. I think it couldn't get enough energy somehow. I eventually stopped using that pc but i'm thinking about purchasing a new computer soon. I want to know if it is a known problem for computers to randomly have a sudden shut-down, and how to circumvent that problem (i already had the machine plugged directly in the wall socket, so no crappy power cords would kill energy flow). I live in Europe (netherlands) in a relatively new house (built around 2000), i don't know if that information helps you. I hope you can help",,,,,Submission,0,0,0
d249yxx,2016-04-15 14:05:02-04:00,artillery129,,"If you're worried about dirty power, or insufficient power, use a UPS:

http://www.amazon.com/b?node=764572

I'm sure there's an equivalent product for sale in NL


as an aside, this is not a computer science question",4exxgx,t3_4exxgx,joshiposhi96,,Comment,3,0,3
d24dsz3,2016-04-15 15:30:04-04:00,njaard,,"No, a UPS will not fix this, the power supply could b insufficient. There's a dozen things that could cause random poweroff.",4exxgx,t1_d249yxx,artillery129,,Reply,1,0,1
d253flc,2016-04-16 06:16:38-04:00,artillery129,,"I only said, if he think it's an issue involving the power socket, there are of course many variables like you said, PSU could be damaged etc",4exxgx,t1_d24dsz3,njaard,,Reply,1,0,1
d24hco4,2016-04-15 16:52:20-04:00,videoj,,Try asking at /r/techsupport,4exxgx,t3_4exxgx,joshiposhi96,,Comment,3,0,3
4ew6n9,2016-04-15 06:55:46-04:00,anonlecter,USB-Dashboard on desk[advice],"Hi Guys, 
I want to rebuild my desk, and I would love to have a USB Dashboard( Maybe also with Card readers and Headphone ins )  in top of my desk, kindof inserted into the surface. Now i found several of those on Amazon,however they are just for the frontpanel (therefore with short cables I guess). Do you guys have any ideas on how to do this ? 
Cheers",,,,,Submission,0,0,0
d249d1z,2016-04-15 13:51:45-04:00,Eracoy,,"This isn't the right place to ask this. However, I don't know where the right place is. Anyone else want to recommend somewhere?",4ew6n9,t3_4ew6n9,anonlecter,,Comment,2,0,2
d249fy0,2016-04-15 13:53:30-04:00,TopherMiami,,perhaps either /r/pcmasterrace or /r/battlestations,4ew6n9,t1_d249d1z,Eracoy,,Reply,3,0,3
4enm2q,2016-04-13 16:46:02-04:00,jt663,What's the big O complexity when calculating the dual Cartesian Product of 2 sets?,4ab?,,,,,Submission,8,0,8
d21xoj7,2016-04-13 20:33:38-04:00,katkov,,Just O(ab). The multiplication of constants isn't taken into consideration,4enm2q,t3_4enm2q,jt663,,Comment,8,0,8
d228x5d,2016-04-14 01:29:44-04:00,JVO1317,,"Shouldn't it be O(n^2)?
I've never seen the use of specific values ....",4enm2q,t1_d21xoj7,katkov,,Reply,2,0,2
d229637,2016-04-14 01:39:02-04:00,zardeh,,"The sets can be different sizes, if one is of size 6, and the other size 7 billion, neither ""36"" nor 4*10^(19) tells the true story.

This is also why you'll see O(|V|+|E|) or O(|V||E|) on graph algorithms, even though for a dense graph |E| = |V|^2, because the behavior of various algorithms can be vastly different on dense vs. sparse graphs and as a result there are variants that can work well on either.",4enm2q,t1_d228x5d,JVO1317,,Reply,7,0,7
d22ad85,2016-04-14 02:30:38-04:00,JVO1317,,"thanks, I didn't remember that.",4enm2q,t1_d229637,zardeh,,Reply,2,0,2
d21xq27,2016-04-13 20:34:37-04:00,jt663,,ahh okay thankyou that makes sense.,4enm2q,t1_d21xoj7,katkov,,Reply,1,0,1
4emxup,2016-04-13 14:29:18-04:00,theacorneater,Is there a video-tutorial somewhere for building a Facebook messenger bot?,I'm unable to follow the the getting started tutorial on the Facebook developers page. I got stuck in the initial steps where I need to enter a callback URL for setting up webhook. Am I required to direct the callback URL to my facebook page or my own personal URL? I am lost. Please help. ,,,,,Submission,1,0,1
4ejjzk,2016-04-12 22:29:36-04:00,Total_InZAINity,Help With Running a Jar on CD Insert,I have a CD-R with a .jar file on it. The jar file needs to be in the same folder as a couple images and another jar file to run correctly. Is there a way to make it so that when the disc is inserted into a PC it will automatically execute the jar file? I run windows 10 if that helps at all. ,,,,,Submission,0,0,0
d20yay5,2016-04-13 05:30:42-04:00,gstuartj,,"Sort of. Windows has autorun.inf files for doing this and defining other default actions, but it's been disabled by default for years due to security reasons.",4ejjzk,t3_4ejjzk,Total_InZAINity,,Comment,1,0,1
4ej9j6,2016-04-12 21:16:13-04:00,furrycow779,c++ help with Visual Studio 2015,"Hello all, I am working on a final project for my intro comp sci class and ive hit a pretty big wall. I am not sure if this is the correct sub for this kind of post but here we go anyways. When i go to run my code it gives me a pop up window saying ""Unhandled exception at 0x77575B68 in finallab.exe(my file name for my project): Microsoft C++ exception: std:: out_of_range at memory location 0x0055F4EC."" the only options it gives me is three buttons, ""Break"" "" Continue"" and ""Ignore"" but ignore is not clickable. Any help would be amazing because i have tried looking all of the web for answers but have not found anything to help. ",,,,,Submission,0,0,0
d20o8u4,2016-04-12 22:25:51-04:00,TheGrue,,"This isn't really the place for this question, as this is more of a theoretical subreddit.  The better place to go would be to a C++ specific subreddit, there seem to be several, including some targeted at beginners. (Just google 'C++ subreddit'.)

To your question, I believe that the error you are getting is that you are attempting to access memory that was not allocated to your program.  This tends to crop up in beginner programs when you step through an array, but go one too far, and step off the end.  

I.E. you have an array with 4 values, but you attempt to read a fifth value - since there isn't one, the memory at that location isn't yours to read/write, resulting in an out of range exception.  Remember that if you have an array of length 4, those are addressed as values at indexes 0, 1, 2, and 3.  If you try to read a value at index 4, that would be the fifth position in your array.  If your array doesn't actually have a length of five, you will get this exception.

You cannot continue through an out of bounds exception because it indicates that your program is attempting to read/write memory that does not belong to it, which is a fatal error and not recoverable.",4ej9j6,t3_4ej9j6,furrycow779,,Comment,3,0,3
d20oxnj,2016-04-12 22:45:00-04:00,furrycow779,,Thank you for the input I really appreciate it and I will try posting on a c++ subreddit. ,4ej9j6,t1_d20o8u4,TheGrue,,Reply,1,0,1
4ej8ot,2016-04-12 21:10:40-04:00,eagle1-2,I built a node.js app. How do I get it on to the web?,"I have a nodes web app that functions fine on localhost 3000. I know I need a virtual server, and I already have a domain. What is the best way that my app apperas when i or anyone types in my domain. I have read alot about dns and tried setting it up on aws and google cloud but failed. What is the simplest way?",,,,,Submission,12,0,12
d20u16n,2016-04-13 01:33:26-04:00,Roybot93,,"Why hasn't anyone mentioned heroku? Seriously it is practically made for fast and easy and free cloud hosting. 

It works together with git so you can push your code just like you would push to a github repo. No configuration needed. 

Once you do that you can go to wherever you bought your domain name and modify the dns records to point to your heroku app. Then update the domain settings on heroku to include your new domain name. Google for the details. 

If you wanna make it a bit hard for yourself sure you can go with a manual deployment. A vps with digital ocean is nice. ",4ej8ot,t3_4ej8ot,eagle1-2,,Comment,9,0,9
d20of17,2016-04-12 22:30:35-04:00,d0m58,,"Typical modern workflow for manual deployment:

Step 1: get cloud-hosted computer - something like AWS, digitalocean or heroku. The different hosting providers basically offer different levels of control you can have over the machine, all the way from a dedicated machine they just keep running and connected to machines that dynamically resize based on the load.

Step 2: install dependencies - install things like node and git using 'apt-get install' if you choose a common Linux OS like Ubuntu (recommended, if only for the sheer number of stackoverflow answers you can find)

Step 3: put your code on the machine - use 'scp' or 'got clone' a repository 

Step 4: run the server program - same as you would on localhost, but if you want to serve HTTP over the Internet like the rest of the web, use port 80. You'll want to keep it up and running with something like 'forever' from npm, or by using 'screen' to leave it running in a detached screen

Step 5: set your DNS - point your domain to the IP address of the computer",4ej8ot,t3_4ej8ot,eagle1-2,,Comment,6,0,6
d20txm7,2016-04-13 01:29:22-04:00,squall14414,,"How would you do this if you wanted to keep your own server instead of a cloud based computer?

Edit: nm clearly just steps 4-5 ",4ej8ot,t1_d20of17,d0m58,,Reply,3,0,3
d20u558,2016-04-13 01:38:10-04:00,zefcfd,,heroku,4ej8ot,t3_4ej8ot,eagle1-2,,Comment,2,0,2
d20ma01,2016-04-12 21:33:48-04:00,Syde80,,"I'd suggest you look for a hosting provider that specifies they will host node.js apps.  That way you don't have to worry about managing the operating system, DNS, etc.

Basically you will sign up with whoever, they'll tell you where to point your domain to, give you a login & pass to upload your code and that's about it.",4ej8ot,t3_4ej8ot,eagle1-2,,Comment,1,0,1
d215t7b,2016-04-13 10:22:34-04:00,dxk3355,,"If you want to do home hosting you'd need a static ip from your ISP ($$), set the DNS to that from whomever you bought your hostname, open a hole in your firewall/router, and make sure you route the traffic for that service to your server pc. ",4ej8ot,t3_4ej8ot,eagle1-2,,Comment,1,0,1
d23kvrl,2016-04-14 23:41:15-04:00,theacorneater,,Get digital ocean. You'll have a virtual Linux machine on there or whatever OS you prefer,4ej8ot,t3_4ej8ot,eagle1-2,,Comment,1,0,1
d28sgo5,2016-04-19 08:52:29-04:00,zthrowaway5,,"Others have given you some good advice so far, but I will throw in something for you to check... When I went to publish my first app, I totally forgot to save my dependencies while developing it. I had to go back through and install each one, one by one. So, before you deploy to a server make sure your package.json has everything you need. Then deployment is as simple as pulling down your repo on the server and running ""npm install"". ",4ej8ot,t3_4ej8ot,eagle1-2,,Comment,1,0,1
4eh7tb,2016-04-12 13:40:56-04:00,CaptainSymph,I'll be a senior next year with freshman level experience and I need advice,"Hey there! I'm in a bit of an odd situation. I transferred to my university after getting an AA. I'm going to be senior-status at my university next year, but essentially only have (approximately) freshman level experience with CS from courses this year. This is made more difficult due to my university heavily reorganizing their CS program, and changes being put in starting next year.

As an example, the current CS class I'm in is a data structures course. One that's a prereq. for just about any of the other CS courses here. This wouldn't be a problem, except next year I'm required to take the senior project capstone courses. I'll be doing it alongside core courses, but I'm coming into this a little nervous. I'm a decent CS student, but I'm nowhere near senior level. It's encouraged to work in groups for the senior projects. I'd be so far behind everyone else I'm not even sure what I could contribute while not knowing some of what they know.

The plan is to just do projects as much as I can this summer when I'm not working, internships aren't really an option for me due to location. Is there anything else I should do to prepare? Anyone who has done these kind of senior projects know what I should be prepared for? I plan on asking my main CS professor as well, but since next year is going to be hectic, I figured I could use all the advice I can get.",,,,,Submission,11,0,11
d20437h,2016-04-12 14:22:03-04:00,sullage,,"Sounds like imposter syndrome. It is common in fields where absolute expertise is impossible and group deliverables are required.
You'll feel equally unprepared your first few years after college. It's ok. Remind yourself that no one expects you to know everything, and the only way to make progress is to continually put your best foot forward. 
The only people who don't feel this way at some point are people who never challenged themselves. ",4eh7tb,t3_4eh7tb,CaptainSymph,,Comment,10,0,10
d20nqrl,2016-04-12 22:11:57-04:00,videoj,,"It sounds like your AA didn't prepare you for being a junior.  It also sounds like you're a year behind.  Maybe you need to consider taking an extra year to graduate, so you can catch up and take all the coursework you'll need.  
",4eh7tb,t3_4eh7tb,CaptainSymph,,Comment,1,0,1
d20pj59,2016-04-12 23:02:00-04:00,CaptainSymph,,"I probably should've mentioned, I'm already set to take two extra quarters (fall-winter) after next year, and graduate following that. Not this next year. Even though we could condense all the courses needed into one year, it doesn't make sense to and would be difficult. I have all my generals, it's just the core CS courses that I'm missing.",4eh7tb,t1_d20nqrl,videoj,,Reply,1,0,1
d21bb12,2016-04-13 12:22:36-04:00,xiongchiamiov,,"As an aside, take a search through r/cscareerquestions: lots of similar stuff in there.",4eh7tb,t3_4eh7tb,CaptainSymph,,Comment,1,0,1
4efr7b,2016-04-12 08:01:34-04:00,BikingFresh,Computers creating Natural Laws from Experimental Data has inspired me to get a degree in CS. What can I do so that one day I can do research on this subject? (link to paper in description),"This paper: [Distilling Free-Form Natural Laws from Experimental Data](http://creativemachines.cornell.edu/natural_laws) is my inspiration for starting a degree in CS. What can I do so that one day I can do research on this subject? 

What are the books I should read or courses I should take? A
re there any dissertations out there on this subject specifically? How would I find those? I'm really excited about this and the prospects of automation used in the scientific method.
",,,,,Submission,2,0,2
d20farf,2016-04-12 18:30:47-04:00,videoj,,"This sounds like a form of [genetic programming](https://en.wikipedia.org/wiki/Genetic_programming), which is a technique for involving programs (or equations).    It is one technique of the field of machine learning.

The simplest way to get started is to learn the Python language and try libraries like [PyBrain](http://pybrain.org/)  or [SciKit-learn](http://scikit-learn.org/stable/).  You can also post over at /r/machinelearning and they can point you to some more current research.",4efr7b,t3_4efr7b,BikingFresh,,Comment,2,0,2
d1zuv9v,2016-04-12 11:01:42-04:00,ToplessTopmodel,,"Look at fft fast fourir transform

Learn to code. Everyone expects you to code in your freetime. Java is big at my uni but python will keep you motivated. Cpp is hard work and takes the most time before you set something up. (But runs fastest if you know what you do).

Get linux to learn how a computer works. Really an open OS is great for learning. Dual boot if you are not used to it.

Learn to use git. You will cry and hate everyone. But learn it. You will see why after half a year.
",4efr7b,t3_4efr7b,BikingFresh,,Comment,1,0,1
d209w5z,2016-04-12 16:25:46-04:00,ollee,,Also....in Linux...back everything up so you're not afraid to try things and break your OS. Some of the best learning experiences I have had were corrupting something or another then going through and figuring out how to fix it. It was fun. I swear.,4efr7b,t1_d1zuv9v,ToplessTopmodel,,Reply,2,0,2
4ef6rg,2016-04-12 04:19:28-04:00,flibbell,Physics vs. Chemistry?,"I'm a first year major in CS and trying to meet class requirements needed to transfer to a university. Nearly all of the schools want you to complete 1-2 classes in the physics OR chemistry series. 

Is knowing one subject more beneficial than the other for CS or should I try to take a subject that seems more interesting/easier? If any of you have taken these classes in college and have any advice that would be great!",,,,,Submission,9,0,9
d1znf2i,2016-04-12 07:12:10-04:00,ggkimmiegal,,"Whichever is more interesting to you. If you are interested, then you are more likely to pass the class(es) with good grades. ",4ef6rg,t3_4ef6rg,flibbell,,Comment,6,0,6
d1zsd68,2016-04-12 10:01:57-04:00,CelticJoe,,"I found Physics more useful: a major part of physics 101-201 is covering electricity and thermodynamics, both of which are more closely related to the physical side of computing than Chem.  I also have always been particular to physics as it basically translates to ""practical math"" to me.

If you could care less about the hardware side of things then it really doesn't matter, which ever subject intrigues you the most I guess. ",4ef6rg,t3_4ef6rg,flibbell,,Comment,6,0,6
d20ey7k,2016-04-12 18:21:59-04:00,cajun_super_coder2,,I think another argument for physics is game design and simulations.  You're more likely to run into a physics engine than a chemistry engine (????).,4ef6rg,t1_d1zsd68,CelticJoe,,Reply,2,0,2
d1zyvm2,2016-04-12 12:30:08-04:00,rm-rf_,,"Physics will be far more beneficial IMO. Physics focuses on applied math and logic to solve problems similar to CSE. Chemistry is much closer to the field of Biology where the skills tested are the ability to soak up and apply knowledge - something still useful, but less so in CSE. In terms of difficulty, they will both be challenging in their own way. The relative difficulty will be much more dependent on your current strengths and the professor teaching the course.

Source: Physics and Chemistry graduate (now working in CSE field) ",4ef6rg,t3_4ef6rg,flibbell,,Comment,3,0,3
d1zw6gf,2016-04-12 11:31:15-04:00,None,,Pick the one that you like the most. At the end of the day it's not that important: both would be beneficial in some ways.,4ef6rg,t3_4ef6rg,flibbell,,Comment,2,0,2
4edzm1,2016-04-11 21:53:35-04:00,noobeeee,Programming Question: histogram the array and then print out the peak solution,"Hi,

I have the following inputs.

matches = [(doc_id, start_time, end_time), ....]

for example, matches = [(1, 44, 55), (2, 33, 99), (1, 45, 56), (1, 66, 77), (2, 88, 99), (1, 22, 24)]

I need to histogram it based on the time_delta (end_time - start_time) and then print out the peak doc_id and largest end_time from doc_id.

After histogram, I'll have the following results.

[Image](http://i.imgur.com/V2q6EUT.png)

From the image, you can see the doc_id is the solution with count = 3 and largest_end_time out of that peak = 99.

How can I do so efficiently?

Please provide code in any language.

Thanks.  ",,,,,Submission,0,0,0
d1zmeq1,2016-04-12 06:14:40-04:00,fa21,,"You could create a class „TimeDelta“

    Create class TimeDelta{
        delta int
        count int
        docIDs int[]
    }

In the main class you loop through the array and create for every new calculated delta value a class and store them in a list. If a value was already in a previous loop, you increase the count variable and store the docid in the array.

Main Class:

    createList<TimeDelta> deltaList
    For every ArrayItem{
        tempDelta = calcDelta(start_time, end_time)
        if tempDelta is a new value then{
            create object TimeDelta(tempDelta, 1, docId)
            append object to deltaList
        else (tempDelta is already in the list) 
            appendToObject(DocId) // (increase variable count and append the docId to the object)
        }
    }
Then sort the list by the count variable. 

For finding the largest end time loop through the array and check for the highest value.

    tmpMax = 0
    DocID = 0
    For every ArrayItem{
        if end_time > tmpMax then{
            tmpMax = end_time
            DocID = doc_id
    }
Or just sort the array by the end time desc and pick the first one. 

I hope it will help you to find a solution. 

I think, there is a better way to do it but i´m not a pro and that was just my first idea. 
",4edzm1,t3_4edzm1,noobeeee,,Comment,2,0,2
d2072ju,2016-04-12 15:25:26-04:00,noobeeee,,This would be O(n^2). And it seems pretty bad? Maybe I should use dictionary to store the states.,4edzm1,t1_d1zmeq1,fa21,,Reply,1,0,1
d20xqfq,2016-04-13 04:52:24-04:00,fa21,,"That is true, it´s pretty bad. 

Maybe you could use [hash](http://www.cplusplus.com/reference/unordered_map/unordered_map/).",4edzm1,t1_d2072ju,noobeeee,,Reply,1,0,1
4eb88j,2016-04-11 11:38:38-04:00,smugglingpeanuts,"Starting a degree in computer science at 23, what can I do for the next 6 months to prepare?","I'm really keen to excel at university. I have been out of education for a few years now but got top grades in Biology, Chemistry and Physics as well as an AS level in maths. (Half as much knowledge as the sciences). Computer science was never offered at my school. 

What can you recommend I learn before I go back so that I have the best chance of a head start and a smooth transition back into academic learning? 

Thanks a lot :) ",,,,,Submission,15,0,15
d1yuthh,2016-04-11 15:46:07-04:00,johnhutch,,"A little different than what everyone else is suggesting, I would say learn *nix. Get comfortable on the shell. Wether it's mac or linux or something else (I know nothing about Windows' new bash thinger, but maybe there?). Unless you plan on programming exclusively in the microsoft universe with .net and whatnot (don't do this), you will at some point find yourself relying more and more on the shell -- whether it's for git operations, or SSH-ing to set up a remote server, doing ruby bundler stuff, or grepping project folders or any number of things. 

By and large, programming happens in the shell -- if not entirely, then in part. As a working web developer, I can tell you that my coworkers and I either spend our entire day in the shell (even using shell-editors like vim or emacs) or they jump through a lot of hoops (that must be re-jumped with every software/server update) to avoid it. So just learn it. You'll be grateful. ",4eb88j,t3_4eb88j,smugglingpeanuts,,Comment,9,0,9
d1z7cob,2016-04-11 20:46:27-04:00,dinaturaltransformat,,"I followed this plan unintentionally and it was super useful. Similarly, learn some basic sysadmin stuff like setting up an Apache server and getting it to serve static and dynamic content. This will make it much easier to not fight OS/server when you are programming for it later.

Also, learning a command-line editor (vim or emacs), version control (git), and LaTeX before they were necessary was super useful for me because it meant that by the time I had to use those tools, I could focus on the problem at hand instead of fighting the tools. (Also, TAs really appreciate nicely typeset homework.)",4eb88j,t1_d1yuthh,johnhutch,,Reply,1,0,1
d1z2rqg,2016-04-11 18:49:32-04:00,neujersey,,"I agree very much.  Any first year program will teach you the language(s) you need - but is often lacking in the not directly academic aspects like working with a linux shell.  A first-year student would do well to learn that stuff and avoid wasting time futzing with the environment.

I recommend [How Linux Works](http://www.amazon.com/How-Linux-Works-Superuser-Should/dp/1593275676/)",4eb88j,t1_d1yuthh,johnhutch,,Reply,1,0,1
d1z6r6j,2016-04-11 20:31:49-04:00,johnhutch,,"I was lucky enough to go to a school (Rowan) with a ""Lab Techniques"" class for first and second years that was billed as a ""how to use the computer lab"" class but was essentially a ""how to use unix"" class. Of all the classes I had in college, it was probably the single most useful. ",4eb88j,t1_d1z2rqg,neujersey,,Reply,2,0,2
d1yqhzn,2016-04-11 14:13:56-04:00,crookedkr,,Relax. Once it starts it will probably get pretty intense with or without starting now so you should relax while you can. ,4eb88j,t3_4eb88j,smugglingpeanuts,,Comment,7,0,7
d1yyv8n,2016-04-11 17:15:01-04:00,bistonut,,"I imagine brushing up on maths skills will be thing to go for. UK unis don't assume any prior programming skills, and remember, comp sci =! programming. Of course there's loads in there, but I think first year stuff will be split between intro to Java/python/C, and theoretical fundamentals. And these require maths.

Not an expert in what types of maths mind you, someone else might suggest that. But if you're up for learning some new stuff beyond the AS, search ""Linear Algebra"", which I know is used a lot. Basically the maths of vectors and operators.",4eb88j,t3_4eb88j,smugglingpeanuts,,Comment,3,0,3
d1zkpky,2016-04-12 04:22:45-04:00,Dassy,,"I second this. My program was primarily focused on math and the more theoretical aspects of CS. Math especially in the first year.

Learning to program was the easiest part for me, and I hadn't done terribly much beforehand. If you do want to get a head start in programming however I would suggest trying to do an internship, or something similar that force you to learn quickly (and maybe gets you some money saved up)",4eb88j,t1_d1yyv8n,bistonut,,Reply,1,0,1
d209j77,2016-04-12 16:18:28-04:00,ollee,,"I find the both single and multi-variable calculus have been extremely helpful. Additionally Discrete, Linear Algebra probably have helped me the most. Numerical Analysis is next on my list of courses to take. 

It hasn't just been the actual math that has been important, so much as how the problems are approached that have helped me approach problem solving while actually doing other CS things.",4eb88j,t1_d1yyv8n,bistonut,,Reply,1,0,1
d1ykwr3,2016-04-11 12:11:51-04:00,Teemperor,,"Start programming, see /r/learnprogramming. Most CS lectures require programming knowledge for the exercises and starting as soon as possible is the best preparation IMHO.",4eb88j,t3_4eb88j,smugglingpeanuts,,Comment,6,0,6
d1ywt3i,2016-04-11 16:29:22-04:00,ilikestring,,"Not really sure that's necessary. He'll likely just pick up bad habits which will have to ironed out when he starts his degree.

I personally think it's better to start reading a book on data structures or algorithms. This sort of stuff is much harder to get your head around initially than programming. Picking up the basics would be a great help and you can't get it wrong",4eb88j,t1_d1ykwr3,Teemperor,,Reply,2,0,2
d1yx9e1,2016-04-11 16:39:26-04:00,gyroda,,"Eh, it might make the first few weeks easier to pick up the basics of functions and loops. I know it did for me, which was useful considering that university was a big change for me. ",4eb88j,t1_d1ywt3i,ilikestring,,Reply,1,0,1
d1ylbhl,2016-04-11 12:20:58-04:00,stop_ttip,,"Learn to program in python. Maybe study some linear algebra, calculus and statistics.",4eb88j,t3_4eb88j,smugglingpeanuts,,Comment,4,0,4
d1yt3xy,2016-04-11 15:09:38-04:00,potifar,,"A couple of non-programming suggestions that may be obvious to people more structured than me:

* Work. Try to save up some money if you can. Economic insecurity and work commitments during the semester can be stressful.
* If possible, take a peek at the curriculum and/or lecture notes. It might give you some ideas about stuff that would be beneficial to learn ahead of time. What programming languages will you learn? What tools will you be using?
* Prepare your work environment. Get your editor(s)/IDE set up, familiarize yourself with source control (Git + GitHub is a safe bet), make sure you have a solid productivity management system in place and practice using it. Todo list/calendar etc.
* Take some proper time off before the semester starts. Wind down, get rested.",4eb88j,t3_4eb88j,smugglingpeanuts,,Comment,2,0,2
d1yvpu0,2016-04-11 16:05:18-04:00,Alx_xl,,"I'd suggest refresh your math skills.  

Don't know why everyone suggest programming.  
It's one of the easier things to learn.

Maybe take a look into physics and electronics",4eb88j,t3_4eb88j,smugglingpeanuts,,Comment,2,0,2
d1ynsvj,2016-04-11 13:15:37-04:00,ObeselyMorbid,,"Java,  c, and c++. ",4eb88j,t3_4eb88j,smugglingpeanuts,,Comment,3,0,3
d1yvuzl,2016-04-11 16:08:24-04:00,arnaudh,,Start with learning C. Can't go wrong.,4eb88j,t3_4eb88j,smugglingpeanuts,,Comment,1,0,1
d1yzlvr,2016-04-11 17:31:53-04:00,XisuperninjaiX,,Could look at what modules you will be covering in the first year and have a quick look at what they cover,4eb88j,t3_4eb88j,smugglingpeanuts,,Comment,1,0,1
d1z5on7,2016-04-11 20:04:48-04:00,DoktorJeep,,"Given your background in the sciences and time till you start classes, I'd ask you to consider reading Donald Knuth's Art of Computer Programming. There are something like 4 volumes, so it's definitely a challenge. But, if you're interested in getting a low level understanding of how a computer can be taught to think, you'd be hard pressed to find another option that is similar in format or subject.",4eb88j,t3_4eb88j,smugglingpeanuts,,Comment,1,0,1
d21mtpa,2016-04-13 16:20:23-04:00,None,,"Codecademy.com

You might not need to learn all of the languages, but at least you can get a handle on typing, syntax and Object Oriented Programming.",4eb88j,t3_4eb88j,smugglingpeanuts,,Comment,1,0,1
d1yn58q,2016-04-11 13:01:05-04:00,KronktheKronk,,"Jump on github, and try your very hardest to use your learning time to contribute anything at all to some projects on github.

It's going to be the thing that differentiates you from 85% of people getting CSC degrees in your class.",4eb88j,t3_4eb88j,smugglingpeanuts,,Comment,1,0,1
d1yn6wy,2016-04-11 13:02:07-04:00,spatialdestiny,,"On top of the development suggestions, I'd also recommend an introduction to SQL and data modeling.  It will help you get started on knowing how to organize data along with their relationships.",4eb88j,t3_4eb88j,smugglingpeanuts,,Comment,1,0,1
d1yp3l7,2016-04-11 13:43:29-04:00,revvupthosefryers,,DO NOT learn to program in Python as your first language. Ignore that suggestion. Follow the others. ,4eb88j,t3_4eb88j,smugglingpeanuts,,Comment,-1,0,-1
d1yrmpw,2016-04-11 14:37:54-04:00,jaredub69,,Why not? It's easy to pick up and learn the basics. ,4eb88j,t1_d1yp3l7,revvupthosefryers,,Reply,2,0,2
d1yu76r,2016-04-11 15:33:00-04:00,revvupthosefryers,,It's way abstract to learn as your first language. Top down learning doesn't work when you're starting out. ,4eb88j,t1_d1yrmpw,jaredub69,,Reply,1,0,1
d1yv2fo,2016-04-11 15:51:24-04:00,johnhutch,,"There are lots of very smart people who disagree with you. I don't have an opinion on who is correct, but I sure as shit know you shouldn't be so damn sure of yourself.",4eb88j,t1_d1yu76r,revvupthosefryers,,Reply,0,0,0
d1yvl2z,2016-04-11 16:02:28-04:00,revvupthosefryers,,Cool ,4eb88j,t1_d1yv2fo,johnhutch,,Reply,0,0,0
d1zi4c3,2016-04-12 02:03:25-04:00,potifar,,[MIT disagrees](https://www.edx.org/course/introduction-computer-science-mitx-6-00-1x-7#!).,4eb88j,t1_d1yu76r,revvupthosefryers,,Reply,0,0,0
d1zjh0c,2016-04-12 03:09:25-04:00,revvupthosefryers,,Cool,4eb88j,t1_d1zi4c3,potifar,,Reply,-1,0,-1
d1ytull,2016-04-11 15:25:30-04:00,5225225,,What's wrong with python? It was my first real language.,4eb88j,t1_d1yp3l7,revvupthosefryers,,Reply,1,0,1
4eakv7,2016-04-11 08:59:21-04:00,noobeeee,The best way to store word_indexes for search engine in MySQL?,"Hi,

Which one would be better choice to store word_indexes for search engine?

1.

    word_index | doc_id | term_freq
    reddit | 1 | 3
    reddit | 2 | 1
    computer | 1 | 1
    bar | 1 | 3
    stack | 2 | 1
    startup | 2 | 4
    price | 1 | 3
    price | 2 | 1

However, this list will grow into very big - billions of rows.

Or
 
2.

    word_index | docs_id_tf
    reddit | {1, 3}, {2, 1}
    price  | {1, 3}, {2, 1}

This will reduce the number of rows, however, it will make it hard to delete rows and to do other operations.


What method is better from search engine inverted index perspective?

Thanks. ",,,,,Submission,7,0,7
d1yehd9,2016-04-11 09:27:01-04:00,Chappit,,"The first one is absolutely the way to go. The second is not normalized.

A database can handle a tremendous amount of rows before its slow. When it gets to be enormous you can just split your data across databases and it will remaim decently fast. All the ""A"" words in one database for example. ",4eakv7,t3_4eakv7,noobeeee,,Comment,1,0,1
d1yexuw,2016-04-11 09:41:25-04:00,noobeeee,,"I see. I can use distributed databases to store that as well - Cassandra etc. 

Another worry is that after I had billions of rows, when I query for ""Reddit"" for example, the results will be hundred thousands of rows. And going through hundred thousands of rows to find the best articles seems redundant. What else can i do to do so efficiently? 

Thanks.",4eakv7,t1_d1yehd9,Chappit,,Reply,1,0,1
d1ygqxp,2016-04-11 10:32:07-04:00,BonzoESC,,"Sort by `term_freq`. You can and should benchmark this, maybe with a Wikipedia dump. Some possibilities would be 

1. no indexes
2. an index on `{word_index}`
3. an index on `{word_index}` and an index on `{term_freq}`
4. an index on `{word_index, term_freq}`
5. an index on `{word_index}` and an index on `{word_index, term_freq}`",4eakv7,t1_d1yexuw,noobeeee,,Reply,2,0,2
d1yf0gy,2016-04-11 09:43:37-04:00,Chappit,,I mean can't you search with a where clause that will automatically yield the best result? That would leave most of the heavy lifting to the database. ,4eakv7,t1_d1yexuw,noobeeee,,Reply,1,0,1
4e72es,2016-04-10 15:01:04-04:00,murtaza64,How does a processor know how many bytes to read from memory? (and other questions),"Hey guys, I'm trying to understand how CPUs work and interact with memory. From what I understand, each memory address corresponds with one byte of data in memory. Therefore, an integer (32 bits) will span 4 memory addresses. Now lets say you wanted to add two such integers. A couple of questions here, assuming a word size of 64 bits:

1. Does the processor load both whole integers into registers then add them, or does it break it up into smaller pieces?

2. How does the processor 'know' how many bytes to access from memory? What if the integer is only 2 bytes?

3. In C, when you increment the value of a pointer by one, are you literally accessing the next byte of memory? Or is it that the increment depends on the data type of the pointer, so incrementing an integer pointer by one will take you to a memory location 4 bytes ahead (e.g. 0x0000 -> 0x0004)?

Finally, are there any specific sources (books, articles, videos, webpages) that you would recommend for learning more about this kind of stuff (computer architecture I guess)?
",,,,,Submission,18,0,18
d1xlhha,2016-04-10 15:45:25-04:00,pencan,,"1. Each integer will be loaded into a single register. I think there's some funky stuff that can be done if you have 32 but registers and are emulating 64 bit mode, but that's not the common case. 
2. Depends on the instruction. You can do a load word, load byte, load long etc.  The processor will mask the higher bits to 0 before loading. (and not affect the higher bit contents on store) 
3. Depends on the pointer type. This is a compile time decision, not a runtime one. If you have a pointer to 32 bit ints,  then cast it to a char* it will only increment by 1 byte each time. ",4e72es,t3_4e72es,murtaza64,,Comment,5,0,5
d1xzc4u,2016-04-10 22:03:48-04:00,murtaza64,,Thanks for the answer. What if your integers are 64 bits on a 32 bit system? Or is that not an operation you can do? ,4e72es,t1_d1xlhha,pencan,,Reply,2,0,2
d1y0awj,2016-04-10 22:32:33-04:00,pencan,,"I'm now speaking out of my comfort zone, but I believe some processors support operations on multiple registers. For instance, if you only held 8 bits per register, but wanted to deal with 16 bit ints:



    LDLRG 0x0123 R1, R2 // R1 = 0x01 R2 = 0x23
    LDLRG 0xabcd R3, R4 // R3 = 0xab R4 = 0xcd
    ADDLRG R1, R2, R3, R4, R5 // R5 = 0x0123 + 0xabcd



Obviously, this is not real code and this instruction format wouldn't fit in the ISA, but you get the point.  If the processor itself doesn't support that type of operation, you could emulate it with some hackish assembly.  There's very little you can't do with enough memory and time on a processor (don't smite me Turing)",4e72es,t1_d1xzc4u,murtaza64,,Reply,1,0,1
d1y0gtx,2016-04-10 22:37:33-04:00,craiig,,"You can add 64 bit numbers on a 32 bit system, the compiler usually uses software emulation to split it into 2x32-bit add instructions plus handling the carry bit.",4e72es,t1_d1xzc4u,murtaza64,,Reply,1,0,1
d1y0ook,2016-04-10 22:44:09-04:00,teraflop,,"Sure, you can operate on values that are larger than your processor's native word size. It just takes more operations.

For instance, if your processor has 32-bit registers and you want to add two 64-bit values, you can add the high and low halves separately, making sure to carry the extra 2^32 in case the low half overflows. The compiler knows what types you're working with, and can drop in either the correct inline assembly, or a call to a built-in function that can do the math for you.",4e72es,t1_d1xzc4u,murtaza64,,Reply,1,0,1
d1ygys0,2016-04-11 10:37:40-04:00,lordvadr,,"Let me expand on the other answers here a bit.  If you want to do something that's not supported directly in hardware, that's software's job.  So either the compiler will do it for you if it's simple enough, or you have to write it yourself.  It was a common exercise in CS to write an arbitrary precision mathematical library.  It's pretty easy to do right up until you implement divide--which get's way more complicated.",4e72es,t1_d1xzc4u,murtaza64,,Reply,1,0,1
d1xtjl9,2016-04-10 19:17:33-04:00,gunnarsvg,,A microprocessor design course might be of interest to you. The Coursera [HW / SW interface](https://www.coursera.org/course/hwswinterface) course seems like it'd be the most relevant.,4e72es,t3_4e72es,murtaza64,,Comment,2,0,2
d1y0f25,2016-04-10 22:36:01-04:00,craiig,,"The width of the value to load is determined by the instruction the processor executes. Here's the [mips ISA](http://www.mrc.uidaho.edu/mrc/people/jff/digital/MIPSir.html). Note there's lb (for loading 1 byte), and lw (for loading 1 word, which is 32 or 64 bites). Most ISAs have different instructions for loading other sizes as well. If your ISA lacks an instruction to load a particular width, the compiler will generate a series of load/mask/shift/extend instructions to load the right number of bits into the register.",4e72es,t3_4e72es,murtaza64,,Comment,2,0,2
d1xn7th,2016-04-10 16:29:59-04:00,Acidom,,"Correct me if I'm wrong, usually memory is fetched in pages (kilobytes aka 1000's of bytes) rather than by individual bytes. The thought process being that most of the time stuff is accessed sequentially, and from a practical standpoint from the CPU's POV going to memory takes FOREVER so while we are there we might as well grab a whole bunch of memory addresses because odds are we will have to come back to this region so lets kill many birds with one stone.

Now after you have this page from main memory, you keep it close by, and not in a register close by, but on an upper level cache just in case we need some extra tid bits of information. Now we got it nice and close by, and we do not have to do that dreadful Mordor like journey too main memory.

Hope this helps in whatever way it can.",4e72es,t3_4e72es,murtaza64,,Comment,0,0,0
d1xuk5o,2016-04-10 19:46:23-04:00,None,,"There's the semantics and then the physical execution process. They're different things.

The semantics of memory are set by the load and store instructions in the ISA to move data between registers and memory. Generally they work on register sized values, the natural word size of the processor (eg 64 bits).

Physically those instructions only work between the lowest level cache and the registers. When a load needs to access memory not currently cached, a fixed size block of memory will be moved into an appropriate line/slot in the cache. Commonly cache lines are 64 bytes.

Pages are how the mapping between virtual and physical memory is managed. They're commonly 4kb, though larger sizes are available and used on some processors. Pages are too large to move in and out of the processor as a single unit without too much waste, hence the much smaller cache line size.",4e72es,t1_d1xn7th,Acidom,,Reply,3,0,3
d1xvt08,2016-04-10 20:22:27-04:00,Acidom,,ah thanks dude! much clearer now,4e72es,t1_d1xuk5o,None,,Reply,1,0,1
d1xugxr,2016-04-10 19:43:55-04:00,ad_tech,,"It's fetched in cache lines, actually. On modern processors, cache lines are usually 64 bytes. [Source 1](https://en.wikipedia.org/wiki/CPU_cache#Cache_entries), [Source 2](http://www.aristeia.com/TalkNotes/ACCU2011_CPUCaches.pdf)",4e72es,t1_d1xn7th,Acidom,,Reply,1,0,1
4e6p1k,2016-04-10 13:35:16-04:00,snake1121,Degree advice,"Hey I am currently graduating with an Econ degree, I am currently thinking about getting my MBA-concentration in Information System. Is that going to lead to a high paying job, and is the transition going to be hard?",,,,,Submission,4,0,4
d1xi553,2016-04-10 14:17:19-04:00,Yourothercat,,You would get a better response from /r/cscareerquestions,4e6p1k,t3_4e6p1k,snake1121,,Comment,2,0,2
4e6api,2016-04-10 11:59:13-04:00,Swansonator2,What Does this Computer Science-related Image Mean,,,,,,Submission,19,0,19
d1xgs4r,2016-04-10 13:41:51-04:00,east_lisp_junk,,The layout it uses is a common way to describe the MIPS 5-stage instruction pipeline (including an ALU at stage 3 and pipeline registers between all the stages).,4e6api,t3_4e6api,Swansonator2,,Comment,13,0,13
d1xlmai,2016-04-10 15:48:54-04:00,pencan,,"It's a diagram for a pipelined processor. Each stage represents one phase of the instruction (fetch, decode, execute, memory, write back). The blocks in between are temporary storage between the stages",4e6api,t3_4e6api,Swansonator2,,Comment,8,0,8
d1xfgmn,2016-04-10 13:07:47-04:00,evilbringer,,"From my limited experience as a 1st year cs student, I think the cs/e part signifies computer science and engineering and the bar names are on logic circuits such as multiplexers and adders",4e6api,t3_4e6api,Swansonator2,,Comment,4,0,4
d1xgomh,2016-04-10 13:39:19-04:00,Swansonator2,,"Thanks, looking up logic circuits, it definitely looks like that. I got the cs/e part but why do they do the ba/r and to/ur?",4e6api,t1_d1xfgmn,evilbringer,,Reply,2,0,2
d1xignj,2016-04-10 14:25:54-04:00,evilbringer,,"No idea, maybe to align with the lines between bars",4e6api,t1_d1xgomh,Swansonator2,,Reply,1,0,1
d1xty6a,2016-04-10 19:29:11-04:00,CSMastermind,,"This is the back of the shirt.  The front is a finite state automata.

http://i.imgur.com/dXU07Iw.png",4e6api,t3_4e6api,Swansonator2,,Comment,2,0,2
d1xd09n,2016-04-10 12:00:54-04:00,Swansonator2,,"Some context: My buddy is study computer science at our university and he post this image related to his computer science bar tour. The top says ""cse bar tour 2016"" but I'm unsure why there are slashes in between random letters like cs/e. Also, Cafe, Darkhorse, etc are all bar names but what are the barriers between each bar and the lines connecting them? Thanks!",4e6api,t3_4e6api,Swansonator2,,Comment,2,0,2
d1xgwcp,2016-04-10 13:44:51-04:00,BearClawWednesda,,"It's been about a year and a half since I did circuit design, but hopefully I have some answers you're looking for.

First, the Mad Mex block is the symbol for a [Multiplexer or Mux](https://en.wikipedia.org/wiki/Multiplexer). All the other blocks just look like ""Black Boxes"". You put some logic in, you get some logic out.     

The gray bars are most likely flip-flops. A [flip-flop](https://en.wikipedia.org/wiki/Flip-flop_%28electronics%29) is used to store information. Without the flip-flops, if you tried to give this circuit input it would immediately race to the end. By using flip-flops you can synchronize the input based on a clock signal. Meaning that the data will only move to the next stage when the clock goes from 0->1 or 1->0 (depending on what you want/need). 

As for the the names up top, those are usually used to denote what stage you are in. As far as I can tell, there is not a special meaning to the ""/""s other than to obfuscate the words or make it look more ""Computer Science-ee"".

Hope that helped!",4e6api,t3_4e6api,Swansonator2,,Comment,2,0,2
d1y41gk,2016-04-11 00:34:03-04:00,ad_tech,,"[Original](http://1.bp.blogspot.com/-hd2EUtOjsVw/TsJi9QNOp9I/AAAAAAAAA5o/2-n4Hux0TsI/s1600/pipeline.png)

The random letters with slashes refer to the names of the pipeline registers. The pipeline stages are IF, ID, EX, MEM, and WB. IF/ID refers to the registers that carry data from the IF stage to the ID stage. The other registers are named similarly.",4e6api,t3_4e6api,Swansonator2,,Comment,1,0,1
d1yejel,2016-04-11 09:28:49-04:00,huhthatscool,,FYI this is for the PSU bar crawl. ,4e6api,t3_4e6api,Swansonator2,,Comment,1,0,1
4e5ld8,2016-04-10 08:16:26-04:00,MBilalZubairi,"I recently learned that since 1998 there have been specially defined prefixes for binary, so why aren't these used?","I read that the International Electro-technical  Commission (IEC) proposed a new set of definitions for prefixes of binary data as to not interfere with the SI units and this proposal was accepted by other international standards bodies.

So 1024 bytes would not be kilobytes( as that would lead to the assumption that it's a 1000 bytes) but rather, Kibi-bytes with symbol Ki.

Likewise Megabytes are Mebibytes and Giga is Gibi.

This was at page 13 in Cambridge International AS and A Level Computer Science Coursebook, published December 2015.

Is this indeed the standard prefix used among the people employed in the field  of computer science? or is the book setting up it's readers to learn a useless and universally incompatible standard?

Because as far as I know, most popular operating systems and web browsers display binary data using the SI prefixes.",,,,,Submission,11,0,11
d1x7y4w,2016-04-10 09:05:15-04:00,zkxs,,"The vast majority of software (OS, file managers, etc.) use the binary sizes, however I've seen very few systems that explicitly use the KiB-type prefixes you point out.  The good news is if you see ""1 GiB"" you know you're dealing with 2^30 bytes.  The bad news is if you see ""1 GB"" there's no standard as to what that means.  You've *probably* got 2^30 bytes, but maybe you've only got 10^9 bytes... 

The only place I see the SI sizes used is on storage devices.  When you buy that 8 GB flash drive you're only getting 8\*10^9 bytes instead of 8\*2^30 , meaning they managed to not sell you 563 MiB of storage, and that's assuming they didn't get their ""8 GB"" figure by rounding up from 7.5 GB.",4e5ld8,t3_4e5ld8,MBilalZubairi,,Comment,11,0,11
d1xgnd1,2016-04-10 13:38:23-04:00,east_lisp_junk,,"> You've probably got 2^30 bytes, but maybe you've only got 10^9 bytes...

I've even seen 2^(10)\*10^6 and 2^(20)\*10^(3) (but these definitions always seem to come from storage device vendors).",4e5ld8,t1_d1x7y4w,zkxs,,Reply,6,0,6
d1xh4uc,2016-04-10 13:51:02-04:00,Bottled_Void,,"Because the people that make computers use the [JEDEC memory standards](https://en.wikipedia.org/wiki/JEDEC_memory_standards).


The people that caused confusion were salemen advertising memory sizes in 1000 multiples to make the numbers seem bigger. They're the ones I blame.",4e5ld8,t3_4e5ld8,MBilalZubairi,,Comment,4,0,4
d1xlzeh,2016-04-10 15:58:32-04:00,thatwasntababyruth,,"Among the other valid reasons given so far, I would argue that a big one is the prior momentum behind the misused SI prefixes. By 1998, they already had decades of usage among the computing community and a few years of popular usage thanks to the Internet. It's really hard to just suddenly change what units you're using (case in point, US conversion to metric). 

Apple actually does use the correct system, by the way. They switched to using the correct sizing of kilo/mega/gigabytes a few years ago (I want to say it was part of the Mountain Lion release, correct me if I'm wrong), and they got blasted for it. Everyone got confused, and fired back at Apple for it.",4e5ld8,t3_4e5ld8,MBilalZubairi,,Comment,5,0,5
d1x8je9,2016-04-10 09:31:33-04:00,BonzoESC,,The quick answer [can be found here](https://xkcd.com/927/).,4e5ld8,t3_4e5ld8,MBilalZubairi,,Comment,7,0,7
d1x8jlp,2016-04-10 09:31:46-04:00,xkcd_transcriber,,"[Image](http://imgs.xkcd.com/comics/standards.png)

[Mobile](https://m.xkcd.com/927/)

**Title:** Standards

**Title-text:** Fortunately, the charging one has been solved now that we've all standardized on mini\-USB\. Or is it micro\-USB? Shit\.

[Comic Explanation](https://www.explainxkcd.com/wiki/index.php/927#Explanation)

**Stats:** This comic has been referenced 2745 times, representing 2.5747% of referenced xkcds.

---
^[xkcd.com](https://www.xkcd.com) ^| ^[xkcd sub](https://www.reddit.com/r/xkcd/) ^| ^[Problems/Bugs?](https://www.reddit.com/r/xkcd_transcriber/) ^| ^[Statistics](http://xkcdref.info/statistics/) ^| ^[Stop Replying](https://reddit.com/message/compose/?to=xkcd_transcriber&subject=ignore%20me&message=ignore%20me) ^| ^[Delete](https://reddit.com/message/compose/?to=xkcd_transcriber&subject=delete&message=delete%20t1_d1x8jlp)",4e5ld8,t1_d1x8je9,BonzoESC,,Reply,2,0,2
d1x8gnh,2016-04-10 09:28:23-04:00,crookedkr,,Its nice to have an explicit difference but it rarely matter in practice. If you are talking about storage or network bandwidth its powers of 10 so they get to sell you less for the same price. Otherwise it powers of 2.,4e5ld8,t3_4e5ld8,MBilalZubairi,,Comment,3,0,3
d1xb47l,2016-04-10 11:03:56-04:00,MCPtz,,"There are some GNU linux command line tools that specifically write out KiB and KB, just in case you actually need to know the difference.

Otherwise, you're almost certainly seeing powers of two, even if it says KB.",4e5ld8,t3_4e5ld8,MBilalZubairi,,Comment,3,0,3
d1x8ws7,2016-04-10 09:47:01-04:00,5225225,,Because they sound stupid.,4e5ld8,t3_4e5ld8,MBilalZubairi,,Comment,3,0,3
4e2grv,2016-04-09 14:46:19-04:00,dev_joy,"Masters, job or independent study?","I am a 25 year old BTech computer science graduate from India. I have been working in a small company for the past 2 years. Although I like working, I would like to pursue higher education.

Should I pursue:

* a full time masters, 
* a part-time course,
* an online course like the Georgia tech OMS CS,
* continue my current job, learning new things, or
* find a job where I can do research, learn new things and earn well

I have worked with Python, JS, CSS, SQL, Linux, Git, project design, development, etc....

I am open to working with any language. But I think I love cryptography. What are the paths that I may take?",,,,,Submission,1,0,1
d1wgq7y,2016-04-09 15:38:45-04:00,gyroda,,You might be better off in /r/cscareerquestions ,4e2grv,t3_4e2grv,dev_joy,,Comment,1,0,1
4e1m6h,2016-04-09 11:18:20-04:00,Illikis,"What would take longer: copying 1024 1MB files, or 1 1GB file?","If one of those takes longer than the other, why?",,,,,Submission,30,0,30
d1w7euv,2016-04-09 11:24:09-04:00,Saetia_V_Neck,,"I'd imagine 1024 1MB files would take longer because you have to fetch file metadata as well as the actual file data. Plus, depending on how much data your buffer can transfer in a single block, you might have unused space in the buffer. ",4e1m6h,t3_4e1m6h,Illikis,,Comment,43,0,43
d1w7shj,2016-04-09 11:35:39-04:00,Illikis,,"That makes sense. I had thought it would be the 1024 1MB files, but couldn't think of why. Thanks!",4e1m6h,t1_d1w7euv,Saetia_V_Neck,,Reply,9,0,9
d1w8bx3,2016-04-09 11:51:51-04:00,5225225,,"In addition to everything else, the 1GB file is likely to be contiguous, the 1K files are going to be in random places on the drive. This only matters on spinning storage though.",4e1m6h,t3_4e1m6h,Illikis,,Comment,22,0,22
d1wbkz2,2016-04-09 13:22:08-04:00,brennanfee,,That's not exactly true.  Fragmentation of the drive (and by extension the file) would have to be taken into account.  I think all we can say is that it is exceeding likely that the 1000 files are in separate places while the 1GB file *might* be contiguous.,4e1m6h,t1_d1w8bx3,5225225,,Reply,13,0,13
d1wc020,2016-04-09 13:33:16-04:00,5225225,,"I said likely.

It does depend how the file was made, but if you defrag, the 1GB file will be in one place.",4e1m6h,t1_d1wbkz2,brennanfee,,Reply,7,0,7
d1wcryx,2016-04-09 13:53:50-04:00,brennanfee,,True.  **If** you defrag.,4e1m6h,t1_d1wc020,5225225,,Reply,1,0,1
d1wdp1y,2016-04-09 14:17:56-04:00,keepdigging,,Or if you use a more sensible filesystem that doesn't fragment files intentionally to begin with.,4e1m6h,t1_d1wcryx,brennanfee,,Reply,3,0,3
d1weirh,2016-04-09 14:40:10-04:00,5225225,,"In some cases, unless you shuffle files around automatically, if you reach high disk space you *have* to fragment.

Though modern file systems like ext4 really don't fragment much.",4e1m6h,t1_d1wdp1y,keepdigging,,Reply,1,0,1
d1wee9w,2016-04-09 14:36:47-04:00,brennanfee,,"I'm unaware of a file system that doesn't have fragmentation at some point.  Most are smart enough to store a file contiguously if possible, but over time and as files get edited fragmentation ensues.  Some of the newer ones do background defrag over time, which is interesting, but still fragmentation is a natural part of file system management.

So, maybe it's me but I'm missing your point.  What filesystem(s) are you referring to?",4e1m6h,t1_d1wdp1y,keepdigging,,Reply,1,0,1
d1wekd6,2016-04-09 14:41:26-04:00,5225225,,"EXT4 hardly needs defragmentation, its more intelligent than NTFS when placing files on disk.

https://superuser.com/questions/536788/do-ext4-filesystems-need-to-be-defragmented",4e1m6h,t1_d1wee9w,brennanfee,,Reply,1,0,1
d1wib82,2016-04-09 16:23:14-04:00,brennanfee,,"Ext4 still has to contend with lots of fragmentation (especially for larger files), so what are you talking about?  It just does what I described and looks for empty ranges that can store an entire file when being written.  NTFS does the same thing.  Ext4 is slightly better because when a file later gets edited it might also move the file to defrag it if necessary (assuming there is a free block range that is large enough).  Gotta love the power of inodes.  NTFS won't do that as the file is being edited but will do it as needed when the computer is idle.  As I said, some of the file systems do work in the background to reduce fragmentation *over time*.  But that doesn't mean they don't have fragmentation.  Nearly all file systems have to contend with this issue, especially as a drive becomes more full, so I am still confused what you are going on about.  

NOTE: I would say ""all file systems"" and drop the nearly but I haven't used or looked at the code for all of them, just most of them.",4e1m6h,t1_d1wekd6,5225225,,Reply,2,0,2
d1x2yf5,2016-04-10 03:44:52-04:00,None,,[deleted],4e1m6h,t1_d1wcryx,brennanfee,,Reply,0,0,0
d1xfv97,2016-04-10 13:18:15-04:00,brennanfee,,That's a big if.,4e1m6h,t1_d1x2yf5,None,,Reply,1,0,1
d1xgbep,2016-04-10 13:29:36-04:00,None,,[deleted],4e1m6h,t1_d1xfv97,brennanfee,,Reply,1,0,1
d1xh3jz,2016-04-10 13:50:07-04:00,brennanfee,,"The install base for spinning disks is still larger than SSD's actually so... even though it is 2016 they are still quite relevant.  http://www.statista.com/statistics/285474/hdds-and-ssds-in-pcs-global-shipments-2012-2017/

Regardless, what I'm talking about applies to spinning disks and SSD's.

I like how you resort to ad hominem without any kind of actual argument.  Given I work with brilliant people and am one myself, I am out of practice dealing with people of your caliber.  [See that's an elegant way of calling someone stupid without just coming out and saying it.]",4e1m6h,t1_d1xgbep,None,,Reply,1,0,1
d1xi1zx,2016-04-10 14:15:04-04:00,None,,[deleted],4e1m6h,t1_d1xh3jz,brennanfee,,Reply,1,0,1
d1xij6a,2016-04-10 14:27:41-04:00,brennanfee,,"> You've got issues with selfesteem.

Nope.

> I'd also put my money on you being mediocrely-successful nerd that craves for peer-approval.

And wrong.

See, you can't just read a few words and think you understand someone.  Instead, you might do better focusing on the conversation rather than the person.  What you failed to realize in the comment you were incensed by is that I was agreeing with him.  I said, ""True"".  And them emphasized the ""if"" you defrag in his comment to punctuate where I was agreeing.  There is nothing wrong with that.  We, that is those of us that were ACTUALLY FUCKING IN THE CONVERSATION, were coming to an agreement - an understanding.  I doubt you have much experience with that which might be the source of your confusion or frustration.  [Another subtle way of calling someone stupid, see it's not hard if you think before you type.]

""I can explain it to you, but I can't understand it for you.""

tata",4e1m6h,t1_d1xi1zx,None,,Reply,0,0,0
d1w87ke,2016-04-09 11:48:14-04:00,PastyPilgrim,,"The 1024x1mb scenario is the reason that we have zip files. There's a lot of overhead in copying many files, so if you flatten the file structure and put everything in one box, then you can move the data more efficiently. ",4e1m6h,t3_4e1m6h,Illikis,,Comment,8,0,8
d1wb4k3,2016-04-09 13:10:11-04:00,elykl33t,,That makes a lot of sense lol. I'd never actually thought enough about archive files to realize.,4e1m6h,t1_d1w87ke,PastyPilgrim,,Reply,2,0,2
d1wbs42,2016-04-09 13:27:20-04:00,brennanfee,,"The 1024 smaller files will take longer.  Drives, both platter and SSD, perform best under sustained read (of contiguous blocks) and sustained writes.  The contiguous block addendum is only for platters, but still.

Also, most copy operations these days do a good deal of buffering to smooth out the seek times on the read end and that buffering is less effective for smaller files.

As /u/PastyPilgrim said that is what container files are for.  Tar, for instance, doesn't even compress but merely creates a container.  Zip and others are both container formats and compression schemes rolled into one.",4e1m6h,t3_4e1m6h,Illikis,,Comment,5,0,5
d1x63rj,2016-04-10 07:22:46-04:00,jokoon,,"Seen how much time it takes for any collection of files to be moved around at once, it's obvious that anything involving more files is going to be slower.

For example, just try and download the C++ boost archive, unzip it (that will take AGES), and try moving its folder to another place.

I don't really know why, but it might involve how OSes use safety precautions when dealing with individual files. Those precautions might incur a cost with several files, and it mostly won't with big single files. Also, I think filesystems are tree-like data structures, so when you deal with many files, it's not really an array, it's more like a map. Most filesystems use inodes, and some pretty sketchy addressing methods to track data so it can be flexible enough to copy/move/delete data with an ""file object"" paradigm. It's easy to handle files, but if you have a lot of data, databases will do a better job. Filesystem are better for diverse content, but not for quantity, so most filesystems will index things pretty poorly and it will result in sluggishness.

There is a lot of science involved with filesystem (ZFS, bigtable, etc), which is mostly about compromises between speed of access, reliability, fault resistance, redundancy, etc.

If you want to understand how filesystems work, try to study simple things like FAT32 or ext3.

Also remember that hardware also works best in a sequential/segmented way. Asking your HDD 1000 segments of 1000 is much slower than 1 segment of 1000000, but OS can already build a buffer of those index so it really depends.",4e1m6h,t3_4e1m6h,Illikis,,Comment,2,0,2
d1xve1z,2016-04-10 20:10:16-04:00,5225225,,"By the way, I wouldn't suggest moving the folder as a benchmark. Most of the time, moving a folder is just a rename, so file systems can move stuff nearly instantly, no matter now full they are.",4e1m6h,t1_d1x63rj,jokoon,,Reply,1,0,1
d1wvsdh,2016-04-09 22:53:51-04:00,Ein_Bear,,Would you rather download 1024 MB sized GBs or one GB sized MB?,4e1m6h,t3_4e1m6h,Illikis,,Comment,2,0,2
d1wcn5a,2016-04-09 13:50:25-04:00,bo1024,,"It could be in some scenarios that the 1024 small files is faster by taking advantage of concurrency (they could be copied in parallel). I doubt it, but just a thought.",4e1m6h,t3_4e1m6h,Illikis,,Comment,1,0,1
d1wd9k3,2016-04-09 14:06:27-04:00,shadowdude777,,Does it work that way? I'm no hardware engineer but a HDD has one head. It can only do one thing at a time.,4e1m6h,t1_d1wcn5a,bo1024,,Reply,2,0,2
d1wgedc,2016-04-09 15:29:52-04:00,gyroda,,The bottleneck is probably the USB connection (if that's what we're using) so copying files to two USB sticks might be faster than to one. Other than that I can't imagine a situation where you'd be able to send two files in parallel but not be able to use that extra bandwidth for more of one file. ,4e1m6h,t1_d1wd9k3,shadowdude777,,Reply,1,0,1
d1wikcc,2016-04-09 16:30:31-04:00,bo1024,,If it is going to disk then I think you're right.,4e1m6h,t1_d1wd9k3,shadowdude777,,Reply,1,0,1
d1wiozh,2016-04-09 16:33:55-04:00,PhDeeezNutz,,"SSDs have multiple parallel channels, but even with that, multiple small files will be slower than one large file. ",4e1m6h,t1_d1wd9k3,shadowdude777,,Reply,1,0,1
d1wgcdm,2016-04-09 15:28:22-04:00,gyroda,,"I doubt it for most conventional hardware. If you had some weird setup with two USB ports then yes, but you'd probably want to just send different bits of the same file in parallel to do the same thing for the one big file. ",4e1m6h,t1_d1wcn5a,bo1024,,Reply,1,0,1
d1wf4fl,2016-04-09 14:56:23-04:00,dxk3355,,This is easy to test so just do it.  In my experience the 1024 files take a lot longer.,4e1m6h,t3_4e1m6h,Illikis,,Comment,1,0,1
d1wfqhk,2016-04-09 15:12:20-04:00,Oxc0ffea,,Asking the important questions here!,4e1m6h,t3_4e1m6h,Illikis,,Comment,1,0,1
d1wkvsi,2016-04-09 17:36:08-04:00,MCPtz,,"If remote copying:  
We can just use tar/zip (not compressed) to stream all the files over ssh and make it one single communication handshake. We can tar them on the fly on the sender's end and untar them on the fly at the receiver's end.

The extra overhead of tar and having to open 1024 individual files will make the single file faster, slightly.

Everyone else covered local copying, so I thought I'd add remote copying.",4e1m6h,t3_4e1m6h,Illikis,,Comment,1,0,1
d1x6knt,2016-04-10 07:52:38-04:00,lgastako,,The feathers!,4e1m6h,t3_4e1m6h,Illikis,,Comment,1,0,1
d1wi2w7,2016-04-09 16:16:40-04:00,Protagoras,,"The only correct answer is to test it. Under normal operations 1024 files should be slower than one single big file, but you can't tell if you're dealing with f*d up hardware before you try it. 

For example, copying a 300MB podcast to my beat up MP3 player can take up to ~10 minutes, while 3 single 100MB podcasts are done within a minute. If I had to guess this would be either because there is no contiguous 300MB region of well operating memory available, or the driver has issues writing single files to multiple chips. Unfortunately I probably can't diagnose the issue without spending $1,000+ to send it to a clean room for inspection. So any attempt to figure out in advance which files would copy faster is pointless. ",4e1m6h,t3_4e1m6h,Illikis,,Comment,1,0,1
4e1ahh,2016-04-09 09:43:56-04:00,PlainclothesmanBaley,Logic. Question regarding theories and quantifier elimination to prove decidability.,"So the idea behind a theory is that it only entails things inside the theory and the theory only contains closed formulas. So the theory can only entail closed formulas.

Then I see this in the notes I'm reading: An important technique to show that a theory is decidable is quantifier elimination. We say that a theory T admits quantifier elimination if for any formula ∃x F, with F quantifier-free, there exists a quantifier-free formula G with the same free variables as ∃x F such that T |= ∃x F ↔ G, that is, for any assignment A that is a model of T , A |= ∃x F if and only if A |= G. (It is worth emphasizing that quantifier elimination is defined on formulas that may have free variables.) We furthermore say that T has a quantifier elimination procedure if there is an algorithm to obtain G given F.

But if they have free variables then they can't be entailed by the theory due to my first paragraph. They aren't closed formulas.

I realise I've misunderstood something along the way, but I've got myself in a rut now where I just keep reasoning in circles and I'm getting nowhere. Any help would be greatly appreciated.",,,,,Submission,0,0,0
d1xh1sm,2016-04-10 13:48:48-04:00,east_lisp_junk,,"> But if they have free variables then they can't be entailed by the theory due to my first paragraph. They aren't closed formulas.

I think you're confusing ⊦ and ⊧ here. You won't have an open formula with ⊦ because you can't prove something that isn't well-formed and meaningful to begin with, but ⊧ is about truth rather than proof. An open formula *can* be ""true for a certain choice of how to interpret its free variables"" (which is usually written as <interpretation> ⊧ <open formula>) or ""true no matter how you interpret its free variables"" (written as <theory> ⊧ <open formula>).",4e1ahh,t3_4e1ahh,PlainclothesmanBaley,,Comment,1,0,1
4e004q,2016-04-09 00:28:18-04:00,JeffersonPoke,Not sure how For Loops work.,"I've been trying to add the ten squared numbers and don't know what to input to sum them together. I would really appreciate if someone here could tell me what to do so I could add those ten squared numbers together with a for loop. The code below is what I have so far:


//Create a C++ program that uses a “for” loop to sum up the squares of the integer from 1 through 10. (The Problem)


include <iostream>


include <iomanip>


include <string>


using namespace std;


int main()


{


int sum;


for (int i = 1; i <= 10; i++)


{


cout << pow(i, 2);


}


system(""pause"");


return 0;


}




NOTE: Sorry if I didn't put the hashtags in some parts of the code. I'm a reddit newbie and am not sure how to format it so I can include hashtags in the beginning.",,,,,Submission,1,0,1
d1vvnk6,2016-04-09 01:07:11-04:00,filljfry,,"So there are some errors in here that I will try to let you know how to fix and explain them. First, you declare the sum variable, but don't initialize it. Think about it, what value will sum have if it is not initialized? Therefore you will have to initialize the sum variable to start at 0 (the logical place to start a sum).  

Then, you are just printing out each of the squares from 1 to 10 instead of summing them. Instead of cout, you should increment the sum variable using ""sum = sum + (amount):"" or ""sum += (amount):"" for a shorter version where amount is the amount you need to increment by.  

The for loop goes like this: starts at the first variable you declared (i=1) and goes through everything in the scope of the for loop (between the {}), then when it reaches the end, it increments i based on what is in the third part of the for loop (i++). This continues until the condition in the second part (i<=10) is no longer true. Therefore, in this example, it will loop through from i = 1 to i = 10.  

I can answer any more questions you have or further clarify if you need.",4e004q,t3_4e004q,JeffersonPoke,,Comment,4,0,4
d1vw4d4,2016-04-09 01:26:48-04:00,JeffersonPoke,,"Thank you! I changed made my "" int sum = 0:"" and ""sum _= pow(i, 2):"" and it worked! Although I now know how to actually write the code to make for loops do this, I still have a bit of trouble understanding the concept behind it, such as why sum += (amount) is able to add the numbers together like that. Anyway, thank you for the help! I'll try to look over my new code to understand it! Below is my rewritten code if you want to see it:



include <iostream>


include <iomanip>


include <string>


using namespace std:


int main()


{


int sum = 0:


for (int i = 1: i <= 10: i++)


{


sum += pow(i, 2):


}


cout << sum:


system(""pause""):


return 0:


}",4e004q,t1_d1vvnk6,filljfry,,Reply,2,0,2
d1vw9xv,2016-04-09 01:33:43-04:00,filljfry,,"When you want to write a for loop, you need to think of 3 things, the start value, the condition for the loop to continue, and the thing that changes in the for loop. This is where you would think, ""I need to start from 1 and go until 10,"" and then you would write the for loop with the given syntax. More on for loops can be found at http://www.cprogramming.com/tutorial/lesson3.html.  

For the += question, it is just shorthand for writing sum = sum + pow(i, 2). This is just saying take sum, and add pow(i, 2) to it and store it in sum. For example, say you have a value and you want to add 10 to the value, the code for that would be ""value += 10:"".",4e004q,t1_d1vw4d4,JeffersonPoke,,Reply,3,0,3
d1vwk5v,2016-04-09 01:47:09-04:00,JeffersonPoke,,"Oh, I understand now. So, for example, if I was to make my value = 5 and wrote something like value += 15 along in my code, the value will become 20, right? At first, I was like how does ""sum = sum + pow(i, 2)"" work because I thought of it as something like ""1 = 1 + 2 ."" Thanks for making it so easy to understand in three sentences.",4e004q,t1_d1vw9xv,filljfry,,Reply,2,0,2
d1vwnlg,2016-04-09 01:51:47-04:00,filljfry,,"Exactly, you need to think of the variable as that, variables. Each variable stores a value depending on the type of variable. Glad I could help.",4e004q,t1_d1vwk5v,JeffersonPoke,,Reply,2,0,2
4dzokh,2016-04-08 22:41:56-04:00,Jolly_Misanthrope,"When remotely accessing a computer, is performance limited by the computer used to access it?",Edit: Not sure if this is the right subreddit as I'm not sure if this is more of a software or hardware discussion (or both?),,,,,Submission,3,0,3
d1vrp6b,2016-04-08 22:46:50-04:00,mcowger,,"Generally no, unless you are some how limited by the bandwidth between the systems (like try to display graphics).",4dzokh,t3_4dzokh,Jolly_Misanthrope,,Comment,6,0,6
d1vry6s,2016-04-08 22:54:43-04:00,Jolly_Misanthrope,,"I'd imagine, then, that this is the major issue (graphics) with cloud gaming, which was what prompted me to ask this question. ",4dzokh,t1_d1vrp6b,mcowger,,Reply,1,0,1
d1vs9a8,2016-04-08 23:04:43-04:00,zefcfd,,isnt what's sent back effectively pre-rendered at that point?,4dzokh,t1_d1vry6s,Jolly_Misanthrope,,Reply,2,0,2
d1vzhmj,2016-04-09 04:44:35-04:00,koorb,,"Yes, you are basically playing an interactive youtube stream. Latency and bandwidth are both super important.",4dzokh,t1_d1vs9a8,zefcfd,,Reply,2,0,2
d1wsz6x,2016-04-09 21:33:44-04:00,mcowger,,Yes - sending back 30+ frames per second of HD screen data is very bandwidth intensive.,4dzokh,t1_d1vry6s,Jolly_Misanthrope,,Reply,1,0,1
4dyjlj,2016-04-08 17:36:18-04:00,collegecoder,How do I loop this? (Python),"This is a program that will prompt the user for their age, and calculate the amount of breaths/heart beats they have had in their lifetime.

It is almost finished, other than one last part.

""Use a loop to prevent users from entering an age over 120. Keep reprompting until the age entered is correct.""

I have no clue what loop to use or how to use it. Can someone incorporate the proper loop into my code and copy paste it?

Thanks!
# greeting
    print (""You will be prompted for your age and name."")
    print (""After entering your age and name, you will be displayed the average amount of breaths and heart beats you have taken in your life."")
# input
    age = int(input(""Enter your age here:""))
    name = str(input(""What is your name?:""))
# process 
    if age == 0:
	    age = .6
	    breathsPerMinute = 45
    if age >= 1 and age <=4:
	    breathsPerMinute = 25
    if age >= 5 and age <= 14:
	     breathsPerMinute = 30
    if age > 14:
	     breathsPerMinute = 16
		
    breathsPerYear = age * 525600 * breathsPerMinute
    heartbeatsPerYear = age * 525600 * 67.5
    breathsPerYear = round(breathsPerYear)
    heartbeatsPerYear = round(heartbeatsPerYear)
# output
    print (name,"","")
    print (""you have had an average of"",breathsPerYear,"" breaths in your lifetime, and"")
    print (""you have had an average of"",heartbeatsPerYear,"" heart beats in your lifetime."")",,,,,Submission,1,0,1
d24f0t4,2016-04-15 15:57:24-04:00,Newcdn,,"This looks like an intro level CompSci assignment question. 

After asking for the age, check if it is >120. If yes, ask age again. Otherwise, continue the calculations. ",4dyjlj,t3_4dyjlj,collegecoder,,Comment,1,0,1
4dxpqs,2016-04-08 14:27:35-04:00,chotu_lala,Which web browser would be best for this system?,"1.83GHZ Atom processor
1GB ram
windows8",,,,,Submission,0,0,0
d1v9hh4,2016-04-08 14:46:45-04:00,Oli_Picard,,[midori](http://midori-browser.org) would be a good fit.,4dxpqs,t3_4dxpqs,chotu_lala,,Comment,1,0,1
4dxc6m,2016-04-08 13:06:13-04:00,2b2a,Considering a Masters in Computer Science,"I am considering pursuing a Masters in Computer Science, but I have no idea what I would want to specialize in. I'm interested in learning more about Computer Science, but I currently do not have anything I'm crazy about. I like both theory and programming. Would it be feasible to get into a Masters program without knowing what I want to specialize in, or should I really need to know what I want to specialize in before I consider applying? I'm a junior in college without any research experience. ",,,,,Submission,7,0,7
d1v9yj0,2016-04-08 14:57:02-04:00,osuwhitey,,"Yes, it's possible. Source: I'm a master's student in CS and left college with no research experience and no idea what I wanted to specialize in. Still don't, nearly a year in!",4dxc6m,t3_4dxc6m,2b2a,,Comment,2,0,2
d1va46e,2016-04-08 15:00:29-04:00,spatialdestiny,,"I'm a bunch of years out of college and I just applied to software engineering grad school.  I probably wouldn't be going into this field if my intention had been to go straight to grad school.  Your motivations change a bit after college.  Things like job availability by location, compensation, fit, company size, etc., start mattering more and more after college.  


Besides interest and finding the right field and program, my guess is you'd have a hard time getting into most grad schools straight from bachelor's unless you have a great resume.  ",4dxc6m,t3_4dxc6m,2b2a,,Comment,1,0,1
d1vcdbi,2016-04-08 15:50:21-04:00,minesasecret,,I don't think you need to specialize in anything. I don't think I had a single coworker who had a Master's and specialized in anything..,4dxc6m,t3_4dxc6m,2b2a,,Comment,1,0,1
d1vflf6,2016-04-08 17:04:32-04:00,pewpsewp,,Look at coursework master's programs. I was in the same position as you and it was a great option for me (I'm graduating in about a month). There is no thesis you just take graduate level CS classes. Myself and some peers ended up joining research groups once we found a niche we liked but there was no research requirement. ,4dxc6m,t3_4dxc6m,2b2a,,Comment,1,0,1
4duxw5,2016-04-08 00:44:25-04:00,coaster367,Does this greedy method produce a graph with maximum connectivity (given # of vertices/edges)?,"We are given the number of vertices and edges, and are tasked to produce a graph with those numbers and largest connectivity. I propose that we can just greedily choose an edge that maximizes connectivity, but don't see a proof of why that would be the maximum or not.

Is there a reference for this?",,,,,Submission,7,0,7
d1ummh6,2016-04-08 02:18:50-04:00,_--__,,What do you mean by connectivity?,4duxw5,t3_4duxw5,coaster367,,Comment,4,0,4
d1uw2jm,2016-04-08 09:49:54-04:00,IAmNotNathaniel,,"How are you determining/defining which new edge would maximize connectivity?

I would think if you checked the connectivity of each pair of non-connected vertices, and then added an edge between the lowest connected pair, it may work.

The potential problem I see is you will have a number of ""ties"" where multiple new edges would result in the same overall connectivity. But maybe that won't matter. I haven't worked with graphs in a long time now.",4duxw5,t3_4duxw5,coaster367,,Comment,1,0,1
d1urkj5,2016-04-08 07:09:05-04:00,motheryaar,,"lets say there are n vertices and m edges
then just make an adjacency matrix of size nxn and fill in m 1s and rest of them with 0s",4duxw5,t3_4duxw5,coaster367,,Comment,0,0,0
4dteuu,2016-04-07 18:02:39-04:00,1KillerMidget,Not sure what area in CS I should study in.,"I want to go to college for CS but I'm not sure as to what area I want to study in. I've been really stressing out because I'm not sure as to what exactly I want to do. I'm wanting to study in a area that is more math heavy but not all math. I'm currently in eleventh grade. Next year I'll be taking AP statistics and AP calculus. I've already taken two chemistry classes, a physics class, and four math classes. Is it a must to know a language when entering college? If not how much does it set you back?",,,,,Submission,1,0,1
d1ucouo,2016-04-07 21:09:49-04:00,InserdGerming,,"I'll add my two cents, I don't think it is necessary to have programming experience before entering college, however it is very useful. Start learning from places like Code Academy, or read a formal CS book, or even self study/take AP Computer Science during your senior year.",4dteuu,t3_4dteuu,1KillerMidget,,Comment,4,0,4
d1ucwjb,2016-04-07 21:15:26-04:00,mplang,,"Don't waste your energy worrying -- you clearly have enough on your plate already. You have plenty of time to figure things out. Nobody expects you to enter college with the next four years planned-out to the letter. Don't forget to leave some time to enjoy your last year-and-change of high school!

I don't think that having little or no programming experience will put you at any great disadvantage in the long-term. It may be a little more difficult in the beginning, but you'll eventually take courses which level the playing field. Your math and science background will probably give you an advantage elsewhere, since programming languages are just a tool in CS, after all. If you're committed to doing well, you'll do well.

But not knowing a language prompts me to ask, why CS? Many people who go to college for CS have programming experience because they have a passion for it. What's your passion? That's not to say that CS isn't for you (as I said, CS != programming), but if you're just chasing the promise of good job prospects and pay, you might be disappointed (not that I have any reason to think this is applies to you!).",4dteuu,t3_4dteuu,1KillerMidget,,Comment,3,0,3
4dsmu8,2016-04-07 15:13:54-04:00,darkbluespark,Getting Clinical Experience as a Computer Science Major,"I'm going to enter university in the fall, and I originally selected biomedical informatics as my major because I thought it would help me to fulfill my pre medical course requirements. However, after studying the major maps, I realized there were still several courses I'd have to take outside of that major to fulfill the requirements. Since I'm more interested in the computer science aspect even though I have no experience, I am considering changing majors. However, I am concerned that doing so would lessen the opportunities I have for research/clinical experience, because I still want to keep medical school as an option in the future. So I guess what I'm asking is, if I major in computer science and take classes to fulfill my pre med requirements, would it be any more difficult for me to get some hospital or research position than if I major in biomedical informatics? Because I know that medical schools really value clinical experience.
I did consider double majoring in those two areas.",,,,,Submission,11,0,11
d1ty8sa,2016-04-07 15:31:58-04:00,BonzoESC,,You might want to talk to someone from a med school's admissions department. I suspect most readers here aren't entertaining dreams of med school.,4dsmu8,t3_4dsmu8,darkbluespark,,Comment,3,0,3
4dq47r,2016-04-07 04:07:25-04:00,Cregavitch,Question about studying at a Uni level with no experience,"I've become heavily interested in computer science after learning so much about computer hardware, but I don't know much about it. The College and Secondary School I went to offered no courses on it so it's never crossed my mind to study it any way before, but I have dabbled a little bit in using Visual Studio to build simple programs and Javascript.

The first thing I'd like to know is, how much do you need to know?

Has anyone interested in Computer Science without any experience went to study at a University level and found it reasonable to learn?

I know that it's best to have some knowledge in a subject before studying it, where would be the best place to learn about these things? The first year of my course includes: Learning Java, Relational Databases, System Analysis, Fundamental of Computing and Web Technologies. 

Or have I just messed up completely?

EDIT: Thanks a lot for all the replies guys, feels great knowing that a few of you were in the same boat and know many others just like me :) ",,,,,Submission,12,0,12
d1tavad,2016-04-07 04:30:04-04:00,luhin,,"> Has anyone interested in Computer Science without any experience went to study at a University level and found it reasonable to learn?

Yep, I'm now in my first year. Had zero experience in programming, but that was never a problem. Courses are built from the ground up. Seriousely, don't worry! The only thing i have problems with, is the math, but thats because I just ignored it in highschool.",4dq47r,t3_4dq47r,Cregavitch,,Comment,9,0,9
d1tavo3,2016-04-07 04:30:44-04:00,Cregavitch,,"Cheers man, this calmed my nerves a lot! :)",4dq47r,t1_d1tavad,luhin,,Reply,1,0,1
d1tikqs,2016-04-07 09:58:26-04:00,thewholehamdamily,,"I'm in the same boat as the guy you replied to. I took an intro to C++ course at my community college last semester and am now finishing Programming I at my university. It moves fast, but it's absolutely doable without prior experience (I had none). As the guy above said, it is a lot of math though. ",4dq47r,t1_d1tavo3,Cregavitch,,Reply,1,0,1
d1tp8wy,2016-04-07 12:24:06-04:00,Xxyr,,"I had the good fortune to take classes in high school, but I found that the intro classes at college were a repeat of what I learned.

The thing I see people having the most trouble with is breaking a problem down into small solvable pieces.",4dq47r,t3_4dq47r,Cregavitch,,Comment,2,0,2
d1tr6o8,2016-04-07 13:04:12-04:00,the_omega99,,"Most people going into CS never had HS classes teach it. And many that did had shitty, horribly teachers that deserve to be insulted because they're so downright awful (probably not really their fault so much as the fault of administration that thought they could make someone with no experience suddenly teach a new topic just like that).

So you're fine.",4dq47r,t3_4dq47r,Cregavitch,,Comment,2,0,2
d1tkuiw,2016-04-07 10:51:26-04:00,sunemori,,"I am currently studying CS, I had a bit of prior experience. However there are tons of people (most of them in fact) in my classes who have had no experience whatsoever and are doing great. We really start from the basics, seriously our first term was only HTML, touching on a bit of basic JavaScript. 

If you have an interest, you should definitely go for it. A lot of colleges teach, expecting that their students know essentially nothing on the subject. Good luck. ",4dq47r,t3_4dq47r,Cregavitch,,Comment,1,0,1
d1txggj,2016-04-07 15:15:19-04:00,Karoal,,"My uni tutor (in the UK) started with absolutely 0 experience, and just in her 2nd year she got an internship at a global-scale company.

She's a very extreme example but it goes to show that you won't be hindered by this!",4dq47r,t3_4dq47r,Cregavitch,,Comment,1,0,1
d1u3byu,2016-04-07 17:19:52-04:00,CheekiBreekiIvDamke,,"I started with what I thought was good knowledge because I had farted around in Ruby. Turns out there is a shit load to learn in CS and it is hard to start from the right point. 

Fortunately that is why almost every CS degree starts with the basics. Not like 'how to operate word' or anything but certainly information on how computers -work-. Your first programming course should be quite relaxing really. At my school (and its peers) you start off with Python doing very very basic stuff. ",4dq47r,t3_4dq47r,Cregavitch,,Comment,1,0,1
4dnpgu,2016-04-06 16:45:43-04:00,LTS1287,Context-oriented programming?,"Can we really have one programming language to rule them all?

I'm wondering what are the implications of a programming language with a grammar that changes based on what you're trying to accomplish. I don't even know if it would be called ""context-oriented programming"".

Example:

Let's say I want to write a program but I want part of the program to be Haskell-like (lazily evaluated and purely functional.

    using context safe{
        list a = [1..]
    }

However, I need IO to do anything and therefore need side effects.

    using context unsafe{
        for(int i=0; i<20; i++):
            print(a);
    }

Is this even possible? What if I were to go even further and say that I want haskell-like semantics in one part of the program while having prototype-based OO in another part?

Am I right in assuming that the grammar would change based on what you're trying to do? What are the implications of this?",,,,,Submission,6,0,6
d1snrbw,2016-04-06 17:03:39-04:00,Majiir,,"First off, you *can* have purely functional IO in Haskell.

I think the problem you'll run into is that things like memory layout are highly influenced by language semantics. Many languages allow you to work in what you call ""contexts"", where the semantics change and new keywords are introduced to help you work with them. But there are limits, and that's because some semantics are simply incompatible (or combining them comes with a large performance cost).",4dnpgu,t3_4dnpgu,LTS1287,,Comment,2,0,2
d1soi7w,2016-04-06 17:20:18-04:00,LTS1287,,"I understand that Haskell has IO wrapped in a monad. My question is not specific to Haskell. Perhaps it was a poor example.

My question was whether or not we can just create a language that does it all.

Perhaps one block of code is purely functional while another is object oriented?",4dnpgu,t1_d1snrbw,Majiir,,Reply,2,0,2
d1svysn,2016-04-06 20:19:09-04:00,Majiir,,"You (kind of) can, but you start to lose the benefits of having those concepts in the first place. Consider that with enough monad transformers and helper functions, you can basically write imperative code in Haskell—but if you try to generalize this, you end up with a gigantic transformer stack that can do everything under the sun. The type of any given expression no longer tells you much at all about what that expression *is* or *does*.

I say ""kind of"" because there are some tradeoffs you simply cannot have. You can't mix the very dynamic nature of Javascript with the type system of Haskell. You can hack it together, by using a Map or somesuch in Haskell, but there are two fundamentally opposed schools of thought here that you just cannot make coexist.

Here's the thing: You *can't* shoehorn Haskell's type system into Javascript. The best you can get is [a strongly typed programming language that compiles to Javascript](http://www.purescript.org/). Some languages are better for imitating the features of *other* languages. C# and Haskell both come to mind: the former because it has such a wide array of language features, and the latter because it's good at crafting abstractions in general.

> Can we really have one programming language to rule them all?

Egh, yes? Maybe? But not by cramming every language feature and design philosophy into one language. ",4dnpgu,t1_d1soi7w,LTS1287,,Reply,2,0,2
d1ssirf,2016-04-06 18:55:16-04:00,PM_ME_YOUR_PROOFS,,"Various forms of this exist. The first thing to note is that for this to be useful these languages have to have a common way of communicating information. That gets tricky but isn't OK impossible. Take .NET or JVM as a examples. They have desperate languages than can all communicate. Many languages provide foreign function interfaces for C or some host language as well. 

LISP is worth mentioning here. It allows you to write domain specific languages that solve sub tasks well.

Haskell has template Haskell and the monad syntax which both accomplish this.

You can even embed domain specific languages in C++ like boost spirit for instance.",4dnpgu,t3_4dnpgu,LTS1287,,Comment,2,0,2
d1su01x,2016-04-06 19:31:30-04:00,LTS1287,,I was aware these things existed but I didn't know how powerful they are. Thank you for your reply.,4dnpgu,t1_d1ssirf,PM_ME_YOUR_PROOFS,,Reply,2,0,2
d1sq1qy,2016-04-06 17:55:20-04:00,avaxzat,,"Sure, you could create a programming language that, as in your example, switches from being purely functional (in case of a safe context) to stateful (in case of an unsafe context). However, this wouldn't ""change the grammar"" of the language: you would define a single grammar that takes into account the context you're currently working in, and based on that context the compiler/interpreter can verify whether your statements are allowed and what they mean at that point in the program.

If you're talking about a language where you get to define new operational semantics on the fly, then such languages exist, too. It's called metaprogramming, and it's supported by various programming languages (perhaps most notably the Lisp family and Prolog).

Basically, you can have a language that ""does it all"", but each extension of a programming language brings with it practical and theoretical problems. On the practical side, the language becomes harder to implement: on the theoretical side, it may become very hard to reason about what your programs are doing. For example, consider the case of C++. The C++ templating system [is itself Turing-complete](http://port70.net/~nsz/c/c%2B%2B/turing.pdf), meaning you could theoretically do anything you can do in full C++ using nothing but templates. However, a consequence of this is that the problem of determining whether or not your C++ program is well-formed [becomes undecidable](http://blog.reverberate.org/2013/08/parsing-c-is-literally-undecidable.html). So you can put as much expressiveness into your language as you want, but it comes with trade-offs.",4dnpgu,t3_4dnpgu,LTS1287,,Comment,1,0,1
d1stx1t,2016-04-06 19:29:29-04:00,LTS1287,,Thank you for your reply. Very informative. Now I have to decide what language to add to my repertoire.,4dnpgu,t1_d1sq1qy,avaxzat,,Reply,2,0,2
d3stiv8,2016-06-02 03:05:08-04:00,TheChance,,"I realize it's been a month, and you've probably decided by now, but I have a stock answer to this, so here it goes:

Stop agonizing over which language. The language is a tool, not an activity. Programming is the activity.

A woodworker can use any of a *number* of saws to make a given cut, whereas only a couple of the saws in their shop will be *utterly incapable* of making that cut.

So should the woodworker spend days poring over catalogs, trying to decide whether he should next learn to use a table saw or a panel saw?

Of course not. The woodworker should pick a saw, and use it to make their straight cuts for a while. If they hate it, they can always try another saw later.

After a while, they'll start to learn which saw is most convenient for which cuts, and which ones just don't ""agree"" with their workflow. It's all down to experience.

In that vein, when you feel like it's time to explore a new language, I *strongly* discourage you from spending more than an hour or two deciding which one. Instead of researching to determine which language is ""appropriate,"" spend that time reading and writing *code*, and *then* decide if the language is appropriate.",4dnpgu,t1_d1stx1t,LTS1287,,Reply,2,0,2
4dl3a0,2016-04-06 05:54:42-04:00,njitjunk,Help with arraylists,"http://pastebin.com/kTn5CNV1

I need to add a function that will take the user input and will search for through the array. And then it will remove it and insert something into the text area. 

else if(temp.startsWith(""get"")){

      String[] temparray = temp.split("" "");

      String newtemp = temparray[temparray.length-1];

      for(int i = 0; i < 10; i++){

         if(right.getName(i).equals(newtemp)){
               }
               else{
                  System.out.print(""not working"");
               }
This is somewhat of what i got but I'm not exactly sure of how to read the correct arraylist. Is there a way to print out the array to see it visually?        ",,,,,Submission,0,0,0
d1rydr4,2016-04-06 06:43:57-04:00,InserdGerming,,"A couple things. One, in this for loop here, were you trying to iterate through a list called right? If so, you need to call get(i).getName().equals(...) instead of what you have (unless I don't understand what method getName is). And to print the contents of an array list, just put the variable name in System.out.print( ).",4dl3a0,t3_4dl3a0,njitjunk,,Comment,4,0,4
d1sgsf9,2016-04-06 14:39:58-04:00,njitjunk,,Thanks.,4dl3a0,t1_d1rydr4,InserdGerming,,Reply,1,0,1
4dl0l2,2016-04-06 05:23:10-04:00,cardboardxbox,Images in python--how to save?,"I'm working on a small project and need each object to store an image. I'm hoping to make it a web app, what's the best way to store and reference image information?",,,,,Submission,1,0,1
d1s1asd,2016-04-06 08:49:46-04:00,brtt3000,,"In web development with Python the [Pillow library](https://pypi.python.org/pypi/Pillow/) is pretty much the standard bitmap image processing library. I've seen it mostly used for whole image processing (thumbnailing/cropping/format conversion etc) but it also has basic drawing capabilities (pixels, rects, lines, shapes etc). There are also a ton of dependents that use it to make higher level packages that might fit your problem.

For storage use a filesystem or a blob storage like S3 and refer to images by their path or URI. Storing binary blobs in a database is usually considered a bad practice. For example Django has a ImageField to put images in db models but it actually only saves the path in the database and uploads the image data to a file storage.

I don't know what the science people use.",4dl0l2,t3_4dl0l2,cardboardxbox,,Comment,1,0,1
4dctse,2016-04-04 16:00:07-04:00,furrycow779,Help making up my mind.,"Hello all, I am in my first comp sci class and it started off pretty well. We are learning c++ and it wasn't to difficult. But recently we are getting in pretty deep and during our lab assignments it's getting very difficult. It's not that I don't understand the coding it's taking the question given and formulating the code to complete it. I feel like someone needs to hold my hand and break down the steps of how I need to look at things for every step. Once they explain it dumbed down I can figure it out from there. My main question is, is it just the fact I'm new to computer science?  I consider myself relatively smart and people tell me I am as well (not to sound egotistical) I just don't know wether or not I should pursue this career if I don't look at problems the way someone should. Thanks for any advice you could give me on this. 

Edit: Thanks everyone for the advice I really appreciate it. After seeing your responses it really opened my eyes that it's a part of the skill I need to learn. I think I just get clogged up trying to work with the code and breaking down the problem. Thanks again! ",,,,,Submission,5,0,5
d1pvx8u,2016-04-04 17:48:25-04:00,MCPtz,,"You have to hold your own hand. Pretend you have to explain the problem to the TA or whatever. 

Break it down, slow it down, and break the problem into small pieces.

Don't worry, everyone has to learn how to do this or you'll never get anywhere.",4dctse,t3_4dctse,furrycow779,,Comment,9,0,9
d1prvue,2016-04-04 16:18:16-04:00,Pyar23,,It is like any other field or skill.  Did you know how to write an essay the first time you had to do it? No your teachers probably broke it down into very simple steps and then you built up to a full essay. Programming and writing code is very similar. The skills will come with time and practice just like anything else.,4dctse,t3_4dctse,furrycow779,,Comment,5,0,5
d1qkmlk,2016-04-05 08:07:43-04:00,east_lisp_junk,,"> It's not that I don't understand the coding it's taking the question given and formulating the code to complete it.

But that *is* coding.

> My main question is, is it just the fact I'm new to computer science?

Also the fact that most introductory classes don't really teach how to think through breaking down a problem.",4dctse,t3_4dctse,furrycow779,,Comment,3,0,3
d1py32k,2016-04-04 18:41:06-04:00,self_raising,,It's something that will get easier and easier the more you learn and the more projects/exercises you complete.,4dctse,t3_4dctse,furrycow779,,Comment,2,0,2
d1qnmeq,2016-04-05 09:45:12-04:00,joshuazed,,[This book](http://www.amazon.com/Think-Like-Programmer-Introduction-Creative/dp/1593274246) is excellent at helping you learn those skills. The author also has [videos](https://www.youtube.com/watch?v=YgzpqlF54lo) to go along with the book.,4dctse,t3_4dctse,furrycow779,,Comment,2,0,2
4d8a8d,2016-04-03 18:34:50-04:00,virt1028,I'm having trouble with Parse Trees.,"I am doing homework and I have the following problem:

Consider the grammar:

    statement → assign 
    statement → sub_call 
    assignment → id = expression 
    sub_call → id ( arguments ) 
    expression → head tail 
    tail → operation expression
    tail → ε 
    head → id 
    head → sub_call 
    head → ( expression ) 
    operation → + | - | * | / 
    arguments → expression arg_tail 
    arg_tail → , arguments 
    arg_tail → ε

Construct a parse tree for the input string:

     foo(a , b)

Add the production rules necessary to extend this grammar to include the definition of a subroutine. For example:

    subroutine foo (x, y) { 
    z = x + y 
    bar(z) } 

**What I've Considered**

I am somewhat familiar with trees that use expressions,literal, and variables but this is so much more and I can't think of where to even begin. Not to mention create a tree for a subroutine. Could someone please help me with this? How do I even start?",,,,,Submission,7,0,7
d1p5usi,2016-04-04 05:59:57-04:00,nawap,,"Well, I will not do all of your homework for you, but here is how you will construct the parse tree:

[Tree](https://dl2.pushbulletusercontent.com/P4Ix8exQsHS4uT3Es5EPCrV1YYjBG3YW/IMG_20160404_152414021.jpg)

As for extending the grammar, you can start with adding a new rule which produces the subroutine string:

sub_def -> subroutine id ( arguments ) body
body -> { ..fill your logic here.. }

",4d8a8d,t3_4d8a8d,virt1028,,Comment,1,0,1
4d823r,2016-04-03 17:37:40-04:00,coconutscentedcat,"Which majors are related to CS, but don't require math?","I'm a mature student and I never took grade 12 math credits in high school. I'm planning on completing them, but I won't be able to do it time for registration next fall.


Are there any majors that are somewhat related to CS that don't require math, bio/chem, or physics credits? I was considering applying to Cognitive Science, and then switching to CS afterwards (when I finish my math credits). Cognitive Science is related UX/UI design, so it can be useful to me later if I want to work in that field. Are there any other majors that can be useful in CS?",,,,,Submission,2,0,2
d1rs9ga,2016-04-06 01:15:35-04:00,_ActionBastard_,,I would be astounded if there were any that didn't require calculus and statistics.,4d823r,t3_4d823r,coconutscentedcat,,Comment,2,0,2
d1ojwif,2016-04-03 18:00:51-04:00,Arkeministern,,"Sorry for not answering your question but may I ask where you are studying? I thought it was a Swedish thing that cogsci was related to UI/UX. 

At my uni in Sweden you could study ""science of systems"" which I don't think require higher maths. ",4d823r,t3_4d823r,coconutscentedcat,,Comment,1,0,1
d1okcl1,2016-04-03 18:12:25-04:00,coconutscentedcat,,"Canada, I don't think we have science of systems here.

I checked job postings (for canada, and for silicon valley) for cog science and the only results I get are for UX design/research positions. ",4d823r,t1_d1ojwif,Arkeministern,,Reply,2,0,2
d1s7z83,2016-04-06 11:36:38-04:00,x_Zoyle_Love_Life_x,,"You can't find any offerings for Information Systems or Business Technology Management?

I am in the US though so I have no idea.

Edit: After typing this out I saw the comment /u/constructivCritic wrote says basically the same thing.",4d823r,t1_d1okcl1,coconutscentedcat,,Reply,1,0,1
d1oqn4i,2016-04-03 20:54:31-04:00,constructivCritic,,"Not if you'lol have these.  But the school of business might have, Business Info. Systems, computer Info. Systems, or Management Info. systems.  They require a bit less math.",4d823r,t3_4d823r,coconutscentedcat,,Comment,1,0,1
4d5fse,2016-04-03 04:07:52-04:00,csrabbit,"Are AND, OR, XOR logic gates just conceptual abstractions for deeper understanding, or are they actual physical structures inside computers?","If so, what do they look like and what are they made out of?

Thanks!",,,,,Submission,29,0,29
d1nwed0,2016-04-03 04:21:40-04:00,PastyPilgrim,,"They're real and made out of transistors and wires. A processor has billions of transistors, many of which are for microscopic logic gates, so it's rather difficult to see what they actually look like. However, you can look up circuit diagrams for any gates that interest you (e.g. [and](https://en.wikipedia.org/wiki/AND_gate#Implementations)).",4d5fse,t3_4d5fse,csrabbit,,Comment,26,0,26
d1o4fun,2016-04-03 11:15:25-04:00,VainWyrm,,"So, lots of good answers here that all say,""yes."" Because it's the correct answer and whatnot. The only caveat I've got is that in reality *most* gates are NAND, NOR, XOR, and NOT. It's just more efficient, space wise.",4d5fse,t3_4d5fse,csrabbit,,Comment,17,0,17
d1olcon,2016-04-03 18:38:15-04:00,spongebue,,"To go into further detail, NAND and NOR are considered universal logic functions, because any gate can be created using just those gates by feeding the output of one NAND into another.

Details: http://www.electrical4u.com/universal-gate-nand-nor-gate-as-universal-gate/",4d5fse,t1_d1o4fun,VainWyrm,,Reply,3,0,3
d1ob8js,2016-04-03 14:12:03-04:00,squall14414,,What is NAND? Is it related to NAND flash memory at all? ,4d5fse,t1_d1o4fun,VainWyrm,,Reply,1,0,1
d1oc3fa,2016-04-03 14:34:28-04:00,DeeJay250,,"A NAND gate stands for  ""Not And"". Think of it like whenever an AND gate would be true for two inputs, the NAND gate would be false and vice versa.",4d5fse,t1_d1ob8js,squall14414,,Reply,8,0,8
d1p1ykl,2016-04-04 02:08:38-04:00,pencan,,"And yes, NAND flash is composed of NAND gates, just like NOR flash is composed of NOR gates. NAND is faster for writing so it's used for general flash, while NOR is faster for reading so it is used for read only flash. ",4d5fse,t1_d1ob8js,squall14414,,Reply,3,0,3
d1p21tu,2016-04-04 02:12:56-04:00,The-Night-Forumer,,"Just curious, how is one faster than the other at things like writing or reading?",4d5fse,t1_d1p1ykl,pencan,,Reply,3,0,3
d1pfdk2,2016-04-04 11:39:55-04:00,pencan,,"That's getting into VLSI (physical chip layout). It has to do with how the gates are physically constructed. NAND has less capacitance, so it takes less time (lower RC constant) to gather a full charge when a voltage is applied. NOR has higher drive strength so it can charge something else faster. 


It's more complicated than that, but that's the general idea ",4d5fse,t1_d1p21tu,The-Night-Forumer,,Reply,2,0,2
d1nwexh,2016-04-03 04:22:44-04:00,tyggerjai,,"It's transistors all the way down. The technology gets smaller and smaller, but as far as I know. CPUs are still made up of patches of doped silicon that either allow or prevent the flow of current, depending on base signal. That's a transistor, and that lets you build logic gates. Most first year EE/CE students will build some simple gates from transistors. Mostly, then, it's all NAND gates. The nand2tetris book/site, and _Code_ by Charles Petzold have good explanations of this.

TL:DR, yes, it's logic gates made from transistors, or equivalent. ",4d5fse,t3_4d5fse,csrabbit,,Comment,7,0,7
d1nyof0,2016-04-03 07:04:52-04:00,R4p354uc3,,"Boolean logic (AND, OR, NOT, and friends) is the foundation of computers as we know them. If you have a system that can implement boolean logic (and some kind of data storage/retrieval mechanism) you can build a computer.

The vast majority of the computers we use implement boolean logic with tiny electronic devices called transistors. A transistor is basically an electronic switch that controls voltage between two ports. Electricity flows in one port and out the other, and a third port controls the voltage between them (Note that this is an extremely basic explanation of transistors. How they actually work is quite a bit more complicated than that). By connecting transistors in serial, you can create an AND gate. By connecting them in parallel, you can create an OR gate. Other configurations create other kinds of gates. 

Knowing how to implement boolean gates with transistors allows us to design computer processors at a ""high level"" as complex patterns of gates. Once the configuration has been tested in a virtual environment, you can convert the gates to transistor configurations. This configuration is then stamped into what is called a die to create the physical processor, which is then tested, packaged, and shipped to be built into computers!

Much of the advancement in processors these days is figuring out how to make transistors smaller and use less power. Standard processors nowadays have transistors that are less than 10 nm wide, and you can fit tens of billions of them in a single processor.

**WARNING: RANT BELOW**. Feel free to stop here.

The fascinating thing about computers is that they are built out of layers of abstraction. At the top level, you have user-facing applications, which can be designed by anyone who knows how to program and build user interfaces. Then you have lower-level applications that talk directly to I/O and might have to deal with things like concurrency. User-facing applications abstract these lower-level pieces so that developers don't have to think about them, but they are still working and someone had to design them. Then you have the operating system level, which is the layer that talks directly to the hardware. All software interfaces with this layer at some point. But operating systems can be virtualized. You can swap out an actual operating system with a virtual one and the software built on top of it wouldn't know the difference. Then you have the hardware, which is implemented using chips designed by someone else, but the specifications of the chips define what they do and which pins serve which purposes. Inside those chips could be even more chips. And at the very bottom is transistors.

But every single level of this stack that makes up a computer can be swapped out with some other technology that does the same thing in a different way. One interesting idea is optical computers, where instead of implementing logic gates with electronic components, they are implemented with optical components like fiber optics. Aside from the energy source, you could build an entire computer using what is basically just glass. Imagine a phone that is truly water-proof. You could take it into a pool with you and you wouldn't have to worry about the electronics being fried, because there aren't any electronics!

And then there are some software-based solutions for implementing logic gates. Most hardware developers implement their circuits in virtual environments first before actually building them. The circuits work exactly as they would in real life, but they are built on top of virtual components. And there are even things like Minecraft, which gives you the ability to build logic gates out of redstone. You can find plenty of videos on youtube where people show the rudimentary computers they built in Minecraft. They are not nearly as powerful as the computers we are used to, but they are still real computers, enabled by the idea of layers of abstraction.",4d5fse,t3_4d5fse,csrabbit,,Comment,7,0,7
d1nx66j,2016-04-03 05:14:55-04:00,None,,[deleted],4d5fse,t3_4d5fse,csrabbit,,Comment,2,0,2
d1o48vf,2016-04-03 11:09:33-04:00,Murikk,,Python is an interpreted language. There is no compiler. This holds for other languages though.,4d5fse,t1_d1nx66j,None,,Reply,3,0,3
d1om91n,2016-04-03 19:01:49-04:00,Natanael_L,,"It can be compiled, both to bytecode (for parsing or for a JIT runtime) and binary ",4d5fse,t1_d1o48vf,Murikk,,Reply,2,0,2
d5c9oz4,2016-07-14 14:22:36-04:00,5225225,,"Though compiling to binaries is usually just including a copy of the bytecode interpreter, isn't it?",4d5fse,t1_d1om91n,Natanael_L,,Reply,1,0,1
d5c9qhv,2016-07-14 14:23:31-04:00,Natanael_L,,Depends. Both can be done ,4d5fse,t1_d5c9oz4,5225225,,Reply,1,0,1
d1pg66r,2016-04-04 11:58:15-04:00,BarqsDew,,"Don't you just hate it when people don't answer the original question? ""What do they look like?""

This is what an actual CMOS AND gate looks like under a (very good) microscope: http://i.imgur.com/aDWIf7Q.jpg or with the top layers removed: http://i.imgur.com/U5feaZM.jpg - different colors indicate different layers, not necessarily different materials.

It's made up of a NAND gate and a NOT gate chained together.

source: https://www.usenix.org/legacy/events/smartcard99/full_papers/kommerling/kommerling_html/

",4d5fse,t3_4d5fse,csrabbit,,Comment,1,0,1
4d59te,2016-04-03 02:49:47-04:00,Calclit,Binary Search Algorithm Question?,"(ii) It is known that, for a binary search, the maximum search length for an array of 1000 data items is 10. Assuming this is true, what would be the maximum search length for an array of 2000 data items?	[1] 

(iii) If a computer takes a maximum of 1 second to do a binary search on a set of 1000 data items, state, with a reason, approximately how long you would expect a binary search to take on a set of 1000000 data items.	[2] 

How would you evaluate and prove these questions using mathematics. 

",,,,,Submission,1,0,1
d1nv0vt,2016-04-03 02:57:48-04:00,dandrino,,"Since this is obviously a homework question, the only hint I will provide is to think of the runtime complexity of binary search. ",4d59te,t3_4d59te,Calclit,,Comment,2,0,2
d1nv46f,2016-04-03 03:02:58-04:00,Calclit,,Yes it's a homework question which I already have done. I have the answers of (ii) of 11 and (iii) of 20. I would just like to see explanations of a mathematical form because that hasn't been shown to us.,4d59te,t1_d1nv0vt,dandrino,,Reply,1,0,1
d1nv4te,2016-04-03 03:03:56-04:00,Calclit,,I additionally have been unable to understand the searches and I think some explanations relevant to the questions I have would help.,4d59te,t1_d1nv46f,Calclit,,Reply,1,0,1
d1nvreh,2016-04-03 03:41:04-04:00,dizzydizzy,,"The key to understanding a binary search is that every iteration halves the number of items remaining to be searched

so if you have 1000 sorted items, and the 500'th one is greater than the value you are looking for you throw away items 500 to 1000 and now search items 0 to 499 (so you look at item at position 250 and repeat)

so effectively each extra search iteration doubles the size you can have for the starting number of items.

so for n search iterations you can search 2^n  items.

so for 10 iteration you can search 1024 items

11   2048

12   4096

13   8192


and so on..



",4d59te,t1_d1nv4te,Calclit,,Reply,1,0,1
d1nvt5w,2016-04-03 03:44:02-04:00,Calclit,,So does that imply that Log2(t) is the time required? and do my answers for ii and iii seem logical and correct?,4d59te,t1_d1nvreh,dizzydizzy,,Reply,1,0,1
d1nw1zf,2016-04-03 03:59:21-04:00,dizzydizzy,,"(ii) is correct

Search Iterations = Log (base 2) Number of Items

(iii) is wrong re read the question.

it takes 1 second to search 1000 items (which is 10 iterations of the search algorithm)
",4d59te,t1_d1nvt5w,Calclit,,Reply,2,0,2
4d3xy6,2016-04-02 19:47:12-04:00,slashquit,Any resources online for learning image based compsci and 3d rendering?,"Anyone have any good resources related to 3d rendering or image/video based programming. Currently i use many different libraries...but i honestly do know a whole lot about how they work.

Ive found some cool videos explaining some basics. but I would love to get much deeper into these subjects.",,,,,Submission,7,0,7
d1nvcka,2016-04-03 03:16:37-04:00,mmishu,,"Great question, interested too, maybe try cross-posting to /r/gamedev and /r/learnprogramming ?",4d3xy6,t3_4d3xy6,slashquit,,Comment,1,0,1
d1pzded,2016-04-04 19:13:06-04:00,slashquit,,"reddit failed me. I had to do my own research.

Anyway, I found a nice course(from 2009) that UC Davis uploaded a few years ago on the subject of 3d computer graphics.

https://youtu.be/01YSK5gIEYQ?list=PL_w_qWAQZtAZhtzPI5pkAtcUVgmzdAP8g

Looks like it will be a good starting point. Obviously a little outdated and also pretty annoying to watch because I'm not used to learning in lecture form anymore haha. ",4d3xy6,t1_d1nvcka,mmishu,,Reply,1,0,1
d1q0mya,2016-04-04 19:45:38-04:00,mmishu,,"Lmk if you start it and how it goes!

I should mention I've seen this book mentioned a lot on Quora: http://www.pbrt.org


""This book is stunning.  Its graphics are all memorable and every page makes you want to write a perfect ray tracer.  On top of that, it uses literate programming to annotate code making the book not a book of code, but a book of explanation.   This is also one of the most thorough books on the topic.  It takes you from not knowing any linear algebra to using multivariable calculus and probability theory to define BRDFs and examining the rendering equation.  On top of that, it ships with open source code for playing with."" - http://qr.ae/R0yEdn

From the book's website:

Physically Based Rendering, Second Edition describes both the mathematical theory behind a modern photorealistic rendering system as well as its practical implementation. A method known as ""literate programming"" combines human-readable documentation and source code into a single reference that is specifically designed to aid comprehension. Through the ideas and software in this book, you will learn to design and employ a full-featured rendering system for creating stunning imagery.",4d3xy6,t1_d1pzded,slashquit,,Reply,1,0,1
4d1ugv,2016-04-02 11:10:38-04:00,danbobdavis,Looking for a video about ISP-based virtual networks,"This Youtube video (maybe a TED talk?) was about the future of the IOT and security where it was proposed that each user have an ID associated with their traffic and that all network interfaces become virtual, with actual physical routing handled by the ISP. This would allow ISPs to identify hackers, terrorism, etc. I'm paraphrasing, but it's driving me crazy not being able to find it.",,,,,Submission,4,0,4
4cxr7e,2016-04-01 16:02:49-04:00,vovchak,Memory Address Allocation Problem,"Guys,

I am stuck with the following question:

*Assume that the MAR contains 20 bits, so that we can get access to a memory of 1 MB, but
our computer has 4 MB of memory. Explain how to address all the 4 MB cells using such an
MAR.*

I understand that using two-dimensional memory would enable the 20bit MAR to access 4MB memory (2^12.5 x 2^12.5). Though I am not sure how to prove it.

Any suggestions?",,,,,Submission,8,0,8
d1mg0am,2016-04-01 19:57:18-04:00,njaard,,Segmentation and extended paging (like the x86's Page Address Extension) are two methods I can think of,4cxr7e,t3_4cxr7e,vovchak,,Comment,2,0,2
d1mmmgo,2016-04-01 23:22:58-04:00,willemreddit,,I think you are correct.  But the simplest way would be to shift the MAR left twice.  This would make 1MB * 2^2.  You can do this since memory is Byte addressable.,4cxr7e,t3_4cxr7e,vovchak,,Comment,2,0,2
d1m9rs5,2016-04-01 17:05:52-04:00,lneutral,,You might consider paging.,4cxr7e,t3_4cxr7e,vovchak,,Comment,0,0,0
4cu7ir,2016-04-01 01:51:28-04:00,vecalciskay,Why relational databases are not used in the architecture and design softwares to facilitate collabirate work?,,,,,,Submission,0,0,0
d1ll3or,2016-04-01 05:11:29-04:00,kinkyaboutjewelry,,"I am not sure I understand the question. You seem to claim relational databases are not included in architecture and design part of projects. If that is what you meant, it is not true. Systems with storage must be designed taking storage, volume, growth and access patterns. In many cases relational databases are chosen and the design is informed by the capabilities and limitations they have. 

I hope that helps. If you would like to add more context to your question, we can help you more. ",4cu7ir,t3_4cu7ir,vecalciskay,,Comment,2,0,2
d1llarl,2016-04-01 05:24:11-04:00,vecalciskay,,"Well, an architect friend of mine works for quite a few years now and he was struggling getting large projects done in a collaborative way. Internet bandwidth is not so cheap here in some parts of south america and therefore having a team of architects working at the same time on a dropbox shared 500Mb file it's just out of the question. So they end up having one guy redoing all stuff to gather everything in one nice piece.... but this is wasted time if you ask me. Why can't they collaborate accessing a single relational database, they get fast, reliable data and only what's needed, they wouldn't open the huge file.

My understanding of architecture and design software is rudimentary at best, but I was wondering whether such context exists without having to purchase a 5 figure software.",4cu7ir,t1_d1ll3or,kinkyaboutjewelry,,Reply,1,0,1
d1lwlpb,2016-04-01 12:00:06-04:00,transpostmeta,,"I think you might be looking for git. It's a tool used to collaboratively work on things in a team, and behaves the way you describe. It's not a relational database, though.",4cu7ir,t1_d1llarl,vecalciskay,,Reply,2,0,2
d1m5hvh,2016-04-01 15:26:14-04:00,kinkyaboutjewelry,,"/u/transpostmeta mentions git as a free tool to collaborate on building software in a distributed way. This is a version control system which allows people to work in parallel, merge their work when they need  and so forth. 

I was unsure if what you meant was the collaboration in the actual coding process (where that suggestion can help a lot) or collaborating in designing and documenting the architecture. I don't know which tools you're friend is using, but there's a load of collaborative tools now. Google Docs, Spreadsheets and Drawings are very helpful if not specialized. Some Google Drive extensions for UML and diagrams in general. I hear Office 365 is becoming very good too but I haven't tried it.

Edit: if what they are sharing access to is simply data then you may be right, a database might help. But if what they are sharing access to is logic (code) or binary encoded content, that might not be too helpful. It seems like whatever they are sharing is being changed and they send the file around to prevent concurrent modification. ",4cu7ir,t1_d1llarl,vecalciskay,,Reply,1,0,1
d1pkfp0,2016-04-04 13:33:43-04:00,2nd_world_settler,,"I guess /u/vecalcikay is talking about CAD and 3D-modeling tools, not software design and architecture.",4cu7ir,t1_d1m5hvh,kinkyaboutjewelry,,Reply,1,0,1
d1pu9vc,2016-04-04 17:10:25-04:00,kinkyaboutjewelry,,"Ah good point. Re-reading in that light I think /u/velacikay is correct and there are ways to implement collaborative tools. These have just historically not been built because collaboration is hard to program (real time editing is a tough problem around concurrency and latency) and the need to collaborate is easy to brush under a rug. ""I work on this part of the design, you work on that part."" Teams hardly ever have more than 2 or 3 people working on the exact part of the design and because most of this work is not distributed, people can easily coordinate. This may be changing and whoever comes up with a collaborative solution may gain a relevant portion of the market. We can only speculate. ",4cu7ir,t1_d1pkfp0,2nd_world_settler,,Reply,1,0,1
4csw9n,2016-03-31 20:15:46-04:00,misturjaykay,Switched majors. Where to start off.,Hello reddit. Im a third year and I just switched from an Accounting Major to a Computer Informations Systems Major in my school (the CS major my school offers) I have no idea where to start or what I should practice on to begin my career. I dont know what path im going to choose either but hopefully some of you guys can help. Is it also helpful to look into self teaching or studying some math concepts and theorys? If so what would you recommend? Thank you!,,,,,Submission,5,0,5
d1ls1me,2016-04-01 10:09:15-04:00,Aquifel,,"Just a heads up, computer information systems isn't... exactly a CS degree.  From what I remember, it's more of a functional degree, you'll focus more on actually doing computer-related things (programming/etc.) and, might have a few business classes but, you're unlikely to have to spend a lot of time on theory and, it's unlikely you'll end up taking anywhere near as much math.

Personally, I hated the theory and thought using extra calculus classes as filler for a degree was pointless (Yes, I know, it teaches you how to think.  I knew how to think by calculus 2, thanks.) but, because of the 'easier' course load, a lot of actual CS people tend to look down on CIS.  That stigma associated with CIS may follow you into the job market.

Anyways, to answer your question more directly.  If your program is anything like the CIS programs when i was going to school, you're not going to get into hardcore math.  Have you checked to make sure that the math requirement isn't already satisfied from your time as an accounting major?  There's a good chance that a lot of your required classes are going to be focused more on getting your hands dirty and making things work and less on theory.  Have you planned out what classes you're going to take?  It's likely changed a bit since i went to school but, I'm guessing it's probably a lot of programming with focuses on specific languages/platforms?",4csw9n,t3_4csw9n,misturjaykay,,Comment,9,0,9
d1p2el2,2016-04-04 02:29:59-04:00,misturjaykay,,"Well this semester im currently taking a C++ object orientated course. along with database management systems (pretty much a ton of Access, ERD's and SQL related work). For next semester im going to be taking a Java coding class, Systems analysis and design, and still deciding between a web app dev class or more C++. 
The only math class required in our business school was up to pretty much applied calc and matrix algebra. Anything other than that would just be extra classes unless i wanted to go for a math minor. Sadly my school dosnt offer a real CS degree.
",4csw9n,t1_d1ls1me,Aquifel,,Reply,1,0,1
d1livuh,2016-04-01 03:02:55-04:00,zanidor,,"(Quick note: the CIS majors I've encountered in the past didn't really have a ""hardcore"" CS degree, and were not really qualified to have jobs as programmers. CS programs can vary wildly, though. Not sure what you're looking for, but make sure the program is a good fit for your goals.)

The curriculum of CS classes you will take is probably your best indication of what to study. What classes will you be taking first? The textbooks for those classes might be a good starting point. If you want some alternate reading material or pointers to prerequisites to brush up on, knowing what classes you'll be taking will help us point you in the right direction.",4csw9n,t3_4csw9n,misturjaykay,,Comment,4,0,4
d1l9zit,2016-03-31 22:05:10-04:00,umib0zu,,[SICP](https://mitpress.mit.edu/sicp/full-text/book/book.html),4csw9n,t3_4csw9n,misturjaykay,,Comment,3,0,3
4csh3x,2016-03-31 18:40:16-04:00,BenRayfield,"Does immutable mean all reads get the same value? If so, an immutable object's value is mutable before its first read. Does this lead to paradoxes?","X and Y are immutable objects each with 2-bit value that have never been written or read, or pointers to them or proofs about them.

By design of these objects, if you first read X before first read of Y, X's value will be 10 or 11, and Y's value will be 01 or 00. Or if Y is read first, the opposite of the first bit.

By choosing which to read first, you write 1 or 0 and read 1 or 0. The internal logic, or whoever on the Internet it may call to, doesnt know which of X or Y you will read first (if you ever read), and you dont know which immutable value you will get from the first read except that it will be of 2 possible values, and after an object is read it will always give the same value for future reads.

If X and Y are not immutable, then what is your definition of immutable in terms of reading and writing?",,,,,Submission,0,0,0
d1l5409,2016-03-31 19:56:39-04:00,0x68656c6c6f,,Why does it matter what the possible values could be before the immutable object is instantiated?  Immutability is only a property of the object after instantiation.,4csh3x,t3_4csh3x,BenRayfield,,Comment,11,0,11
d1m725i,2016-04-01 16:01:28-04:00,BenRayfield,,"> after instantiation

what if its lazy-eval?",4csh3x,t1_d1l5409,0x68656c6c6f,,Reply,1,0,1
d1ml740,2016-04-01 22:38:23-04:00,0x68656c6c6f,,"Lazy evaluation (or lazy instantiation) does not preclude immutability.

Think of immutability more of as a compile-time property of an object.  All you need for an object to be immutable is for there to be no operations on that object that change its state.  As others mentioned, there are no guarantees that an immutable object will not change state at runtime (via reflection in Java, for instance).",4csh3x,t1_d1m725i,BenRayfield,,Reply,2,0,2
d1m1hne,2016-04-01 13:53:41-04:00,rlcute,,"This guy is [either in high school or he's really stoned all the time](http://i.imgur.com/oFtT7f2.jpg). Or both. 7/10 /r/iamverysmart . Move along.

Edit: Or possibly some [mental health issues](https://np.reddit.com/r/conspiracy/comments/4apn2i/there_are_basic_math_questions_i_ask_the_math/).",4csh3x,t3_4csh3x,BenRayfield,,Comment,5,0,5
d1l5e87,2016-03-31 20:03:55-04:00,anamorphism,,"immutable means it can't be written to after it has been created.

your situation is not possible, since you wouldn't be able to write to x or y when x or y is read. by design your immutable objects are created without a value. since they can't be altered, they will always have no value.",4csh3x,t3_4csh3x,BenRayfield,,Comment,4,0,4
d1l8vza,2016-03-31 21:35:39-04:00,TheXanatosGambit,,"> immutable means it can't be written to after it has been created

Not by traditional means. But allow me to note that immutable doesn't mean the object's store in computer memory is unwriteable. I know firsthand, in Java, you can use reflection to circumvent immutability (but there's a lot of idiotic things you can do with reflection.) ",4csh3x,t1_d1l5e87,anamorphism,,Reply,1,0,1
d1lwjiq,2016-04-01 11:58:38-04:00,anamorphism,,none of it really means anything if you want to go that far. you can always hand-modify memory and bypass the enforcement of any of these concepts.,4csh3x,t1_d1l8vza,TheXanatosGambit,,Reply,0,0,0
d1m298b,2016-04-01 14:11:46-04:00,TheXanatosGambit,,"Wouldn't really call reflection ""hand modification"" of memory.",4csh3x,t1_d1lwjiq,anamorphism,,Reply,1,0,1
d1m2dnc,2016-04-01 14:14:41-04:00,anamorphism,,"never said it was.

edit: my statement was meant to imply that if you want to start talking about ways to bypass the concept of immutability, then there is really no reason to talk about the topic. at the end of the day, things are just bits in memory that you can always modify. 'hand modification' was meant as you can just point to a memory address in code and write whatever bits you want to that location.

the concept is just there to mostly have a compiler or interpreter tell you you're tying to do something we don't want you to do. same thing with type systems, access modifiers, etc.",4csh3x,t1_d1m298b,TheXanatosGambit,,Reply,0,0,0
d1m6sgz,2016-04-01 15:55:19-04:00,BenRayfield,,"Depends what you mean by object. If its a function that returns a value that happens to be the same every time its called, that function could read the current content at a url when its first called, remember that value, and return it from all future calls. So you can write x or y when they are read by not returning from the read until that download returns.",4csh3x,t1_d1l5e87,anamorphism,,Reply,0,0,0
d1m84ff,2016-04-01 16:26:00-04:00,anamorphism,,"what does any of that have to do with immutability?

it's seems you're getting caught up on the value not changing, the value is not equivalent to the object you're using to store that value. in order to ""remember that value"" you're going to need to have a predefined object to hold that value that's created before the value is determined. you're then writing to that on first read, meaning your object is not immutable.

you can create a new immutable object each time with the appropriate value, but that really has nothing to do with any of what you're proposing since the immutable object doesn't exist prior to a read.

i guess the whole problem is that you can't define an immutable object without a value. it's either going to just be uninitialized garbage memory or some other default value like 0 or null, but all of those are still values. a program has no concept of you merely thinking that this immutable object exists.",4csh3x,t1_d1m6sgz,BenRayfield,,Reply,2,0,2
d1o2m0p,2016-04-03 10:18:09-04:00,BenRayfield,,"Not all programming is object-oriented. In functional-programming, object and value are the same.

> i guess the whole problem is that you can't define an immutable object without a value.

https://en.wikipedia.org/wiki/Dangling_pointer to where the value will be",4csh3x,t1_d1m84ff,anamorphism,,Reply,1,0,1
d1o8p4i,2016-04-03 13:08:22-04:00,anamorphism,,"no they aren't. when you return values from a function, an object is created with metadata about that value, whether that's an entry in some symbols table used by your compiler or interpreter or something else. object means thing or item in this case. the word is not meant to imply some application of concepts from oop.

a dangling pointer still has a value (the memory address that contains garbage memory), and a different value is still defined in that memory. you just don't have any idea of what's stored in that memory anymore.",4csh3x,t1_d1o2m0p,BenRayfield,,Reply,1,0,1
4cqemd,2016-03-31 11:06:13-04:00,wike_mithrow,What is a website or program that will help me develop a specific type of web form for my job?,"So let me begin by saying that I am not a CompSci guy, but I am somewhat tech savvy. Mostly I just need help finding the right tool for me...so here is what I need:
I am in sales, and whenever I get a new client, I have to send them a form with 10 questions that they need to fill out and return to me before I can move forward. What i've been doing is sending them the questions via email, and asking them to return them to me the same way...what ends up happening is a lot of confusion. I get the questions back in the wrong file types, wrong formatting that i have to fix myself, and a lot of the time it takes them weeks to get it back to me. SO! What I want is to make a web based form, where they can click a link, go to the form, fill it out, and when it is submitted it goes to my email in a .txt file or something like that. A button to upload a photo would go a long way as well but is not necessary. So I guess what I need is a design tool and a place to host the form. Sorry for being long winded, but HELP!",,,,,Submission,0,0,0
d1l689q,2016-03-31 20:25:16-04:00,fragmede,,"https://apps.google.com/intx/en_uk/products/forms/
Also survey monkey and a bunch of other ones...",4cqemd,t3_4cqemd,wike_mithrow,,Comment,2,0,2
d1khty4,2016-03-31 11:15:18-04:00,worldDev,,Wufoo is one I've seen used by an ad agency client of mine.,4cqemd,t3_4cqemd,wike_mithrow,,Comment,1,0,1
4cn13t,2016-03-30 17:55:46-04:00,ronsoness,I want to make a beginner python project. What should I do?,"Hi everyone. I am a web developer. I work in PHP SQL, but no other languages. Primarily object-oriented usage. I learned Python through codecademy.com. It's cool because they give you a terminal and give you exercises. The problem is I have no idea what I can do with Python. I hear a lot of business people use it. Is there a cool looking project that I can make and then upload onto a site in order to show off my skills and more importantly, help me cement my knowledge in python by forcing me to use it in a project? I would definitely prefer a recommendation that is business-based. Thank you.",,,,,Submission,4,0,4
d1jrnkf,2016-03-30 19:41:03-04:00,pwbdecker,,"[This](http://www.djangobook.com/en/2.0/index.html) will teach you how to build a web application using Python and Django.  This will teach you about request handlers, database layers, data validation, unit testing, etc.  All very useful skills.",4cn13t,t3_4cn13t,ronsoness,,Comment,7,0,7
d1l24vb,2016-03-31 18:39:55-04:00,ronsoness,,"Thank you. Very complicated to run and get setup so far, compared to Xamp and PHP, but it looks like a good guide. Any recommendations for a project? ",4cn13t,t1_d1jrnkf,pwbdecker,,Reply,1,0,1
d1lesc6,2016-04-01 00:21:40-04:00,pwbdecker,,I dunno I mean there's lots of things you could do.  Do you like sports? Make a website that helps you keep track of your favourite team.  Do you like cooking? Make a simple recipe site.  Make a simple photo sharing site that supports comments and 'liking' photos.  Something that lets people put in data and then render it back out.  Could be the next billion dollar idea :),4cn13t,t1_d1l24vb,ronsoness,,Reply,1,0,1
d1ro03m,2016-04-05 23:06:54-04:00,coconutscentedcat,,Maybe you can create an algorithm that visualizes data into colorful graphs. And then if you want to build off that you can make it fetch data online and update the graph in real time.,4cn13t,t3_4cn13t,ronsoness,,Comment,2,0,2
d1rttwi,2016-04-06 02:17:12-04:00,ronsoness,,"CSC, thanks. But, do you think creating an algorithm is within my reach? I'm self-taught with web dev, but I am not familiar with computer science and algorithm principles.  Also, is there something in django that allows you to refresh - ajax style ? I'm assuming there probably is, otherwise Django wouldn't be used for web applications? ",4cn13t,t1_d1ro03m,coconutscentedcat,,Reply,1,0,1
4cm2gb,2016-03-30 14:24:54-04:00,FilthyPuns,Is being on a password secured wifi with a large number of people any more secure than being on an unsecured wifi with the same number of people?,"I apologize if this question has been answered before or if it's really stupid and demonstrates that I know nothing about wifi or encryption. 

Like a good little boy, I avoid transmitting any sensitive information over a network that is open and not secured with a password. I've heard the reasoning put forth for this that other people on the network can somehow access the information since it's not secured. 

But are large password-protected networks subject to the same security risk, with the security coming from the fact that there are limited people on the network? Or is there a layer of encryption that happens at the device level on password-secured networks to keep others on the same network from peeping your infos? 

",,,,,Submission,6,0,6
d1jff4d,2016-03-30 15:03:03-04:00,BukkitBoss,,"I'm far from an expert, but a good way to look at this is not so much whether the network is secure or not- but whether you implicitly trust the people on the network. I'd save important online transactions for networks that I know are reasonably secure.

From what I understand about WEP/WPA, if anyone else is already on the network and has the key they can pretty much work backwards to see what you're doing. Since you all have a single *shared key* it's just a matter of using software to decrypt the data you're sending. This is still better than an open network though, as it keeps out hooligans who might lazily snoop about if the network was open to air.

Now this isn't touching on whether or not the website you're using is secure in general. Is the website using HTTPS? Is that app you're using sending data encypted, or pain text? These are other aspects that could come into play and frankly I don't know enough about to add much on. :P

*Edit* - TL:DR: **Yes**, they're still vulnerable to snooping. But they are more secure in general as you have control who is on the network.",4cm2gb,t3_4cm2gb,FilthyPuns,,Comment,2,0,2
d1jgsx8,2016-03-30 15:32:17-04:00,njaard,,"If the web site is SSL, you're safe, and there's no reason to be worried.",4cm2gb,t3_4cm2gb,FilthyPuns,,Comment,2,0,2
d1juvkk,2016-03-30 21:03:26-04:00,ldpreload,,"Anyone who knows the password can set up a rogue router with the same SSID and password, and computers will connect to whichever one is stronger.

If you're doing financial transactions, those transactions should be over SSL/TLS or some similar end-to-end-secure method. If you're using some actual physical credit card device (including a Square reader), it should encrypt the transaction on the device and send it encrypted to your payment processor, but check its documentation. If you're using a website and typing in a credit card number, make sure the website is using HTTPS competently (at the least, _every_ page and link and resource involved in the shopping flow needs to be HTTPS: any non-HTTPS page can be compromised). As long as the transaction is end-to-end-secure, you can safely do transactions over untrusted networks.",4cm2gb,t3_4cm2gb,FilthyPuns,,Comment,2,0,2
d1lswba,2016-04-01 10:31:29-04:00,FilthyPuns,,"Perfect. That's what I needed to know!

",4cm2gb,t1_d1juvkk,ldpreload,,Reply,1,0,1
d2ucy73,2016-05-05 17:40:02-04:00,TheTarquin,,"The two major consume WiFi security protocols are WEP and WPA2.  WEP is easily decrypted just by listening to enough network traffic, and so its use is discouraged.

WPA2 uses different encryption keys for every connection, so just having the WiFi password isn't enough to listen to someone else's traffic.  It might be possible under certain scenarios, but it would take some significant effort.

All your credit card transactions should be going over an HTTPS connection, however, which would be secure from eavesdropping from even someone who had compromised the keys for the WPA2 WiFi network.

TL:DR: WPA2 networks are meaningfully more secure than unprotected or WEP-protected networks under all conditions.",4cm2gb,t3_4cm2gb,FilthyPuns,,Comment,2,0,2
d1jdq9f,2016-03-30 14:27:01-04:00,FilthyPuns,,"So let's say I run a pop up shop and the venue provides wifi for vendors that requires a password. Is it safe to run credit card transactions over this network, or is it possible that another vendor with some computer savvy could steal 'em? ",4cm2gb,t3_4cm2gb,FilthyPuns,,Comment,1,0,1
d1jg8hl,2016-03-30 15:20:28-04:00,BukkitBoss,,"If you're unsure of the network in general, you can always err on the side of caution. For a POS terminal you could potentially just use cellular data for transactions. There are also devices (such as the square reader) that allow you to turn a tablet/phone into a point of sale terminal, bypassing the need for a wireless network/computer entirely.

Same caveat as my other post, I'm not an expert on this stuff. As they say, a little knowledge is a dangerous thing.",4cm2gb,t1_d1jdq9f,FilthyPuns,,Reply,2,0,2
d1js0us,2016-03-30 19:50:40-04:00,Pseudofailure,,"In this specific case, wifi could be problematic, but it's not the biggest problem: you should be most concerned with the security of the system to which you're sending the transactions. If that's properly secured with TLS, then theoretically you should be fine, even on public wifi _in terms of protecting credit card transactions_. ",4cm2gb,t1_d1jdq9f,FilthyPuns,,Reply,1,0,1
4cln4d,2016-03-30 12:54:24-04:00,imaque,Is it theoretically possible to reverse engineer someone's two factor authentication key if you were to have a list of past codes and timestamps to go with those codes?,"If so, how would it work, and what's the n for how many codes/timestamps would be needed?

I'm just curious. I wonder if my employer could potentially figure this out, given that I login with two factor on a number of personal accounts all the time. Not that I think that they would. I guess it's more of a curiosity about the overall security of two-factor in general.

Thanks!",,,,,Submission,22,0,22
d1jausu,2016-03-30 13:25:41-04:00,dmazzoni,,"It's designed to be practically impossible.

It's similar to most other pseudorandom number generators in that the internal state is a large number (64 bits, maybe) but the sequence is only the last few digits.",4cln4d,t3_4cln4d,imaque,,Comment,15,0,15
d1jexe9,2016-03-30 14:52:39-04:00,Jacob2040,,Practically impossible like cracking 4096 but encryption? Or closer than that. ,4cln4d,t1_d1jausu,dmazzoni,,Reply,2,0,2
d1jvla8,2016-03-30 21:22:23-04:00,ldpreload,,"HOTP and TOTP (the two open standards for two-factor) are based on HMAC-SHA1. That's a 160-bit hash, with all 160 bits being useful for the security level (collision attacks don't apply to HMAC). Even if the entire output were revealed, a MAC is basically a symmetric version of a digital signature: it's supposed to be safe to reveal large numbers of MAC outputs without risking the key, just like you can publish large numbers of digitally-signed documents without risking the signing key. According to [keylength.com](https://www.keylength.com/en/compare/), a 160-bit symmetric key is roughly equivalent to something in the range of 5000&ndash:7000-bit asymmetric encryption, depending on who you ask. So it should be roughly as hard as seeing a bunch of documents signed with a more-than-4096-bit key, and trying to guess the key or forge a new signature.",4cln4d,t1_d1jexe9,Jacob2040,,Reply,2,0,2
d1jgpt1,2016-03-30 15:30:31-04:00,101C8AAE,,"If the two-factor is implemented properly according to RFC 4226, having knowledge of previous keys shouldn't give you any upper hand in cracking it. Generation is deterministic, but it involves some secret that you don't know, so you would need to crack that secret.",4cln4d,t3_4cln4d,imaque,,Comment,5,0,5
4citia,2016-03-29 22:20:59-04:00,Cheeseologist,What's more secure: packet-filtering firewalls or proxy servers?,"I'm kinda new to this stuff, but I know that packet filters can be stateless, stateful, and/or application level. I know that the difference with stateful filters is their ability to recognise data in the context of whatever ""bigger picture"" it exists as a part of. I don't know much about application-level filtering, to be honest -- that is, I'm not entirely sure what the comparative advantages and disadvantages are. I also know about Deep Packet Inspection, which, from what I understand, differs from a proxy in that relies on signature verification (whereas proxies rely on...?).

Evidently, I'm not entirely sure on how these technologies work. I know that proxies inspect the contents of a packet (unlike stateless and stateful packet filters), but I have little idea of what it is they do when they inspect that data.

Now, back to the question... I can surmise that, on a surface level, proxy servers are more secure than stateless/stateful packet-filtering firewalls. I'm less able to determine whether proxies or DPI firewalls are more secure... I've [read](http://www.ranum.com/security/computer_security/editorials/deepinspect/) that DPI firewalls permit by default, looking for matches with a blacklisted signature list, while proxies refuse by default. My problem with that notion is two-fold:

1. If DPI firewalls are defined as such purely based on their ability to match signatures... How is that any more secure than a stateful packet-filtering firewall (which I understand recognises context based on protocols)?

2. Wouldn't permitting by default and refusing by default be a customisable thing, in that a DPI firewall could use a whitelist instead of a blacklist? And for proxies, I thought that they search for *unwanted* packet data; that is, I thought that proxies permit by default?

Jeeeez I swear the more I look into stuff the more I get confused. I'm just trying to figure out whether one really is more secure (fundamentally) than the other. Can you guys make this make a bit more sense?

Also, extra question: Is the best security option to use both a firewall and proxy server? Why or why not? Would doing so significantly hamper performance?

I don't know if I'm breaking a bunch of rules here. I hope not!",,,,,Submission,6,0,6
d1t6nk1,2016-04-07 01:04:55-04:00,TheXanatosGambit,,"> If DPI firewalls are defined as such purely based on their ability to match signatures... How is that any more secure than a stateful packet-filtering firewall (which I understand recognises context based on protocols)?

I think you're a bit confused about packet filtering. SPI alone can't examine the payload of a packet. That's DPI's job.

Think of SPI as a parking ramp attendant. I can control who enters, who leaves, and keep track of the traffic via a ticket system. But that's about it. I can't examine the contents of your car to smoke out any undesirable payloads. You could bring in weapons, drugs, and I would be none the wiser. (If it were stateless, we wouldn't even have a ticket system, it would be more like an honor system.)

Think of DPI as border patrol. I can stop you and subject your vehicle to rigorous examination to root out those 10 kilos of cocaine you're trying to smuggle into the country. If I find a payload I don't like, I can detain you indefinitely. (If the stream was encrypted, however, you would essentially trump my authority. Well shit, you're using a military vehicle, I have no jurisdiction to examine it.)

> Wouldn't permitting by default and refusing by default be a customisable thing, in that a DPI firewall could use a whitelist instead of a blacklist?

It is, but *generally speaking*, defining what isn't allowed is more feasible than defining what is. Think of censorship. The English language exceeds 1 million words. If I want to prevent you from using 10 specific keywords in any emails you send, wouldn't it be easier to blacklist 10 words than whitelist the other million?

> I also know about Deep Packet Inspection, which, from what I understand, differs from a proxy [server] in that relies on signature verification (whereas proxies rely on...?).

You're comparing apples and oranges here. Perhaps you're thinking of a proxy firewall, which is not the same thing as a proxy server. A proxy server alone doesn't perform the functions of a firewall. It sits as a man-in-the-middle and forwards requests. At its most basic level, that's its only job. That's not to say a proxy server can't be given additional functionality. You can use it to cache web pages for performance, filter content for security, etc. Indeed, they are often used for these purposes.

As for proxy firewalls, their usage has been steadily declining. They essentially perform the same deep inspection functions as DPI, but incur more overhead since (like a proxy server) they function as a man-in-the-middle, thus slowing down traffic by preventing direct connections.

I understand the water can look a bit muddy at times: you can blame that on the frequent misuse of terminology. Anyone feel free to correct any mistakes, it's been a few years since college.",4citia,t3_4citia,Cheeseologist,,Comment,1,0,1
4cimox,2016-03-29 21:34:05-04:00,mk3s,I need to find an online equivalent of a course as a prerequisite for grad school.,"I am applying to the University of Maryland (college park) cyber security masters program and as a prerequisite for admission i need to satisfy a requirement for intermediate level programming concepts (preferably C) before I can be considered for admission. I need to find a course that is similar to the one offered in the UMD CS undergrad (shown in the link below).

http://www.ece.umd.edu/undergrad/courses/100-level/enee150

I need it to be an online course from an accredited university but I am not sure the best way of going about finding something aside from emailing each and every university and asking them about it individually. Anyone able to help out??

Thanks!",,,,,Submission,6,0,6
d1ixm4r,2016-03-30 07:49:11-04:00,falafel_eater,,"If the course requirement you need to satisfy is just an introduction to programming with C, ask whether just having experience in the language is sufficient or if there is a quiz/exam you can take to show mastery.  
If this is possible, it may be easier to study C by yourself.",4cimox,t3_4cimox,mk3s,,Comment,2,0,2
d1iy8hb,2016-03-30 08:15:14-04:00,mk3s,,"The requirement is ""intermediate"" level programming. The admissions office didn't specifically mention a test-out option but I also didn't ask. I have a background in programming but haven't used it in any real capacity in a few years outside of some Python and shell scripting. I think the best avenue for me (and their admissions department agrees) is that I find a one-off course to refresh my knowledge and then apply. ",4cimox,t1_d1ixm4r,falafel_eater,,Reply,2,0,2
d1izgwy,2016-03-30 08:59:28-04:00,rich_jj,,"There are a lot of accredited brick-and-mortar colleges that offer online computer science programs, but most that I've seen are graduate level.  A while ago I needed to fill some undergrad pre-requisites and decided on Regis University, which has been heavily emphasizing and marketing its online programs.  My reasons were that (1) price wasn't bad, (2) they had a lot of courses, (3) courses are accelerated.  They also advertise that they have the only ABET accredited *online* BSCS program in the country.  What are the downsides?  Regis isn't an elite top university.  They don't have online lectures, just lots of reading and programming on your own, with required posting to the online forum.  And you often get an ""instructor"" instead of a professor (but does that really matter?).  

http://www.regis.edu/CCIS/Academics/Degrees-Programs/Undergraduate-Programs/BS-Computer-Science.aspx

You can google for any reputable school name along with ""computer science"" and ""online"" or ""distance"" to see what they offer.  (I don't have my old notes on this, but I think I remember Oregon State, Old Dominion, and something in Maryland among my options for online undergrad CS.)  Once you think you have a good candidate, call or email their admissions office and find out if you can just take individual courses without being admitted to one of their programs.  I think Regis let me take up to 6 courses before being admitted.  

Edit: Now that I looked a little closer at the class you need (""ENEE150 Intermediate Programming Concepts for Engineers""), I'm not sure that you'll easily find an exact match.  That looks fairly specific to C unix engineering applications.  Maybe you can talk with an advisor at the University of Maryland to see what their requirements are for an ""equivalent"" course.  You might want to find online *embedded* software courses.  On the Regis course list for their [BSCS](http://www.regis.edu/CCIS/Academics/Degrees-Programs/Undergraduate-Programs/BS-Computer-Science.aspx) program there isn't a match that's obvious to me.  I'm not confident about it, but some looked like they could possibly be related:

* Data Structures
* Principles of Programming Languages
* Advanced Programming and Algorithms
* UNIX Operating System
* Digital Design Theory and Technologies
",4cimox,t3_4cimox,mk3s,,Comment,2,0,2
d1j5ins,2016-03-30 11:32:54-04:00,mk3s,,Thanks. I really appreciate the time you took looking into all this. I will definitely check these out!,4cimox,t1_d1izgwy,rich_jj,,Reply,1,0,1
d1induv,2016-03-29 23:32:28-04:00,crookedkr,,Will edx courses work? You can take courses from many places most of which would probably cover this requirement.,4cimox,t3_4cimox,mk3s,,Comment,1,0,1
d1iy5sv,2016-03-30 08:12:14-04:00,mk3s,,I believe it needs to be course credit from an accredited university. Not sure edx would be sufficient unfortunately. ,4cimox,t1_d1induv,crookedkr,,Reply,1,0,1
d1j2ka9,2016-03-30 10:25:02-04:00,crookedkr,,"The other one i was thinking of is Harvard Extension School. It's a degree granting program (so both accredited and the course can be taken for credit) and you can take the courses from any location.

Not sure that they have a course that covers all of the things in the UMD one (honestly it sounds more overview than anything). CS61 is probably your best bet in terms of C/unix course.",4cimox,t1_d1iy5sv,mk3s,,Reply,2,0,2
d1j5jlm,2016-03-30 11:33:28-04:00,mk3s,,Thanks I will definitely check this out!,4cimox,t1_d1j2ka9,crookedkr,,Reply,1,0,1
d1j2wt0,2016-03-30 10:33:28-04:00,randopoit,,"There are some limited options for university credit with edx.
https://www.edx.org/credit",4cimox,t1_d1iy5sv,mk3s,,Reply,1,0,1
d1jfhrb,2016-03-30 15:04:37-04:00,mk3s,,Checked it out but didn't see anything pop out that might work for me. Thanks though. ,4cimox,t1_d1j2wt0,randopoit,,Reply,1,0,1
4cilgv,2016-03-29 21:25:24-04:00,kamakaZ101,How did you learn to code?,"As the title suggest, I'm wondering how people in the Computer Science field learned how to code? I'm currently taking classes for my bachelor's degree and I am having trouble. I understand the concepts fairly well and often am asked how I can explain it but can't code it and I don't know what to say. I am struggling on translating my thoughts to code and I don't know how to improve on this skill. All I get is ""just keep practicing"" but that doesn't help if I can't finish assignments and am going to fail a class. I guess I'm just looking for any suggestions or help at this point. ",,,,,Submission,9,0,9
d1iinf5,2016-03-29 21:31:24-04:00,cheryllium,,"Instead of answering your title question, I will try to offer some advice for your situation: 

> I am struggling on translating my thoughts to code and I don't know how to improve on this skill. 

I usually walk my students through these steps to help them learn how to translate thoughts to code: 

1. Explain how to solve the problem, in paragraph form (like you are  telling a friend how to do it). 
2. Break this down into a numbered list of steps. 
3. If you find yourself repeating steps, you probably need a loop or a function. 
4. If you have information you refer to in multiple steps, you probably need a variable. 
5. Finally, you can try turning these steps into comments, and then filling it in with code. 

If you can think of a small example, I can walk you through it, if you think it might help. 

Good luck! ",4cilgv,t3_4cilgv,kamakaZ101,,Comment,9,0,9
d1iji46,2016-03-29 21:51:53-04:00,DoItForMom,,"Also when you face something insane just start swinging.  Just go nuts, solve something small, nomatter how nasty the solution. Then when it works polish it. Google stuff(or consult your classlitterature) that doesn't work like you want or thought. 

This is how I did it 5 years ago when I started CS and I surrvived and learned to love it! 

plus ofcourse borrow and try to explain/understand alot of code from slides and what not that they try to teach you. 

All this should be done in small iterations together with the solid tips above!",4cilgv,t1_d1iinf5,cheryllium,,Reply,7,0,7
d1isl0l,2016-03-30 02:50:43-04:00,kamakaZ101,,"Thanks for the tips. I will try to implement that methodology. As far as an example goes I can't think of one specific one. I guess where I feel my classes has lacked is code structure. Like I can understand reading code and coding concepts, but then I get an assignment that is vaguely described(though he has a specific framework in mind) and I can't envision the code needed. I feel this comes with practice, but I don't know how to reinforce the idea of translating thoughts of what you want done to organized clean code.",4cilgv,t1_d1iinf5,cheryllium,,Reply,3,0,3
d1j8s8y,2016-03-30 12:42:04-04:00,rfinger1337,,"I agree entirely, and I will add one thing:

Break it down to it's smallest parts, then write the easy parts.  When you get everything easy written, then look at the last problem (the one you didn't understand from the beginning) and see if you can cut that problem in half.  

If you can, and you understand how to do half of it, do that.  Then look at the problem again, and cut it in half again.  If you can solve the easy part, do it.

when you finally hit the little bit that you are completely stuck on, hit up google (stack overflow) with the one tiny question.  The answer will be there, and you can implement their solution.

suddenly, you have a working solution and you didn't stress about the difficult part until you needed to.

good luck!",4cilgv,t1_d1iinf5,cheryllium,,Reply,2,0,2
d1ildu8,2016-03-29 22:38:15-04:00,SolarAir,,"I just tried to learn to code from online sources, then I sought out some books from the library.  Neither one of those helped me at all. I ended up taking AP CompSci in high school, and actually having a teacher helped to make some of the concepts 'click' for me. I'm now taking a *Engineering Problem Solving with C* class in college, and if I didn't have the AP CompSci class in high school, I think I would be completely lost right now.

In both C and java, I really only know the basics (up to like pointers, malloc/calloc/realloc/free, and arrays in C, and up to interfaces, classes, the merge sort, basic GUI design, and simple objects), and haven't gone too far into advanced stuff with either language. 

---
Whenever I don't know how to do something, I first try to code what I can. There is probably a good way to code something, but I often may not see the optimal way at first. I may write something that takes 16 lines of code that should have been done in 4 lines. Once I get something down though, I generally find it easier to improve it based on what I think needs to happen next.

 As far as class assignments go, sometimes I'll just get fixated on something, thinking it needs to be done some way, and not being able to figure it out. Sometimes I'll need to ask the professor or a classmate if I'm on the right track, and they'll possible point out I wasn't going about it the right way, or I may have been over-thinking it.  

Some good advice, is never try to learn coding alone. Classmates in both my highschool AP CompSci and my college C class were able to help me in rough spots. While you want to be able to be an independent coder, being able to ask for help is something you should take advantage of as a student when you're learning, as it may not be offered in the real world when you're tasked to work on some private project alone when you're expected to apply and use everything you learned.",4cilgv,t3_4cilgv,kamakaZ101,,Comment,2,0,2
d1ivug2,2016-03-30 06:13:24-04:00,mrdevlar,,"The most ""aha"" moment of learning to code came to me when I started trying to get different languages to do the same thing. Once you start doing that you realize that coding is far more abstract than simply memorizing which command does what you want. ",4cilgv,t3_4cilgv,kamakaZ101,,Comment,2,0,2
d1ix7yh,2016-03-30 07:30:38-04:00,vz83,,"Do something fun. It took me a while before everything ""clicked"" but when you're working on a project you actually enjoy, it's much easier to put up with any problems that occur. 

(Also StackOverflow)

Hope this helps",4cilgv,t3_4cilgv,kamakaZ101,,Comment,2,0,2
d1ixsjg,2016-03-30 07:56:54-04:00,koorb,,"I kept doing projects and learning languages and platforms until I got it. TBH, it wasn't until I learned Unity3D that it all snapped into place. I knew about OO and had used it, but now I can see through it all and architect fractal design patterns that scale.

The only advice I can really give is, outline what your thing should do and then break it down into small steps. Computers are very simple things, it is only their ability to do simple things very quickly that makes them seem complicated.",4cilgv,t3_4cilgv,kamakaZ101,,Comment,2,0,2
d1jfg89,2016-03-30 15:03:44-04:00,ksryn,,"What you need to know is that coding and CS are not directly related: you can code without having taken a single CS class, and can struggle with code while being good in CS.

Coding/programming is problem-solving using a programming language. So, if you can solve a problem, you can write the code for it too. It helps if you're comfortable with the language.

Have you tried starting with simple things like:

* printing even/odd numbers less than N
* printing the first N prime numbers (more such problems on [Project Euler](https://projecteuler.net/))

and then moving on to implementing basic data structures and algorithms? Once you're used to this, you'll be able to attempt more complicated stuff.",4cilgv,t3_4cilgv,kamakaZ101,,Comment,2,0,2
d1jg0hy,2016-03-30 15:15:55-04:00,kamakaZ101,,"I've only attempted homework programs. They tend to be my focus and I feel so drained on trying to make them work I never try to start something new on my own. I'll try looking at those test projects though.

At this point I'll try pretty much anything.",4cilgv,t1_d1jfg89,ksryn,,Reply,2,0,2
d1jgaoy,2016-03-30 15:21:43-04:00,ksryn,,Just wondering what kind of programs are giving you a tough time?,4cilgv,t1_d1jg0hy,kamakaZ101,,Reply,2,0,2
d1jhlrz,2016-03-30 15:49:16-04:00,kamakaZ101,,"Mainly the longer assignments. Labs (which are shorter) I do ok on. 

[These are the assignments I've gotten for example ](http://www.cs.uwec.edu/~stevende/cs245/programs.htm)

[While these are labs](http://www.cs.uwec.edu/~stevende/cs245/labs.htm)

As for specific problems I couldn't tell ya. Sometimes I get it and can do fine and worry about debugging. Other times I'm at a total loss.",4cilgv,t1_d1jgaoy,ksryn,,Reply,2,0,2
d1jjuhd,2016-03-30 16:36:24-04:00,ksryn,,"These assignments are not beginner-level (that is for some one with a month or two of general programming experience). They not only require programming knowledge, but also knowledge of the respective domains.

For e.g., the grammar assignment requires that you know about Context Free Grammars and BNF. You read a grammar file and convert it into an in-memory version of the grammar. You then use a random number generator to resolve non-terminals repeatedly till all parts are terminals.

The mosaic one requires that you understand how to divide an image into N number of pieces, determine the color of each piece and then find a matching tile to replace the piece.

For someone who knows how these things work it's something that one can do in a few days (less than a week).

---

I would suggest going through the instructions a few times and then start with the smallest things possible, say reading a file. You then keep adding additional functionality incrementally till you have the entire program working.",4cilgv,t1_d1jhlrz,kamakaZ101,,Reply,1,0,1
d1jkhmi,2016-03-30 16:50:15-04:00,kamakaZ101,,"They are suppose to be harder. Normally we get like 2 weeks on them. 

I try going through the instructions and splitting it up the tasks. It's just rough. Sorry not trying to complain, it's just very frustrating when idk what to do to improve on these areas",4cilgv,t1_d1jjuhd,ksryn,,Reply,1,0,1
d1jkut1,2016-03-30 16:58:06-04:00,ksryn,,"Forget coding for a second. Are you able to go through the instructions and devise a general solution to the problem? That is, decide on a series of steps that will solve the problem?",4cilgv,t1_d1jkhmi,kamakaZ101,,Reply,1,0,1
d1jbaq0,2016-03-30 13:35:06-04:00,CARGLE,,I'm right where you are bro?,4cilgv,t3_4cilgv,kamakaZ101,,Comment,0,0,0
4cfhaf,2016-03-29 09:45:53-04:00,enzio901,How does sustainable development apply to software engineering?,"World bank website defines sustainable development as follows.

>""Development that meets the needs of the present without compromising the ability of future generations to meet their own needs.""

For me it is clear how it applies to fields like agriculture, construction and manufacturing because they produce tangible products. So, it is easy to measure the positive and negative impacts of their practices on the natural environment and society.

But the concept is a bit abstract for professionals creating software. The product (software) is intangible. There is no direct impact on the natural environment due to the process of typing the code. It's like an author writing a novel. But except he writes it on his computer without using paper. 

So, how does software industries and professionals who strive sustainable development relate it to their fields. Do they consider the end result of the software. That is what it will be used for. For example -writing software for functioning of a solar powered device can be called sustainable development in comparison to writing software for a (hypothetical) coal powered lawn mower. What do you think?",,,,,Submission,2,0,2
d1hpt7o,2016-03-29 10:54:14-04:00,panderingPenguin,,"Well you may be just ""typing code"" but think about software actually runs. The environmental hit from a car doesn't come when the automotive engineer is making CAD drawings and doing aero simulations either. The hit comes when you actually run your software. More efficient code decreases power consumption and energy usage. If you're righting something of large scale, say something that'll run in a datacenter, the efficiency of that code can have a pretty massive impact.",4cfhaf,t3_4cfhaf,enzio901,,Comment,2,0,2
4cc9zl,2016-03-28 17:30:09-04:00,hudsok,"Let S(a) be the sequence of all pairs of positive integers (b, c), such that a is b times c, sorted by ascending c. Let T be the concatenation of S(a) for all positive integers a. Let T[a] be the item of T at index a. How can I find T[a] if a is given? How can I find a if T[a] is given?","When calculating T[a] for a given a or a for a given T[a], a may have around 2000 digits in its decimal representation. The algorithm must process integers with this order of magnitude in a reasonable time frame.


I'm not sure if this is the right forum to ask this question. If it isn't, please point me to a site where this question fits better.",,,,,Submission,6,0,6
d1gvsiw,2016-03-28 17:50:41-04:00,james41235,,"Well, by my reading of it:

T[a] is S(a) for a given a.  If that's not correct, disregard the rest.  
So for any a, every item in T[a] is a pair that will multiply together to make a.  Personally I'm not sure there's a reasonable algorithm (by big-O notation) for finding T[a].  For some a the amount of items in T[a] is exponential, not to mention it depends on the value of a, rather than the representation.

However, the other direction, finding a for T[a] is easy.  Just pick the first element of T[a] and multiply, that's a.",4cc9zl,t3_4cc9zl,hudsok,,Comment,1,0,1
d1gx8j5,2016-03-28 18:25:56-04:00,hudsok,,"Unfortunately, that's not correct. Maybe it helps if I provide you some examples?

For example, S(12) is the following sequence:

S(12)[0] = (12, 1)

S(12)[1] = (6, 2)

S(12)[2] = (4, 3)

S(12)[3] = (3, 4)

S(12)[4] = (2, 6)

S(12)[5] = (1, 12)

Now, T is the concatenation of S(a) for all positive integers a. That means that T begins like this:

T[0] = (1, 1) = S(1)[0]

T[1] = (2, 1) = S(2)[0]

T[2] = (1, 2) = S(2)[1]

T[3] = (3, 1) = S(3)[0]

T[4] = (1, 3) = S(3)[1]

T[5] = (4, 1) = S(4)[0]

T[6] = (2, 2) = S(4)[1]

T[7] = (1, 4) = S(4)[2]

T[8] = (5, 1) = S(5)[0]

T[9] = (1, 5) = S(5)[1]

T[10] = (6, 1) = S(6)[0]

T[11] = (3, 2) = S(6)[1]

T[12] = (2, 3) = S(6)[2]

T[13] = (1, 6) = S(6)[3]

T[14] = (7, 1) = S(7)[0]

T[15] = (1, 7) = S(7)[1]

T[16] = (8, 1) = S(8)[0]

T[17] = (4, 2) = S(8)[1]

T[18] = (2, 4) = S(8)[2]

T[19] = (1, 8) = S(8)[3]",4cc9zl,t1_d1gvsiw,james41235,,Reply,1,0,1
d1igij4,2016-03-29 20:41:49-04:00,TheSageMage,,"Have you discovered a solution yet? Algorithmically this reminds me of the Fibonacci sequence in that you have the know the previous state because there's no constant you can program against. 

Every number a is going to have a different number of divisors, which is going to complicate T. 

Are you looking for an proof of correctness or an algorithm to find the T[a]?",4cc9zl,t3_4cc9zl,hudsok,,Comment,1,0,1
d1iofbp,2016-03-30 00:04:03-04:00,james41235,,"That's what I'm thinking.  I can't think of any mathematical relationship in the number of divisors and the number.  I mean... finding all the divisors of a number X is O(X), and then you'd have to do that for all X <= a, so you're still talking O(a^2 ).  This isn't a real polynomially bound function.",4cc9zl,t1_d1igij4,TheSageMage,,Reply,1,0,1
d1irzew,2016-03-30 02:20:19-04:00,TheSageMage,,Are you looking for a mathematical proof of why this function cannot be constructed without iteration? I'm wondering where this question came from...,4cc9zl,t1_d1iofbp,james41235,,Reply,1,0,1
d1jkfbn,2016-03-30 16:48:53-04:00,hudsok,,"Unfortunately, I haven't discovered a solution yet.

I'm looking for two algorithms: One algorithm to find T[a] for a given a. And another one to find a for a given T[a].

However, if you know an algorithm that, given a and (b, c), checks whether T[a] does or does not equal (b, c), then please let me know, because that might still be helpful.",4cc9zl,t1_d1igij4,TheSageMage,,Reply,1,0,1
4cby03,2016-03-28 16:16:23-04:00,CSachen,"Difference between IP, MA, and AM?","I'm reading about complexity theory about classes.

In particular, Interactive Proof, Merlin-Arthur, and Arthur-Merlin.

What's the difference between them?",,,,,Submission,6,0,6
d1hj3j6,2016-03-29 07:22:55-04:00,knightry,,"https://complexityzoo.uwaterloo.ca/Complexity_Zoo:I#ip

The see also has links to AM and MA.",4cby03,t3_4cby03,CSachen,,Comment,1,0,1
4caiga,2016-03-28 11:06:18-04:00,cybermyth,Choosing a sentinel in a skip list,"To avoid having to check if the next value is null, we add a sentinel to a skip list. What would be a good value for a sentinel node? Here's two options I considered:

1. A value bigger then the biggest element in the skip list (let's say we have a skip list with elements [1,2,3] the sentinel would be 4
1. Integer.MAX_VALUE (if we have a skip list of integers, there is no such value that is bigger than MAX_VALUE)

I've thought about the first option and I assume it isn't that good, since if we're adding a value that is bigger then the current sentinel, then we have to fix the sentinel values on all levels.

Well I think the second option would be good, but I'm a bit confused. Let's say we add a value that is the same as sentinel. In practice, there is a case where that could happen. To avoid this we could set a sentinel's value to MAX_VALUE +1. How do I store that value in an integer if I'm using java for example? We have  a skip list of integers and the sentinel is Integer.MAX_VALUE +1. How do i store that?

If we don't talk language specific, a good sentinel would be infinity, right? ",,,,,Submission,1,0,1
d1gft0v,2016-03-28 11:52:54-04:00,crookedkr,,I use INT_MAX for sentinels. You lose one usable spot (which makes sense since you are giving one value special meaning). In my experience this has always been fine...really how often do you need to insert INT_MAX in your structure?,4caiga,t3_4caiga,cybermyth,,Comment,1,0,1
4ca6vw,2016-03-28 09:46:40-04:00,Tonananagram,How can I supplement my (inadequate) CompSci education?,"I feel like my uni isn't teaching me enough. I'm almost at the end of my first year and I've barely learned anything. The boring courses and the poor layout of the interesting courses have really made me lose interest in CompSci in general, even though I'm fairly certain this is what I want to do with my life. What books or other material would you recommend for me to get back the spark I've lost? I just want to learn some computer science in a way that isn't completely soulless or dull, because I know it can be a very fun and interesting subject.

Thanks.",,,,,Submission,14,0,14
d1gf5yz,2016-03-28 11:37:40-04:00,gaymuslimsocialist,,"I think the great thing about CS is that you can just start building things out of thin air, no money required. 

Of course, there is more to CS than programming. Building stuff will definitely improve your programming abilities, but it can also be great for improving your understanding of all kinds of concepts. Either you try to build something related to your courses, a personal area of interest or you just start and stumble upon something interesting by accident. 

Not only will you learn while doing so, you'll also have something to show for it. That is always a great motivation for me. 

",4ca6vw,t3_4ca6vw,Tonananagram,,Comment,8,0,8
d1gh4zt,2016-03-28 12:24:15-04:00,Tonananagram,,Thanks for the reply. I have been interested in working on some open-source stuff maybe or tinkering with game engines. I'll look up some literature for that.,4ca6vw,t1_d1gf5yz,gaymuslimsocialist,,Reply,1,0,1
d1gklg1,2016-03-28 13:41:50-04:00,ForTheBread,,"I felt the same for the first year or two of my comp sci program. It picked up later though. My classes are definitely more interesting and challenging now. But as others have said just find something you are interested in building and build it, play around with easy and challenging projects. I usually visit /r/dailyprogrammer for ideas. Generally just to do side research in the comp sci field. ",4ca6vw,t3_4ca6vw,Tonananagram,,Comment,4,0,4
d1gct3x,2016-03-28 10:38:44-04:00,KrustyKrab111,,"Id advice you to know where in CS you might want to end up. The field is diverse and there are many applications of CS knowledge. Secondly, I'd recommend checking out websites like Udemy, they often have great courses that teach you things that could be beneficial to your understanding of material. There are plenty of books out  there since the subject is so diverse, so I'd recommend finding a path in CS and finding books and material related to that. Also, if you haven't had data structures already I'd recommend you start with that material soon. It'll be helpful in interviews",4ca6vw,t3_4ca6vw,Tonananagram,,Comment,3,0,3
d1gdk0i,2016-03-28 10:58:11-04:00,Tonananagram,,"Thanks for the reply. I'm mainly interested in AI, VR and game-development. I still have four years left of my five year education though, so it seems a bit too early to say for sure. I've been considering a phD in AI as well, but again, bit too early for that. I have had data structures, but it was fairly basic stuff. I suppose atm what I'm looking for is general CS textbooks, so I can branch out once I've gotten a better insight into the subject as a whole. All I've done so far is some math (Linear Algebra, Single Var Calculus, Discrete Math) and some java coding. I just feel like I'm losing interest and I want to combat that. I'll check out Udemy.",4ca6vw,t1_d1gct3x,KrustyKrab111,,Reply,1,0,1
d1h9rvt,2016-03-28 23:42:22-04:00,Xxyr,,"If you have a reasonable handle on basic data structures and actually want a good text book I highly recommend Introduction to Algorithms, 3rd Edition (MIT Press) http://www.amazon.com/dp/0262033844/ref=cm_sw_r_tw_awdm_2BF-wb1BY0EY4 via @amazon

It was by far my favorite text. I didn't start using it until grad school but some schools use it in undergraduate work so it shouldn't be too advanced for a dedicated student.

Now if you don't actually want a nearly 2k page textbook but something to keep you engaged in programming I'd recommend https://www.nostarch.com/lisp.htm Land of Lisp is a very approachable book that teaches functional programming in the context of a text based game. 

Lisp is pretty different from the C family of languages and still introduces a number of great concepts that will help with most other styles of programming. Specifically around immutability and side effects.

If you want to just dip your feet I recommend the 7 in 7 series from pragprog https://pragprog.com/categories/7in7 where they walk you through seven X over a seven week period. 

If you want to solve bigger problems just start a project and see what happens :)

Ps. If you have a specific topic - not game dev - that you want a book recommendation on just ask. I read way too many of them.",4ca6vw,t1_d1gdk0i,Tonananagram,,Reply,2,0,2
d1gn1w8,2016-03-28 14:36:51-04:00,Tetheta,,"Contribute to open source if you can, get some projects going you can talk about in an interview, look at what internships are looking for (Google Google's requirements) and work on that. Tons of things to do, go build something!",4ca6vw,t3_4ca6vw,Tonananagram,,Comment,3,0,3
d1gf55s,2016-03-28 11:37:07-04:00,dxk3355,,"""I just want to learn some computer science in a way that isn't completely soulless or dull""  

Computer science is soulless and dull.  If you're into games then you should of applied to a game program like RIT's (https://www.rit.edu/gccis/igm/) ",4ca6vw,t3_4ca6vw,Tonananagram,,Comment,7,0,7
d1gh3kk,2016-03-28 12:23:19-04:00,Tonananagram,,"I disagree. There are plenty of aspects of it that I find very interesting and just solving problems is enjoyable for me, we just don't get enough of the good parts at uni. I enjoy the math but I don't feel that the actual CS courses are up to par. If I went with the game program you linked (I live in Sweden so wouldn't be able to anyway), I'd be severely limited in future options, and since I'm interested in maybe going for a phD and I would like to contribute to research it doesn't seem like a good choice. Thanks, though.",4ca6vw,t1_d1gf55s,dxk3355,,Reply,4,0,4
d1gm1j4,2016-03-28 14:14:09-04:00,TheKoolAidThatKares,,"I have to side with you. I remain fascinated by computers and im excited to work with them every day, although I do pity the people who have chosen this major without having a passion for it.",4ca6vw,t1_d1gh3kk,Tonananagram,,Reply,3,0,3
d1gsher,2016-03-28 16:34:59-04:00,umib0zu,,"It totally depends on what you're into. What exactly did you find soulless and dull? I would suspect your school is probably job training you by making you learn computer engineering first. It's just a reality that most people will not go into research, so focusing on getting you skills to feed yourself is the first priority. Other parts of the program will likely give you a taste of a research area, so if you do get into grad school, you can build on the basics.",4ca6vw,t1_d1gh3kk,Tonananagram,,Reply,1,0,1
d1hj8xd,2016-03-29 07:30:17-04:00,knightry,,"Check out some online algorithm solving sites like topcoder or codechef or the like. If you like problem solving these are really fun to test your skills and learn new things.

And if your university competes in programming contests, get involved.",4ca6vw,t1_d1gh3kk,Tonananagram,,Reply,1,0,1
d1hkxxp,2016-03-29 08:40:39-04:00,Citricot,,"Yeah, I'd agree with you. There's definitely other career options than  making games and maintaining shitty enterprise applications. You can do whatever you want during your free time, and that can include doing webdev (frontend, backend), security (data transmission, reverse engineering, etc.), networking, whatever, and all of them are interesting to different people.",4ca6vw,t1_d1gh3kk,Tonananagram,,Reply,1,0,1
d1gr855,2016-03-28 16:07:33-04:00,ppc23,,"Get a job as a research assistant at any chair that might suit you. From my experience there are nearly always vacancies and if they know you you're half way in.
My job is really what keeps me interested in my study. I started as a gofer but made my way up. I'm now working on real research projects with interesting topics, do some sessions of the lectures myself and go to conferences to present some of our results. Just like phDs only for less money xD
For me it's the path to go and I would recommend it.",4ca6vw,t3_4ca6vw,Tonananagram,,Comment,1,0,1
d1gwbqw,2016-03-28 18:03:40-04:00,impala454,,"I've got a CS degree and been a developer for for about 12 years.  The classes aren't always going to be some amazing neat new tech.  Software development will definitely have some ""soulless or dull"" times, the key is do you still enjoy it enough during those times.  I'm a code monkey at NASA and even with projects flying in space, there will be those times.  If you think you've just lost the ""spark"" don't worry about it, it will come and go.  Maybe find some small electronics projects that will make you learn some new languages or something.  Or write a mobile app.  Find an old project and redo it in another language.  The key is always be teaching yourself something new.  Your degree really does end up just being a one time entrance ticket to your first job.  After that it's all about your experience and versatility.  Best of luck!",4ca6vw,t3_4ca6vw,Tonananagram,,Comment,1,0,1
d1he1xx,2016-03-29 02:21:18-04:00,Devvils,,"As a math and computer science grad, I find CS more interesting than say real analysis. Its all perspective.

I suggest reading more of which CS topics interests you.

Also try the new Raspberry Pi.",4ca6vw,t3_4ca6vw,Tonananagram,,Comment,1,0,1
4c9bbf,2016-03-28 03:59:12-04:00,Solaire_Of_Astora_77,Computer science vs. Software engineering,"I am currently a freshman in college and am still major undeclared. I am attending Kansas State university and am looking to go into a CIS major, however I am unsure which option to go with.

Here is the link to the required classes:
https://www.cis.ksu.edu/programs/undergrad/cs

I want to know the differences between the two, and whether one is more beneficial than the other. What are the possible pros and cons of each?

*Edit: Thanks all for the quick and well written responses. I am definitely leaning towards CS because of the open options, and because theory holds more interest for me.",,,,,Submission,4,0,4
d1g5t8h,2016-03-28 05:17:40-04:00,self_raising,,"If you're certain that you want to go into software engineering then specialising will give you a firmer basis to start and more software knowledge to use in interviews. 

Having said that, I ""majored"" (British - we don't have major/minor, we just study one degree) in computer science and ended up going straight into software engineering. I wasn't completely sure what I wanted to do as a career and, having been exposed to many different things in my CS studies, I realised that I enjoyed the software modules best (and also got my best exam results in them).

In CS you will learn software engineering as well as other things. And it's a good degree to have on your cv. On the flipside of that, I probably would have got a higher classed degree if I studied software engineering. There were a few CS modules that I had absolutely no interest in and as a consequence performed poorly in the exams/coursework.",4c9bbf,t3_4c9bbf,Solaire_Of_Astora_77,,Comment,7,0,7
d1g7sdy,2016-03-28 07:33:00-04:00,DownRUpLYB,,"> I ""majored"" (British - we don't have major/minor, we just study one degree)

Although not that common, we absolutely do have major/minor, I have one myself! 

I even know some people who did 'double major'.",4c9bbf,t1_d1g5t8h,self_raising,,Reply,1,0,1
d1gesms,2016-03-28 11:28:44-04:00,self_raising,,Really? Where was that? Oxbridge?,4c9bbf,t1_d1g7sdy,DownRUpLYB,,Reply,1,0,1
d1gj1yy,2016-03-28 13:07:28-04:00,DownRUpLYB,,"Oxbridge? Ha! I wish... No, it was at a London University",4c9bbf,t1_d1gesms,self_raising,,Reply,1,0,1
d1gjtt5,2016-03-28 13:24:54-04:00,Solaire_Of_Astora_77,,"I'll probably go into CS since I am unsure about my career. Software engineering does have some appeal, but I'd like to keep my options open.",4c9bbf,t1_d1g5t8h,self_raising,,Reply,1,0,1
d1g6wcb,2016-03-28 06:36:49-04:00,Imoa,,"I majored in Computer Science at a private university in California, and have many friends who went into Software Engineering. The major difference between the two is that a CS degree is much more theoretical than a software engineering degree. Both fields focus on computers, however software engineering is the study and practice of, well, creating and improving software. Computer Science is more the study of how computers work, how computers interface with data, and how computers can be used to process data.

Computer Science touches on a lot of fields and is a more general degree. You'll learn about how programming languages work, how compilers work, and programming algorithms. As you start to move into upper division coursework, the material becomes more theoretical (at least mine did), and simultaneously low level. You'll be able to tailor your studies to what you want. I took a lot of AI and algorithm / math courses (I also minored in math). I also took a compilers course, and a programming languages course. I see a lot of the same courses in the link you provided, so I feel confident that you experience would be similar sans the electives. 

Software engineering will start out similar to CS with a lot of programming (most likely), but the upper division work will be much more project oriented than theory. Most of the software engineering courses at my school, starting around junior year, either involved app programming or group projects, some of them a semester long or more. 

Which one is more useful is up to you. Personally I was way more into theory (still am, working on masters) than I ever was into just programming. If you want to get into software development, SE is the way to go. If you are interested in data processing / theory, AI, or any of the more mathy parts of programming, CS is the degree for you.

As a final caveat, I would add that CS is a more general degree for the field. If your goal is specifically to be a software developer, then I suggest software engineering. However, if you are unsure of what you want I would go with computer science. It's less specialized, but it leaves open the option to go into software development later.",4c9bbf,t3_4c9bbf,Solaire_Of_Astora_77,,Comment,5,0,5
d1gjqim,2016-03-28 13:22:51-04:00,Solaire_Of_Astora_77,,"Based on your experience, it looks like CS is the right choice for me. I am interested in theory, and the generalization in the field is appealing because I am unsure of what I'll be doing.",4c9bbf,t1_d1g6wcb,Imoa,,Reply,1,0,1
d1gj0ui,2016-03-28 13:06:47-04:00,dxk3355,,"Software engineering will make you ready for the workplace based on what I'm seeing in that link.    The other stuff is nice to learn I'll admit, but the SE stuff is stuff that you experience as a group with your classmates.",4c9bbf,t3_4c9bbf,Solaire_Of_Astora_77,,Comment,1,0,1
d1ik1x6,2016-03-29 22:04:49-04:00,CheekiBreekiIvDamke,,"I'm studying Software Engineering in a fairly high ranked Australian university. Our Software Engineering is essentially the same as Computer Science with the distinction that Software Engineering contains several mandatory ""Project"" classes as well as an Industry Experience segment whereas CompSci allows you to do other theory classes instead.

As a personal note, if I could turn back time I'd consider doing CompSci as the introductory ""Project"" classes are absolute bullshit designed purely to filter out the students who did well, have no idea what they want to do and hear Engineers bring bulk dollars. However, I personally feel that despite the similarities a ""Software Engineer"" would be more employable than a ""Computer Scientist"". ",4c9bbf,t3_4c9bbf,Solaire_Of_Astora_77,,Comment,1,0,1
d1gbe9d,2016-03-28 09:59:10-04:00,Chandon,,"I'd recommend CS over software engineering.

It'll probably have a couple extra math classes, which are harder to self-study / learn on the job than the material in some specific project course. It'll also force you into more, more interesting electives senior year.

Specifically, stuff like stats / machine learning or computer architecture / operating systems are things that could be left out of SE but are really nice things to have taken as classes.

Looking at the concrete class lists for your school, I'd say CS by one course:

In SE, Enterprise Information Systems and Advanced Software Design and Development are probably lame. In CS, Database System Concepts is probably lame. On the other hand, Parallel Programming from SE is likely to introduce really useful stuff.",4c9bbf,t3_4c9bbf,Solaire_Of_Astora_77,,Comment,1,0,1
d1gqunc,2016-03-28 15:59:28-04:00,ToplessTopmodel,,Are we turning this sub into computerScienceCareer again?,4c9bbf,t3_4c9bbf,Solaire_Of_Astora_77,,Comment,0,0,0
4c4amq,2016-03-26 23:55:27-04:00,redditsyderaccount,Can someone help me evaluate this program's courses and if it's the full package?,"HeyGuys,

Can someone cool and smart help me see what is missing and what kind of stuff I should learn on my own if I take this program: [this one](http://www.cs.manchester.ac.uk/study/undergraduate/courses/computer-science/?code=00587&pg=options)

So I just want options when I graduate, whether that's doing a PhD or just going into industry.

I narrowed my places to study comp sci to Manchester or Edinburgh (don't have course listings for Edinburgh besides [this](http://www.drps.ed.ac.uk/16-17/dpt/utcmpsi.htm) but it doesn't show year 3/4.) 

The only thing I notice since I don't know what I should be looking for is that there isn't any functional programming in Manchester, and I don't see the classic Algo and Data Structures course. Though that might be the one called 'Algo and Imperative Programming' with details [here](http://studentnet.cs.manchester.ac.uk/syllabus/?code=COMP26120#learningoutcomes), but I'm not an expert on this so not sure.

So yeah if you people could help me see what the program is lacking in content and how I could cover it, I would be happy. I'm leaning more towards Manchester over Edinburgh since they at least have the 'year in industry' and Edinburgh doesn't have any co-op system.

I might regret this all and just go to something close like Toronto but this way it could be like a 3 year vacation.",,,,,Submission,8,0,8
d1f0eyv,2016-03-27 02:12:08-04:00,onemanandhishat,,"Looks pretty comprehensive at a quick look, and given that it's in the QS top-50 global ranking for CS, you won't do much better in the UK (Oxbridge, Edinburgh, UCL and Imperial are ahead).

Re algorithms and data structures, it looks like the course you mentioned covers that topic. If it's missing anything it looks like it might be a little light on the discrete maths side of things. But I went to Warwick which I've heard leans a little heavy on the maths component.

Generally if a CS course at a good university (which Manchester is) is missing anything, it's in terms of practical application. The course will teach you good coding practice and offer practical opportunities, but if you want to build your range of languages and get more experience, that's something you'll need to do for yourself.

A CS degree is about giving you the possibility to go into anything in the CS world, and I think this course looks as good as any other in terms of topical coverage.",4c4amq,t3_4c4amq,redditsyderaccount,,Comment,1,0,1
d1faj71,2016-03-27 11:33:48-04:00,redditsyderaccount,,"Yep that's what everyone says, it's the same one (same classic algo course). That's good though. I'll need to start learning about web dev stuff and I found the FCC to cover the content. ",4c4amq,t1_d1f0eyv,onemanandhishat,,Reply,1,0,1
d1f56zw,2016-03-27 07:37:51-04:00,assface,,"> So I just want options when I graduate, whether that's doing a PhD or just going into industry.

FYI, nobody cares about what courses you took when applying for a PhD program. They care about your research experience. This means working on an open-ended project and getting a publication.",4c4amq,t3_4c4amq,redditsyderaccount,,Comment,0,0,0
4c1tlq,2016-03-26 12:35:32-04:00,BenRayfield,Could a software economy work where prizes are offered for any software which a certain recognizer function matches? Or is the main work in dealing with peoples inability to define what they want?,"Instead of paying someone to do a ""job description"", you would pay them to solve a well defined math statement. Any solution, even if you dont like it, gets the money, so make sure you create the right math statement that will only recognize things that actually get the job done in a way you can use.

Example: If you want a program that factors integers of a certain size and efficiency, you define a function that takes 1 parameter: any possible software which may factor integers.

The program is accepted and money paid when either all possible integers up to that size are factored correctly, or more practically, only a test set of integers generated by a hashing algorithm.

Of if you want a program that modifies any image of clothed people to naked people, you define a recognizer function that can measure if a pair of images has about the same edge locations and the left image has more clothes than the right image and they still look like adult Humans.

In general its easier to recognize solutions than to generate them, unless P = NP. I'm not sure if thats true for the second example of changing images since neuralnets tend to do those symmetricly, but there could be some simpler way to define what you want. If jobs were easier to do than to describe what needs doing, then nobody would bother writing a job description. They'd just do the job.",,,,,Submission,3,0,3
d1ebftv,2016-03-26 13:12:17-04:00,UncleMeat,,"I don't think this would work for several reasons. 

The first and most obvious is that clients rarely actually give a shit about algorithms. They care about their business purpose. In the real world almost nobody hires somebody to produce an algorithm to solve an abstract problem like sorting lists. They hire people to build a CC processing system for their website or a CRUD system for their web app or some other tool for their business purpose. These business purposes are fundamentally fuzzy rather than rigorously defined. Its part of your job as a professional to convert the client's stated desires into rigorous requirements. Its not their job. 

The second problem is that verifying the correctness of programs is hard as shit. Even when you are writing tests alongside development its often a nightmare and tests are fundamentally limited if you really want to prove correctness. Asking the client to provide some abstract interpretation engine that can prove the correctness of your code is ludicrous and asking them to provide detailed tests will hinder your ability to flexibly design the system by locking you into a particular structure from the beginning. 

You've also got other non-correctness requirements. What are the security requirements? Time and memory efficiency requirements? Maintainability requirements? These are things that aren't tested by some correctness validator but are super relevant in real world software engineering.",4c1tlq,t3_4c1tlq,BenRayfield,,Comment,8,0,8
d1ebpie,2016-03-26 13:20:04-04:00,iknighty,,"Continuing on your point of correctness, you could define a number of properties you would want a program to satisfy but you cannot always prove that the program satisfies (or violates) them statically (without running the program). But some properties (or parts of them) can only be verified at runtime.

Thus for most cases such a recognizer function is impossible, since in most cases you wouldn't be able to verify completely that the program does what you need it to do without actually running it on each possible execution.",4c1tlq,t1_d1ebftv,UncleMeat,,Reply,2,0,2
d1eh48s,2016-03-26 15:59:27-04:00,daymi,,"Yes, to add to this: more info is at https://en.wikipedia.org/wiki/G%C3%B6del%27s_incompleteness_theorems",4c1tlq,t1_d1ebpie,iknighty,,Reply,2,0,2
d1ei0td,2016-03-26 16:25:48-04:00,iknighty,,Any papers on how Gödel's incompleteness theorems affect static analysis?,4c1tlq,t1_d1eh48s,daymi,,Reply,2,0,2
d1emijz,2016-03-26 18:33:16-04:00,UncleMeat,,It doesn't really in any more meaningful way than Turing's original paper on undecidability. There's *way* more applicable stuff elsewhere about the power and limitation of static analyses.,4c1tlq,t1_d1ei0td,iknighty,,Reply,3,0,3
d1el3jf,2016-03-26 17:52:04-04:00,BenRayfield,,"Then we limit these parts to things that can be proven. Its still a huge improvement to go to a website and type in you need a piece of code to do this, and 43 seconds later, instead of 7 minutes later as it would take for you to figure it out, you have the code. You might make 3 times more money per hour being that productive, and pay less of it to those who helped you.",4c1tlq,t1_d1ebpie,iknighty,,Reply,-2,0,-2
d1emn0i,2016-03-26 18:36:53-04:00,UncleMeat,,"> Then we limit these parts to things that can be proven.

Great. So you can only ask for incredibly simple programs or you force people to write systems with massive additional complexity by supplying proofs with something like Coq alongside your program. For the large majority of applications this is just unnecessary overhead. Even proving the termination of your implementation is massively challenging, let alone more interesting properties. 

> 43 seconds later, instead of 7 minutes later

I think you are grossly underestimating the time it would take to prove the correctness of the implementation you ask for. How much experience do you have with static analysis? ",4c1tlq,t1_d1el3jf,BenRayfield,,Reply,2,0,2
d1eoolm,2016-03-26 19:37:20-04:00,iknighty,,Can't you ask for the program to conform to some interface and then analyse the interface method calls? That would avoid the need for the programmer to use Coq or proof-carrying code themselves.,4c1tlq,t1_d1emn0i,UncleMeat,,Reply,1,0,1
d1eql9j,2016-03-26 20:34:52-04:00,UncleMeat,,If you care about any actual behavior of the program then this isn't enough. I can write a nonterminating function that fits an interface. I can write a function with horrible side effects that fits an interface. There's a reason why type checkers generally don't tell you if your sort function actually correctly sorts lists. ,4c1tlq,t1_d1eoolm,iknighty,,Reply,2,0,2
d1enexd,2016-03-26 18:59:52-04:00,iknighty,,"Yup, there is work in that direction, i.e. to try and prove whatever is provable statically then leave the rest for runtime.

Something to consider is that for people to satisfy your formal specification then you must show it to them. However there is a learning curve for them to understand it. 

Another problem that arises is that since they know your specification (against which you will verify the program) they can insert malicious code that does not trigger violation (since the formal specification will probably not be complete, or not completely statically provable).",4c1tlq,t1_d1el3jf,BenRayfield,,Reply,1,0,1
d1eoc11,2016-03-26 19:26:45-04:00,BenRayfield,,"> learning curve for them to understand it.

If it takes minutes instead of weeks to understand, success of the paradigm is extreme

> malicious code that does not trigger violation (since the formal specification will probably not be complete, or not completely statically provable).

then such malicious code is what was asked for. Please be more specific next time if you're not happy with your product as requested.",4c1tlq,t1_d1enexd,iknighty,,Reply,-1,0,-1
d1eou3l,2016-03-26 19:42:01-04:00,iknighty,,"If you specify something that you cannot prove statically doesn't mean that the lack of conformance to it is something you asked for. Runtime verification is necessary for the paradigm you propose, meaning you can suddenly realize that the program is not as you specified during runtime.

I'm not saying that is a bad thing, but just pointing out the realities of the situation.",4c1tlq,t1_d1eoc11,BenRayfield,,Reply,1,0,1
d1ez9xo,2016-03-27 01:21:03-04:00,BenRayfield,,"> you specify something

> you asked for",4c1tlq,t1_d1eou3l,iknighty,,Reply,1,0,1
d1ekvy1,2016-03-26 17:46:24-04:00,BenRayfield,,"> Maintainability requirements?

Thats nonsense. Software is math. Math does not need maintenance. You may not like the number 49999, but just because you prefer 52000 on tuesdays and 49999 on other days, does not make 49999 buggy.

> They hire people to build a CC processing system for their website or a CRUD system for their web app or some other tool for their business purpose. These business purposes are fundamentally fuzzy rather than rigorously defined.

That answers ""Or is the main work in dealing with peoples inability to define what they want?"" and opens a practical question: Among programmers who know what is needed of them and what they need to do it, can a practical economy work between programmers who tell eachother the relevant math and money offered for solving parts of it? If not, then whats stopping it, and what kind of deals would need to happen to obsolete paradigms preventing math people from working together effectively?",4c1tlq,t1_d1ebftv,UncleMeat,,Reply,-2,0,-2
d1emhh8,2016-03-26 18:32:21-04:00,UncleMeat,,"> Thats nonsense. Software is math. Math does not need maintenance. You may not like the number 49999, but just because you prefer 52000 on tuesdays and 49999 on other days, does not make 49999 buggy.

Go talk to a businessperson and explain this to them. Tell them to describe their desired software as a pure function and ask them to provide an abstract interpretation engine that can statically verify the correctness of the implementation you provide. They will immediately go look somewhere else.

I'm in academia. I'm well aware of the ""mathy"" nature of computer programs. But you are a naive fool if you think that software as it is practiced in the real world can be approached this way. 

And even if you accept the ""software is math"" argument completely and ignore ""businessy"" questions like maintainability, how the heck are you going to describe and check things like security properties. If you are capable of describing the properties that you want in such perfect detail, you are almost programming the system yourself already. ",4c1tlq,t1_d1ekvy1,BenRayfield,,Reply,3,0,3
d1emrou,2016-03-26 18:40:42-04:00,BenRayfield,,Business is a subset of gametheory and will eventually be dominated by those who understand gametheory in general.,4c1tlq,t1_d1emhh8,UncleMeat,,Reply,-4,0,-4
d1eqlvu,2016-03-26 20:35:21-04:00,UncleMeat,,"> Business is a subset of gametheory and will eventually be dominated by those who understand gametheory in general.

.... are you in high school? This is like saying that biology is just a subset of physics so eventually the field will be dominated by people who understand physics.

This is also completely unrelated to my post or anything you've posted earlier. You are completely unable to explain how a non-technical person who hires a software engineer is going to be able to provide a precise description of the program he wants and efficiently evaluate the correctness of the implementation you create.",4c1tlq,t1_d1emrou,BenRayfield,,Reply,6,0,6
d1em49w,2016-03-26 18:21:31-04:00,ebix,,"Just so you know, other people are interested in this, you should check out [Ethereum](https://www.ethereum.org/) as well as the topic of [Zero Knowledge Proofs](https://en.wikipedia.org/wiki/Zero-knowledge_proof) upon which Ethereum contracts are built. 

That said, as someone who has spent time doing both math research, and software engineering, I doubt that it will ever take off for one main reason: Software Engineers don't converse in perfect mathematical problem statements, and the reason for this isn't necessarily because they are unable to (although certainly some are). 

Event quite mathematically proficient SEs build imperfect abstractions for two big reasons:

* Building perfect abstractions is very difficult.
* Communicating perfect abstractions is slow and laborious

Take a VM (or a container running in a VM for that matter): Is it a perfect abstraction of a physical machine? No. Do we almost always treat it as a perfect abstraction when building on top of it? Yes.

That is because thinking about the mapping between the VM and the physical machine(s) running it is almost always wasted thought. 

Ultimately, giving this up isn't worth the nebulous benefit of a theoretically perfect market for software. ",4c1tlq,t1_d1ekvy1,BenRayfield,,Reply,1,0,1
d1eq032,2016-03-26 20:17:13-04:00,iknighty,,"Is zero-knowledge proof applicable to proving a program correct? I can't see how it is.. 

Firstly it is probabilistic, but usually we want certainty. Secondly how can you show that you know that a program is correct without showing the proof?",4c1tlq,t1_d1em49w,ebix,,Reply,1,0,1
d1ewsp6,2016-03-26 23:49:16-04:00,ldpreload,,"> Thats nonsense. Software is math. Math does not need maintenance. You may not like the number 49999, but just because you prefer 52000 on tuesdays and 49999 on other days, does not make 49999 buggy.

Sure. But people pay for 52000 on Tuesdays and 49999 on other days. You can give them 49999 every day of the week as much as you want, and you won't get paid for it.

The profession of software engineering (as opposed to computer science, which is a field of academic study, not a profession) is the job of providing people with the specific math that solves the problem they actually have today. Not the problem they had yesterday, not the problem they _think_ they have today (which is a common trap).

Maintainability is important because it is generally easier to adapt software that almost fits today's requirements (e.g. because it fit yesterday's requirements) than to start afresh. If you think you can start afresh, by all means, go for it. Just realize that the requirements change daily.

> Among programmers who know what is needed of them and what they need to do it, can a practical economy work between programmers who tell eachother the relevant math and money offered for solving parts of it? 

Yes, it can, and it does. This is exactly what you do with your life as a professional non-entry-level worker in software engineering. Sometimes your title changes from ""software engineer"" to ""software architect,"" ""project manager,"" ""engineering manager,"" or something, but you're still in the field of software engineering. As you move on from entry-level positions, a smaller portion of your time goes towards actually writing software, and a larger portion goes towards figuring out what the requirements are, and figuring out how to provide code for it (working with / managing co-workers, recruiting and interviewing and hiring new ones, buying commercial software, adapting open-source software, etc.).",4c1tlq,t1_d1ekvy1,BenRayfield,,Reply,1,0,1
d1ersta,2016-03-26 21:11:04-04:00,Bottled_Void,,"I work in aerospace. And for some of the jobs the problem is very well defined. Press this switch, check for weight on wheels, raise the gear etc.


But the people building the planes want someone to provide a box that does everything along with all the certification evidence all within a certain timescale.


So for that sort of job, I can't really see your approach working. Because it's a big investment of money before you get to the point where you have something to deliver.


Most businesses want to put something in a plan to know when something is going to be done by. They don't want to just hope that someone comes along with a solution.


That being said, a lot of Open source stuff works like the way you said. The best (or sometimes earliest good) version is adopted. Although there isn't usually cash involved.",4c1tlq,t3_4c1tlq,BenRayfield,,Comment,1,0,1
d1ewteu,2016-03-26 23:49:58-04:00,throwaway,,"Check out Kaggle.  Machine learning competitions work roughly the way you describe, because the problem can be described very precisely.",4c1tlq,t3_4c1tlq,BenRayfield,,Comment,1,0,1
4c0z1c,2016-03-26 08:18:00-04:00,Bilalaftabraja,Computer Tec Software,,,,,,Submission,0,0,0
d1e3jgm,2016-03-26 08:19:48-04:00,Bilalaftabraja,,I  want to change the world through computer,4c0z1c,t3_4c0z1c,Bilalaftabraja,,Comment,-1,0,-1
4c02vt,2016-03-26 01:10:32-04:00,coolkid3860,How to use RubyGems???,"Greetings! I am a high schooler who is currently trying to build a robot that spins its head whenever I receive an email. I have been using a tutorial online, which I will link here: http://www.sitepoint.com/arduino-and-ruby-create-a-gmail-notifier/ 
to help me out a little bit. The tutorial required the usage of Ruby and RubyGems and to be honest, I don't know much about either. I did install Ruby and I think that it is legit. I also installed the newest version of RubyGems. What is puzzling is that I went to go check on if I installed RubyGems on the Interactive Ruby thing, it would pop up as a ""undefined local variable or method."" What am I doing wrong? How should I go about to fix it? Please help! Thank you! ",,,,,Submission,0,0,0
d1e6s9r,2016-03-26 10:46:33-04:00,SEMW,,"The reason you've been downvoted is that this sort of question is better suited to /r/learnruby/ or /r/learnprogramming/: this sub is more geared around computer science.

That said: make sure you've `require`'d the gem. Eg if using the `hello_world` gem, after installing it (`gem install hello_world`) and opening irb, you need to do:

`require 'hello_world'`

only then can you do

`HelloWorld.say_hello`",4c02vt,t3_4c02vt,coolkid3860,,Comment,3,0,3
4byk7c,2016-03-25 17:48:06-04:00,AsteriskTheServer,How do I determine the tag size in a 32 bit word address in a (directly mapped | full associative | set-associative) cache?,"For example if we a some cache with n lines  and w word blocks. How do I determine the length of the tag? 

So for example, 
*cache = direct-mapped
*lines = 16
*w = 2 

Store 6  which is a 32 bit memort address reference, but given as a word address. 

I understand the offset would be 1 as log_2(*w) = 1.  I also understand that the bit length for the indices is 4 as log_2(*lines) = 2. Lastly, I get that the tag is then tag - 1 - 4, but I don't understand how to get the tag length. Which as far as I can tell is not 27. ",,,,,Submission,0,0,0
4bxqn1,2016-03-25 14:34:46-04:00,griffin3141,Important CS topics to study as a self-taught software engineer,"I am a mostly self-taught engineer. My primary background is in applied mathematics and statistics. These days I mostly do full-stack javascript work with some functional programming work on the side for fun. No experience with low-level languages.

I want to improve my CS fundamentals, given that I didn't study CS in undergrad. What are the most important topics/projects to work on? I am already strong in data structures and algorithms, given my math background. I am not trying to do this to get a job. I just want to learn and become a better engineer

I've been told to try my hand as writing a compiler and writing on OS . I've found guides online with great references for writing a scheme interpreter in Haskell and writing an OS in rust. Thinking about reading up on these topics then diving in. Are these projects worthwhile? Anything else that would be important to study up on?",,,,,Submission,16,0,16
d1degva,2016-03-25 15:58:13-04:00,videoj,,">I've found guides online with great references for writing a scheme interpreter in Haskell and writing an OS in rust. Thinking about reading up on these topics then diving in. Are these projects worthwhile?

Absolutely.   

[This guy](https://www.scotthyoung.com/blog/myprojects/mit-challenge-2/) talks about doing a self-directed computer science degree using MIT course material.  You might find it useful.

Also take a look at machine learning and graphics.  With a statistics background, machine learning will be easy to pick up.

",4bxqn1,t3_4bxqn1,griffin3141,,Comment,3,0,3
d1e3rgt,2016-03-26 08:32:34-04:00,ProfessorAlgorithm,,"With a math and stats background, I'd recommend looking into parallelization, and for a real challenge, quantum algorithms.  The first is so badly needed, and yet undertaught.  The second has far too few people working on it, with a SE background (no offense intended, physicists!).",4bxqn1,t3_4bxqn1,griffin3141,,Comment,3,0,3
d1do8ym,2016-03-25 20:33:26-04:00,the_omega99,,">Are these projects worthwhile?

Yes. You'll learn a lot from them. The impact won't be groundbreaking to your actual job, but it does have implications. For example, do you know what goes into writing a file? The heavy lifting there is all part of the OS and thus an understanding of how the OS works lets you know what is going on when you do things in a higher level language. I think the most valuable part of learning about OSes is going to be where processes and threading are concerned.

As for writing an interpreter, I would consider this invaluable because you'll learn a lot about parsing (which is a valuable skill on its own) as well as a lot about language design (and can thus understand why languages are designed the way they are) and some intricacies of interpreted languages (which naturally applies to JS).

>Anything else that would be important to study up on?

Well, there's plenty of more specific fields of CS that could be very interesting to learn if you wanted to make something involving those fields. Eg, working with computer vision could be a useful skill if you ever made an application that utilized some CV approach (such as classifying an image).

I mentioned threading casually before, but it's really quite a complex topic that there's a lot to learn about. The fact that you work largely in JS makes me think you probably won't have much experience with multi-threading, since JS defaults to being single threaded (web workers are JS's approach to multithreading). For example, how can you prevent deadlocks? Do you know how to avoid race conditions?

There's some advanced parts of algorithms that I'm not sure if you're well versed on (it's unclear exactly how much you know about them). The problem of NP algorithms is an advanced topic of interest. The main point of interest to you would be the ability to identify when an algorithm is going to be grossly inefficient and there's no better approach. Stuff like approximation algorithms can be a useful compromise for solving these issues.",4bxqn1,t3_4bxqn1,griffin3141,,Comment,2,0,2
d1di3t1,2016-03-25 17:34:09-04:00,apendleton,,"Not sure how fundamental you're talking, and sort of depends on the flavor of work you're doing, but if anything that you work on is performance-sensitive, definitely worth making sure your grasp of data structures and algorithms is solid. The first semester of that in a CS program would probably cover assessing space and time complexity of algorithms (""big-O,"" etc.), and survey the general approaches to searching and sorting, and talk about contiguous vs linked data structures. Second semester would probably get into binary and other trees (with some specialized versions like red-black or AVL trees, b-trees, etc.), graphs, hash sets/tables, and maybe some more exotic stuff like bloom filters.

Knowing the details of how individual algorithms or data structures are implemented is not that important (if for some reason you ever needed write a red black tree implementation instead of grabbing one off the shelf you could just look it up), but having a toolbelt of familiar algorithmic problem-solving strategies is super useful if you ever find yourself needing to make a slow thing faster.",4bxqn1,t3_4bxqn1,griffin3141,,Comment,1,0,1
d1domux,2016-03-25 20:45:17-04:00,mraikentv,,I'm in my second semester as a Econ and CS major and I've yet to even cover half of what you say I should've already learnt by now. Should I be reading about these topics in my free time? ,4bxqn1,t1_d1di3t1,apendleton,,Reply,1,0,1
d1e0avb,2016-03-26 04:23:19-04:00,Yithar,,"No, at least not here. Here Algorithms are covered in the 4th semester and big o is covered in it. Hash tables in 3rd. Binary trees and graphs covered very briefly in 2nd, but covered more deeply in 400 level data structures course. Also brought up in algorithms because minimal spanning tree, Dijkstra's algorithm, traveling salesman problem.
  
I'm in my second to last semester and I know most of what he's talking about.",4bxqn1,t1_d1domux,mraikentv,,Reply,1,0,1
d1efva5,2016-03-26 15:22:56-04:00,apendleton,,"No, sorry, I just meant the first and second semesters of data structures and algorithms. Lots of programs do this subject matter as a two-semester sequence. Not necessarily your first and second semesters of school.",4bxqn1,t1_d1domux,mraikentv,,Reply,1,0,1
d1e8k09,2016-03-26 11:44:41-04:00,None,,"Modularity, unit testing, mocking, dependency inversion. These are very useful software engineering principles. ",4bxqn1,t3_4bxqn1,griffin3141,,Comment,1,0,1
d1e8o8n,2016-03-26 11:48:23-04:00,griffin3141,,"All things I do, but could always stand to get better with. ",4bxqn1,t1_d1e8k09,None,,Reply,2,0,2
4bxl97,2016-03-25 14:02:28-04:00,alexkaz0,Help understanding OAuth Processes.,Can someone break down the OAuth process to simpler terms to understand in regards to APIs and how its implemented. ,,,,,Submission,9,0,9
d1dchi0,2016-03-25 15:09:18-04:00,stangelm,,Stack Overflow is great for questions like this: http://stackoverflow.com/questions/4727226/on-a-high-level-how-does-oauth-2-work,4bxl97,t3_4bxl97,alexkaz0,,Comment,3,0,3
d1dtq5m,2016-03-25 23:22:19-04:00,wafflestealer654,,"Here's the top answer for future viewers.

> How OAuth 2.0 works in real life:

> So I was driving by Olaf's bakery on my way to work and I saw the most delicious donut in the window, I mean the thing was dripping chocolatey goodness. So I went inside and demanded ""I must have that donut!"". He said sure that will be $30.

> Yeah I know $30 for one donut! It must be delicious! I reached for my wallet when suddenly I hear the chef yell ""NO! No donut for you"". I asked why? He said he only accepts bank transfers.

> Seriously? Yep, he was serious. I almost walked away right there, but then the donut called out to me. It said ""Eat me, I'm delicious..."" Who am I not to obey orders from a donut.

> So I said ok. He hands me a note with his name on it (the chef not the donut), ""Tell them Olaf sent you"". He wrote his name on the note, I don't know why he said that, but ok.

> So I drive an hour and a half to my bank. I hand the note to the cashier, I tell her Olaf sent me. She gives one of those looks, the kind of look that says ""I can read"".

> She takes my note, asks for my id, then asks me how much money is ok to give him. I tell her $30 dollars. She does some scribbling and hands me another note. This one has a bunch of numbers on it, I guess that's how they keep track of the notes.

> At this point I'm starving. I rush out of there, an hour and a half later I'm standing in front of Olaf with my hand extended to his face. He takes my note, looks it over and says ""I'll be back"".

> I thought he was getting my donut, but after 30 minutes I started to get suspicious. So I asked the guy behind the counter ""Where's Olaf?"". He says ""He went to get money"". ""What do you mean?"", ""He take note to bank"".

> Huh. So Olaf took the note that the bank gave me and went back to the bank to take out money from my account. Because he has the note that the bank gave me, the bank knows he's the guy I was talking about. And they know to only give them $30 because I told them that's all I would allow them to give him.

> It must have took me a long time to figure that out because by the time I looked up there was Olaf standing in front of me finally handing me my donut. Before I left I had to ask ""Olaf, did you always sell donuts this way?"", ""No, I used to do it different.""

> Huh. As I walked to the car my phone rings. I didn't bother answering, it was probably my job calling to fire me, my boss is such a ***. Besides I was caught up thinking about this whole process I just went through.

> I mean think about it, I was able to let Olaf take $30 out of my bank account without having to give him my account information. And I didn't have to worry that he would take too much because I already told the bank he was only allowed to take $30. And the bank knew he was the right guy because he had the note they gave me and I gave to Olaf.

> Ok, sure I would rather have given him $30 from my pocket. But now that he has that note I could just tell the bank to let him withdraw $30 each week, then I can just show up at the bakery and I don't have to go to the bank anymore. I could even order the donut by phone.

> Of course I'd never do that - that donut was disgusting.

> I wonder if this approach has broader application. He mentioned this was his second approach, I could call it Olaf 2.0. Anyway I better get home, I gotta start looking for a new job. But not before I get one of those strawberry shakes from that new place across town, I need something to wash away that taste of that donut.
",4bxl97,t1_d1dchi0,stangelm,,Reply,3,0,3
4bxgfe,2016-03-25 13:33:27-04:00,bdjorn,Does my phone work faster on mute?,"Say I'm playing a game on my phone that has sound effects. When my phone is muted, am I removing one of the things that it has to ""think"" about? Does it process other things faster as a result? Or does it need to go through the same steps even when it's not producing sound?

I hope I made my question clear. I'm sure I'm not using the correct words, and I thank you in advance for overlooking that.",,,,,Submission,0,0,0
d1d936i,2016-03-25 13:49:04-04:00,Orionsbelt,,"If you turn off sound effects in the game menu it's possible you might see better performance. If you are just turning volume off it's unlikely because the audio is still playing just with a volume setting at zero.

Not an Android programmer so I can be wrong. ",4bxgfe,t3_4bxgfe,bdjorn,,Comment,2,0,2
d1dhiqd,2016-03-25 17:17:54-04:00,ToplessTopmodel,,Sound is the least worry when you think about Performance.,4bxgfe,t1_d1d936i,Orionsbelt,,Reply,1,0,1
d1dk6x4,2016-03-25 18:32:53-04:00,Orionsbelt,,"Not always true, with gaming desktops sound drivers can cause DPC latency and cause micro lag ect. I'm not saying that performance will get much better or this is a good troubleshooting step just drawing the destination between turning off output of sound and turning the volume of said output to zero. ",4bxgfe,t1_d1dhiqd,ToplessTopmodel,,Reply,1,0,1
d1ddrxj,2016-03-25 15:40:58-04:00,Aquifel,,"There's a good chance it may still be processing the sounds but, just not outputting them (i.e., going through the same steps).  It also may be completely dropping sound processing (not thinking about it), it just depends on how things are programmed, it can go either way.  Either way, it may work faster but, it will likely be an incredibly small performance increase. 

**tl:dr**: Performance increase: potentially (but, very small).",4bxgfe,t3_4bxgfe,bdjorn,,Comment,1,0,1
d1fqnfm,2016-03-27 19:32:54-04:00,Gavinhenderson5,,If you are just turning your phone onto mute before you play a game or something its not going to make a difference as the game would still send out all the same audio just the android system would play it at a volume of zero causing no change to whats being processed but if a game has some kind of sound off setting in it then maybe but that then really depends on how that game sound is designed,4bxgfe,t3_4bxgfe,bdjorn,,Comment,1,0,1
4bx5ir,2016-03-25 12:23:07-04:00,kattwebb,COMPRESSION HELP PLEASE!!,"I was wondering if anyone knew any thing on compression? I am studying computer science and am finding it difficult. Our task is to compress a 1MB file into as small as we can. Do you know how to compress a large file into the smallest amount you can? Or any ways on how to code it? Any guidance would be very very much appreciated :))

Thanks!",,,,,Submission,3,0,3
d1d7pkh,2016-03-25 13:17:12-04:00,instant_cat_torque,,"Compression is a huge area of research in information theory. However, reading up on a basic techniques you will be able to understand strategies that are relatively easy to implement. Perhaps the simplest form of compression is a dictionary-based scheme.

Imagine you have a text file that you'd like to compress. One approach is to create a data structure that maps a word X to a unique integer Y. Next, read the file you wish to compress. For each word you encounter add the word to the dictionary, and instead of writing the word to the output file, write the integer. Finally, write the data structure dictionary at the end of the file. To decompress, replace integers with their words.

This scheme will achieve compression when the size of the integer replacing words is smaller than the words themselves.

Generally speaking compression is challenging, but creating a workflow with a basic structure (e.g. the dictionary approach above), lets you start playing with different strategies and heuristics.",4bx5ir,t3_4bx5ir,kattwebb,,Comment,2,0,2
d1dco1i,2016-03-25 15:13:45-04:00,stangelm,,"Even simpler, as a starting point, is [Run-Length Encoding](https://en.wikipedia.org/wiki/Run-length_encoding).  ",4bx5ir,t1_d1d7pkh,instant_cat_torque,,Reply,2,0,2
d1dgnzv,2016-03-25 16:54:50-04:00,splenetic,,"Do you know in advance what kind of data is in the file? Some compression algorithms work brilliantly with particular data formats but not well with others.

The original PKZIP used a variety of compression algorithms depending on the data. There's an overview of both the ZIP data file format as well as the algorithms it uses [here](https://pkware.cachefly.net/webdocs/casestudies/APPNOTE.TXT).
",4bx5ir,t3_4bx5ir,kattwebb,,Comment,1,0,1
d1e36x8,2016-03-26 07:59:28-04:00,avaxzat,,"What compression you use depends on what type of file you want to compress. If the compression has to be lossless (which I assume is the case here), you might want to look into the following algorithms:

* Huffman coding
* Burrows-Wheeler transform
* Lempel-Ziv (LZ77 and LZ78)
* Lempel-Ziv-Welch (LZW, my personal favorite)

Usually you're going to want to combine some of these together. For example, you might want to preprocess the file using the Burrows-Wheeler transform, then apply LZW and finally Huffman code it. Almost all compression algorithms start with the BWT, then do some other magic and Huffman code as a final step. It's a general recipe that works well.",4bx5ir,t3_4bx5ir,kattwebb,,Comment,1,0,1
4bvgss,2016-03-25 02:02:31-04:00,docares,Have $145 leftover from my research project! What should I buy?,"I recently assisted a prof with some undergrad level research and now I have $$ that I need to spend before the end of March. What should I buy??

My research was on flowshop scheduling optimization for 2 machines with a breakdown period. My professor / mentor recommended books on MPI, Heuristics, Parallel Processing, and Scheduling to further my knowledge of the subject.

I like the idea of GPU processing but not sure if that has much value at this point. 

Thanks for any and all suggestions!
",,,,,Submission,5,0,5
d1cz4oz,2016-03-25 09:36:29-04:00,umib0zu,,Computer time on AWS to test your results.,4bvgss,t3_4bvgss,docares,,Comment,3,0,3
d1czglm,2016-03-25 09:47:18-04:00,docares,,"I'm learning how to take advantage of our University computer cluster. Looks like I have it mostly to myself, so would there be an advantage to AWS?",4bvgss,t1_d1cz4oz,umib0zu,,Reply,1,0,1
d1czwov,2016-03-25 10:01:18-04:00,umib0zu,,"It's just to check it out. Clusters are nice, but being able to spin up your own small cluster might be useful if you're in a pinch. Also, trying to recreate your results on separate machines and documenting them will add to your researches ability to be reproducible.",4bvgss,t1_d1czglm,docares,,Reply,2,0,2
d1d25zc,2016-03-25 11:04:34-04:00,docares,,"Oh that is an excellent point, thanks!",4bvgss,t1_d1czwov,umib0zu,,Reply,1,0,1
d1cxpml,2016-03-25 08:43:23-04:00,Am0s,,"What do you mean about gpu processing not having much value? Is it that you already know a lot about it, or that you don't think it's relevant? 


If the latter, I'm really not sure where you're coming from. 


What I would suggest if you like the idea if gpu processing but want something more broadly useful would be the parallel processing book. GPUs essentially are designed for embarrassingly parallel tasks, and it's also a very relevant skill for programming multicore CPUs. Win/win? ",4bvgss,t3_4bvgss,docares,,Comment,2,0,2
d1czatv,2016-03-25 09:42:08-04:00,docares,,"I wasn't sure how relevant GPU would be because I don't know what I don't know.

Parallel processing sounds like the way to go! ",4bvgss,t1_d1cxpml,Am0s,,Reply,2,0,2
4bv6i0,2016-03-25 00:17:41-04:00,eagle1-2,Check my Cache Simulator out,"I have been working on this cache simulator for my Computer Architecture class and was wondering If anybody would want to take a look at my output and see if I am understanding caches correctly. 

    R
    Read Selected
    Reading Address 5
    Cache Miss
    The value at that address is: 5

    DC
    Display Cache Selected
    Slot  Valid  Tag     Data
    0     1      0       0 1 2 3 4 5 6 7 
    1     0      0       0 0 0 0 0 0 0 0 
    2     0      0       0 0 0 0 0 0 0 0 
    3     0      0       0 0 0 0 0 0 0 0 
    4     0      0       0 0 0 0 0 0 0 0 
    5     0      0       0 0 0 0 0 0 0 0 
    6     0      0       0 0 0 0 0 0 0 0 
    7     0      0       0 0 0 0 0 0 0 0 

    R
    Read Selected
    Reading Address 6
    Cache Hit
    The value at that address is: 6

    DC
    Display Cache Selected
    Slot  Valid  Tag     Data
    0     1      0       0 1 2 3 4 5 6 7 
    1     0      0       0 0 0 0 0 0 0 0 
    2     0      0       0 0 0 0 0 0 0 0 
    3     0      0       0 0 0 0 0 0 0 0 
    4     0      0       0 0 0 0 0 0 0 0 
    5     0      0       0 0 0 0 0 0 0 0 
    6     0      0       0 0 0 0 0 0 0 0 
    7     0      0       0 0 0 0 0 0 0 0 

    R
    Read Selected
    Reading Address 7
    Cache Hit
    The value at that address is: 7

    DC
    Display Cache Selected
    Slot  Valid  Tag     Data
    0     1      0       0 1 2 3 4 5 6 7 
    1     0      0       0 0 0 0 0 0 0 0 
    2     0      0       0 0 0 0 0 0 0 0 
    3     0      0       0 0 0 0 0 0 0 0 
    4     0      0       0 0 0 0 0 0 0 0 
    5     0      0       0 0 0 0 0 0 0 0 
    6     0      0       0 0 0 0 0 0 0 0 
    7     0      0       0 0 0 0 0 0 0 0 

    R
    Read Selected
    Reading Address 14c
    Cache Miss
    The value at that address is: 4C

    DC
    Display Cache Selected
    Slot  Valid  Tag     Data
    0     1      0       0 1 2 3 4 5 6 7 
    1     1      5       48 49 4A 4B 4C 4D 4E 4F 
    2     0      0       0 0 0 0 0 0 0 0 
    3     0      0       0 0 0 0 0 0 0 0 
    4     0      0       0 0 0 0 0 0 0 0 
    5     0      0       0 0 0 0 0 0 0 0 
    6     0      0       0 0 0 0 0 0 0 0 
    7     0      0       0 0 0 0 0 0 0 0 

    R
    Read Selected
    Reading Address 14d 
    Cache Hit
    The value at that address is: 4D

    DC
    Display Cache Selected
    Slot  Valid  Tag     Data
    0     1      0       0 1 2 3 4 5 6 7 
    1     1      5       48 49 4A 4B 4C 4D 4E 4F 
    2     0      0       0 0 0 0 0 0 0 0 
    3     0      0       0 0 0 0 0 0 0 0 
    4     0      0       0 0 0 0 0 0 0 0 
    5     0      0       0 0 0 0 0 0 0 0 
    6     0      0       0 0 0 0 0 0 0 0 
    7     0      0       0 0 0 0 0 0 0 0 

    R
    Read Selected
    Reading Address 14e
    Cache Hit
    The value at that address is: 4E

    DC
    Display Cache Selected
    Slot  Valid  Tag     Data
    0     1      0       0 1 2 3 4 5 6 7 
    1     1      5       48 49 4A 4B 4C 4D 4E 4F 
    2     0      0       0 0 0 0 0 0 0 0 
    3     0      0       0 0 0 0 0 0 0 0 
    4     0      0       0 0 0 0 0 0 0 0 
    5     0      0       0 0 0 0 0 0 0 0 
    6     0      0       0 0 0 0 0 0 0 0 
    7     0      0       0 0 0 0 0 0 0 0 

    R
    Read Selected
    Reading Address 14f
    Cache Hit
    The value at that address is: 4F

    DC
    Display Cache Selected
    Slot  Valid  Tag     Data
    0     1      0       0 1 2 3 4 5 6 7 
    1     1      5       48 49 4A 4B 4C 4D 4E 4F 
    2     0      0       0 0 0 0 0 0 0 0 
    3     0      0       0 0 0 0 0 0 0 0 
    4     0      0       0 0 0 0 0 0 0 0 
    5     0      0       0 0 0 0 0 0 0 0  
    6     0      0       0 0 0 0 0 0 0 0 
    7     0      0       0 0 0 0 0 0 0 0 

    R
    Read Selected
    Reading Address 150
    Cache Miss
    The value at that address is: 50

    DC
    Display Cache Selected
    Slot  Valid  Tag     Data
    0     1      0       0 1 2 3 4 5 6 7 
    1     1      5       48 49 4A 4B 4C 4D 4E 4F 
    2     1      5       50 51 52 53 54 55 56 57 
    3     0      0       0 0 0 0 0 0 0 0 
    4     0      0       0 0 0 0 0 0 0 0 
    5     0      0       0 0 0 0 0 0 0 0 
    6     0      0       0 0 0 0 0 0 0 0 
    7     0      0       0 0 0 0 0 0 0 0 

    R
    Read Selected
    Reading Address 151
    Cache Hit
    The value at that address is: 51

    DC
    Display Cache Selected
    Slot  Valid  Tag     Data
    0     1      0       0 1 2 3 4 5 6 7 
    1     1      5       48 49 4A 4B 4C 4D 4E 4F 
    2     1      5       50 51 52 53 54 55 56 57 
    3     0      0       0 0 0 0 0 0 0 0 
    4     0      0       0 0 0 0 0 0 0 0 
    5     0      0       0 0 0 0 0 0 0 0 
    6     0      0       0 0 0 0 0 0 0 0 
    7     0      0       0 0 0 0 0 0 0 0 

    R
    Read Selected
    Reading Address 3A6
    Cache Miss
    The value at that address is: A6

    DC
    Display Cache Selected
    Slot  Valid  Tag     Data
    0     1      0       0 1 2 3 4 5 6 7 
    1     1      5       48 49 4A 4B 4C 4D 4E 4F 
    2     1      5       50 51 52 53 54 55 56 57 
    3     0      0       0 0 0 0 0 0 0 0 
    4     1      E       0 1 2 3 4 5 6 7 
    5     0      0       0 0 0 0 0 0 0 0 
    6     0      0       0 0 0 0 0 0 0 0 
    7     0      0       0 0 0 0 0 0 0 0 

    R
    Read Selected
    Reading Address 4C3
    Cache Miss 
    Start Address 1216


    DC
    Display Cache Selected
    Slot  Valid  Tag     Data
    0     1      13       C0 C1 C2 C3 C4 C5 C6 C7 
    1     1      5       48 49 4A 4B 4C 4D 4E 4F 
    2     1      5       50 51 52 53 54 55 56 57 
    3     0      0       0 0 0 0 0 0 0 0 
    4     1      E       0 1 2 3 4 5 6 7 
    5     0      0       0 0 0 0 0 0 0 0 
    6     0      0       0 0 0 0 0 0 0 0 
    7     0      0       0 0 0 0 0 0 0 0 

    WR
    Write Selected
    Writing 99 to 14c
    Cache Hit at 4C

    DC
    Display Cache Selected
    Slot  Valid  Tag     Data
    0     1      13       C0 C1 C2 C3 C4 C5 C6 C7 
    1     1      5       48 49 4A 4B 99 4D 4E 4F 
    2     1      5       50 51 52 53 54 55 56 57 
    3     0      0       0 0 0 0 0 0 0 0 
    4     1      E       0 1 2 3 4 5 6 7 
    5     0      0       0 0 0 0 0 0 0 0 
    6     0      0       0 0 0 0 0 0 0 0 
    7     0      0       0 0 0 0 0 0 0 0 

    WR
    Write Selected
    Writing 7 to 63B
    Cache Miss


    DC
    Display Cache Selected
    Slot  Valid  Tag     Data
    0     1      13       C0 C1 C2 C3 C4 C5 C6 C7 
    1     1      5       48 49 4A 4B 99 4D 4E 4F 
    2     1      5       50 51 52 53 54 55 56 57 
    3     0      0       0 0 0 0 0 0 0 0 
    4     1      E       0 1 2 3 4 5 6 7 
    5     0      0       0 0 0 0 0 0 0 0 
    6     0      0       0 0 0 0 0 0 0 0 
    7     1      18       18 19 1A 1B 1C 1D 1E 1F 

    R 
    Read Selected
    Reading Address 582
    Cache Miss 
    Start Address 1408


    DC
    Display Cache Selected
    Slot  Valid  Tag     Data
    0     1      16       80 81 82 83 84 85 86 87 
    1     1      5       48 49 4A 4B 99 4D 4E 4F 
    2     1      5       50 51 52 53 54 55 56 57 
    3     0      0       0 0 0 0 0 0 0 0 
    4     1      E       0 1 2 3 4 5 6 7 
    5     0      0       0 0 0 0 0 0 0 0 
    6     0      0       0 0 0 0 0 0 0 0 
    7     1      18       18 19 1A 1B 1C 1D 1E 1F 

    R
    Read Selected
    Reading Address 348
    Cache Miss 
    Start Address 840


    DC
    Display Cache Selected
    Slot  Valid  Tag     Data
    0     1      16       80 81 82 83 84 85 86 87 
    1     1      D       48 49 4A 4B 4C 4D 4E 4F 
    2     1      5       50 51 52 53 54 55 56 57 
    3     0      0       0 0 0 0 0 0 0 0 
    4     1      E       0 1 2 3 4 5 6 7 
    5     0      0       0 0 0 0 0 0 0 0 
    6     0      0       0 0 0 0 0 0 0 0 
    7     1      18       18 19 1A 1B 1C 1D 1E 1F 

    R
    Read Selected
    Reading Address 3F
    Cache Miss 
    Start Address 56


    DC
    Display Cache Selected
    Slot  Valid  Tag     Data
    0     1      16       80 81 82 83 84 85 86 87 
    1     1      D       48 49 4A 4B 4C 4D 4E 4F 
    2     1      5       50 51 52 53 54 55 56 57 
    3     0      0       0 0 0 0 0 0 0 0 
    4     1      E       0 1 2 3 4 5 6 7 
    5     0      0       0 0 0 0 0 0 0 0 
    6     0      0       0 0 0 0 0 0 0 0 
    7     1      0       38 39 3A 3B 3C 3D 3E 3F 

    R
    Read Selected
    Reading Address 14b
    Cache Miss 
    Start Address 328


    DC
    Display Cache Selected
    Slot  Valid  Tag     Data
    0     1      16       80 81 82 83 84 85 86 87 
    1     1      5       48 49 4A 4B 99 4D 4E 4F 
    2     1      5       50 51 52 53 54 55 56 57 
    3     0      0       0 0 0 0 0 0 0 0 
    4     1      E       0 1 2 3 4 5 6 7 
    5     0      0       0 0 0 0 0 0 0 0 
    6     0      0       0 0 0 0 0 0 0 0 
    7     1      0       38 39 3A 3B 3C 3D 3E 3F 

    R
    Read Selected
    Reading Address 14c
    Cache Hit
    The value at that address is: 4C

    DC
    Display Cache Selected
    Slot  Valid  Tag     Data
    0     1      16       80 81 82 83 84 85 86 87 
    1     1      5       48 49 4A 4B 99 4D 4E 4F 
    2     1      5       50 51 52 53 54 55 56 57 
    3     0      0       0 0 0 0 0 0 0 0 
    4     1      E       0 1 2 3 4 5 6 7 
    5     0      0       0 0 0 0 0 0 0 0 
    6     0      0       0 0 0 0 0 0 0 0 
    7     1      0       38 39 3A 3B 3C 3D 3E 3F 

    R
    Read Selected
    Reading Address 63F
    Cache Miss 
    Start Address 1592


    DC
    Display Cache Selected
    Slot  Valid  Tag     Data
    0     1      16       80 81 82 83 84 85 86 87 
    1     1      5       48 49 4A 4B 99 4D 4E 4F 
    2     1      5       50 51 52 53 54 55 56 57 
    3     0      0       0 0 0 0 0 0 0 0 
    4     1      E       0 1 2 3 4 5 6 7 
    5     0      0       0 0 0 0 0 0 0 0 
    6     0      0       0 0 0 0 0 0 0 0 
    7     1      18       38 39 3A 3B 3C 3D 3E 3F 

    R
    Read Selected
    Reading Address 83 
    Cache Miss 
    Start Address 128


    DC
    Display Cache Selected
    Slot  Valid  Tag     Data
    0     1      2       80 81 82 83 84 85 86 87 
    1     1      5       48 49 4A 4B 99 4D 4E 4F 
    2     1      5       50 51 52 53 54 55 56 57 
    3     0      0       0 0 0 0 0 0 0 0 
    4     1      E       0 1 2 3 4 5 6 7 
    5     0      0       0 0 0 0 0 0 0 0 
    6     0      0       0 0 0 0 0 0 0 0 
    7     1      18       38 39 3A 3B 3C 3D 3E 3F 

",,,,,Submission,4,0,4
4bv387,2016-03-24 23:49:03-04:00,haragan42,Help with interviewing a software developer,"I am not sure if this is the correct place for this but I need to interview a person in the career I want to enter and since I do not know anyone I could interview, I was wondering if I could receive some help here. I do not have a lot of questions (I only have 15), but the assignment does require for contact information to be provided. 
If this is not the appropriate place for this request, could someone tell me where to go? Thank you.",,,,,Submission,2,0,2
d1cpy5n,2016-03-25 00:36:52-04:00,taricorp,,Have a look at the [list of willing interviewees](https://www.reddit.com/r/AskEngineers/comments/39xf9o/call_for_engineers_willing_to_be_interviewed_june/) at /r/AskEngineers. I see at least one SE in there.,4bv387,t3_4bv387,haragan42,,Comment,3,0,3
d1dqqdi,2016-03-25 21:49:06-04:00,haragan42,,"Thank you so much, this is really going to help, thanks!",4bv387,t1_d1cpy5n,taricorp,,Reply,1,0,1
4bqqzd,2016-03-24 04:09:51-04:00,alpha173,(newbie) what kind of application/program that use Automata theory?.,"i'm new to this informatics thingy..so is there like a specific apps to use automata theory? idk if this a dumb question or not since im new and curious about this thing hehe.
",,,,,Submission,5,0,5
d1bk662,2016-03-24 04:54:59-04:00,Syrak,,"Regular expressions are a very common tool for search/replace tasks. UNIX's `sed` and `grep` tools, some text editors. Regular expressions are also present (in various flavors) in several libraries in all programming languages.

Lexer generators such as `flex` use regexps to turn byte streams into tokens. They are commonly used in combination with parser generators such as `yacc`, which technically don't rely on ""automata"" in the formal language theory terminology, but do use state machines (usually with a stack) in order to parse many context free languages efficiently.

A lot of electronic equipment is programmed as finite state machines: coffee machine, vending machine, automatic doors, dish-washer... though ""automata theory"" might not be necessary to design them as such. Representation as state machines is useful for model checking and program verification, which help make cars/planes/rockets safer.",4bqqzd,t3_4bqqzd,alpha173,,Comment,4,0,4
d1cdb1h,2016-03-24 18:33:25-04:00,Devvils,,"Its essentially everywhere in out modern electronic society, people don't recognise it.

I took the online Automata course offered by Stanford, the finite state machine given by Jeff Ullman was tennis scoring!",4bqqzd,t1_d1bk662,Syrak,,Reply,2,0,2
d1bri37,2016-03-24 10:14:29-04:00,None,,[deleted],4bqqzd,t3_4bqqzd,alpha173,,Comment,1,0,1
d1f20ld,2016-03-27 03:38:52-04:00,alpha173,,wow thx!! :D but do u know any other apps ? so i know there is visual automata simulator and JFLAP which u told me..is there any other apps than that or thats the only two?,4bqqzd,t1_d1bri37,None,,Reply,1,0,1
d1ffp73,2016-03-27 14:07:36-04:00,None,,[deleted],4bqqzd,t1_d1f20ld,alpha173,,Reply,2,0,2
d1g9m84,2016-03-28 08:58:43-04:00,alpha173,,"awesome!, thank you!",4bqqzd,t1_d1ffp73,None,,Reply,1,0,1
d1bym3i,2016-03-24 12:56:24-04:00,wlu56,,"When you loaded this webpage, your web browser had to parse HTTP and HTML. Yes, it uses automata theory to do that!",4bqqzd,t3_4bqqzd,alpha173,,Comment,1,0,1
4bqh2d,2016-03-24 02:02:37-04:00,noobeeee,Implementing Search Engine Steps,"Hi,

I'm working on a school project. I need to crawl about 100 websites and their pages. It will come down to about 20,000 different pages.
I need to be able to search with acceptable recalls and acceptable precision.

I'm using [Scrapy](http://scrapy.org) with [BeautifulSoup4](http://www.crummy.com/software/BeautifulSoup/) to crawl and parse the them. I'm also using [cfscrape](https://github.com/Anorov/cloudflare-scrape) to bypass cloudflare protected pages.

Things I'm doing...

1: Crawl and store the whole HTML dom in MySQL

2: Remove stopwords and segment the words

3: Use MapReduce to index and store in the MySQL in the following format.


    id | word | doc_id
    ---------------------
    1  | Katy | 1
    
    2  | Katy | 3 
    
    3  | Katy | 4
    
    4  | visit  | 5
    
    5  | Katy  | 5 ...



4: When user searches, remove stopwords, segment and then search from our index.

For example, when user searches for ""Katy visited Zoo on Sunday"", doc_id 5 will return at the top.


5: There are some more tweaks involved in ranking the pages - published time of the articles, popularity of the page, previous viewed count, etc.



My worries and questions are...

1. Is storing HTML DOM in MySQL a good idea? I believe it will blow up the storage of my database. 

2. My Index table. It seems that after a while, the index table will have billions of rows. Is that how to correctly store the words index?
For example, the word ""visit"" will be in a l. ortant results.

3. What other important things did I miss from my implementation?

I'm not trying to make another google. But, I want to make my implementation as good as a single me could.

Thanks everyone.

Have a lovely day.",,,,,Submission,6,0,6
d1buhyp,2016-03-24 11:26:27-04:00,justlikestoargue,,"Instead of storing everything in the database, can you strip out things that ought to be irrelevant, like html tags, JavaScript code, etc.? Might be an issue if someone is searching for specific snippets of html code, but aside from that it would save a lot of space and wouldn't affect non-Web-dev searches afaik",4bqh2d,t3_4bqh2d,noobeeee,,Comment,2,0,2
d1byqo4,2016-03-24 12:59:09-04:00,noobeeee,,Awesome.,4bqh2d,t1_d1buhyp,justlikestoargue,,Reply,1,0,1
d1bt5we,2016-03-24 10:55:12-04:00,armpit_puppet,,"The [Stanford Information Retrieval book](http://nlp.stanford.edu/IR-book/html/htmledition/irbook.html) would probably be most helpful here. The [first chapter](http://nlp.stanford.edu/IR-book/html/htmledition/an-example-information-retrieval-problem-1.html) on Boolean retrieval would really help you set up your indexes. 

By no means am I an IR expert, but hopefully this is helpful. 

To answer your specific points:

* Storing the DOM in MySQL should be fine for the purposes of your problem. If you have a bunch of data (like too much for the disk on your machine), you could put it on cloud storage or something with links to the HTML blobs. You could also apply the same idea locally, where the MySQL table points to a path on the local filesystem. 

* You're looking for a postings list, probably with position information in the index. The book above has details in the beginning chapters. The position information should help rank results as well. Imagine crawling a page with ""Katy Perry's new tour dates released"" and ""Obama's visit to Cuba"". Without positions, this ranks the same as your example. 

* My back-of-the-napkin math thinks a billion rows in MySQL is do-able, but inefficient. 1B rows * (64bit ID + varchar + 64bit doc ID) puts you at at least 20-25GB on disk. Postings lists are where it's at. 

* Probably no reason to remove stop words if you need to support free text queries. With stop words removed, could you support queries like ""to be or not to be""?

* Could you use Map-Reduce at the query stage to make things faster?",4bqh2d,t3_4bqh2d,noobeeee,,Comment,1,0,1
d1bu9ou,2016-03-24 11:21:09-04:00,noobeeee,,"Thank you. I'll be reading the book.

I think I'll just put a pointer to HTML DOM in my database.

About stopwords, true. My engine won't be able to search it. However, index-ing stopwords will be pretty a overkill in our storage. Isn't it? However, I could look into index-ing phrases.

MapReduce yeah definitely. However, how should I use it for search?

Thanks.",4bqh2d,t1_d1bt5we,armpit_puppet,,Reply,1,0,1
4bovi4,2016-03-23 18:34:53-04:00,BenRayfield,Whats a good turing complete form of the Odds And Evens game?,"[Odds And Evens](https://en.wikipedia.org/wiki/Odds_and_evens), aka [Matching Pennies](https://en.wikipedia.org/wiki/Matching_pennies), is a zero-sum game of infinite strategy. Its simpler than [Rock Paper Scissors](https://en.wikipedia.org/wiki/Rock-paper-scissors) since it has 2 choices instead of 3, and there are no ties.

If we represent a snapshot of a computing/thinking process as an integer, then 2 times that integer plus 0 or 1 (appending a lowest bit) can represent a single bit input, and the lowest bit can be an output. The rest is state of code and data which may self-reference itself, depending what kind of turing completeness is used.

A snapshot of a player, represented as an integer, gets input by checking if that integer is even or odd, and gives output as the returned integer being even or odd. The returned integer, other than the lowest bit, is the next state of that player, stored statelessly as an integer.

Considering that we cant let a player and a copy of itself win both times or lose both times, the bit output of one is copied into the other literally, and the bit output from that one is NOT'ed, so there is never a [nash-equilibrium](https://en.wikipedia.org/wiki/Nash_equilibrium) between a player and a copy of themself.

These simple semantics define a way to play (Matching Pennies / Odds And Evens) between any 2 turing complete processes, and to explore any or all possible branches independently of who they're playing against since the game world outside is defined entirely by a growing bitstring, of 1 more bit per cycle.

The big question is, whats a good mapping between integer and turing complete process, which plays this interactive bit streaming game better as it gets deeper into the games? Keep in mind that memory and computing time are a cost, so the function of integer to integer could limit that cost by only returning integers up to some maximum.",,,,,Submission,1,0,1
4bo91z,2016-03-23 16:14:39-04:00,gilgameshskytrooper,Wanting advice for a C++ project I have in mind,"I am looking for some advice for a very large scale C++ project me and my friend came up with for our current software design course. Now I have had experience with Python, R, C#, and Java, but the majority of the code has to be in C++.

So the problem our project is trying to solve is the issue that students at our school have no idea if a laundry machine is in use or not. Instead of physically having to check the state of the machines, we thought it would be nice to be able to see the state of all machines on campus at once, through a website or app. To create such a service, we plan to install a system of sensor assisted microcomputers (C.H.I.P. or Raspberry Pi), that would communicate ON or OFF state changes to the machines through WiFi to a central server, which would then update the changes to a database, and then update the information on a web server.

Now this is a fairly gargantuan task for two people, and we expect about 2 teams of 4 people each to be assigned once we start recruiting.

But in the meantime, I just wanted to ask the informed and experienced community at Stackoverflow how you guys would attempt such a project.

First and foremost, how would I make a C++ structure that could send a package of data from the microcomputer to the central server? How could I design a server side object that would take that data, analyze it, and call the meaningful functions that will then work on the tasks of updating the database and the webserver?

Another question is, is this how you would approach the problem? or is my thinking inefficient for this project?

How should be split the teams for maximum efficiency? My thoughts are one team for the microcomputer end and one team for the server side.

Is there a way to have C++ and another language, say PHP (or any other web scripting language), to be able to update the website in realtime (without the user having the refresh?)

Please excuse my ignorance on a lot of these areas. I really love this idea, and I am willing to put in the effort to make it work, but I do not know where to start. So Any advice on this topic would be much appreciated.

Cheers, happy Seg Fault 11's",,,,,Submission,12,0,12
d1ay6jw,2016-03-23 16:31:32-04:00,instant_cat_torque,,"It seems that there are three primary components to this project which might work well for three teams:

1. Detecting and communicating to a microcomputer the state of the machine.
2. Having a microcomputer communicate machine state to a database service.
3. Display the state of the machines found in the database (e.g. organized by building).

# Detecting and Communicating Machine State

Ultimately this step requires aggregating machine ON/OFF state at one or more microcomputers for a physical location.

1. How will ON/OFF state be detected?
2. You could potentially save money by structuring the system so that one microcomputer could monitor multiple machines.

# Post Machine State

This microcomputer will periodically 'call home' to a centralized location (e.g. a REST web endpoint). The primary concern here is probably security and reliability. For instance, make sure the device can recover from intermittent errors such as wifi and power issues. Security can as simple or as complicated as you want. Mostly the concern here is that an open REST endpoint should be restricted to authorized microcomputers. You could just do this with a password, or something more robust.

# Website

This is basically just a webserver and database that collects machine state.

Ultimately I don't see much need for any C++. Depending on how you detect machine state, you might need some low-level coding that is harder in high-level languages.",4bo91z,t3_4bo91z,gilgameshskytrooper,,Comment,8,0,8
d1b049o,2016-03-23 17:14:17-04:00,CoopNine,,"I'm assuming the majority the code has to be in C++ because of course requirements... Because you could do this with a scripting language... probably a lot easier :)

As for your sensors sending data, I'd probably look at [gSOAP] (http://www.cs.fsu.edu/~engelen/soap.html)  It's pretty simple to use.  Alternatively you could use cURL to hit a rest webservice.  If you're using pi's for your sensors, you could use cron to schedule the interval that they report.  That's a lot cleaner than using a resident program which sleeps (in my opinion).  

If you use gSOAP, your server could be written in C++ rather easily, and exist as a CGI module... (You could run this on a pi too!)  You would give each sensor an ID and update its status when it checks in.  (Keep the last reported date to identify bad sensors).  

Then you can build a simple web site using PHP or whatever you like, which would query your DB and report the status of each sensor.  If you want the page to update 'automatically'  you can either refresh the entire page on an interval, or make more discreet calls using AJAX,  I'd recommend a lib like jQuery that makes this easy.  

This is rather easily a two person job if you work smart and don't over-complicate things.  I'd have one team/person work on the sensors, and the other work on the web service and web site.  The web service (server) should be done first,  the WSDL passed to the guys making the client so they can generate their calls.

The risk I see is interfacing with the washing machines.  Everything else has been solved by others out there, and there are libs to build it rather quickly.  (If allowed) re-use that work and don't reinvent any wheels, or data-packet transmission methods :).",4bo91z,t3_4bo91z,gilgameshskytrooper,,Comment,4,0,4
d1dfrnv,2016-03-25 16:31:22-04:00,gilgameshskytrooper,,"gSOAP is golden advice! Thank you. I thought I would have to invoke Rails action, but if I can do it directly in C++, this would be more worthy.",4bo91z,t1_d1b049o,CoopNine,,Reply,1,0,1
d1c78b6,2016-03-24 16:07:13-04:00,dmazzoni,,"> Now this is a fairly gargantuan task for two people, and we expect about 2 teams of 4 people each to be assigned once we start recruiting.

Are you sure it'd actually be faster with 8 people involved?

Read up on [The Mythical Man-Month](https://en.wikipedia.org/wiki/The_Mythical_Man-Month) and remember that nine women can't make a baby in a month.

The problem with having too many people on a software project is that reaching consensus and communicating decisions ends up taking more time than actually doing the work.

I'd recommend breaking the task into separate pieces that can be built and tested as independently as possible. /u/instant_cat_torque did a great job in his/her comment, and that only involves three pieces.

Personally I'd try to just do it with three people, one for each of those. At most, make each a team of two, and try to have each team do their parts as independently as possible, then put them together and plan on a lot of time for integration.

To recap:

**Manager:** How long will it take?

**Lead programmer:** About two more weeks

**Manager:** Okay, then what if I add two more people to your team?

**Lead programmer:** Then it will take at least a month
",4bo91z,t3_4bo91z,gilgameshskytrooper,,Comment,2,0,2
d1c8hu6,2016-03-24 16:35:29-04:00,lordvadr,,"Just throwing out some ideas and thoughts.

If you go look at C code for this type of stuff, you see some neat tricks that help with this.  One problem with objects is that they don't translate well to a data-stream, so you have to write a stream-out and a stream-in method to send/receive your object code.  Also remember that you should never trust ANYTHING from the network.  Double check everything.

For starters, you need to start thinking about different architectures and such when you start sending network data.  Some platforms an int is 4 bytes, sometimes it's 2.  So it's wise to use datatypes that are specific.

But, just an example you see this in some database C code to define variable length strings....

    typedef struct str {
        uint32_t len:
        char data[0]:
    } str_s:

Which is a clever trick where you just

    str_s *p:
    
    p = malloc( sizeof(str_s) + strlen(mystring) ): // you can +1 here if you want to null terminate it too
    p.len = strlen(mystring):
    strncpy(p.data, mystring, p.len): //again, +1 if you want to null terminate
    p.len = htonl(p.len): // get the byte order right
    .... send the message ....
    free(p):

But this is a good way to get data into something you can stuff into a TCP stream.  Checkout the documentation for ntohl/htonl and ntohs/htons for some interesting reading.  Getting that right means you know what you're doing.

The trick plays off of the fact that sizeof(char[0]) is zero, but you've already allocated enough data, and at runtime, there's no bounds checking.

Now, with that said, this being a C++ class, you'll need to use new and delete and such, but nothing stops you from using malloc too (in fact, if you look at the generated code, that's what new and delete do anyway).

Now, when you receive data, you do something like...

    str_s *p:
    int c:
    p = malloc( sizeof(str_s) ):
    c = read( socket, p, sizeof(str_s) ): // you're only reading the length
    p.len = ntohl(p.len):
    if (c < sizeof(str_s) || p.len > SOME_MAX_LEN) { .... close connection, bad data.... }
    
    p = realloc(p, sizeof(str_s) + p.len ) // make more space in the buffer
    
    if (read(socket, p.data, p.len) < p.len) { .... bad data .... }

    // and process as normal, though, in this case, you'll proabably want to make sure data is printable characters or some other sanity check.

This can be extended to a message or command type channel too, even an array of commands.

Also, be sure to read up about how to do non-blocking I/O as any network daemon should be capable of.",4bo91z,t3_4bo91z,gilgameshskytrooper,,Comment,2,0,2
4bnf7x,2016-03-23 13:13:29-04:00,jaggernought,[Help] What topics should I have researched for my dissertation?,"For my dissertation I built a price of software (IDE) for developing web application. The software can: automatically include frameworks such as bootstrap, generate HTML code with a WYSIWYG editor, create code snippets, edit and preview CSS code and connect to a FTP server. (There's more but they're the main features). Now I have to write up a final report.

My issue is that I did no research and I'm not sure what my research topics should have been. So could somebody please suggest what I should research?",,,,,Submission,3,0,3
d1as70f,2016-03-23 14:24:24-04:00,None,,[deleted],4bnf7x,t3_4bnf7x,jaggernought,,Comment,5,0,5
d1asq0t,2016-03-23 14:35:28-04:00,jaggernought,,Yeah Bsc Hons. Should have but didn't so really need to do some now.,4bnf7x,t1_d1as70f,None,,Reply,1,0,1
d1b9o7b,2016-03-23 21:40:42-04:00,n_OP_e,,Depends what your course is but Human computer interaction with web development could be a start,4bnf7x,t1_d1asq0t,jaggernought,,Reply,1,0,1
d1bjynz,2016-03-24 04:41:02-04:00,markgraydk,,"You may be able to salvage it but it may be hard. First, ask yourself a few questions. Why did you write the software? What was the goal? What did you try to archive? How did you write it? What language, other tech did you use? Did you use some special methodology to write it (think development processes and architecture and patterns). Evaluate the development experience. What went well, what didn't. Should you have used different tech or methodology to archive your goals.

Next, look at your courses and see what might fit with what you wrote. Do some research in those areas and write some notes. Contact a professor and sit down with them for help. Bring your notes and the answers to the questions up top. You need some advisor to help you I think. I forget the details but a professor once told me a cautionary tale of a couple of students that did what you did. They made a webshop, I think it was, but with little reference to any research. They also came to the realisation they made a mistake but talked to him and he got them on track. 

Perhaps you may have to make some changes to the software to fit it with a research topic. But put that off till you have a better idea of what you want to do. Human computer interaction as someone suggested could be a good topic but that may require some changes, perhaps even some testing/experiments with users using the software. If you used special tech or methodology you may be able to do a write up of that by itself. Even if you didn't, with few or no changes you could pick a topic like OOP and do a detailed analysis of the pros and cons and whys and whynots. 

",4bnf7x,t3_4bnf7x,jaggernought,,Comment,1,0,1
4bl6cx,2016-03-23 00:49:05-04:00,freemarco,Advice or help. Major in computer science?,"I've been out of high school for 3 years. I never took calc or trig in high school. I want to say I'm bad/average at math. 

I know I'll have to take remedial classes in college. And there is a lot of math, calc, physics, etc. This scares me. 

I never coded before (tried java at codeacademy and it was hard, but I kinda liked it). 

I'm looking into majoring in cs at my community college. Should I do it? Is it too late?",,,,,Submission,1,0,1
d1a4q4y,2016-03-23 00:52:44-04:00,Xxyr,,"What do you want to do with the degree? You might want to consider coding bootcamp as an alternative, especially if math is your fear.",4bl6cx,t3_4bl6cx,freemarco,,Comment,1,0,1
d1bemgu,2016-03-24 00:12:06-04:00,Ignyted,,"I went to school for Computer Science without taking trig or calc and with zero programming knowledge. In fact, it was actually a couple years that I had actually even taken a math course. 

I just went straight into it and took calculus with a positive mindset and focused heavily on my programming course and the concepts. 

So no, it's not too late. Colleges have to assume most students are coming in with almost zero experience and that was true for a lot of my classmates. Whether or not you should do it is if it's something you're passionate and excited about. I was a little nervous diving into something where I felt like I was in no position to attempt compared to others, but I was looking forward to learning and being challenged and I enjoyed it. ",4bl6cx,t3_4bl6cx,freemarco,,Comment,1,0,1
d1imdk3,2016-03-29 23:04:33-04:00,freemarco,,Thank you for this,4bl6cx,t1_d1bemgu,Ignyted,,Reply,1,0,1
d1dveeb,2016-03-26 00:17:04-04:00,heyitsvini,,"Same. I suck at math or related and I know very little programming too, plus, I'm in the last year of high school and sometimes I wonder if I should follow a different path. 

After reading the answers here I felt more confident and I guess we should give cs a try, once it is the thing we like.   ( : ",4bl6cx,t3_4bl6cx,freemarco,,Comment,1,0,1
d1imdt2,2016-03-29 23:04:45-04:00,freemarco,,Good luck bro,4bl6cx,t1_d1dveeb,heyitsvini,,Reply,2,0,2
d1e8hr7,2016-03-26 11:42:42-04:00,Yithar,,"It's not too late. It's going to be hard as CS does involve quite a bit of math. But if you work hard you can do well.  
  
As to whether it's worth it, is this what you want to do in life? You have to ask yourself that.",4bl6cx,t3_4bl6cx,freemarco,,Comment,1,0,1
d1ime0b,2016-03-29 23:04:55-04:00,freemarco,,Thank you,4bl6cx,t1_d1e8hr7,Yithar,,Reply,2,0,2
4bkyop,2016-03-22 23:41:28-04:00,courtenayplacedrinks,Why do O/S ever hang? Why don't they prioritise the basic UI operations so you can close things down?,"I was using Linux today and the browser took up all the CPU cycles to the point where the mouse and keyboard weren't even working. I tried to switch to a terminal screen so I could terminate the offending process but they key-sequence didn't do anything.

It's not just Linux though, I've had exactly the same problem on Windows.

Why aren't there some basic UI activities that are given processor cycles above all others? For Windows it would be mouse, keyboard, window operations (but not repaints), menus, power management and task manager. For Linux it would just need to be mouse/keyboard, window operations and power management. In Linux if you can switch to a terminal screen you can usually interact, if slowly.
",,,,,Submission,12,0,12
d1a31ac,2016-03-23 00:00:24-04:00,sandwichsaregood,,"A full system lockup is not usually because of the CPU being maxed out, it is because something critical in the OS has crashed and taken everything else with it. Rendering windows and handling inputs is a much more complicated process nowadays, involving multiple devices communicating that might even partially bypass the CPU, like with composited window rendering.

The mouse and keyboard *are* prioritized over almost everything. If your web browser caused them to lock up, it probably was a bug that caused the xserver (responsible for drawing windows as well as the mouse and keyboard on the screen in Linux) to crash. You get similar effects on Windows or MacOS if the process responsible for drawing the screen or handling input crashes.

This is not quite the same as a kernel panic (where the core of the OS itself crashes, AKA a blue screen on Windows), though they can be related.",4bkyop,t3_4bkyop,courtenayplacedrinks,,Comment,9,0,9
d1aizql,2016-03-23 11:06:09-04:00,yes_thats_right,,"> A full system lockup is not usually because of the CPU being maxed out, it is because something critical in the OS has crashed and taken everything else with it.

I have read commentary from Microsoft that the primary reason for such crashes is almost always 3rd party drivers rather than the OS.",4bkyop,t1_d1a31ac,sandwichsaregood,,Reply,1,0,1
d1a469g,2016-03-23 00:34:16-04:00,courtenayplacedrinks,,"It all sorted itself by the time I came back with a coffee. I think if it was the xserver I would have had to log in again (unless it can restore all the windows after the crash, maybe it can).

I like your answer though, I can certainly imagine that there's a particular system call that has a high scheduling priority. Perhaps it usually responds instantaneously but it got blocked for some reason and my application started retrying over and over, rapidly slowly the computer down to a halt.

The more I think about it I can see how hard it would be to draw a clear line between UI-critical tasks and background work. I'm sure the UI relies on many of the same system processes than applications do.
",4bkyop,t1_d1a31ac,sandwichsaregood,,Reply,1,0,1
d1a4pze,2016-03-23 00:52:34-04:00,sandwichsaregood,,"Yeah, what you describe is probably it if it managed to recover- a race condition involving a system call. I routinely use my workstation pinned at 3200% CPU utilization for hours and I have never really noticed a difference in responsiveness. There's lots of queuing and prioritization going on to make it that way.

The other major cause of slowdowns is IO contention. Reading from disk is the major bottleneck in most software nowadays and it can cause slowdowns if it is overwhelmed. This shouldn't cause your mouse and keyboard to freeze on its own, but it could in combination with other circumstances I suppose. Swapping RAM to disk also used to be another cause, but that is much rarer these days.",4bkyop,t1_d1a469g,courtenayplacedrinks,,Reply,4,0,4
d1afjab,2016-03-23 09:37:17-04:00,crookedkr,,Sounded like a swap issue to me. It's not that uncommon on laptops to hit the swap. I've seen my browser use much more memory than you might think it needs...,4bkyop,t1_d1a4pze,sandwichsaregood,,Reply,2,0,2
d1a7ykw,2016-03-23 03:11:17-04:00,zanidor,,"If you aren't familiar with hardware interrupts, I'd recommend reading up on that topic: https://en.wikipedia.org/wiki/Interrupt

When your keyboard triggers an interrupt, your operating system is responsible for distributing the keypress event to any process that might care. The processes then respond accordingly.

If your OS is handing key press events off to xserver, which hands them off to your web browser, etc., it's still up to each individual process to respond appropriately. If your web browser is waiting on some other unavailable system reasource or in a blocked state because of a programming error, it will simply never respond to the key press. The process is ""deadlocked"" and is simply not moving forward in its execution. If xserver itself is deadlocked, no application will respond.

Processes in Linux can spawn sub-processes to handle tasks in the ""background"" so the main task is free to continue responding to user input events from the operating system. However, writing code that correctly synchronizes across multiple sub-processes is notoriously hard. Applications like xserver are very complex, and there are lots of states a computer can be in. Taken together, this means there are lots of opportunities for subtle, rarely-encountered programming errors to hang a process from responding to user input.

In these cases, the OS is still handling hardware interrupts, but some running process has stopped responding to them.",4bkyop,t3_4bkyop,courtenayplacedrinks,,Comment,3,0,3
4bkjnc,2016-03-22 21:43:33-04:00,-Rum-Ham-,Have any of you seen this kind of behaviour in a (RFID) protocol before? Need some help! (x/post /r/cryptography),"Hello! 

I'm trying to analyse an RFID chip by first looking at it's protocols. I've used a snooping device to snoop on the transactions between the tag and a reader and it all looks pretty standard for 13.56MHz chip (ISO14443-A) up to a point. Once the standard protocol stuff is out of the way (past the anti collision and selection) it starts some form of proprietary protocol:

1) The tag sends 4 bytes of some form of number (with 4 bytes of 0 preceding or succeeding it)

2) The reader sends back those 4 bytes decremented

3) The next time the tag gets to this point (in another instance of the protocol) it sends back to the reader what it was sent in part 2 and the reader decrements it again.

In hex:

------------------------------------------------------------------------------

TRANSACTION 1:

Tag -&gt; Reader : 00 00 00 00 *FA FA FA FA* + some bytes that don't change

Reader -&gt; Tag : *F9 FA FA FA* + some bytes

------------------------------------------------------------------------------

TRANSACTION 2:

Tag -&gt; Reader : *F9 FA FA FA* 00 00 00 00 + some bytes

Reader -&gt; Tag : *F6 FA FA FA* + some bytes

------------------------------------------------------------------------------

TRANSACTION 3:

Tag -&gt; Reader : 00 00 00 00 *F6 FA FA FA* + some bytes

Reader -&gt; Tag : *F5 FA FA FA* + some bytes

------------------------------------------------------------------------------

This occurs every time, with the 4 bytes of 0s alternating each time.

It is also odd because each byte decrements in an odd way:

FA -&gt; F9 -&gt; F6 -&gt; F5 -&gt; EA -&gt; ... -&gt; 0A -&gt; 09 -&gt; 06 -&gt; 05

Upon the first byte reaching 05, the second byte starts to decrement in the same sequence. So 05 FA FA FA -&gt; FA F9 FA FA.

Has anyone seen anything like this? Is it some form of odd counter of interactions? And why does it go down instead of up? Any ideas? 

I have run the snoop over many iterations of the tag interacting and managed to get it down to FA FA F9 FA, and I assume once the 3rd byte reaches 05 the 4th shall start decrementing too.

Thanks in advance.",,,,,Submission,5,0,5
d1a6nh4,2016-03-23 02:06:50-04:00,doyouevenbinary,,Very interesting. Maybe a counter for a rolling key combo? Maybe it's just used to distinguish interactions from each other? I'm a student who knows next to nothing but would love to hear the thoughts of someone much smarter! ,4bkjnc,t3_4bkjnc,-Rum-Ham-,,Comment,2,0,2
d1a6unh,2016-03-23 02:15:38-04:00,-Rum-Ham-,,"I thought it might be to distinguish between interactions, it follows a clear pattern, and depending on the following hex character it puts the 00's on the right side (every time the first byte ends in A it puts the 00's first). 

Might prevent cloning of the card if the reader keeps a store in memory of what number it should be on, as it seems to carry on through transactions.

Will just have to wait and see what people think.",4bkjnc,t1_d1a6nh4,doyouevenbinary,,Reply,1,0,1
4bgt1m,2016-03-22 06:05:32-04:00,4nexus,I need help with this. I've to submit it on analytic view and in C++ code.,,,,,,Submission,6,0,6
d18yin7,2016-03-22 06:06:31-04:00,4nexus,,"k=cift means k=even numbers 

If one could help me displaying it on block diagram and analytic view then for code I think I could do it myself. Help appreciated.  
  
**Edited for clarity**  
An example of block diagram https://gyazo.com/131300cbd5daaebccb9988184504cb7c",4bgt1m,t3_4bgt1m,4nexus,,Comment,3,0,3
d1957cw,2016-03-22 10:29:39-04:00,ToplessTopmodel,,I could help you with the cpp part but the other thing i have never hears about,4bgt1m,t3_4bgt1m,4nexus,,Comment,3,0,3
d19793h,2016-03-22 11:19:42-04:00,4nexus,,Can you give me a hint about the part after SUM? I know I'll have to declare variables and then give a value to N,4bgt1m,t1_d1957cw,ToplessTopmodel,,Reply,1,0,1
d19cr7i,2016-03-22 13:20:21-04:00,nawap,,"I think by analytic view you mean a flowchart? You can find many guides on how to draw a flowchart online. Start from there.

The C++ part should be easy once you separate out the various parts of the expression. The sum is a loop from lower to upper bound and you perform the inner expression that number of times.",4bgt1m,t3_4bgt1m,4nexus,,Comment,2,0,2
d19dho2,2016-03-22 13:36:34-04:00,4nexus,,Yea we also have to put it on flow chart and analytic view. Thank you ,4bgt1m,t1_d19cr7i,nawap,,Reply,1,0,1
d190q8b,2016-03-22 08:06:53-04:00,None,,[deleted],4bgt1m,t3_4bgt1m,4nexus,,Comment,-4,0,-4
d191b5p,2016-03-22 08:30:40-04:00,4nexus,,I didn't ask for solution I just asked for tips and help...,4bgt1m,t1_d190q8b,None,,Reply,3,0,3
d195yg0,2016-03-22 10:48:35-04:00,946789987649,,"Typically when asking for help with programming, you have an attempt at least and then once you're stuck, you come and ask for help. If you don't even know how to start, it might be worth talking to your lecturer rather than us.",4bgt1m,t1_d191b5p,4nexus,,Reply,1,0,1
d196wph,2016-03-22 11:11:45-04:00,4nexus,,"I did start it but I'm stuck on the part after SUM as seen here
https://gyazo.com/c4748075511092cbf61a0069925a553a ",4bgt1m,t1_d195yg0,946789987649,,Reply,2,0,2
d19ic1h,2016-03-22 15:22:41-04:00,HollowWiener,,"Based on what you've shown us, n should not be defined as 1, n should be user defined and passed in from above.",4bgt1m,t1_d196wph,4nexus,,Reply,1,0,1
d19iwyr,2016-03-22 15:35:29-04:00,4nexus,,Yes n will be asked with cin command for input ,4bgt1m,t1_d19ic1h,HollowWiener,,Reply,1,0,1
d197l5b,2016-03-22 11:27:15-04:00,946789987649,,No idea what on earth that is. You didn't really describe what analytic view is since I don't think many people use that (whatever it is). Where's your code? ,4bgt1m,t1_d196wph,4nexus,,Reply,0,0,0
d198c2w,2016-03-22 11:43:52-04:00,4nexus,,"Sorry if I wasn't clear enough
Here's an example: 
https://gyazo.com/131300cbd5daaebccb9988184504cb7c
After I interpret it on diagram I have to make the code for it which I think it shouldn't be hard but I'm struggling on other views 
",4bgt1m,t1_d197l5b,946789987649,,Reply,2,0,2
d19csao,2016-03-22 13:21:02-04:00,ToplessTopmodel,,"Looks like a typical flow chart. A sum is easily just a for loop. You will have to declare a sum Variable and then enter a for loop in which you calculate each step of the sum and add it to the sum variable.

Function (x) {
Sum = 2

For k = 0: k < n : k+2{
Sum += x*x+ ...
}}

Here some pseudo code. I think that should be more then enough to help you.",4bgt1m,t1_d198c2w,4nexus,,Reply,2,0,2
d19dfzx,2016-03-22 13:35:32-04:00,4nexus,,Thank-you very much! I'll be sure to try this ,4bgt1m,t1_d19csao,ToplessTopmodel,,Reply,1,0,1
4bgeyn,2016-03-22 03:06:17-04:00,nbaplaya101,Best Cyber Security Books to Read in College,"Any suggestions? First year in college and want to get ahead of the game

Appreciate the help! THANKS",,,,,Submission,16,0,16
d19i0vg,2016-03-22 15:15:52-04:00,high_side,,~~Deception Point~~ Smashing the Stack.,4bgeyn,t3_4bgeyn,nbaplaya101,,Comment,1,0,1
4bg89o,2016-03-22 01:48:34-04:00,ThePopemobile,Best final year modules?,"I am having a difficult time choosing my optional modules for my final year.    
I'm allowed to choose 4 and i've narrowed the options that i am most interested in down to the following.

-MACHINE LEARNING    
-LANGUAGE, DESIGN AND IMPLEMENTATION (compiler construction)    
-COMPUTER SECURITY    
-ARTIFICIAL INTELLIGENCE    
-DATA MINING    
-NEURAL NETWORKS    
-DATA COMPRESSION    
-PHYSICAL COMPUTING    

I want to choose modules that are most relevant and employable. However, i don't want to choose a combination of 4 that will be extremely difficult.

My main request:     
Please rank these options 1-8 by employability/relevancy then also rank them 1-8 by difficulty. Feel free to justify your rankings.

Any kind of other information is appreciated. e.g which modules i should absolutely take. which combination would be far too difficult etc.
",,,,,Submission,5,0,5
d191swg,2016-03-22 08:49:03-04:00,BarqsDew,,/r/cscareerquestions,4bg89o,t3_4bg89o,ThePopemobile,,Comment,3,0,3
4besyq,2016-03-21 19:05:24-04:00,asdfqwertylol,Is blossom algorithm appropriate?,"I need to provide and answer to the following question: ""A directed graph may have many different minimum capacity s−t cuts. Give a
polynomial time algorithm that, amongst all the minimum capacity cuts, finds one that contains the largest number of arcs.""  
  
I was thinking of using the blossom algorithm , but it returns the matching with the max number of nodes, whereas I am asked to find the one with the max number of arcs. Is this the right approach?  
  
I can use the residual graph of ford fulkerson to find a min s-t cut, but I don't think it guarantees that the cardinality of the deges of this cut will be maximal.  
Thanks for the input!",,,,,Submission,9,0,9
d18tker,2016-03-22 01:16:12-04:00,paulblarr,,"the solution I found is to run FF, then run DFS on the residual graph to get the edges in the min st cut.  
Then loop through all the edges that are not in the min s-t cut and increase their capacity by one at a time.  
Then run FF with this new edge, and see if the flow you get has increased. If this is the case, a new min st cut
has been found, so I check its cardinality and see if it is an improvement.  
 The runtime of FF is (VE^2), and it runs 
on at most e times (edges not in cut), so the runtime is O(VE^3).   No need for blossom algorithm after all.",4besyq,t3_4besyq,asdfqwertylol,,Comment,1,0,1
d190b93,2016-03-22 07:47:55-04:00,thewataru,,"You could modify capacities of all edges to Cx(initial capacity)-1.  
This way cost of any cut in the modified network is Cx(cost in given network)-number of arcs. If you choose C more than |E| then min cut in the new network will correspond to min cut in the original network but also will have a maximum number of arcs in it.

Edit: then apply any known polynomial algorithm for min-cut/max-flow problem.",4besyq,t3_4besyq,asdfqwertylol,,Comment,1,0,1
4bagd3,2016-03-20 22:09:34-04:00,Wowsoysauce,Have you ever failed a class?,in your undergrad... have you ever failed a class? was it for your major or a  GE...? whats your story?,,,,,Submission,7,0,7
d17k9f9,2016-03-21 01:24:46-04:00,orlybach,,"yes, physics my freshman year",4bagd3,t3_4bagd3,Wowsoysauce,,Comment,5,0,5
d17rjif,2016-03-21 08:42:04-04:00,Wowsoysauce,,Sigh this might be me ...,4bagd3,t1_d17k9f9,orlybach,,Reply,1,0,1
d18jlcg,2016-03-21 20:20:48-04:00,orlybach,,I should have dropped it so I didn't take the bad grade. I retook the class over the summer at an easier campus and got an A though. ,4bagd3,t1_d17rjif,Wowsoysauce,,Reply,2,0,2
d1ajngs,2016-03-23 11:21:18-04:00,None,,[deleted],4bagd3,t1_d17rjif,Wowsoysauce,,Reply,1,0,1
d1ajpz0,2016-03-23 11:22:54-04:00,Wowsoysauce,,should i drop out if im struggling with calc 1...,4bagd3,t1_d1ajngs,None,,Reply,1,0,1
d1ak0ol,2016-03-23 11:29:32-04:00,zefcfd,,"no, i failed calc one, and got an A the second time through. then a ""B"" in calc II and III. 

What you should do, is take your first programming class ASAP. if you like it, stick with it.

seriously, past the first 1.5 years its interesting. I wish I could just keep taking classes part time because they're interesting. Some of the classes are hard, but in a much different way than a calculus class. So take a programming class, and if things click for you pretty easily, you should stick with it. And thats not to say that comp sci == programming. but if you can talk the language of computers pretty easily, then you can probably intuitively follow the  comp sci behind the scenes",4bagd3,t1_d1ajpz0,Wowsoysauce,,Reply,1,0,1
d1ak2nm,2016-03-23 11:30:48-04:00,Wowsoysauce,,how many years did it take for you to graduate?,4bagd3,t1_d1ak0ol,zefcfd,,Reply,1,0,1
d17roml,2016-03-21 08:47:58-04:00,veltrop,,"2 tries on linear algebra.  (Though I ""dropped"" it after bombing the first exam so no penalty)
2 tries on statistics/probability.  Actually got a failing grade on that one.

Major was computer science.  It hasn't hurt my career in anyway.  Linear algebra is very important in my field (robotics).  But 99% of the people *use* libraries for that, and just a few actually *make* said libraries.
",4bagd3,t3_4bagd3,Wowsoysauce,,Comment,5,0,5
d17wdel,2016-03-21 11:12:23-04:00,None,,[deleted],4bagd3,t1_d17roml,veltrop,,Reply,5,0,5
d180sof,2016-03-21 12:57:41-04:00,veltrop,,"In my case they expected us to memorize every possible permutation and corner-case of the method to solving systems, *and* do all of the calculations at the exam precisely without error, and not hand out partial credit with a strategy that is effective for recognizing your understanding.

So... your typical worst case scenario for a math class.",4bagd3,t1_d17wdel,None,,Reply,3,0,3
d1830no,2016-03-21 13:49:08-04:00,Wowsoysauce,,how did you end up passing?,4bagd3,t1_d17wdel,None,,Reply,1,0,1
d185dv2,2016-03-21 14:41:37-04:00,None,,[deleted],4bagd3,t1_d1830no,Wowsoysauce,,Reply,2,0,2
d185jdb,2016-03-21 14:44:56-04:00,Wowsoysauce,,so the class wasnt required ,4bagd3,t1_d185dv2,None,,Reply,1,0,1
d185lt1,2016-03-21 14:46:29-04:00,None,,[deleted],4bagd3,t1_d185jdb,Wowsoysauce,,Reply,2,0,2
d185qfj,2016-03-21 14:49:19-04:00,Wowsoysauce,,how many years did it take to graduate,4bagd3,t1_d185lt1,None,,Reply,1,0,1
d185tbv,2016-03-21 14:51:05-04:00,None,,[deleted],4bagd3,t1_d185qfj,Wowsoysauce,,Reply,2,0,2
d185xa8,2016-03-21 14:53:30-04:00,Wowsoysauce,,wow. what do you do now?,4bagd3,t1_d185tbv,None,,Reply,1,0,1
d1860ao,2016-03-21 14:55:24-04:00,None,,[deleted],4bagd3,t1_d185xa8,Wowsoysauce,,Reply,1,0,1
d17m7wa,2016-03-21 03:08:48-04:00,None,,[deleted],4bagd3,t3_4bagd3,Wowsoysauce,,Comment,2,0,2
d17rj6e,2016-03-21 08:41:41-04:00,Wowsoysauce,,:( the feels,4bagd3,t1_d17m7wa,None,,Reply,1,0,1
d17mj6r,2016-03-21 03:28:19-04:00,Blaposte,,"I failed Chemistry 2 lab. And then I failed Computer Architecture. I considered dropping the latter, as I did horribly in the first exam, but I did pretty well on my second, which made the professor advised me to stay in the class since I guess it gave him faith that I would do decently on the third one but I didn't so I failed lolol",4bagd3,t3_4bagd3,Wowsoysauce,,Comment,2,0,2
d17qmew,2016-03-21 08:00:09-04:00,BonzoESC,,"I flunked quite a few gen-Ed classes, enough that I dropped out, worked at a few jobs, and realized I wanted to do CS more than EE. Got an associate's degree at a community college, went back to university, did fine. Nobody's cared since I got admitted to the CS department, including the on-campus jobs I had and the handful of jobs I've had since graduating. ",4bagd3,t3_4bagd3,Wowsoysauce,,Comment,2,0,2
d17v7zy,2016-03-21 10:42:17-04:00,None,,[deleted],4bagd3,t3_4bagd3,Wowsoysauce,,Comment,2,0,2
d17vf9v,2016-03-21 10:47:40-04:00,Wowsoysauce,,I'm asking because I failed 2 of 4 of my physics exams. I'm scared.,4bagd3,t1_d17v7zy,None,,Reply,2,0,2
d17wgio,2016-03-21 11:14:34-04:00,rhiaaryx,,"I had an extremely bad semester, a bunch of personal stuff that got in the way, way too much partying, etc. I failed three of four classes. The fourth I got a D in. Some of those were classes in the major I wanted to be in, some weren't. I was already on probation (essentially because I was trying to switch majors, so I wasn't taking classes in ""my major""), so I got a year-long suspension.

I worked for Target stocking shelves for about two weeks before I got a job at Radioshack. Best Buy GeekSquad wouldn't hire me--as a CS major I was ""overqualified"". I had to move since I couldn't afford my apartment to a slumlord apartment. I had < $50 to spend on groceries some months, even with my dad still paying for my car insurance.

After a year I convinced them to let me back in, finished the major swap, and graduated. It set me back over a year and I had to explain the GPA to my recruiter when I was getting hired, but it didn't stop me from getting an awesome job as a software engineer. And it taught me a hell of a lot about discipline and personal responsibility.

It worked out for me, but seriously, **just don't**. Check out /r/getdisciplined if you need help getting a study schedule.",4bagd3,t3_4bagd3,Wowsoysauce,,Comment,2,0,2
d17y95s,2016-03-21 11:58:19-04:00,xiongchiamiov,,"I failed about half the courses I ever took in my college career, including one quarter where I failed every class I was in. And yes, many of these were computer science courses.

GPA is used as a filtering tool when you don't have any job experience, but after the first job no one cares.",4bagd3,t3_4bagd3,Wowsoysauce,,Comment,2,0,2
d17z1tr,2016-03-21 12:17:13-04:00,Wowsoysauce,,We're they hard courses or was it something else,4bagd3,t1_d17y95s,xiongchiamiov,,Reply,1,0,1
d17zgw3,2016-03-21 12:26:58-04:00,xiongchiamiov,,"The courses were difficult, but I generally failed them because of my own personal issues.  Did you know that [almost half of American college students report signs of depression](http://www.healthline.com/health/depression/college-students#3)?  For most people, it's your first foray into really being in charge of your own life, and while that brings a lot of freedom, it also brings a lot of responsibilities.  Please, pay attention [to the signs](https://www.nimh.nih.gov/health/publications/depression-and-college-students-new/index.shtml) in yourself and your friends, and get professional help (your uni will offer this for free!) instead of struggling through it on your own.",4bagd3,t1_d17z1tr,Wowsoysauce,,Reply,2,0,2
d17qjv7,2016-03-21 07:56:38-04:00,SneakingNinjaCat,,Never failed a class...,4bagd3,t3_4bagd3,Wowsoysauce,,Comment,1,0,1
d17r3g2,2016-03-21 08:22:37-04:00,GunslingerJones,,"I failed a couple math courses, but dropped them beforehand. I retook them the next year and passed thankfully.",4bagd3,t3_4bagd3,Wowsoysauce,,Comment,1,0,1
d17sdne,2016-03-21 09:14:58-04:00,Azims,,CALCULUS 2,4bagd3,t3_4bagd3,Wowsoysauce,,Comment,1,0,1
d17snsa,2016-03-21 09:24:54-04:00,uksheep,,"Operating Systems, Maths (Geometry)

Should have learned Unix before the first one. Should have actually attempted to figure out maths",4bagd3,t3_4bagd3,Wowsoysauce,,Comment,1,0,1
d18q972,2016-03-21 23:16:22-04:00,brunokim,,"Whew, plenty. In Brazil, you pick a major and must go through with it until the end. I started in Mechatronics, bombed almost my whole second semester, went with it for 2 years before giving up and changing to Computer Engineering. Even then, I failed plenty of classes because I simply didn't attend the minimum required, never got the discipline to watch 70% of classes. Some professors are cool with it if you pass the exams, others not.

Still, every time I got a sliver of chance to redeem myself, I worked my ass off to use it. In one semester I got 4.9 with a professor (passing grade: 5.0), and he said I didn't dedicate myself enough. The next semester, with the same professor, I gave my best, did all extra projects, aced the exams. He went back to the previous semester's grade and changed it to... 4.95. The system then rounded it to 5.0 and I passed... but due to a software glitch, not by his hands :)

Enjoy undergrad school, where professors are kings and the rules don't matter!",4bagd3,t3_4bagd3,Wowsoysauce,,Comment,1,0,1
4ba991,2016-03-20 21:14:29-04:00,BenRayfield,Whats the connection between rule110 and 2-3-turingMachine,"rule110:
https://en.wikipedia.org/wiki/Rule_110#/media/File:CA_rule110s.png

2-3-turingMachine:
http://blog.wolfram.com/images/swolfram/turing_rule.gif

Both discovered by Stephen Wolfram. Both the simplest known of their kind of turingComplete process. Both made of triangles.

Possible differences in angles and combinations of the triangles. Whats happening? Why do they look so similar?

As a fact of math, both being turing complete, they are equal in computing ability, but what is the translation between them?",,,,,Submission,1,0,1
d17deke,2016-03-20 21:38:24-04:00,dandrino,,"They look similar because they are examples of 1D cellular automata. Triangles appear frequently in cellular automata images (including many cases that are not turing complete), so the presence of triangles should not be interpreted as having some extra special meaning here. 

[Here are some examples](https://en.wikipedia.org/wiki/Elementary_cellular_automaton#Random_initial_state). Only rule 110 is known to be Turing complete, but many others also look visually similar.",4ba991,t3_4ba991,BenRayfield,,Comment,1,0,1
d17duxr,2016-03-20 21:51:15-04:00,BenRayfield,,"> Only rule 110 is known to be Turing complete

You disagree 2-3-turingMachine is turing complete? Which part of https://en.wikipedia.org/wiki/Wolfram%27s_2-state_3-symbol_Turing_machine#Proof_of_universality do you disagree with?",4ba991,t1_d17deke,dandrino,,Reply,0,0,0
d17gapz,2016-03-20 23:01:50-04:00,dandrino,,The 2-3 turing machine is not an example of an elementary cellular automaton. I was incorrect in my original statement. My point was that the presence of triangles is not special.,4ba991,t1_d17duxr,BenRayfield,,Reply,1,0,1
d17k9ci,2016-03-21 01:24:40-04:00,BenRayfield,,The measure called special is not special. Be specific. What is its https://en.wikipedia.org/wiki/Kolmogorov_complexity,4ba991,t1_d17gapz,dandrino,,Reply,-1,0,-1
4ba5za,2016-03-20 20:49:52-04:00,thecodingturtle,Microsoft Surface for College,Do any of you have experience using either a Surface Pro 4 or Surface Book for a college laptop? Is it a good idea or should I be looking for something that is a little more powerful?,,,,,Submission,0,0,0
d17c6a0,2016-03-20 21:03:14-04:00,None,,[deleted],4ba5za,t3_4ba5za,thecodingturtle,,Comment,5,0,5
d17eic2,2016-03-20 22:09:55-04:00,YourFavoriteBandSux,,Did you run into HDD space limitations?,4ba5za,t1_d17c6a0,None,,Reply,1,0,1
d17c576,2016-03-20 21:02:24-04:00,polarbearurine,,Having only a weak laptop is slightly problematic in some of the higher level classes that require using powerful VMs. Other than that you'll be fine. ,4ba5za,t3_4ba5za,thecodingturtle,,Comment,2,0,2
d17cc1v,2016-03-20 21:07:51-04:00,None,,[deleted],4ba5za,t1_d17c576,polarbearurine,,Reply,1,0,1
d17hm1d,2016-03-20 23:43:18-04:00,polarbearurine,,"I have to use a VM for a class I'm taking in the hadoop ecosystem. A lot of the hadoop tools work better with 8+ gbs of ram, but I can only allocate 6 gbs. My teacher was nice enough to build a custom VM for us that ran lighter, but, if he hadn't, my laptop wouldn't have been adequate. If you get a top of the line surface book you should be fine though.",4ba5za,t1_d17cc1v,None,,Reply,2,0,2
d18qrm2,2016-03-21 23:32:33-04:00,p2pp2,,"I have a Surface Pro 3.  It is really good for note taking.  But 3rd party softwares are not that good when using Surface pens for notetaking.  'Windows Journal', which comes with Windows 10, does not have enough features.  So as 'OneNote mobile'.  And the free 'OneNote desktop' needs to connect to the Internet to have multiple Notebooks.   So, I bought 'OneNote 2013 desktop'.  But OneNote is not good to do things with pdf.  So, I bought 'Drawboad PDF'.  

Normally I write program on another older computer which has Linux.

I doubt I should have bought a cheaper one, Surface 3 (not pro).  It is less powerful, but it is lighter, longer battery hours, ...

Surface Book is too expensive.  If I loss it, or drop it, or forget it in a car ..., I will :(",4ba5za,t3_4ba5za,thecodingturtle,,Comment,2,0,2
4b9vhq,2016-03-20 19:34:42-04:00,BenRayfield,"Since no single-stack automata is turing-complete, but two-stack automata can be, what does lambda look like as 2 stacks?","https://en.wikipedia.org/wiki/Lambda_calculus

I think of lambda(x,y,z){ some code, including more lambdas, using x y andOr z } as having 0 parameters and all code inside is binary trees where some leafs are an integer pointing lower in the stack. The parameter of a lambda is at the top of the stack. When copying a lambda to just above top of stack, all its code is changed to point that much higher in stack.

Lambdas that take n params point n higher on stack, so code of a lambda with ""free variables"" may point above its current stack position, but this can also be viewed as there is no such thing as ""free variables"" and you just cant push a lambda until binding its vars, so all vars are negative integer pointers at lower in stack.

But a stack made of lambdas (as every lambda is a function of lambda to lambda) needs another stack of the code in each lambda, as you recurse like lambda(x){ lambda(y){ lambda(z){ ((y x) (z x)) } } }. The ((y x) (z x)) first recurses into (y x), then y, then after popping some, (z x) then z, then the return of one called on the return of the other. This must be the part that happens on the second stack.

Or can it be done on one stack since pointers are allowed?

Can this be done without rewriting the code of each lambda as its used again? Can each lambda have a ""stack pointer"" context only?

How does it work with ( lambda(x){ (x x) } lambda(y){ (y y) } ), which returns itself? What normalized form of lambda would recognize that lambda returns itself and, if all earlier call-and-return pairs were cached, recognize its in that specific loop?

How do functions like https://en.wikipedia.org/wiki/Church_encoding of plus and multiply work on 1 or 2 stacks? How many stacks are needed to use no variables, only pointers into stack?",,,,,Submission,3,0,3
4b9r4a,2016-03-20 19:02:42-04:00,lewisj489,Which collections are useful when?,"I'm trying to keep this about CS and not a language, so please excuse language specific stuff.

It seems like List<T> or Dictionary<T, T> have pretty much 99% of my use cases. I know about loads of collections, singly linked list, double, hash tables, tress. Whatever. Is there a useful guide on when to use which in terms of performance and such? Or am I good sticking with list and Dictionary",,,,,Submission,8,0,8
d17c5lm,2016-03-20 21:02:42-04:00,dandrino,,"Lots of data structures have different trade-offs depending on how you use them and knowing there tradeoffs can make a huge difference in performance.

**Lists**

* Array-based lists. These lists use an array as the underlying data structures. This means that they allow fast (i.e. O(1)) random-access, but suffer with random insertion and deletion (i.e. O(n)). Most implementations will allow on average O(1) appending to the back of the list. This is because the array is allocated with more space than is actually used (often referred to as capacity). When the capacity is exceeded, a new array is created internally and all values are copied over. In general, array lists are useful if you are frequently reading and infrequently updating. Java uses ArrayList for this implementation, whereas C++ uses std::vector.

* Linked lists. These use a series of nodes and pointers as their underlying representation. Random access is bad (i.e. O(n)), but insertion and deletion is O(1). Because each node has a value and a pointer, memory overhead is a bit higher, and because these nodes are typically allocated on the heap as opposed to a single block of memory for array lists, performance can suffer due to cache considerations. Linked lists are good if you are updating frequently, but read infrequently (and reads are done via iteration). Java uses LinkedList and C++ uses std::list.

**Associative Arrays (a.k.a maps or dictionaries)**

* Hash maps. Hash maps use a hash table as an underlying data structure, so lookup and updates are amortized O(1). This is generally the data structure of choice for dictionaries. One downside is that a decent hash function must be implemented for your key types otherwise performance could degrade quite significantly. Usually works well with keys that are primitive types (e.g. strings, integers, etc). Java uses HashMap and C++ uses std::unordered_map. If you need a defined iteration order, then Java also offers LinkedHashMap (no corresponding structure in STL C++, but maybe in boost).

* Sorted maps. Sorted maps uses a binary search tree as the underlying data structure (usually a red-black tree). Lookup and modifications are performed in O(log n) time, but sorted maps allow you to additionally query for the largest and smallest values. As a result, sorted maps are useful when you want to process one value sorted by another value (e.g. a time series data structure). Keys in a sorted map just need to implement some ordering operator, which makes them a bit easier to adapt to custom types than hash maps. Java uses TreeMap and C++ uses std::map.

**Other**

* Sets. Sets are useful for querying membership. Like maps/dictionaries, they come in hash and sorted flavors and have the same tradeoffs as mentioned above. If you find yourself writing a dictionary where the value is a boolean, you might want to reconsider if a set is a better option. For hash sets, Java has HashSet and C++ has std::unordered_set. For sorted sets, Java has TreeSet and C++ has std::set.

* Stacks. Stacks are simplified lists that only allow appending and removal from the end. Both operations are O(1). Not used too often (linked lists usually supply all the necessary functionality), but you may see them in recursive algorithms (e.g. depth-first search). Java uses Stack and C++ uses std::stack.

* Priority Queue. Priority queues are usually implemented as heaps, and allow you to add values and query/remove the smallest value (or largest depending on how you implemented your ordering operator). These are useful in all sorts of algorithms (e.g. best first search). They do not allow for membership querying, so if you find yourself needing that operation you may want to consider a sorted set instead. Java uses PriorityQueue and C++ uses std::priority_queue

These are the basic data structures offered out the box by most (mature) programming languages. More obscure or sophisticated data structures (e.g. disjoint set (i.e. union-find), trie, kd-tree, graphs, etc) usually require some custom coding or third-party support. Also keep in mind that Java out of the box comes with thread-safe versions of a lot of the aforementioned data structures (C++ does not have separate  concurrent data structures, instead relying on you to use mutexes and locks when accessing and modifying them).


",4b9r4a,t3_4b9r4a,lewisj489,,Comment,15,0,15
d17zboo,2016-03-21 12:23:38-04:00,Xxyr,,"That's not quite how ArrayList works. It doesn't allocate a new memory block and copy everything when you run out of space. It allocates a new chunk of memory and any accesses is first be routed to the right chunk then the chunk is indexed. 

So adding doesn't devolve into O (n) but access is a high ish overhead O (1).",4b9r4a,t1_d17c5lm,dandrino,,Reply,2,0,2
d17zzfm,2016-03-21 12:39:05-04:00,dandrino,,"Fair enough. I do believe this is what happens with std::vector, however (and to be specific, the values are moved, not copied)",4b9r4a,t1_d17zboo,Xxyr,,Reply,1,0,1
d17o9n9,2016-03-21 05:30:15-04:00,luisbg,,Great summary!,4b9r4a,t1_d17c5lm,dandrino,,Reply,1,0,1
d17v4iw,2016-03-21 10:39:43-04:00,None,,"Great summary, thanks!  Anyone interviewing for a programming job should probably study the hell out of this.",4b9r4a,t1_d17c5lm,dandrino,,Reply,1,0,1
d17k8nz,2016-03-21 01:23:49-04:00,panderingPenguin,,"List<T> and Dictionary<T> just describe interfaces for features the collection has. Those other things you mention, singly linked lists, hash tables, etc, are how you actually implement these collection interfaces behind the scenes.",4b9r4a,t3_4b9r4a,lewisj489,,Comment,2,0,2
d18r7rn,2016-03-21 23:47:23-04:00,brunokim,,"You're correct that 99% of the time, arrays, lists and a hash table suffice for most needs (which, incidentally, are the only generic data structures provided in Go). Still, data structures are not an academic endeavor, each one comes with different tradeoffs that must be matched to your use case.

Take, for example, a set, as in a collection of unique elements. You can reuse a hash table and simply map all elements in the set to a constant. Look, O(1) access and insertion, O(n) iteration, how can it get any better? Well, perhaps your use case requires that you iterate the elements in sorted order, and now you have to do an O(n log n) operation for every traversal. In this case, you could be better off by using a tree set, with O(log n) insertion and O(n) sorted iteration.

Even more, if the space of possible elements is finite and known in advance, you can use an integer! Just associate each bit with an element, and flip it to indicate absence/presence. That's how EnumSet is implemented in Java.

As for a guide, I'd indicate ""The Art of Computer Programming"", by Knuth. There are plenty of other references out there, but I find it enjoyable and gives you a broad vista. You should at least know what's available for when the need comes you know what to look for. ",4b9r4a,t3_4b9r4a,lewisj489,,Comment,1,0,1
4b6ohr,2016-03-20 02:37:39-04:00,dyl_up,Parallel Programming Examples/Ideas?,"I was given a Parallella Board by a professor to tinker around with, and I'm trying to find out some somewhat interesting parallel programming things to do with it. So far I've done some stuff with openmp and Mandlebrot Sets, and I've been looking at some stuff about Sleep Sort. Anyone have any good ideas and/or resources to point me to, preferably in C/C++?",,,,,Submission,8,0,8
d16skz1,2016-03-20 11:53:02-04:00,videoj,,"[Machine learning](http://hunch.net/?p=249) is a field that is filled with problems that benefit from parallelism.    

[Graphics Rendering](https://en.wikipedia.org/wiki/Parallel_rendering) is another place to look for ideas. ",4b6ohr,t3_4b6ohr,dyl_up,,Comment,3,0,3
d16nj2a,2016-03-20 08:35:43-04:00,rocketbunny77,,"No answer for you, but you might get success posting in /r/learnprogramming",4b6ohr,t3_4b6ohr,dyl_up,,Comment,2,0,2
d171rw5,2016-03-20 16:16:16-04:00,okiyama,,Cellular automata and sorting networks parallelize quite well,4b6ohr,t3_4b6ohr,dyl_up,,Comment,1,0,1
d176l4h,2016-03-20 18:28:31-04:00,Worzel666,,You may also want to look at MPI.,4b6ohr,t3_4b6ohr,dyl_up,,Comment,1,0,1
d17j6ku,2016-03-21 00:39:10-04:00,ToplessTopmodel,,"Do a gravitational force simulation. O(n^2 )

Or boids...",4b6ohr,t3_4b6ohr,dyl_up,,Comment,1,0,1
d16pitf,2016-03-20 10:08:58-04:00,None,,Sounds like you know enough about this topic to not have to ask Reddit for sources,4b6ohr,t3_4b6ohr,dyl_up,,Comment,-3,0,-3
4b5rhs,2016-03-19 21:30:05-04:00,fewdea,Question about processing SQL join results,"Is it faster to use (several) joins in a single query and then process the results programmatically, or to break the join into several select statements?

A silly example:

    SELECT * 
    FROM projects p 
    JOIN users u 
      ON (p.user_id = u.id) 
    JOIN comments c 
      ON (c.user_id = u.id) 
    WHERE u.id = 5 
      AND p.id = 2;

Using this select statement, I receive a 2 dimensional array. Columns like u.name and p.name would contain duplicate results for every row. I would have to (somehow) separate this data into a more useful structure before displaying/using it. If I were to iterate the result set, I would have to set the $user_name and $project_name variables to the same value in every iteration. This is a simple example, but imagine something more complex with 4 or 5 joins.

This seems like a waste of processing time. Would it be faster and easier to process if I did a select for each join instead? Is there a name/concept I can research that can tell me more about this problem? What is the purpose of a JOIN if I spend a bunch CPU cycles processing the result set before I can use it?",,,,,Submission,6,0,6
d16f61p,2016-03-20 00:23:22-04:00,theobromus,,"You don't have to do select * - you can select precisely the columns you want. In this case you probably don't really need to join the users table because you are only checking that the user id is present in it. If you have a foreign key constraint on the comments and projects tables, then you'll know it's a valid userid if it's in those tables.

To answer your main question - doing a join is almost always **way** faster than running a loop to produce the result fields. This is because the SQL plan system knows how to do joins really well. On the other hand interpreting and running individual SQL statements is really slow compared to almost any other language. You want to do only a few SQL statements, but have them do all the work for you.

Even besides this interpreted language penalty, joins are often a lot faster because SQL knows how to structure the join to make most efficient use of the indices you have and the disk layout of data. At least for MS SQL server, it can choose to do hash joins, loop joins (left or right), and merge joins (and maybe other kinds). These might enforce the join constraint and where clause constraints before even reading a bunch of data from disk (usually by doing index seeks).
A hash join builds a hash table from one side of the join and compares elements from the other side. This is usually most efficient when you can't do a merge join and both sides have a roughly similar number of possible matches (or neither side has an appropriate index).

A loop join loops over all possibilities from one side of the join and then searches for the match on the other side (usually by seeking into the index). This basically requires a disk seek for each row in the table driving the join (of course it may only look at the ones that match the where clause). That's assuming the data is too large to fit in memory - when everything fits in memory, any query will be **much** faster because memory access is much faster than reading from disk.
A loop join where the side being searched on each iteration of the loop has an index is often what you're going for if the number of possible matches is reasonably small. Also, this can be best if the final sort order is different from the index order and especially if you are doing a TOP N query. In that case the query planner may be smart enough (and often is in my experience) to find the top N results in the left table of the join and then seek to find the match in the right table. This means you're only doing N seeks into the right table. This is often faster than a merge join (more on that below) which requires reading a bigger portion of both tables and then re-sorting the results.

A merge join is often the fastest possibility. It can happen when each side has an index sorted on the column (or columns) used in the join predicate. In this case those two indices are scanned at the same time (well I assume it can seek to the relevant point in each). Because they are in the same order, only one pass through each list can find all the matches. It also means you might have to sort the result set to get it into a different order.

All told, there's a ton of depth you can go into about this. I'd say the first thing to realize is that your intuition of thinking about CPU cycles is very often wrong in large databases. Usually CPU is quite abundant (not always, but it's not throwing away some column data that wastes it). Usually your queries will be constrained by the amount of disk and memory access they have to make. Every time you have to read from a disk, the processor is waiting for millions of instructions to get the data it needs to continue (realistically it switches to do something else while waiting, but that doesn't help your query). So start thinking about it based on that.",4b5rhs,t3_4b5rhs,fewdea,,Comment,3,0,3
d16hfxq,2016-03-20 01:52:33-04:00,fewdea,,"Thank you for your very informative response. I don't think I made my question very clear enough though.

I'm considering two situations. In the first scenario, using an arbitrary programming language (let's use python), I select a project row and store that data in variables such as $project_name, $user_id, $start_date, $etc. Using the $user_id value, I do another select on the user table to retrieve $user_name, $member_since, and so on. Last, I do a third select on the comments table to get the user's comments for the project. 

In option two, I use the example query in my post and return as many rows as there are comments. In my script, I would do something like this:

    for row in results.iteritems:
        user = row['user_name']
        project = row['project_name']
        comments.append( [ row['comment_date'], row['comment_text'], row['etc'] ] )

And then I would pass these variables on to my view to display this information.

In option 1, I fetch my data and do zero post-processing, no looping, nothing. I just pass the query results directly to my view. *But* I did have to make 3 database queries.

Now let's scale these two solutions up. Python, in this example, has pretty fast list structures, but looping, say, a million rows is going to be fairly time consuming. Especially if I'm overwriting the $user and $project values with the same thing each time.

In this particular example, I think either would be fine, but if I were to scale the application to a large size, am I looking at a different set of (non-negligible) pros and cons? Is the speed of a JOIN query going to outweigh having to loop the entire result set? Perhaps the first two queries don't return single results, but instead account for 10% each of the results, while the last part is 80% of the data. My looping structure becomes more convoluted, as I must somehow detect what the important part of the row is. Did the user change from the last row? The project? Are there comments for this post?

Intuitively, it feels like 3 succinct selects with no post-processing is the way to go, but I don't really know, so I'm asking here. At the end of the day, I should probably just run some tests to find the optimal solution. I was just wondering if there was a standard way of doing this, as it seems like practically anyone running a join in against a relational data set would have run into this problem.",4b5rhs,t1_d16f61p,theobromus,,Reply,2,0,2
d17d4ji,2016-03-20 21:30:15-04:00,theobromus,,"Ah I think I understand what you're asking now. Yes, it makes sense to return multiple result sets in this case. You could probably just have two assuming there's only 1 user and 1 project row. You wouldn't even need a join in this case I think.

In general, you can have a lot of more complicated queries where a join is really what you want. For example, you might do a join of the comments table to the users table and GROUP BY userid to get the count of comments by user. Or you could join the projects onto the comments table to only return comments for a specific type of project.",4b5rhs,t1_d16hfxq,fewdea,,Reply,1,0,1
4b4fw1,2016-03-19 15:36:03-04:00,Aiiight,Counting Integer Problem Help!,"Hi!

So I'm a freshman CS major and currently on spring break, and I'd figure I could do a few simple programs in my textbook to practice and try to improve as time goes on.

This program I'm having trouble with involves the user typing in integers, and the input ends when the user enters 0. After that, the program should calculate the amount of negative numbers, positive numbers, and the total amount of numbers.


I feel like I'm almost there, but could anybody explain why when I compile, no matter what numbers I enter, the output for positive/negative numbers is so close, but not quite the right answer?

Example output:
https://gyazo.com/c50796659f34162a65a8d5d6b5a47260


Thank you so much!


And here is my source code:
http://pastebin.com/Z4v0nU2H",,,,,Submission,5,0,5
d15zxez,2016-03-19 16:25:59-04:00,None,,[deleted],4b4fw1,t3_4b4fw1,Aiiight,,Comment,2,0,2
d160p1a,2016-03-19 16:49:36-04:00,Aiiight,,Thank you! Much appreciation for the advice: I'm definitely going to start practicing and writing out the problem in paper and then translate it into code.,4b4fw1,t1_d15zxez,None,,Reply,1,0,1
d15zfrv,2016-03-19 16:11:14-04:00,tannich,,"you should increment total (by 1) instead of adding the variable ""numbers"" to it on each iteration of the loop

Additionally, think about whether or not you want to have the ""cin << numbers"" line that is outside of the loop at all. Won't you be doing the exact same thing again just as you enter the loop (from the condition of the while)?",4b4fw1,t3_4b4fw1,Aiiight,,Comment,1,0,1
d160nll,2016-03-19 16:48:22-04:00,Aiiight,,"
Yeah, just realized that: thank you!!",4b4fw1,t1_d15zfrv,tannich,,Reply,1,0,1
4b3be2,2016-03-19 10:41:44-04:00,BigSwerty,What are the most interesting examples of using genetic algorithms?,"I remember seeing a Super Mario level solved by a genetic algorithm, are there any more interesting stuff like that? Thanks!",,,,,Submission,16,0,16
d15ufsc,2016-03-19 13:42:29-04:00,videoj,,Large list here: https://en.wikipedia.org/wiki/List_of_genetic_algorithm_applications,4b3be2,t3_4b3be2,BigSwerty,,Comment,3,0,3
d15y68w,2016-03-19 15:32:47-04:00,Merad,,"Don't have a link handy, but one of the coolest things I've seen was a guy who made AIs that learned to walk using GAs.",4b3be2,t3_4b3be2,BigSwerty,,Comment,3,0,3
4b2vzf,2016-03-19 08:14:54-04:00,moeseth,What are the best ways to manually collect stock quote?,"Hi,

I'm collecting live stock quotes data for a few stocks. 

My plan to do so is to write a small android app and then send a few people to stay at stock exchange to live input the quote as it changes.

Is it how people do it? Is there any other ways without collaborating with officials who update the data?

Thanks.",,,,,Submission,3,0,3
d15lv7l,2016-03-19 08:30:47-04:00,dxk3355,,I don't think they actually place bids and asks like that anymore.  It's all computerized through APIs.,4b2vzf,t3_4b2vzf,moeseth,,Comment,1,0,1
d15vop8,2016-03-19 14:19:19-04:00,moeseth,,I see. How does the API provider get their quote though? ,4b2vzf,t1_d15lv7l,dxk3355,,Reply,1,0,1
d15pfw8,2016-03-19 11:08:00-04:00,yes_thats_right,,"What is your end goal here?  I work in technology at a bank and can tell you that all market data is obtained by feeds from companies like reuters, Bloomberg etc. I have a good imagination generally but can't begin to think of a reason for having people at an exchange manually entering stock quotes.",4b2vzf,t3_4b2vzf,moeseth,,Comment,1,0,1
d15vnt0,2016-03-19 14:18:34-04:00,moeseth,,"My end goal here is to be a data feeder to a bank for a specific stock exchange. 

How does bloomberg get the quote? Do they have deals with stock exchange or do they have people putting the data at stock exchange?

Thanks.",4b2vzf,t1_d15pfw8,yes_thats_right,,Reply,1,0,1
d15w0i8,2016-03-19 14:28:56-04:00,yes_thats_right,,"It is all electronic. The days of traders going to a stock exchange to execute deals is many years in the past. Nowdays market data is exchanged, orders placed and matched all in the blink of an eye. One of your people will not be able to even reach for their app before every bank, hedgefund and other traders have already got the price from a feed. On top of this, the data they are receiving is significantly richer, not just prices but vol curves, yield curves etc.

I think your idea is a clever idea, but if I understand it correctly, it is solving a problem for which there is already an established and better solution.

You can read more [here](https://en.wikipedia.org/wiki/Financial_data_vendor). As it mentions, this is a multi billion dollar industry, and unless you have a really ingenious idea which significantly changes the game, you would be looking at hundreds of millions of investment required to be a player.

If you are interested in this space, perhaps it might make more sense to focus your idea on what you could do with the data rather than how you collect it.",4b2vzf,t1_d15vnt0,moeseth,,Reply,3,0,3
4azxxp,2016-03-18 15:55:00-04:00,dfacts1,Need quick clarification on worst-case running time,"Hello, I am trying to teach myself some CS theory and I'm having some confusion with this concept:

Say there is an operation that takes O(log n) time,

If a program accepts an array of *n* elements, and the program runs the operation **twice** for each element of an array, is the running time of the program

- O(log n)

- O(n log n)

- O(2n log n)?

My guess is that it is O(n log n), but I'm not entirely sure. Any help is appreciated!",,,,,Submission,1,0,1
d14zief,2016-03-18 17:03:36-04:00,VideotapeReturn,,"O(n log n) = O(2n log n) (refer to an algorithms text, big-oh drops multiplicative constants). 
 
If the algorithm does the log n operation in each of the n elements then it's O(n log n). I'd it runs the log n operation on the entire list then it's O(log n) total. 

The number of times it runs the operation doesn't matter unless it's a non constant function of n (it can do it twice, it can do it a hundred times). ",4azxxp,t3_4azxxp,dfacts1,,Comment,3,0,3
d15521m,2016-03-18 19:37:09-04:00,_--__,,"People seem to missing this point, *both* O(n log n) and O(2n log n) are correct answers. For completeness, so is O(n^(2)), O(n^(3)), O(2^(n)) even though none of these are equal to O(n log n).",4azxxp,t1_d14zief,VideotapeReturn,,Reply,2,0,2
d14zf70,2016-03-18 17:01:21-04:00,sathoro,,Your guess is correct. It doesn't matter how many times you run the operation as long as that number is constant (not a function of n) because the 2 just gets dropped. You could run it 1 trillion times for each n and the answer would be the same,4azxxp,t3_4azxxp,dfacts1,,Comment,2,0,2
d14zg9u,2016-03-18 17:02:06-04:00,avaxzat,,"If the operation takes O(log n) time on n items, and you run it twice for each element in an array of size n, then the running time is O(2n log n), which is the same as O(n log n). The constant factor of 2 has no influence on Big-O notation.",4azxxp,t3_4azxxp,dfacts1,,Comment,2,0,2
d14zgor,2016-03-18 17:02:23-04:00,Xxyr,,"So, you won't see a constant co-efficient in [BigO notation](https://www.khanacademy.org/computing/computer-science/algorithms/asymptotic-notation/a/big-o-notation) because it is asymtotic. The only times you generally see constants if when n is exponential time ie O(n^2) or geometric time ie O(2^n). 

Lets start with a simple example, where you loop through an array and do something with every entry. This has a runtime of O(n). It doesn't matter if you do one statement, two statements or twenty statements its still O(n). For small values of n a large constant factory has a noticeable impact, but as n approaches infinity the constant (1/2/20) has very little impact on the overall runtime. 

What about O(log n) ? Well lets say you had a sorted array and were doing a binary search in that array to find a value. Each iteration of the algorithm discards half the dataset, so the number of operations is log_base2 (n) where n is the dataset size. 

What about O(n * n) ? Well that would be an example where we have an array and for each element in that array we have to compare it to every other element in the array. Since for each of the n times we go through the array we have to go through through the whole array again, with a cost of n. (This is an example of the naive sorting approach of finding the smallest and putting it in the first slot, then the next smallest, and so on).

What about O (n log n) ? That is the standard cost for an efficient sort ie quicksort/mergesort. Where you have to do processing for every element (to get it in the right spot) but the number of operations for each element is approximately log n. 

The main takeaway here is that the only thing that impacts the BigO of an algorithm is the way it grows with respect to the input size. So since it says ""it runs an O(log n) operation twice for each element of the array"" you can break this down into 

    --> Run an O(log n) operation ~~twice~~ for **each element of the array** 
    --> Run an O(log n) operation O (n) times 
    --> O (n log n).

Sidenote: I highly recommend this book if you want a really good understanding of basic algorithms / theory. http://www.amazon.com/Introduction-Algorithms-3rd-MIT-Press/dp/0262033844/ref=sr_1_1?ie=UTF8&qid=1458334047&sr=8-1&keywords=algorithms",4azxxp,t3_4azxxp,dfacts1,,Comment,2,0,2
d154x06,2016-03-18 19:33:04-04:00,_--__,,"> So, you won't see a constant co-efficient in BigO notation because it is asymtotic

This is not correct.  It is perfectly acceptable to have co-efficients in Big-O notation, you can take the big-O of *any* function.  You generally do not see it because it is ""cleaner"" to have a leading co-efficient of 1 - just as it is more elegant to drop unnecessary terms like O(n^(2)) instead of O(n^(2)+3n) [or O(n log n) instead of O(2 n log n)].",4azxxp,t1_d14zgor,Xxyr,,Reply,1,0,1
d14zziz,2016-03-18 17:15:45-04:00,dfacts1,,"thank you all for the help, really appreciate it",4azxxp,t3_4azxxp,dfacts1,,Comment,1,0,1
4ax7wz,2016-03-18 02:50:40-04:00,pas43,Programming for 10 years and I'm terrible at it,"So I been coding get for 10years or so and I decided too go uni at the age of 26 because I'm kinda good at it and I like doing it. I mean we get paid to solve problems which problems are always changing which makes the job exciting. So I'm in my second year of uni and I have trouble digesting and understanding the software specifications which leads especially me with a mess of  program which not sure they asked for.  There are kids in my course who have never programmed before but with In a few months they just seem to be able to think better about the program design than me and can actually read the spec and digest the info,  this is frustrating  and tbh makes me want to quit,  I overcomplicate problems and I have real trouble making things simple, I am quite heavily dyslexic  and have repeated the first year twice and now I'm in the second year which I'm about to repeat again because of an injury I had I'm coming up to nearly 30 and these kids seem to think better than me, even though I probably started coding before some of them was born.  Any one else feel like they are not cut out for coding? I feel like I'm able to to do so much better than I can, I just can't.",,,,,Submission,27,0,27
d14cznl,2016-03-18 06:34:06-04:00,chully,,"There are lots of good jobs for technically inclined people related to Software Engineering, that are not programming. Project management of any interest?  Maybe writing software specs will help you grok the specs that others have written. It is not easier or harder, but different in a way that suits different mental skills. Find out which niche *is* right for you. Practice all the time. You don't have to be better than every programmer, but as long as you can hold a conversation there are lots of possibilities.",4ax7wz,t3_4ax7wz,pas43,,Comment,12,0,12
d14ed8o,2016-03-18 07:51:27-04:00,EquationTAKEN,,"I had similar feelings when I went to uni as a CS major. I was slow in terms of learning the concepts, and my peers were immediately much better than me at programming in general. I was able to get OK-ish grades, and thankfully it landed me a job.

At the time, I felt like CS wasn't for me at all, but I had committed to it, and switching seemed like the wrong route, and I was right.

I got a job as a software developer, and I'm just now realizing that I *am* good at this shit! 

You know why I didn't shine in school? **Because they have one single curriculum, and they expect it to fit everyone!** There is no one-size-fits-all curriculum, and sadly, you're getting the business end of that dick-shaped stick.

I was the same way with math. Didn't grasp SHIT! But then I discovered online resources, and the idea of learning at my own pace. Now I love math, and I'm taking a second BSc as a part-time student, next to my full-time job, and I'm breezing through it.

My advice: You're just experiencing a curriculum that isn't paced the way you need it. Find online resources, and learn stuff at your own pace. There are SO many resources out there, and you can use any subset of them.

You can start off by finding video tutorials on the concepts you're struggling with, because those are holes that need to be filled, because these holes prevent you from building more knowledge on top of them.

Then when you're caught up, you can pre-emptively learn next week's concept, etc. That's what I did starting in my second semester, when it dawned on me that I wasn't keeping up.

It's going to take some time in front of the computer on your own time, but it should be pretty damn rewarding when the curriculum starts feeling easier.",4ax7wz,t3_4ax7wz,pas43,,Comment,16,0,16
d14aklf,2016-03-18 03:50:10-04:00,RosscoGiordano,,"Never give up. Never surrender. If you put your mind to it, you can do it.",4ax7wz,t3_4ax7wz,pas43,,Comment,25,0,25
d14s6ic,2016-03-18 14:08:19-04:00,Kid_CharlaHEYMAYNE,,"Watch this talk by DHH, the guy who invented Ruby on Rails. The first half of the talk is about how he struggled to learn to write software. 
https://www.youtube.com/watch?v=9LfmrkyP81M
Maybe Computer 'Science' isn't for you, but 'writing software' is. 

Next, maybe consider doing a bootcamp/3-6 month program. I never got a CS degree, like DHH, but I was able to get a job coding.",4ax7wz,t3_4ax7wz,pas43,,Comment,5,0,5
d14ba0n,2016-03-18 04:38:11-04:00,jackdh,,Perhaps university is just not for you. That's fine not everyone needs it especially in our field. ,4ax7wz,t3_4ax7wz,pas43,,Comment,7,0,7
d14y8fb,2016-03-18 16:32:51-04:00,umib0zu,,"You probably just haven't realized abstraction yet, and that there are abstractions across programming languages. You should check out some Type Theory and maybe some category theory. Once you realize there are abstractions above just the particular program language and get a grasp of those, translating a problem to a particular programming language is simply a matter of syntax.",4ax7wz,t3_4ax7wz,pas43,,Comment,2,0,2
d152eo9,2016-03-18 18:21:03-04:00,Bottled_Void,,"From what you've described there, your problem isn't really 'programming'. It's understanding the problem you're solving and design work.


It might help to write the features of the program in a list. I had a similarly abled co-worker who found colour-coding things helped a little. Colours and diagrams.


But maybe think of a few scenarios of how the software is used. Maybe drawing use cases would help. Once you've got a good idea of what you need to do, start creating objects to perform those functions. Top down analysis of the problem should give you an idea of these.


Then before the coding part, personally I found scribbling down some [nassi shneiderman diagrams](http://www.breezetree.com/articles/article_images/nassi-shneiderman-diagram.png) helped me. I've since switched to control flow diagrams.


If what you're doing (which a lot of people that can already code actually fall into the trap of) is just sitting down at the computer and bashing out some code, then yeah, you're probably not going to have a well designed piece of software and it will be hard to tie it all together.


But doing CS, this is what they should teaching you on your course.


Also:

> even though I probably started coding before some of them was born

Doesn't magically make you better than them.",4ax7wz,t3_4ax7wz,pas43,,Comment,2,0,2
d152v9q,2016-03-18 18:34:06-04:00,greeedy,,"I definitely know that feeling. I'm a programmer for around 7-8 years. I was a software developer trainee and after that did my bachelor in computer science. I always thought I'm kind of good at programming, but at university were people who never did programming or even other computer science things before and took only some months before they were better than me. Even in the company I now work are people who are 10 years younger than me and just straight out wayyy better than I ever will or could become. 

I always wanted to become a really good professional developer and wanted to become the best in every company. But my view changed a bit, the guys who are way better are bad in other things, were I'm definitly better at, like collaboration with others, thinking about edge cases, maintaing software (the best guys do incredible things but they do only the 20% most complex ones and I do the last 80% and connect all the lose ends). 

Developing software isn't always about writing code, there are many more things, developing is actually most of the time the least you do. There is bug hunting, maintaining, planning, presenting, writing down specifications, discuss features and so on.

Find your place it's not always hardcore complex logic programming and if the whole programming thing in any way fun for you, don't give up.",4ax7wz,t3_4ax7wz,pas43,,Comment,2,0,2
d16i2n5,2016-03-20 02:22:31-04:00,coconutscentedcat,,"respect to you sir. I'm going to begin my CS studies at 28 yrs old. What's it like being the older guy? Did you have trouble fitting in? 

Oh, and did you ever consider getting a tutor?",4ax7wz,t3_4ax7wz,pas43,,Comment,1,0,1
d16i37o,2016-03-20 02:23:17-04:00,p2pp2,,"It seems you have problem with understanding and digesting what they want you to program (or do) -- not programming itself.  Is it just when you read?   If someone explains it to you, can you understand it better?

Do you have anyone who is an expert in dyslexic to give you advices?  I think they are the one who can help you.",4ax7wz,t3_4ax7wz,pas43,,Comment,1,0,1
d16j6gi,2016-03-20 03:24:10-04:00,p2pp2,,"Perhaps, the problem is that these kids have less knowledge than you.   Most older people have knowledge more than younger people.  But the assignments in the course are tailored for students who have knowledge up to what the professor has taught in the course (including the prerequisites of the course).  Someone who has more knowledge may overthink on the assignments.  

For instance, small kids will answer 2 for this question, 1 + 1.  But someone who has basic in computer science may answer: 

* 2  (if '+' is the arithmetic plus sign)
* 1  (if '+' is the Boolean OR sign)
* 10 (if we are dealing with base-2)",4ax7wz,t3_4ax7wz,pas43,,Comment,1,0,1
4awqg1,2016-03-17 23:55:41-04:00,moeseth,What are some ways to encrypt ogg files for streaming service?,"We are making an online streaming app with offline support.

Could you please point me to some documents where it states how to do encryption?

For offline support, user will be able to listen to songs for about one week without an internet connection. After that, they will have to connect to internet to renew their keys.

Could you please let me know.

Thanks.",,,,,Submission,4,0,4
d14zeen,2016-03-18 17:00:47-04:00,UtterlyDisposable,,"Here's the thing, there is no such thing as good DRM. If you're doing online-only streaming then you're fine tunneling through HTTPS until it gets to the client, but at some point once it's there it will have to be decrypted and turned into sound.

This has commonly been referred to as ""the analog hole"" and there have been lots of suggestions on how to ""fix"" it but the reality of it is that any sufficiently motivated pirate will defeat whatever measure you put in place. Look at CSS and HDCP, the encryption used on DVDs and for HDMI connections respectively. Both have been completely broken because it's a lot of work to make a standard, and once a standard is in place you can't just go and change it so once the technology is broken, it's broken completely and forever. Since you're talking about a software platform, your situation is ever so slightly better in that sure you don't have to use static keys but you're still going to be fighting an ongoing war with the people who want to use your streams in ways that you don't intend for them to and you will always lose in the end. Not because you aren't smart, or talented, or whatever, but because you (or your employers) will have to pay a limited number of smart and talented people to work constantly on keeping your platform secure. The people hacking your platform? They don't have to pay anybody. The bigger the challenge you set up, the more people and talent you'll attract to breaking it. They will be doing it for fun, just to spite you, and in the end you will just end up having to settle for ""good enough."" 

Your ""answer"" or at least the best one, will be to use asymmetric cryptography. Encrypt the content with a short-lived transaction-specific private key and store the public keys on the device. The user can decrypt the specific media file with the public key and listen to it, but if they try to transfer the music file to something else it will be unusable... Except, despite a method like this uses sound cryptography, it still has major, glaring problems just like every other DRM system ever:

One: you'll have to store those public keys somewhere. You can obfuscate them, encrypt them with some other method, it doesn't really matter because at some point they'll have to be decrypted in order to read the media and if your streaming service becomes successful you'll become a target for hackers.

Two: once those keys expire, the user would have to redownload the media using a different keypair which will be annoying for the user. If you're running a streaming service, which is a pretty competitive field nowadays, the *last* thing you want to be doing is annoying users because annoyed users won't be users for long.

Three: there is no way to make a key that actually won't work after a set amount of time. All you can do is enforce that in software somehow which means that a sufficiently clever hacker would could just extract the encrypted media and public key from the app, decrypt the file permanently, and be on their way.

Four: the biggest, baddest of them all, the bane of all music DRM ever... sound is super, incredibly easy to record during playback. Just run it from a plug on the player to the mic port on any computer and bam, instant high fidelity recording to the format that you want. 

So the lesson here is basically this: implement the absolute minimum amount of DRM needed to shut your content producers and IP holders up. If you push the issue, you're going either going to end up with an app that works, but becomes a target for hackers, or you'll end up with an app that is a pain to use and that won't succeed. 

Please understand that I'm not trying to be mean here, I'm just trying to give you a more realistic outlook on what you'll be able to accomplish. Based on the way you've phrased your question, I imagine that you're probably somewhat new to this and you've probably got a lot of really optimistic ideas about what you can do, and that's great. This is a field that is governed by the laws of mathematics though, and it's better to give you a ""hard knock"" lesson than it would be to give you a half-answer and send you on your way thinking that your platform will be completely ""piracy proof."" It won't because it can't, and I'd advise you to put the minimum amount of effort needed to secure the content, and maximum effort into securing your users' data.
",4awqg1,t3_4awqg1,moeseth,,Comment,3,0,3
d152fj8,2016-03-18 18:21:43-04:00,moeseth,,Thanks for details.,4awqg1,t1_d14zeen,UtterlyDisposable,,Reply,2,0,2
d146bxg,2016-03-18 00:22:40-04:00,sathoro,,Sounds like you are looking for DRM?,4awqg1,t3_4awqg1,moeseth,,Comment,2,0,2
d146evs,2016-03-18 00:25:39-04:00,moeseth,,Correct. What would be the optimal DRM for my case?,4awqg1,t1_d146bxg,sathoro,,Reply,2,0,2
d148as2,2016-03-18 01:43:22-04:00,sathoro,,"When building a streaming app, that sounds like one of the most non-trivial aspects so I wouldn't want to do your job for you :)",4awqg1,t1_d146evs,moeseth,,Reply,2,0,2
d149fd2,2016-03-18 02:41:18-04:00,moeseth,,:-)  I understand. But would be great to hear some voices to add to my knowledge.,4awqg1,t1_d148as2,sathoro,,Reply,1,0,1
d14ajt0,2016-03-18 03:48:43-04:00,sathoro,,"It is going to be heavily dependent on what you are streaming to, which you never stated in your post. For example a browser, mobile device, etc",4awqg1,t1_d149fd2,moeseth,,Reply,2,0,2
d14chfa,2016-03-18 06:01:34-04:00,moeseth,,mobile devices,4awqg1,t1_d14ajt0,sathoro,,Reply,1,0,1
4auwlx,2016-03-17 16:12:40-04:00,BearFloofin,Need help with mystring.cpp,"Note: The boolean expression is outrageously long. I was told this can be simplified to pretty much one line of strcmp if someone can help. Thank you very much. Also, if someone can help reduce my replace. I'm having a hard time using library functions because I keep manually typing it out.
=======================================

#include ""mystring.h""

// default constructor
mystring::mystring() 
{
    ptr_buffer = new char[1];
    *ptr_buffer = '\0';
    buf_size = 1;
    len = 0;
}

// constructor from c-style string
mystring::mystring(const char * s) 
{
len = strlen(s);
buf_size = len + 1;
ptr_buffer = new char[buf_size];
strcpy(ptr_buffer, s);

}

// copy constructor
mystring::mystring(const mystring& orig) 
{
    len = orig.length();
    ptr_buffer = new char[len+1];
    buf_size = len+1;
    strcpy(ptr_buffer, orig.ptr_buffer);
}

//Overload operators mystring objects can be manipulated as 1st class objects.
mystring& mystring::operator=(const mystring& orig)
{    
      delete [] ptr_buffer;
      len = orig.length();
      ptr_buffer = new char[len+1];
      buf_size = len+1;
      strcpy(ptr_buffer, orig.ptr_buffer);
}

mystring& mystring::operator=(const char* orig) 
{    
        delete [] ptr_buffer;
        len = strlen(orig);
        buf_size = len + 1;
        ptr_buffer = new char[buf_size];
        strcpy(ptr_buffer, orig);
}

char mystring::operator[](size_type pos) const 
{
    return ptr_buffer[pos];
}

char& mystring::operator[](size_type pos) 
{
    return ptr_buffer[pos];
}

mystring& mystring::operator+=(const mystring& str) 
{
    this->append(str);
    return *this;
}

mystring& mystring::operator+=(const char* str) 
{
    this->append(str);
    return *this;
}

//Methods for modifying mystring objects with other mystring objects and c-style strings.
void mystring::clear() 
{
    len = 0;
    ptr_buffer[0] = '\0';
}

void mystring::push_back(char c) 
{
    reserve(len + 1);
    ptr_buffer[len] = c;
    ptr_buffer[len + 1] = '\0';
    len++;
}

mystring& mystring::append(const mystring& str) 
{
    return append(str.c_str());
}

mystring& mystring::append(const char* str) 
{
    int size = strlen(str);
    reserve(size + len);
    strcat(ptr_buffer, str);
    len += size;
    return *this;
}

mystring& mystring::insert(size_type pos, const mystring& str) 
{
    return insert(pos, str.c_str());
}

mystring& mystring::insert(size_type pos, const char* str) 
{
    int size = strlen(str);
    reserve(len + size);
    mystring b(ptr_buffer + pos);
    
    for (int i = 0; i <= size; ++i)
    {
        ptr_buffer[i + len] = str[i];
    }
    
    len = (pos + size);
    append(b);
    return *this;
}

mystring& mystring::replace(size_type start, size_type span, const mystring& str) 
{
    char* temp = new char[buf_size];
    
    for(int c = 0; c < buf_size; c++) 
    {
        if(c >= start && (c-start) < span && str[c-start] != '\0')
            temp[c] = str[c-start];
        if(c < start || c >= (start + span))
            temp[c] = ptr_buffer[c];
    }
    temp[buf_size-1] = '\0';
    
    delete [] ptr_buffer;
    ptr_buffer = temp;
    temp = NULL;
}

mystring& mystring::replace(size_type start, size_type span, const char* str) 
{
    char* temp = new char[buf_size];
    
    for(int c = 0; c < buf_size; c++) {
        if(c >= start && (c-start) < span && str[c-start] != '\0')
            temp[c] = str[c-start];
        if(c < start || c >= (start + span))
            temp[c] = ptr_buffer[c];
    }
    temp[buf_size-1] = '\0';
    
    delete [] ptr_buffer;
    ptr_buffer = temp;
    temp = NULL;
}

void mystring::reserve(size_type n) 
{
    if(n < (len + 1))
        return;
    
    buf_size = n;
    len = buf_size - 1;
    char* temp = new char[buf_size];
    strcpy(temp, ptr_buffer);
    delete [] ptr_buffer;
    ptr_buffer = temp;
    temp = NULL;
}

//Methods to view properties of mystring objects.
mystring::size_type mystring::size() const 
{
     return len;
}
 
mystring::size_type mystring::length() const
{
     return len;
}
 
mystring::size_type mystring::capacity() const
{
     return buf_size;
}
 
mystring::size_type mystring::max_size() const
{
       return (int)pow(2,30) -4 ;
}

bool mystring::empty() const
{
    if(len == 0)
        return true;
    
    else
        return false;
}

//Destructor
mystring::~mystring() 
{
    delete [] ptr_buffer;
}

//Boolean operators to compare mystring objectss and c-style strings.
// I need to clean this up with basically one strcmp function
bool operator==(const mystring& lhs, const mystring& rhs) 
{
    if(lhs.length() != rhs.length())
        return false;
    
    int i = 0;
    while(lhs[i] != '\0') {
        if(lhs[i] != rhs[i])
            return false;
        i++;
    }
    return true;
}

bool operator==(const char* lhs, const mystring& rhs) 
{
    if(strlen(lhs) != rhs.length())
        return false;
    
    int r = 0;
    while(lhs[r] != '\0') 
    {
        if(lhs[r] != rhs[r])
            return false;
        r++;
    }
    
    return true;
}

bool operator==(const mystring& lhs, const char* rhs) 
{
    if(lhs.length() != strlen(rhs))
        return false;
    
    int r = 0;
    while(lhs[r] != '\0') {
        if(lhs[r] != rhs[r])
            return false;
        r++;
    }
    
    return true;
}

bool operator!=(const mystring& lhs, const mystring& rhs) 
{
    if(lhs.length() != rhs.length())
        return true;
    
    int i = 0;
    while(lhs[i] != '\0') 
    {
        if(lhs[i] != rhs[i])
            return true;
        i++;
    }
    
    return false;
}

bool operator!=(const char* lhs, const mystring& rhs) 
{
    if(strlen(lhs) != rhs.length())
        return true;
    
    int r = 0;
    while(lhs[r] != '\0') 
    {
        if(lhs[r] != rhs[r])
            return true;
        r++;
    }
    
    return false;
}

bool operator!=(const mystring& lhs, const char* rhs) 
{
    if(lhs.length() != strlen(rhs))
        return true;
    
    int r = 0;
    while(lhs[r] != '\0') 
    {
        if(lhs[r] != rhs[r])
            return true;
        r++;
    }
    
    return false;
}

mystring operator+(const mystring& lhs, const mystring& rhs) 
{
    mystring concat = mystring(lhs);
    concat.append(rhs);
    return concat;
}

//Overload output operator to simplify displaying mystring objects.
ostream& operator<<(ostream& out, const mystring& str) 
{
    out << str.c_str();
    return out;
}

//Method that returns c-style string of mystring object.
const char * mystring::c_str() const 
{
    return ptr_buffer;
}

mystring::iterator mystring::begin() 
{
    return ptr_buffer;
}
",,,,,Submission,0,0,0
d13pvih,2016-03-17 16:45:11-04:00,dandrino,,"First of all, never use strcpy, always use strncpy

strncmp takes two char pointers and a length and returns -1 if the first char pointer is lexicographically less than the second, 0 if they are equal, and 1 if it is greater. So your check will be something along the lines of:

    len == rhs.len && strncmp(ptr, rhs.ptr, len) == 0

Your replace function could be something along the lines of:

    size_t str_len = strlen(str):
    char* temp = new char[length + str_len - span]:
    strncpy(&temp[0], ptr_buffer, start):
    strncpy(&temp[start], str, str_len):
    strncpy(&temp[start+str_len], &ptr_buffer[start + span], length - start - span):
    std::swap(temp, ptr_buffer):
    delete[] temp:

Make sure you reuse this logic (instead of copying it) for all the other specializations of replace.

There are a lot of other issues with your code, much of which can be avoided by simply maintaining a std::vector<char> instead of a char pointer and a length.",4auwlx,t3_4auwlx,BearFloofin,,Comment,2,0,2
4auuwx,2016-03-17 16:02:24-04:00,LegatusDivinae,I need help starting research-news library software,"Basically, I am required to do a project on News libraries (Hemeroteca).

But I don't know how such softwares work, what is their general idea or how to start modeling them.

""Hemeroteca"" yield results in spanish, while news library yields some corporate web sites.",,,,,Submission,1,0,1
4audgi,2016-03-17 14:15:24-04:00,bigal75,My son is a senior in high school and wants to get an early head start on some coding knowledge. What book would be a good starter for him to start reading before he gets to any college courses?,,,,,,Submission,20,0,20
d13yz1m,2016-03-17 20:38:07-04:00,shamankous,,"https://mitpress.mit.edu/sicp/


If you can learn one programming language you can learn just about any other. I would focus instead on learning more about the commonality between them and the underlying principles of getting computers to do useful things.",4audgi,t3_4audgi,bigal75,,Comment,10,0,10
d141925,2016-03-17 21:41:39-04:00,pangrametry,,This!,4audgi,t1_d13yz1m,shamankous,,Reply,1,0,1
d13xtli,2016-03-17 20:05:57-04:00,metaobject,,"it might be a good idea to find out a bit about the CS program he'll be going into.  For instance, do they use C++, Java, or Python as their teaching language?  It might be a good idea to get a head start in the language he's going to have to use for his classes.",4audgi,t3_4audgi,bigal75,,Comment,6,0,6
d13lw9f,2016-03-17 15:16:22-04:00,SneakingNinjaCat,,I read [Computer Systems A programmer's perspective](http://www.amazon.com/Computer-Systems-Programmers-Perspective-Edition/dp/0136108040) on my first BSc year in Software Engineering.,4audgi,t3_4audgi,bigal75,,Comment,5,0,5
d13jq02,2016-03-17 14:28:08-04:00,thatguywhorows,,"codecademy.com
",4audgi,t3_4audgi,bigal75,,Comment,15,0,15
d13l9wg,2016-03-17 15:02:24-04:00,tatertitzmcgee,,This. Way better than any book for a starting programmer. ,4audgi,t1_d13jq02,thatguywhorows,,Reply,5,0,5
d13pog6,2016-03-17 16:40:59-04:00,None,,"I'd disagee that it's good in all cases, codecademy.com is good, but it really depends on the learner. I taught myself reading Java For Dummies, then playing with the JDK.",4audgi,t1_d13l9wg,tatertitzmcgee,,Reply,8,0,8
d14a3ab,2016-03-18 03:19:47-04:00,None,,He'll be reading lots in college and most of it will be without any real world context. College isn't good at teaching programming so I would suggest learning a bit of programming and then building something. Code academy is good to get him started an then he should be able to build something for himself. This will make what he learns in college more grounded.,4audgi,t3_4audgi,bigal75,,Comment,3,0,3
d14bp3f,2016-03-18 05:07:42-04:00,pharma_joe,,I wish i had read this pages.cs.wisc.edu/~remzi/OSTEP before any of the texts I was prescribed at uni. Good luck to your son. ,4audgi,t3_4audgi,bigal75,,Comment,1,0,1
d14esni,2016-03-18 08:11:27-04:00,beachhouse21,,"Most colleges have either gone Microsoft and teach .Net or have gone Oracle and teach Java.  Knowing which path your school has gone is pretty important to trying to get ahead.  The tools used are obviously totally different.  I'd check into that, and then start learning about which ever your school teaches.

Also note, that this can vary by department.  At my college, CS is Oracle, but MIS in Microsoft.",4audgi,t3_4audgi,bigal75,,Comment,1,0,1
d14mabu,2016-03-18 11:51:34-04:00,xiongchiamiov,,r/learnprogramming has a bunch of useful resources. I'd also advise he keep r/cscareerquestions bookmarked for a few years down the line.,4audgi,t3_4audgi,bigal75,,Comment,1,0,1
d13z8sh,2016-03-17 20:45:42-04:00,high_side,,"Code, don't read.",4audgi,t3_4audgi,bigal75,,Comment,0,0,0
d14a3wl,2016-03-18 03:20:50-04:00,None,,yup,4audgi,t1_d13z8sh,high_side,,Reply,1,0,1
4asbod,2016-03-17 06:08:25-04:00,blond0_0,MPD plugin for VLC,"Hi anyone,

I'm looking for a plugin that could allow me to control my (remote) Music Player Daemon with VLC.

Currently, I connect to my server, then use 'mpc toggle' command and then clic Play on (local) VLC for it to read the stream my MPD generates.

Do any of you know a mean to only have to click once / type only one command to start both streaming for MPD and to tell VLC to read it directly ?

Any help will be appreciated.
I have this problem at work and so, platform is Windows.

Thanks in advance",,,,,Submission,0,0,0
4arx9e,2016-03-17 03:08:56-04:00,jamjam89,"Can I say ""Ruby on Rails is to Ruby as Javascript is to Jquery""?",,,,,,Submission,3,0,3
d130zvu,2016-03-17 03:40:51-04:00,nanermaner,,"I don't think that's quite right, since jquery isn't a full server side web development framework. Maybe: Ruby is to Rails as JavaScript is to Node.js ",4arx9e,t3_4arx9e,jamjam89,,Comment,16,0,16
d132qkb,2016-03-17 05:40:15-04:00,whalt,,A much better analogy.,4arx9e,t1_d130zvu,nanermaner,,Reply,3,0,3
d13ho0x,2016-03-17 13:42:26-04:00,exneo002,,Node strikes me as a roll your own tech stack. Ror doesn't need you install a ton of package to build your app. >.<,4arx9e,t1_d130zvu,nanermaner,,Reply,2,0,2
d13cypm,2016-03-17 11:55:57-04:00,Croned,,You still have the structure of the analogy messed up. Its RoR is to Ruby as Node.js is to Javascript,4arx9e,t1_d130zvu,nanermaner,,Reply,3,0,3
d13hewp,2016-03-17 13:36:48-04:00,anamorphism,,ruby on rails is to ruby as angularjs is to javascript as asp.net mvc is to c#.,4arx9e,t3_4arx9e,jamjam89,,Comment,2,0,2
d130zs2,2016-03-17 03:40:42-04:00,whalt,,"You've got the analogy mixed-up. Ruby and Javascript are both programming languages while RoR and JQuery are popular frameworks/toolkits for each. Even then it's a bit of a stretch since they cover different domains, i.e. server-side web and database programming versus client-side DOM manipulation.",4arx9e,t3_4arx9e,jamjam89,,Comment,4,0,4
4aqlxy,2016-03-16 20:19:33-04:00,SUsudo,Encryption vs password,What's the difference between the two? On android I'm aware you can set a password for your phone but there is also the option to encrypt. What's going on here? ,,,,,Submission,6,0,6
d12r1db,2016-03-16 21:33:27-04:00,AbouBenAdhem,,"No password: You don’t have a library card, but you could still break into the library.

Encrypted data: All the library books have been re-written in Sumerian.",4aqlxy,t3_4aqlxy,SUsudo,,Comment,12,0,12
d13hxa1,2016-03-17 13:48:06-04:00,101C8AAE,,"More like,

Encrypted data: the library has been burned down, and your password will expand into the instructions on how to reassemble the ash into books.",4aqlxy,t1_d12r1db,AbouBenAdhem,,Reply,2,0,2
d12plh5,2016-03-16 20:53:29-04:00,rewardiflost,,"Password - a barrier to entry, like a door with a lock.   

Encryption - actually scrambles data.  Even if someone could bypass / brute force / guess the password and get into the system,  the data is still unreadable without a lot more work. 
",4aqlxy,t3_4aqlxy,SUsudo,,Comment,12,0,12
d12t1qm,2016-03-16 22:29:51-04:00,njaard,,"These answers are marginally correct, but mostly not.

When you lock your phone with a password, you have to enter your password to access your phone. A malicious person could open up your phone with a screwdriver and physically remove the storage chip, and attach it to something else and access all your data.

When you choose to encrypt your phone, you still give it a password. That password is used to put all that data on a chip under a code that can only be mathematically unlocked with that password. If that malicious person tried to access the data by removing the storage chip, they would still need your password to be able to decipher it.",4aqlxy,t3_4aqlxy,SUsudo,,Comment,8,0,8
d141j75,2016-03-17 21:49:45-04:00,kinkyaboutjewelry,,"I really liked your answer. It is correct, it is witty and it is short. I tried to be funny myself but it didn't work. That's ok, this is not my day job :) ",4aqlxy,t3_4aqlxy,SUsudo,,Comment,1,0,1
d12rc8h,2016-03-16 21:41:48-04:00,None,,[deleted],4aqlxy,t3_4aqlxy,SUsudo,,Comment,-8,0,-8
d1306ct,2016-03-17 02:51:41-04:00,101C8AAE,,"Not relevant, unsalted, and using a broken hash function.",4aqlxy,t1_d12rc8h,None,,Reply,3,0,3
d131l52,2016-03-17 04:19:17-04:00,kinkyaboutjewelry,,You must be terrific at parties. :) ,4aqlxy,t1_d1306ct,101C8AAE,,Reply,-2,0,-2
d13hmus,2016-03-17 13:41:42-04:00,101C8AAE,,You mean I need to make light of poor answers to be fun at parties? ,4aqlxy,t1_d131l52,kinkyaboutjewelry,,Reply,2,0,2
4anmx0,2016-03-16 08:47:04-04:00,benjamini88,What are possible Big Data subtopics for a presentation?,"For a computer science seminar at the university, I have to conduct a 60 minute presentation. The seminar is called _New Concepts of Computer Science_ and we were allowed to find a topic by ourselves. The presentation should be like a TED talk, which introduces the audience to the topics and make them being interested for further research. My topic is _Big Data_, and since this is a current topic, I thought this would fit to the seminar.

However, I find it difficult to find subtopics about _Big Data_ that I could talk about.

My currently list of contents so far:

* Motivation / What is Big Data?
* Benefits of Big Data for Companies
* Tools & Technologies
    * Apache Hadoop
    * MapReduce
* Outlook

> __What are other possible subtopics I could talk about?__

> __Is there literature that you know I should definitely read?__",,,,,Submission,7,0,7
d12b5mp,2016-03-16 15:06:55-04:00,BonzoESC,,"CAP theorem, how it relates to network reliability, what it means for data consistency and system reliability.

All of https://aphyr.com/tags/Jepsen is worth reading, but https://aphyr.com/posts/281-jepsen-on-the-perils-of-network-partitions and https://aphyr.com/posts/288-the-network-is-reliable especially.",4anmx0,t3_4anmx0,benjamini88,,Comment,1,0,1
4am7mi,2016-03-16 00:20:32-04:00,norberg1,Career Advice for a 31 Year Old Semi-regretful Software Guy,"Hi guys. I have been struggling with this problem now for almost a year. I built a great career for myself; moved out of my hometown and to the Bay Area, worked at several notable companies, and in my last had a real opportunity to do some green field work, and took on a ton of responsibility. Now, I know I'm not leadership material (at least not yet), and while I enjoy responsibility and trust, what's most important to me is life balance. Because of this, I want to stay at the mid-senior level, if possible.

So here comes the problem. I've always had trouble getting up in the morning, and since I turned 29 or so it's been getting a lot worse. I used to rely on coffee a lot to get me going, but it's not working as well as it used to. I started exercising (mainly walking) in my mid twenties, and I lost a lot of weight, but due to work stress I'd gained a lot of the weight back. I also have a thyroid condition, but it's borderline and the doctors have always told me it's not treatable with medication at this point. I found myself getting a little depressed, and I started to isolate a little bit, immersing myself in video games in my spare time as it's the only thing that brought me any joy.

The job I was working afforded me an opportunity to take a break with the option to come back. I decided to go back home and spend some time living cheaply and work on a side project. The problem is, as soon as I moved out of my apartment and headed home, I was overcome with panic. The panic has not abated since. It's actually grown, contributed to by a string of bad news - deaths of family members, losing a pet, regretting bad decisions about how I spent my twenties, being alone... 

I wanted to simply head back to the Bay Area with the resolve to change, but.. the problem is, I've been unable to get myself motivated enough to change there, partially due to the fact that I'm alone out there and have no support - no family, few good friends, and stuck in a culture of work work work. Most of my coworkers are married, and I found it immensely difficult to make friends out there. To be honest, I've always found it difficult to make friends, but in the Bay Area it seemed impossible. No matter how much I pushed myself to head back there, I couldn't bring myself to do it. Moving back there alone seemed so incredibly depressing. To top it off, my energy problems have gotten worse, and I wasn't even sure I could handle full time software development anymore. The thought of it made me panic a little bit. So I told them no, and decided to continue to pursue the only thing in my life that was bringing me any joy - the relationship I started, while living off of savings and working on my side project. I continued to be more or less racked with depression and anxiety.

To skip ahead a little, that relationship eventually led me to another country. I don't yet have health insurance here, and can't see anyone about the increasingly bad energy problems I've been having and the depression. I knew it was foolish idea to come here in the condition I was in, but being here made me feel alive again, and I was hoping it would provide me with some answer if I spent some more time here. I am now a year older, and dealt with severe panic attacks for a month after arriving here. I am still unemployed, and to top it off, the relationship isn't working.

The one positive is, in many ways I feel stronger than before - like I am able to handle things on my own again - but I am still not ready to take on full time work again. I'm trying to soften the blow by looking for remote work possibilities (I strongly dislike the way most offices work), or building a consultancy (but not sure if I want to deal with clients), but I'm still fighting off depression on a daily basis, and while every fiber of my body is screaming not to go back full time work, I panic about my future, and retirement, and being alone, and being unhealthy and unhappy.

The problem I have is, I'm 31 now, and I feel like it's my last chance to really change my life before I don't have much recourse except to work and save for retirement. I'm very concerned about my health problems making it torture for me to work regularly, and I am very smart, so I am terrified of the thought of having to do unskilled work for little pay, likely until I am of age for social security, if the program doesn't collapse. Occasionally I'm filled with joy and motivated to take up creative endeavors, but I never seem to commit to anything. I'm scared that I will go back to my working life and become complacent, play video games to self medicate, and my life will never go anywhere. I'm also scared that my ability to deal with the disappointment of all of this is decreasing with age.

It seems like all the pressures of life just dropped on my head. I'm incredibly lonely, need friends, want a significant other, want to be happy in my career, have a creative urge that I just can't satisfy, and I don't have any of the answers. I'm making progress, but the stress of it all is at times, unbearable.

I'm in another country now, and, surprisingly, I'd actually like to try and stay here, if possible. The fun of it is one of the few things making me feel better. It's a big problem not being able to communicate with a lot of people, but it's a large place and lots of people speak English, so it might be OK.

I'm living my life day to day right now... trying to focus on the next thing, and make the decisions that are the best for me. Apply for jobs, try to get a little creative work done.. I need to not feel overwhelmed - because when I do, it sucks the joy out of my life. But it's hard to find fun, creative software development work that isn't constantly filled with pressure and overwork, and I cannot enjoy my life this way. I always thought of myself as a creative person, but I actually do enjoy doing software development! I just hate the way working life is, I suppose. What other answers are there for me? I thought about taking a low paying job with lots of free time, but it seems like a real risk, and it means giving up my software career that I've built - as far as I know, basically, forever, since getting back in at 40+ probably means starting as a junior. Is it incredibly stupid to throw away this career? Will I regret it in a year? I don't have a plan for what else I could do, and I need to start working now... please help, anyone! ",,,,,Submission,8,0,8
d11n9la,2016-03-16 01:14:51-04:00,artillery129,,"Firstly, you sound super depressed, and your first course of action should be to seek the best therapist you can find to help you work through your problems and figure out whether you're depressed or not. I am not a therapist and cannot decide what the case is.


If you are depressed, you won't be productive again until you're healthy, and that won't happen unless you focus on yourself. 


Having said that, it is kind of cyclic, it is hard to be healthy and productive when you are broke, and it is hard to be not broke when you're unproductive. 


If necessary, move back to the US in your hometown and relax, take some time off for yourself. Also in the short term you may do software contracts without long term commitments, this will give you revenue without forcing you to give up on your side projects. Best of luck.",4am7mi,t3_4am7mi,norberg1,,Comment,13,0,13
d1210i8,2016-03-16 11:20:27-04:00,chromaticgliss,,"I tried moving in order to solve my problems. I moved 4 different times over the course of 5 years. It always feels great for awhile, the novelty of a new place is great for awhile, but ultimately the root of your issues creeps up again. I also thought a relationship would solve my problems... unfortunately codependency issues rear their ugly heads. You have the right idea by mentioning balance though. I wouldn't recommend dropping out completely since you do enjoy it, but perhaps a more junior level position might work for you...some thoughts.

**Replace escape with Productive things:** This is the cruxus of escaping depression/burnout. And here, productive does not mean work, it means self-development. You have to give equal attention to every aspect of your being: emotional, physical, intellectual, social...

**Join a Meetup/Exercise:** A lot of men seem to have loneliness issues that result in depression because we get pressured into being ""lone wolf"" idealogues. I'd strongly suggest joining a meetup/club in your area that meets weekly. Kill two birds with one stone by making it something active (dance, cycling, kyaking... whatever floats your boat :) ) Energy problems are often pardoxically a symptom of inactivity and poor health. Unfortunately it makes it difficult to pull out of, because... well... low energy. Power through though, make it your goal to do *something* active every day. Even if it's just a walk around the block. The meetup will help make your ""active"" thing more fun... and you'll make friends (also, ladies love a good dancer by the way).

**Try Meditation:** It still has a new-agey/annoying twinge, but there is a lot of science popping up that shows its efficacy. Ignore the gimmicks and cosmologies that get wrapped up into it. I've talked to tons of people that have tried 7-10 day retreats and come out feeling like a brand new person. You don't have to do that necessarily, but it's a thought. My favorite practice is called Vipassana (though I'm not a huge fan of many of the groups that practice Vipassana, they get a little culty). Keep a daily practice of 10 or 20 minutes...increase as you get better. The Zen proverb is, *""You should sit in practice for 20 minutes a day -- unless you are too busy. Then you should sit for an hour.""* For me it has improved my self discipline, patience, and attitude toward every other aspect of my life. There's a reason every religion has some sort of contemplative practice, it's one of the best ways to deal with emotional well being. The secular west has lost sight of that aspect of humanity.

**Cut Instant Gratification** (Drugs, Alcohol, Porn, Video Games, Junk Food): These are all simply ways of tricking yourself into believing you are achieving things that you need. Drugs and Alcohol tend to suppress problems and hinder emotional healing. Porn masks a human need for intimacy and connection ([Russell Brand](https://www.youtube.com/watch?v=5kvzamjQW9M)  said it well). Video Games cater the need for achievement. Junk Food replaces real nourishment. I could go on, but I'm sure you already know all of this.

**Pick A Creative Outlet:** Painting, music, writing... whatever. *Not personal coding projects.* This is key, keep work creativity separate from your outlet. Expect to suck. A lot. For a long time. Embrace the suck. Sit down and say ""I'm going to suck at this, and it's going to be a great time."" And keep sucking every day. Eventually you'll suck less, and you'll gain maximum enjoyment multipliers.

Edit: **Get Outdoors**: Get some of that juicy Vitamin-Nature. It's well documented that time outside helps curb depression/anxiety. 

All in all, take a more holistic view to your career/life. The benefits will bleed over into your software development career.

Good luck!
",4am7mi,t3_4am7mi,norberg1,,Comment,3,0,3
d11w3ns,2016-03-16 09:05:13-04:00,GrizzlyDev,,"I'd recommend getting right before making moves. If I hadn't seen a psychiatrist and gotten the right meds, I wouldn't have a job now",4am7mi,t3_4am7mi,norberg1,,Comment,3,0,3
d12d4oy,2016-03-16 15:51:17-04:00,rocketbunny77,,"Wowzers. This post hit home. Thank you for sharing. I have already found some great advice in the comments. 

I feel like I'm in the exact same state of mind right now. (Not trying to say that my situation is worse or anything, I just feel like I understand how you feel).

 I burned myself out in my career early on (started working at the age of 19) until I was absolutely sick of the thought of working a 9 to 5 and quit to start freelancing. The freelancing has been fun and I've been doing it for about a year but I haven't really been making ends meet and have been living off of savings as a result. This is mostly due to me not wanting to take on too much concurrent work and get trapped in a situation where I'm working around the clock to get everything done.

Right now I'm in a state of depression and anxiety every day because I feel like I need to get a job again, or take on a decent paying contract, to be able to live comfortably but that comes with the stress of ridiculous working hours, as is common in our industry. I have absolutely zero motivation to get back into the cycle of draining myself every day of the week, only to ""relax"" on weekends so that I have the energy to do it the following week. 

The last few months have been the most stressful of my entire life due to external circumstances and some stupid actions on my part and my IBS has returned with full force. I feel like I had things under control in December - was feeling happy, positive about the new year and my health was the best it had been in ages - but right now I'm feeling so mentally ruined that I'm struggling to focus on the work I need to complete for some projects I've taken on.

The future seems so uncertain and to be honest I'm just fucking scared. ",4am7mi,t3_4am7mi,norberg1,,Comment,1,0,1
4alurw,2016-03-15 22:40:14-04:00,zSilverFox,Is a Turing Machine an extension of a Finite State Automaton?,,,,,,Submission,3,0,3
d11jy24,2016-03-15 23:28:14-04:00,bhrgunatha,,"You could argue that push down automata are an extension of finite state automata and turing machines are an extension of push down automata by looking at their attributes. I think it's more important to understand that they are minimal tools capable of solving different classes of problems, from the ability to recognise regular languages right up to anything that is computable.

I can recommend the [Coursera course on Automata](https://www.coursera.org/course/automata) if you are interested in looking in more detail.",4alurw,t3_4alurw,zSilverFox,,Comment,7,0,7
4akm0y,2016-03-15 17:39:03-04:00,Erica993,I need someone to talk to about Computer Science.,"I want to talk with someone about different fields of computer science. We can talk over skype or something.
I am a student. I am in the last year of my Bachelor studies. I plan to get my Masters degree next year probably in the field of Cryptography, Computer Networking, something like that. 

Anyone interested send me a private message.

inb4 mods delete my post sigh",,,,,Submission,0,0,0
d11btjg,2016-03-15 20:19:06-04:00,algorithmsWAttitude,,"As a student, why wouldn't you talk to a professor at your school, one that you respect?  Perhaps from a previous semester?",4akm0y,t3_4akm0y,Erica993,,Comment,2,0,2
d129hv0,2016-03-16 14:29:55-04:00,dxk3355,,Why not call a college department or admissions.  Usually the department advisors can explain the differences.,4akm0y,t3_4akm0y,Erica993,,Comment,1,0,1
4afblm,2016-03-14 17:52:21-04:00,AuraX,Manhattan Distance fill tool algorithm,"I have been asked this question 3 times in 3 unrelated events, and I have not been able to find the optimal algorithm to this question. I am starting to feel like a fool for not being able to even google the answer to this question correctly. Here it is:

You have a 2D array filled with False, a list of indices L, and a distance D. For each index in L, set all indices that are D or less away from the index on the grid to True. Basically, draw filled diamonds around each point on the 2D array.

The algorithm I thought of is the one obvious one, the one exactly stated by the problem. For each point, find each point that is D away from the point (checking for bounds of the grid), and set it to True. 

My algorithm is O(n*m) where n is the number indices in the array, and m is the number of points.

I am told that there is a more efficient way to solve this with 2 passes of the grid. What is it?",,,,,Submission,1,0,1
d0zw1pd,2016-03-14 18:13:07-04:00,RamsesA,,"You could expand a frontier around each of the points, in which case you don't even need to traverse the whole array. The complexity of that is O(mDD).",4afblm,t3_4afblm,AuraX,,Comment,1,0,1
d0zwv14,2016-03-14 18:33:43-04:00,AuraX,,Can you expand upon your answer? I don't see how this is more efficient than my answer.,4afblm,t1_d0zw1pd,RamsesA,,Reply,1,0,1
d107sxx,2016-03-14 23:17:26-04:00,RamsesA,,"Do you traverse all indices in the 2D array, or only the ones within D of the points in L? My algorithm does the latter, but I may have misread your description.

If your current algorithm is traversing only the points within D of L, then it's already pretty good, but you can do better by making sure you traverse each point only once. This does not change the bigO complexity, but does change the average performance in certain scenarios.

Consider a scenario where all the points in L are directly adjacent, leading to a huge amount of overlap and ""refilling"" of the same points repeatedly. This can be eliminated by expanding outward from the center of each point, and stopping when you reach a point that is already filled. However, you need to expand outward from all the points in L simultaneously to avoid ""blocking"" some diamonds from forming.",4afblm,t1_d0zwv14,AuraX,,Reply,1,0,1
d10q074,2016-03-15 11:58:45-04:00,AuraX,,"How are you expanding your frontier?

If you expand without checking, you are setting same points repeatedly, creating the same complexity as my answer.

If you are expanding and stopping when a point is already colored, your answer does not work in the case where any points in L are located inside other frontiers.",4afblm,t1_d107sxx,RamsesA,,Reply,1,0,1
d10y4sf,2016-03-15 15:02:03-04:00,RamsesA,,"You have to check to get any performance boost. To avoid trapping points inside another point's frontier, you must expand all points simultaneously, such that all points distance k from L are set before any point distance > k from L. This can be done by queuing each starting point with the value D and adding all their free neighbors to the queue with value D - 1, etc.",4afblm,t1_d10q074,AuraX,,Reply,1,0,1
d1116pv,2016-03-15 16:09:53-04:00,AuraX,,"I see. I understand now. This would work, and would be linear complexity. Great job.

I am still curious as to what the answer which requires 2 passes is though.",4afblm,t1_d10y4sf,RamsesA,,Reply,1,0,1
4aeyx7,2016-03-14 16:35:43-04:00,TheMooseLodge,Is a Computer Science degree the best degree for someone who's interested in web development/front-end web development?,I would like to go back to school and am very interested in a career in web development. Is this the correct degree for me? ,,,,,Submission,19,0,19
d0ztfv7,2016-03-14 17:10:25-04:00,nanermaner,,"I am going to say yes. Sure some schools might have software engineering or something else that might be relevant, but I would say Computer Science is the most popular path to becoming a web developer. ",4aeyx7,t3_4aeyx7,TheMooseLodge,,Comment,13,0,13
d0zu48j,2016-03-14 17:26:14-04:00,TheMooseLodge,,"I probably should've clarified more in my description, but isn't software engineering more for programs/apps, rather than websites? My goal in mind is dealing with websites, rather than apps. I'm not sure how similar they are to each other, though.",4aeyx7,t1_d0ztfv7,nanermaner,,Reply,2,0,2
d0zuah4,2016-03-14 17:30:21-04:00,RHC1,,"It depends. A lot of websites today serve as apps. If you think about it facebook's website is an app so is Yelp and grubhub and your bank account, etc. A lot of big applications depend on some web back end and front end unless we are talking about mobile apps and even software like phone gap lets you write in JavaScript for phone apps.  ",4aeyx7,t1_d0zu48j,TheMooseLodge,,Reply,8,0,8
d0zui33,2016-03-14 17:35:27-04:00,TheMooseLodge,,"Yeah, I see what you mean. That's a great point now that you mention it. I was talking about computer based applications, not mobile apps so you understood me right the first time, haha. ",4aeyx7,t1_d0zuah4,RHC1,,Reply,3,0,3
d0zuy55,2016-03-14 17:46:12-04:00,RHC1,,"I mean it depends what you're talking about. If you are talking about static websites then you don't really need a CS degree. But there isn't much money in that anymore as far as I know. A lot of people get turned off when it comes to web dev because they think they're making static websites but a lot of popular apps rely heavily on the web. I'd say if you want to build sites like Facebook and Yelp, etc. go for a CS degree because the math and theory will help you create efficient backend code. But if you want to make simple static sites then I don't see the need for a degree a lot of that stuff can be accomplished with proper googling or just using word press. ",4aeyx7,t1_d0zui33,TheMooseLodge,,Reply,3,0,3
d0zv9ol,2016-03-14 17:53:53-04:00,TheMooseLodge,,"I'm still new to all of this, so what exactly are static websites compared to non-static(?) websites? ",4aeyx7,t1_d0zuy55,RHC1,,Reply,1,0,1
d0zzutl,2016-03-14 19:50:33-04:00,keepdigging,,"'Static' has to do with what logic is handled by the server.
You request a page, if I just go and grab page.html off the disk and send it to you it's 'static'.
Wordpress isn't exactly static because it grabs the post from the database, but your role in formatting it for people probably would be.

If your request was actually a click on a 'add friend' button then I'm supposed to do a bunch of work now making sure you're who you say you are, that the friend is add-able and what that means to the associated database models. Now I need to send that person a notification, but if I do that now it will be a full second before I can let you know you've even clicked the button correctly in the first place - these kinds of problems are dynamic.

If you think you want to position and style elements, and handle the interactive and visual aspects of web design you might not find a computer science degree to be worthwhile.
If you want to decide for your boss what 'adding a friend' is in the most discrete of terms so that a computer can understand it and do it a million times a second than you might love being a backend web developer, or db admin or system architect or one of many other roles in the field where 'B.Sc. Computer Science' is seen as a great foundation.

You mentioned an interest in front end web-development, which is gaining more and more dynamic logic as well. (Right now all in javascript but soon webassembly as well hopefully) A lot of what is served statically is then run on your computer, and not the server, and front end developers are doing increasingly more 'real programming' than static markup and styling. If you want to tackle facebook's behemoth chat bar you'll probably want a computer science degree as well.

Ultimately as has been mentioned before in this field employers are most concerned with portfolio and interview performance because if you can do the work you're more valuable than someone who understands the theory and can't do the work. In this sense even with the degree your 'web development' career will be primarily self-taught regardless of how you go about it. - You can do probably at the same time, and will likely pick up the high-level concepts, data-structures and algorithms that you need to excel as a programmer much faster with formal instruction.

School for 4 years vs a salary and the opportunity cost/effectiveness of a degree against your own ability to self-motivate and learn is entirely up to you...",4aeyx7,t1_d0zv9ol,TheMooseLodge,,Reply,4,0,4
d0zvmlo,2016-03-14 18:02:37-04:00,RHC1,,"Static means it stays the same throughout. I personally think it's a bad term because sites can change overtime but to define it think of it as this: I'm a journalist who wants to do a blog so I start a word press that easily and quickly creates a unique site that lets people view my articles. My site is static because the layout is the same. I don't need a backend to manage things like status updates and comments. I don't need to organize user data efficiently. I don't need to represent large gathered data sets in a visually comprehensive form. A static sight is purely HTML, CSS, and/or JavaScript. No backend exists. ",4aeyx7,t1_d0zv9ol,TheMooseLodge,,Reply,2,0,2
d0zvro6,2016-03-14 18:06:09-04:00,TheMooseLodge,,"That's actually not what I had in mind at all, so that's good news. I like the more modern looks/layouts/features of the websites of today.",4aeyx7,t1_d0zvmlo,RHC1,,Reply,1,0,1
d0zw3bk,2016-03-14 18:14:13-04:00,RHC1,,"Then your probably more geared towards web development. The thing is that it's so vague because the web is so standardized. For example when I start working this summer I will be a Systems programming analyst. But my description is basically software engineering and my job involves Java for the web and HTML, CSS, JavaScript. So honestly I'm a web developer. If you are interested in everything I said go get a CS degree it will do you wonders in the field. ",4aeyx7,t1_d0zvro6,TheMooseLodge,,Reply,1,0,1
d0zvnon,2016-03-14 18:03:23-04:00,None,,[deleted],4aeyx7,t1_d0zu48j,TheMooseLodge,,Reply,3,0,3
d0zvsfy,2016-03-14 18:06:42-04:00,TheMooseLodge,,So is it recommended to learn back end before learning front end? Or can I learn both at the same time?,4aeyx7,t1_d0zvnon,None,,Reply,2,0,2
d0zx2sf,2016-03-14 18:39:12-04:00,None,,[deleted],4aeyx7,t1_d0zvsfy,TheMooseLodge,,Reply,4,0,4
d0zxi6l,2016-03-14 18:49:58-04:00,TheMooseLodge,,So is school not really required in this field? Am I better off self teaching?,4aeyx7,t1_d0zx2sf,None,,Reply,1,0,1
d0zzhfp,2016-03-14 19:40:43-04:00,None,,[deleted],4aeyx7,t1_d0zxi6l,TheMooseLodge,,Reply,5,0,5
d10pw1y,2016-03-15 11:56:00-04:00,TheMooseLodge,,"Yes, it does. Thank you very much for your help. I appreciate it",4aeyx7,t1_d0zzhfp,None,,Reply,1,0,1
d0zwntm,2016-03-14 18:28:37-04:00,raegx,,"I posted in this in another thread long ago. Web sites fall into ""information systems"".

> I feel like this the thing that isn't easy to understand pre-college.
> 
> CS Major:
> 
> * Programming / Programming Language Concepts
> * Heavy theory
> * Some SE Process & Best Practices
> * Generally heads towards some specialization dealing with computation algorithms/data models.
> 
> SE Major:
> 
> * Programming / Programming Language Concepts
> * Some theory
> * Heavy SE Process & Best Practices
> * Generally heads towards some specialization dealing with an applied domain (Networking, Information Systems, etc).
> 
> I think it has to do with the fact that for SE, it is really hard to conceptualize what is a ""software engineering process"" and what that means. There are so many concerns in there that the courses don't even make a dent in what really goes into a large scale application development (Unit Testing, Integration Testing, Acceptance Testing, Continuous Integration/Deployment, Source Control strategies, Estimation, etc).
> 
> 
> That said, I don't think one is better than the other in general. I would say if you think you want to pursue an academic career do CS. If you want to go out and make business applications go SE. Games/Graphics I would tend to say CS over SE, but that isn't one I'm willing to stand and fight for. A well rounded out set of electives can make either major viable for any career path. 
> 
> The SE world is slowly getting the academic side. However, the issue is that it is hard to apply scientific principles to processes that vary so widely from company/department/project to company/department/project (hard to reproduce), are completed on infrequent basis (i.e. it can take years for development to ""finish""), and doesn't have a easily available data source for analyzing (since  businesses usually aren't interested in having academic involvement in their business).

For what it is worth I studied Software Engineering, but I took many CS electives. My friends were a mixture of SE and CS and I now worth with another mixture of the same. I have also worked with some IT/MIS backgrounds.",4aeyx7,t1_d0zu48j,TheMooseLodge,,Reply,2,0,2
d0zv2mz,2016-03-14 17:49:16-04:00,nanermaner,,"Honestly I don't really know anything about a Software Engineering degree. but I'm studying Computer Science and if you study Computer Science and take a few electives relating to web dev then you're good to go in my opinion. 

Plus if you decide you don't want to do web dev, CS degree opens plenty of doors.

Plus its an interesting degree.",4aeyx7,t1_d0zu48j,TheMooseLodge,,Reply,1,0,1
d10vf2y,2016-03-15 14:02:06-04:00,chunalt787,,Most good websites now days are as complex or more so than traditional applications.  It's easy for a software engineer to go into front end development.  Invest in the harder program up front.  You will have more opportunity in the future.,4aeyx7,t1_d0zu48j,TheMooseLodge,,Reply,1,0,1
d0zsxp4,2016-03-14 16:58:42-04:00,None,,[deleted],4aeyx7,t3_4aeyx7,TheMooseLodge,,Comment,9,0,9
d0zv4hu,2016-03-14 17:50:31-04:00,nanermaner,,"I feel like this is misleading, sure Software Engineering might be slightly more accurate, but a CS grad going into web dev is VERY common. ",4aeyx7,t1_d0zsxp4,None,,Reply,9,0,9
d10e7gl,2016-03-15 03:45:55-04:00,deftware,,"I think a CS going into web dev is just a sign of the times. Everybody is getting into debt, getting a degree in something that there just aren't many jobs for, so they 'settle for less' and work at a job they are overqualified for as a result.. There are many instances of this in a variety of academic fields.",4aeyx7,t1_d0zv4hu,nanermaner,,Reply,1,0,1
d0zvkgz,2016-03-14 18:01:07-04:00,None,,[deleted],4aeyx7,t1_d0zv4hu,nanermaner,,Reply,0,0,0
d0zwfe2,2016-03-14 18:22:38-04:00,nanermaner,,"Fair enough, I guess it's different since my school doesn't offer software engineering, I just feel like all the cs grads go into web dev.",4aeyx7,t1_d0zvkgz,None,,Reply,1,0,1
d0ztryw,2016-03-14 17:18:17-04:00,high_side,,"It might be at a trade school.  For a more traditional university, you'll have a lot of irrelevant (and hard) coursework.

Many universities have human-computer interaction-type majors, that'll do a lot more for you.  Look into cognitive science with technical emphases.",4aeyx7,t3_4aeyx7,TheMooseLodge,,Comment,5,0,5
d0zttzu,2016-03-14 17:19:35-04:00,None,,[deleted],4aeyx7,t3_4aeyx7,TheMooseLodge,,Comment,2,0,2
d0zu1m2,2016-03-14 17:24:32-04:00,TheMooseLodge,,"Sorry, I meant go back to school as in I never went to college after high school. I currently don't have a degree.",4aeyx7,t1_d0zttzu,None,,Reply,1,0,1
d0zugur,2016-03-14 17:34:37-04:00,None,,[deleted],4aeyx7,t1_d0zu1m2,TheMooseLodge,,Reply,3,0,3
d0zur16,2016-03-14 17:41:25-04:00,TheMooseLodge,,"I've been doing Free Code Camp as well as Codecademy and have been enjoying it, but don't feel committed to it due to them not being an actual class. I'm not sure how similar the MIT or Stanford classes are, but I'll have to check them out.",4aeyx7,t1_d0zugur,None,,Reply,1,0,1
d0zuzhv,2016-03-14 17:47:07-04:00,friedclams,,"It depends on what your college offers. My college (and many others I know of) only offer computer science and IT majors, so computer science would be the best in that case. See if your school offers any web development related electives as well",4aeyx7,t3_4aeyx7,TheMooseLodge,,Comment,2,0,2
d1032e9,2016-03-14 21:13:36-04:00,None,,"I would argue that whether you major in computer science or engineering, the most important thing is that you learn the tools of the web developer trade.  After you get your degree nobody cares if you took a class that used some front end technology.  On the other hand, if you have a nice github repository which shows you are invested in web dev work, that is really valuable.",4aeyx7,t3_4aeyx7,TheMooseLodge,,Comment,2,0,2
d10q02x,2016-03-15 11:58:40-04:00,TheMooseLodge,,It seems my github portfolio could be more valuable than any degree ,4aeyx7,t1_d1032e9,None,,Reply,1,0,1
d10apy3,2016-03-15 00:53:39-04:00,BonzoESC,,"No. Front end and web stuff feels like it changes annually, while university curricula change less than every decade. If you want to build web apps and can keep up, just get a job doing that. 

The value of a CS background in that field is more about building new tools and systems to make front end work nicer, but there are far fewer firms interested in paying for that (Facebook and Google are the only two that spring to mind right now.)",4aeyx7,t3_4aeyx7,TheMooseLodge,,Comment,2,0,2
d10q7po,2016-03-15 12:03:39-04:00,TheMooseLodge,,Is it possible to intern or find a job of some sort where they teach you or train you to prepare to work for their company and to get  experience?,4aeyx7,t1_d10apy3,BonzoESC,,Reply,1,0,1
d10qp3j,2016-03-15 12:15:09-04:00,BonzoESC,,"Many places want somebody with some level of familiarity with the tools and libraries they're using, but you're not usually expected to know everything about it up front. Learn the basics on your time, including fundamentals like a good text editor, git, etc. (these are also things you won't learn in a CS program), and some of the popular modern frameworks (as well as some of the older ones because most of the money is in maintenance, upkeep, and incremental changes).

Training is expensive for companies, but learning as you go and picking up techniques from coworkers on the job is very standard.",4aeyx7,t1_d10q7po,TheMooseLodge,,Reply,1,0,1
d10qttz,2016-03-15 12:18:13-04:00,TheMooseLodge,,"Thank you. How will I know when I am prepared and ready to begin applying for jobs? How soon can I expect that to be? The sooner I can begin my future career, the better, but of course I don't expect it to be a short amount of time. ",4aeyx7,t1_d10qp3j,BonzoESC,,Reply,1,0,1
d10yjd9,2016-03-15 15:11:11-04:00,BonzoESC,,"When you start getting hired for them?

If you live in a sufficiently large city, there is probably one or more web or front-end development group that you can attend meetings for, network with, and learn from. For example, we have http://www.meetup.com/Front-end-Developers-of-Miami/ in Miami. These meetings are frequently attended by people looking to hire developers, and can be one of the best ways to make a good impression as someone who is pleasant to work with and eager to learn on other developers.",4aeyx7,t1_d10qttz,TheMooseLodge,,Reply,1,0,1
d10zzhi,2016-03-15 15:43:22-04:00,TheMooseLodge,,This is good to know. Thank you.,4aeyx7,t1_d10yjd9,BonzoESC,,Reply,1,0,1
d16guge,2016-03-20 01:26:49-04:00,coconutscentedcat,,"There's plenty of resources online to teach yourself. These days many employers in tech (especially in web development) mostly care about experience. You don't need a degree to work in this field and all of the tools to learn are available online for free. All you need is a good portfolio.

Computer Science is almost completely unrelated to web development. You won't learn to program much (unless you learn on your own) and you won't learn anything about design. CS is about theory and more for people who want to build algorithms, AI, work in data science, software etc.. If you go into CS with the goal of becoming a web developer, you'll be in for a rough time.

A 2 year Web Dev community college program or coding bootcamp will get you there quicker than a CS degree. 

If you decide to learn on your own, start by learning HTML, then CSS, and a bit of Javascript..build a portfolio with those skills. then you can then the more complex languages on the job (more JS, jquery, SQL, PHP..)",4aeyx7,t1_d10q7po,TheMooseLodge,,Reply,1,0,1
d10ajpe,2016-03-15 00:47:03-04:00,Newmanator29,,If your school has an integrated media major then I say go with that. In my CS program at never mentioned anything to do with web. However my friend that dropped CS and switched to integrated media wanted to go into front end web dev and they did some pretty advanced stuff with JavaScript,4aeyx7,t3_4aeyx7,TheMooseLodge,,Comment,1,0,1
d10e3yu,2016-03-15 03:39:58-04:00,deftware,,"compsci is heavy. Sounds like you're more Communication Arts, etc... Do something with design/graphics and you're bound to delve into front-end stuff development stuff.",4aeyx7,t3_4aeyx7,TheMooseLodge,,Comment,1,0,1
d0zswju,2016-03-14 16:58:00-04:00,justlikestoargue,,"Relevant to me. 

RemindMe! 24 hours",4aeyx7,t3_4aeyx7,TheMooseLodge,,Comment,-1,0,-1
d0zsxqk,2016-03-14 16:58:44-04:00,RemindMeBot,,"I will be messaging you on [**2016-03-15 20:58:38 UTC**](http://www.wolframalpha.com/input/?i=2016-03-15 20:58:38 UTC To Local Time) to remind you of [**this link.**](https://www.reddit.com/r/AskComputerScience/comments/4aeyx7/is_a_computer_science_degree_the_best_degree_for/d0zswju)

[**CLICK THIS LINK**](http://www.reddit.com/message/compose/?to=RemindMeBot&subject=Reminder&message=[https://www.reddit.com/r/AskComputerScience/comments/4aeyx7/is_a_computer_science_degree_the_best_degree_for/d0zswju]%0A%0ARemindMe!  24 hours) to send a PM to also be reminded and to reduce spam.

^(Parent commenter can ) [^(delete this message to hide from others.)](http://www.reddit.com/message/compose/?to=RemindMeBot&subject=Delete Comment&message=Delete! d0zsxqk)

_____

|[^([FAQs])](http://www.reddit.com/r/RemindMeBot/comments/24duzp/remindmebot_info/)|[^([Custom])](http://www.reddit.com/message/compose/?to=RemindMeBot&subject=Reminder&message=[LINK INSIDE SQUARE BRACKETS else default to FAQs]%0A%0ANOTE: Don't forget to add the time options after the command.%0A%0ARemindMe!)|[^([Your Reminders])](http://www.reddit.com/message/compose/?to=RemindMeBot&subject=List Of Reminders&message=MyReminders!)|[^([Feedback])](http://www.reddit.com/message/compose/?to=RemindMeBotWrangler&subject=Feedback)|[^([Code])](https://github.com/SIlver--/remindmebot-reddit)
|-|-|-|-|-|",4aeyx7,t1_d0zswju,justlikestoargue,,Reply,1,0,1
4a9nfq,2016-03-13 15:37:47-04:00,CyborgOtter,Suggessted Primer for going from Math BS to Comp Sci MS,"Hey Everyone,

Thanks for your help in advance and I'm sorry if this is the wrong section :). I have a BS in Math(had a 4.0 Math GPA took Calc I-III, Real Analysis, Abstract Algebra, and a bunch of stat theory courses). Anyways after undergrad I've been working as an analysts for a couple years and I'm looking back to go for my masters and Computer Science. So I'm a decent programmer/developer. I know SQL,VBA,R, and C# so I'm familiar with concepts but unclear on some of the more formal parts of computer science. What books would you recommend or how would you suggest preparing for a computer science program?",,,,,Submission,6,0,6
d0ysfjf,2016-03-13 20:29:49-04:00,None,,"As a math major you should check out introduction to the theory of computation by Alan Sipser.  Try to actually do the proofs, a lot of them should be very familiar to you.  It will give you a good theoretical foundation for a lot of your theory courses",4a9nfq,t3_4a9nfq,CyborgOtter,,Comment,8,0,8
d0ysoo5,2016-03-13 20:37:19-04:00,crookedkr,,"If you are thinking of doing theory you will fit right in. If something else, scan some of the course overviews of what you will take. Pick the most common language and do a couple exercises. You will probably be fine anyway. While having more programming experience can be helpful, unless you are doing low level systems stuff you won't really need to much more than a couple semesters in some language. ",4a9nfq,t3_4a9nfq,CyborgOtter,,Comment,5,0,5
d0zbrtc,2016-03-14 10:10:11-04:00,IAmNotNathaniel,,"Sounds to me like you shouldn't have much trouble.

You probably already know what Lamba Calculus is from the mathematics work: it also gets used with computation, esp. for things like functional programming.

Automata Theory and Grammars are the things I think of most when I think back to my CS days. [Stamford](http://cs.stanford.edu/people/eroberts/courses/soco/projects/2004-05/automata-theory/basics.html) has a pretty quick overview to give you a better idea what it is.

If you are looking for terms to google, the big ones I remember are:

Inductive proofs: turing machines: pumping lemma: grammars: state machines:

I loved analyzing/making state machines. Each one is like a little puzzle to solve...

Edit: I wouldn't worry too much about the languages. Sounds like you already have a good enough knowledge of general programming that you'll pick up anything else you need within the coursework.",4a9nfq,t3_4a9nfq,CyborgOtter,,Comment,1,0,1
4a6olj,2016-03-12 21:35:25-05:00,CARGLE,Junior In College Majoring In Computer Science,"Hello everyone, this is my first post here. Please forgive me if I end up doing something wrong. Just as the title says, i'm a junior in college majoring in computer science and right now I feel quite lost. 

So far I've taken 

* two object oriented classes
* computer systems
* discrete mathematics
* client side web development (currently)
* algorithms (currently) 
* (Others that I'm forgetting)

These classes have allowed me to get experience in 

* python
* basic c
* c#
* c++
* java
* javascript (currently)

So with all of the classes I have taken so far, I personally feel as though I've gotten a lot of breadth but very little depth as far as actual programming goes. I'm very worried that by the time I'm due to graduate, I still won't be very confident in programming. I've got a pretty good understanding of the concepts I think but if I were to try out for a job right now I'm quite positive I would not be picked up. 

If anyone has experienced this before or has any advice/insight/suggestions that could possibly assist me, I would extremely appreciate it. Thank you for taking the time to read my post. 


EDIT: Thank you all for the insight and advice - it has really been helpful. It seems as though the consensus is that the best way to get a good grasp of programming is to self teach so I think I should definitely do a project over my next break. I've been having trouble disciplining myself to sit down and do certain tasks lately so it may be a challenge but it's one worth overcoming. If I'm not willing to do a project for myself, how could I ever do one for others. Thanks again everyone!",,,,,Submission,3,0,3
d0xtyyt,2016-03-12 22:02:31-05:00,PastyPilgrim,,"Your classes won't supply nearly enough depth for you to get really confident with a specific language.

First note that you're studying computer science, not programming, so the goal of your classes isn't to make you a master programmer. That said, it's important to understand what programming is: it isn't Python or Java or C, it's a more fundamental language between you and a machine. Once you've mastered that language and understand how to communicate with computers, then programming languages become little more than dialects.

To better understand the language that I'm talking about, you need to spend a ton of time with one language until you can see past its grammar and syntax. One semester that included a handful of labs that you waited until the night before to do and a semester long project that you waited until the weekend before to do isn't nearly enough exposure. I wouldn't even put a language with that kind of exposure on my resume.

You want to spend your summer working on a big project that requires tons and tons of programming. All of the programming that you do, along with the debugging, and the learning (as you research how to do what it is that you want to do), will put you on the right track. Then, maybe throughout the school year you want to prototype your schoolwork in whatever language it is that you've chosen to study most intently. I did that (for new languages) and the process of writing programs in multiple languages helped me a lot to understand how things worked behind the scenes.

It's kind of hard to teach you how to teach yourself better because we all learn differently, but programming, like most skills (music, foreign language, art, etc.) is something you need to practice a ton before you get good/confident with it. It's hard to see that at first because you can write programs that pass all the tests without the implementations being good (unlike most other skills), but it's crucial that you practice as much as you can.",4a6olj,t3_4a6olj,CARGLE,,Comment,10,0,10
d16htiq,2016-03-20 02:10:07-04:00,coconutscentedcat,,What sort of things did you build over the summer? I'm wondering what kind of big project I could build with my limited knowledge - any ideas?,4a6olj,t1_d0xtyyt,PastyPilgrim,,Reply,1,0,1
d0xvdl3,2016-03-12 22:47:07-05:00,videoj,,Talk to your professors about an independent study project to help you gain experience in a larger project then a simple homework assignment.  They often have something related to their research they can give to you and you may even get a paper credit out of it.,4a6olj,t3_4a6olj,CARGLE,,Comment,3,0,3
d0xuydd,2016-03-12 22:33:19-05:00,__cxa_throw,,"What really helped me was picking a decent sized project that I personally wanted to do, not related to school, and putting an hour or two per day of time into it.    Usually the nature of the project doesn't matter, you'll cruise through some parts and have to drill down much deeper than expected in others.  Part of being a good scientist/engineer is being able to get something done from start to finish including learning what you need along the way (learning to self teach in a sense).  In the process you'll end up gaining significant depth in certain areas was well as the basics in lots of others.

This has a nice bonus effect that when you interview for jobs you have something with more depth than a 2 month class project that you know inside and out.  You'll be able to explain tradeoffs you made, what you'd do better next time, and all the other stuff interviewers like to ask.  And you took the initiative to do it.

Edited to add:
Second piece of advice:  If you ever see something in class that you think is really interesting spend some time digging into it.",4a6olj,t3_4a6olj,CARGLE,,Comment,3,0,3
d0y8d8p,2016-03-13 10:52:05-04:00,mplang,,"Pretty much everything already said here is spot-on. There are two additional things that really helped me that I'd like to share.

Certain courses were instrumental in furthering my understanding of programming in general: theory of programming languages: compilers: operating systems: computer architecture. Naturally, every course helped in one way or another, but these were especially helpful in pushing me to think about programming in a different way. If you have the opportunity to take one or more of these, I'd very much recommend it.

Second, I'm really bad at coming up with project ideas, but I compensate by being pretty good at ""taking the ball and running with it"". In school, I never had a side-project, but I would take full advantage of assignments. I would implement almost everything I learned, even when not required (data structures and algorithms were great courses for this). I took my time with big projects, and generally went all-out -- never do the bare minimum last-minute! For an extra challenge, do the assignment in a language you've never used before. In school, you're given the gift of a starting point -- use it!",4a6olj,t3_4a6olj,CARGLE,,Comment,2,0,2
d0ybd8v,2016-03-13 12:32:10-04:00,CARGLE,,Alright. I was looking forward to taking an OS class next semester so that's good. Also I will definitely keep it in mind to try to take the extra step in every project from now on if it helps.,4a6olj,t1_d0y8d8p,mplang,,Reply,1,0,1
d0yckee,2016-03-13 13:07:57-04:00,xiongchiamiov,,See also r/cscareerquestions.,4a6olj,t3_4a6olj,CARGLE,,Comment,1,0,1
d118wkf,2016-03-15 19:09:58-04:00,CARGLE,,Thanks!,4a6olj,t1_d0yckee,xiongchiamiov,,Reply,1,0,1
4a6l6z,2016-03-12 21:07:12-05:00,Batmantosh,"What's the difference between the Neural Network in AlphaGo, and a Neural Network I can download (for example a Python module), besides the training.","I'm wondering what's the difference between the neural network AlphaGo's and the one's you can download for free?

Is AlphaGo special because it has a ton of layers/nuerons and a ton of data it was trained on?

Or did they do something to the NN itself, so it's different than the open source NNs out there?",,,,,Submission,7,0,7
d0xtfia,2016-03-12 21:45:45-05:00,PastyPilgrim,,"Well, you can and should fine tune a network to the problem that you wish to solve (especially for a really hard problem like Go), so it's different in that respect. We don't know how Google has optimized the problem, but we do know that they've thrown tons and tons of hardware at the problem to generate a mind that you wouldn't be able to generate with a thousand years of computation on your desktop.

So, there's a similar theory behind most neural networks, but the implementation, training data, and hardware are all pretty important when it comes to a specific neural network.",4a6l6z,t3_4a6l6z,Batmantosh,,Comment,10,0,10
d0y7l7d,2016-03-13 10:20:00-04:00,videoj,,"AlphaGo is built, in part, on top of [TensorFlow](https://www.tensorflow.org/), Google's open source machine learning library ([source](http://googleresearch.blogspot.com/2016/01/alphago-mastering-ancient-game-of-go.html)).  So you can download it and build your own Go playing system.  Google's secret sauce is the ability to use [1000's of CPUs and 100's of GPUs ](https://en.wikipedia.org/wiki/AlphaGo#Hardware) to calculate moves in the limited time available.",4a6l6z,t3_4a6l6z,Batmantosh,,Comment,8,0,8
d16hd7t,2016-03-20 01:49:13-04:00,coconutscentedcat,,had no idea that much hardware was used for alpha go. Now that I know this I feel less thrilled about their win.. it's more powerful than intelligent. ,4a6l6z,t1_d0y7l7d,videoj,,Reply,1,0,1
4a44xe,2016-03-12 10:11:05-05:00,brunokim,"How would you create a breakable encryption, as the government is asking for?","Just to move it off the way, my personal view is that law enforcement can suck it up and consider encrypted devices a fact of life. Professional criminals are already encrypting their data, so creating a backdoor on the device level will just lead to an unbreakable blob anyway... and the information of every regular user as well.

Still, the techie in me wants to tackle this problem: **how to design an encryption that only law enforcement can break, given a warrant?** This system should be resistant to mass surveillance, and difficult enough for the government agency or adversaries to use without the consent of the software manufacturer.",,,,,Submission,11,0,11
d0xqlg8,2016-03-12 20:21:17-05:00,ohlson,,"The easiest way of doing this is probably to just embed a device unique key/id (every mobile device has at least one such id available) as a ""salt"" in the signature calculation of the software, and develop a special ""government OS"", that the hardware manufacturer signs on demand for one specific device at a time. This is not exactly **breaking** the encryption, but it has the same effect.",4a44xe,t3_4a44xe,brunokim,,Comment,2,0,2
d0y2oh0,2016-03-13 05:06:14-04:00,ohlson,,"Why the downvote? Compared to the other suggestions here (key escrow systems and secret sharing schemes) this option is very attractive. It does not require storing all encryption keys (imagine if that database leaks), it does not require a significant monetary investment, and the government cannot unlock a device without the manufacturer's consent. It's also very easy to implement: the hardest part is to keep the actual encryption key hidden inside the device, but that is a general problem which we have even today.

I'm not a cryptographer, but I find it highly unlikely that we will ever devise a *strong* encryption scheme that can be broken by select parties, without actually knowing the encryption key.",4a44xe,t1_d0xqlg8,ohlson,,Reply,2,0,2
d0x75cf,2016-03-12 10:21:20-05:00,brunokim,,"So, my simple and dumb solution would require that the manufacturer provide an incomplete key to the government, that would still have to spend significant resources to crack the device. This difficulty would be adjusted to represent, say, 1 million dollars in energy cost using ASICs, so the judge dispatching such a warrant would (we hope) make sure that the agency took every other measure they could before asking for the decryption key and sending a 1m bill to taxpayers.

I don't know enough about cryptography to create a way that this key can't be reused, but perhaps we can mitigate this risk by creating, say, 10k keys, each corresponding to a number of devices. So, even if law enforcement (or the NSA) were able to break in several phones after this, they would correspond to 0.01% of all users at random. I also don't know how feasible it is to provide an incomplete key per device, or how to secure this many keys from invasion.

The point is: breaking into a cryptographed device should still be hard, and only an institution with large resources should be able to do it. I don't see other ways to prevent abuse. Of course, once this tool exists, other countries would also make use of them, and perhaps Chinese law enforcement demands all keys, or the key to a congressman device :)",4a44xe,t3_4a44xe,brunokim,,Comment,1,0,1
d0xaw0a,2016-03-12 12:25:54-05:00,VainWyrm,,The most trivial problem here is that technology isn't static. What costs $1mm today costs $500k 18 months from now. In addition brute forcing a password is easily multithreaded. A hacker with a bot network can harness a massive amount of computing power.,4a44xe,t1_d0x75cf,brunokim,,Reply,8,0,8
d0ykowl,2016-03-13 16:55:56-04:00,conradsymes,,"> What costs $1mm today costs $500k 18 months from now.

Not completely true.

Moore's law has recently halted. Until the advent of quantum computers, 2^72 security will be secure from everyone without ASICs.",4a44xe,t1_d0xaw0a,VainWyrm,,Reply,1,0,1
d0ynrmv,2016-03-13 18:17:30-04:00,VainWyrm,,Computing speed isn't the only factor.,4a44xe,t1_d0ykowl,conradsymes,,Reply,1,0,1
d0xbnjj,2016-03-12 12:49:30-05:00,brunokim,,"> The most trivial problem here is that technology isn't static. What costs $1mm today costs $500k 18 months from now.

Well, manufacturers can promote this as planned obsolescence! Update your software or risk your privacy! Better yet, buy new hardware!

> In addition brute forcing a password is easily multithreaded. A hacker with a bot network can harness a massive amount of computing power.

I imagine that botnets are less efficient than ASICs, but I had no idea about the scale of botnets available out there, in the scale of millions of computers.
Still, they'd only be able to try to crack the incomplete key if the agency leaked it, which (we hope) would be a rare scenario, right?",4a44xe,t1_d0xaw0a,VainWyrm,,Reply,1,0,1
d0xjju6,2016-03-12 16:45:45-05:00,bo1024,,"> Well, manufacturers can promote this as planned obsolescence!

Your idea of costly break-in is cool, but unfortunately this part doesn't work. Once you send some message that's weakly encrypted, you can't fix it with a software update. That message is vulnerable forever to someone who records it, waits for hardware to improve, then brute-force cracks it.",4a44xe,t1_d0xbnjj,brunokim,,Reply,7,0,7
d0xc26l,2016-03-12 13:01:35-05:00,TotesMessenger,,"I'm a bot, *bleep*, *bloop*. Someone has linked to this thread from another place on reddit:

- [/r/cryptography] [AskComputerScience: How would you create a breakable encryption for law enforcement agencies?](https://np.reddit.com/r/cryptography/comments/4a4qxz/askcomputerscience_how_would_you_create_a/)

[](#footer)*^(If you follow any of the above links, please respect the rules of reddit and don't vote in the other threads.) ^\([Info](/r/TotesMessenger) ^/ ^[Contact](/message/compose?to=/r/TotesMessenger))*

[](#bot)",4a44xe,t3_4a44xe,brunokim,,Comment,1,0,1
d0xjpej,2016-03-12 16:50:19-05:00,None,,[deleted],4a44xe,t3_4a44xe,brunokim,,Comment,1,0,1
d0xm5qp,2016-03-12 18:04:40-05:00,Chappit,,Yeah the problem with this is that I don't trust the law enforcement teams to not leak their key at some point,4a44xe,t1_d0xjpej,None,,Reply,3,0,3
d0xkdk4,2016-03-12 17:10:26-05:00,zefyear,,"Numerous algorithms exist that permit for more than one key individually decrypt data, as well as algorithms that require coordination of N/M keyholders (where N and M are arbitrary values).

The former has many approaches but we already are aware of the most naive -- namely concatenating the data, each encrypted with two different keys.

The latter generally takes the form of Shamir's Secret sharing and might be ideal if several branches of the government as well as the corporation (for example) should have to collaborate in order to decrypt a piece of data.",4a44xe,t3_4a44xe,brunokim,,Comment,1,0,1
4a18g0,2016-03-11 18:01:11-05:00,Tman910,CS the way to go into GeoScientest or Spatial Statistics?,"So a little background on myself. I have about 2.5 years of Geospatial analysis experience because of the military (remote sensing and other airborne assets) and I absolutely LOVED it. 

That being said I've been looking at a B.S. in Geography while taking electives in GIS (not sure if i can minor online) and getting certs in GIS and data management to help improve my hire-ability when i get out of the military (say 5 years from now). I'm really intrigued by Spatial Statistics and GeoScience.

After reading a lot, it seems the combination of CS/Geo/GIS is the golden ticket for a combination for a GeoScientest or Spatial Statistics. I was wondering what you all thought so I can get a large overall picture of everything and all sides.

I'm looking for some foresight, experience and anything else anyone is willing to provide. I greatly appreciate everything. Thanks all.",,,,,Submission,8,0,8
d0xupvp,2016-03-12 22:25:59-05:00,spiral-galaxy,,"In case you haven't seen it: https://www.reddit.com/r/gis

Read its sidebar for more.",4a18g0,t3_4a18g0,Tman910,,Comment,1,0,1
49ym84,2016-03-11 07:39:36-05:00,Shibacoin,Can't figure out the difference between Computer Science and Data Science degree,"Hello,

I'm currently in high school and I can't decide in which area to study next year.
I'm very interested in technology since a decade, making stuff with technology everydays, but don't know how to code. I'm currently learning python.

I'm very very interested in entrepreneurship and I plan to create my startup in the next year.

But I can't figure out which thing to study. I'm very interested in computer science stuff, would love to learn it but I'm also very interested in data science, since data is described as 21st century oil.
CS contains a lot of mathematics, which is not a problem for me, but mathematics are not the thing I'm the most in love on earth. I prefer statistics than linear algebra ie, this is why I'm thinking about studying data science rather than computer science.
Also, it seems that there are way much graduated CS students each year than DS students, isn't it ? A DS could be a better choice, if data scientists are more rare.

Last thing is that I can't clearly figure out the difference between Computer Science and Data Science : CS seems to be a very large area while DS seems more focused.

Considering my background and my intentions, what do you advise me ?

Thank you a lot in advance.

NB : I'm living in Europe. Studying in America is not possible for me.",,,,,Submission,9,0,9
d0w5syc,2016-03-11 12:20:44-05:00,markgraydk,,"Actual degrees in data science are a pretty new thing so they differ more than your typical CS degree. I'd suggest you look more closely at what courses you are expected to take in either program and see if they match your expectations.

CS is not a bad degree for a data science career either (in either case you may have to get a masters or more for some jobs). It may be broad, as you say, but with the right choices you can make it very relevant. It's not clear what future employers will think about the two degrees for similar jobs. Perhaps they will favor one over the other for certain jobs or there will be few differences. One thing to note is that they likely will have more experience hiring people with the CS degree since it has been there longer. On the other hand, perhaps a new degree tailored data science will open other doors.

That there are fewer data science graduates doesn't necessarily mean it is better. It may just as well reflect that there are fewer jobs there, which is obviously true.

With all that said, I think data science will only grow in the future. I think it is entirely possible for you to get a career in the field with either degree if you want to.

oh, and don't do the business degree as some other commenter said. You really don't need it if you want to be an entrepreneur. ",49ym84,t3_49ym84,Shibacoin,,Comment,3,0,3
d0vxatj,2016-03-11 08:31:27-05:00,Hargbarglin,,"I think the people you need to ask are the professors and advisers at the college or university you are looking at getting a degree from. I've  been out of school for a while, but I recall there being like 30 variations of degree in technical computer related fields when I went, and I definitely could not tell you about the subtle differences between those degrees currently.",49ym84,t3_49ym84,Shibacoin,,Comment,2,0,2
d0vzypq,2016-03-11 09:57:18-05:00,videoj,,">I'm very very interested in entrepreneurship and I plan to create my startup in the next year.

Get a business degree, or at least minor in business.  

> I can't clearly figure out the difference between Computer Science and Data Science

Computer Science is the study of general computation.  It is a very broad field that covers everything from operating systems to machine learning.  There are numerous sub-fields that you can study within Computer Science, ranging from very theoretical/mathematical to very practical/engineering-oriented.

Data Science is the study of collecting information from large sources of data.  It is based in statistics and machine learning.  Its definitely getting a lot of hype right now, but be careful about predicting long-term use based on current hype.  I've seen a lot of hyped technologies disappear, only to be replaced by others.  Visit /r/datascience/ to learn more or read up at [Wikipedia](https://en.wikipedia.org/wiki/Data_science).
",49ym84,t3_49ym84,Shibacoin,,Comment,2,0,2
49vvps,2016-03-10 17:32:38-05:00,OutOfMemory,Question about A* and admissible heuristics,"Here's the short version: I'm writing an astrodynamics simulator, and my agent does an A* search (technically SMA*) to find the most efficient route from some initial state to a particular orbit. I'm in a bit of a jam, because once it's in orbit, the best possible action it can take is *nothing*.

This makes sense to me, because heuristics have to be overly optimistic. The cost to get to some orbit plus the estimated delta-v to get to the desired orbit will always be strictly less than the actual delta-v cost to get to the desired orbit.

Am I missing something? How do I make it sufficiently favorable to pursue the goal? My [heuristic](https://github.com/stevenvergenz/ksp-launch-sim/blob/astar/src/simulator.cpp#L409) is as perfect as I know how to make it. It's an exact solution to the problem that the simulator is solving numerically.

Thanks for the input!",,,,,Submission,7,0,7
d1heskx,2016-03-29 02:58:59-04:00,_ActionBastard_,,"Totally guessing here, but RK4's accumulated or local error could be responsible for an eventual overestimate, causing the heuristic to _become_ inadmissible.",49vvps,t3_49vvps,OutOfMemory,,Comment,1,0,1
49v2yl,2016-03-10 14:42:14-05:00,michael1026,What needs to be done when rehashing a hash table?,"I feel like I'm overthinking this (I hope I am), but if I'm rehashing a hash table, wouldn't I have to take out every element, rehash it, then reinsert it into the hash table? Or is there a simpler way?

If that's the case, how would I handle collisions when rehashing? I wouldn't be able to tell what elements contain the old hash and which have been rehashed.",,,,,Submission,6,0,6
d0vkj1m,2016-03-10 22:33:49-05:00,IAmNotNathaniel,,"Are you rehashing because you are getting too many collisions/too much load?

If you rehash, you'd make a new hash table, scan the old one, and insert into the newer, bigger table.

Collisions would be handled the same way, but since you are in a new table you don't have to worry about the old hash at all.

Then, of course, free the old hash memory.",49v2yl,t3_49v2yl,michael1026,,Comment,2,0,2
d0vkoax,2016-03-10 22:37:48-05:00,michael1026,,"> Are you rehashing because you are getting too many collisions/too much load?

The client decides when to rehash. I'm only writing the code that handles it. 

I didn't think about just making a new hashtable. I'll do that. Thanks!",49v2yl,t1_d0vkj1m,IAmNotNathaniel,,Reply,3,0,3
d17bdwd,2016-03-20 20:41:11-04:00,BenRayfield,,"Unless you use tombstones which build up wasted space until you must rehash... To remove x, you must first remove everything which hash-collisions with x and therefore has a different address in the hashtable than x. For example, if both b and c have collided multiple times and both collide at index d, and b is at index d now, you can remove both then hash c (still colliding at the same other indexs) into index d, and later you may hash b into some other index as it will find it has been evicted from index d and hash at least 1 jump farther.",49v2yl,t3_49v2yl,michael1026,,Comment,1,0,1
49traa,2016-03-10 09:59:31-05:00,Sarcon5673,Solving a Recurrence by Change of Variable?,"So I have this recurrence: f(n) = f(n/2) + sqrt(n), f(n) = 2 if n < 4

I tried the following: n = 2^m

f(2^m ) = f(2^m-1 ) + sqrt(2)^m

f(2^m ) = g(m)

g(m) = g(m-1) + sqrt(2)^m

r = 1 --> g(m) = A(1)^m + B*sqrt(2)^m = A + B*(2^m )^0.5

Now here's the part I'm not sure of: f(n) = A + B*sqrt(n)

Is this correct?",,,,,Submission,3,0,3
d0uult6,2016-03-10 11:44:29-05:00,Sarcon5673,,"Nevermind, it's correct. B = sqrt(2) + 2 using undetermined coefficients.

There's a problem with the question since f(n) = 2 if n < 4 implies that there is more than one solution since f(1) = 2 and f(2) = 2 and we only have one constant to solve for (A).",49traa,t3_49traa,Sarcon5673,,Comment,1,0,1
49tnub,2016-03-10 09:35:56-05:00,shivered-wolf,Do employers look at a highschool deploma for any conputer science job or do they look more at your certifications and degrees?,"Do employers look at a highschool deploma for any conputer science job or do they look more at your certifications and degrees?

I have several certification in networking. I also have my A+ but i might have an issue getting my deploma due to my school strict attendance polocy i might not be able to get it. I was thinking about getting my GED instead of going through senior year again. I was wondering if the GED might mess me up when getting a job later in life?

( im sorry for any typos. I am on mobile)",,,,,Submission,0,0,0
d0uqmuu,2016-03-10 10:09:24-05:00,SquirreI,,"They will be more interested in your spelling than your high school diploma, but how do you plan to get onto a degree course without it? ",49tnub,t3_49tnub,shivered-wolf,,Comment,5,0,5
d0uqxff,2016-03-10 10:17:11-05:00,shivered-wolf,,By getting my GED,49tnub,t1_d0uqmuu,SquirreI,,Reply,1,0,1
d0uzjtx,2016-03-10 13:32:24-05:00,ttstte,,It's always a possibility that they'll look but your accomplishments after your GED are much more important. It's hard to say what hiring practices are between different companies. Just stick to it and pull some good grades.  Congratulations on what you're doing and good luck!,49tnub,t1_d0uqxff,shivered-wolf,,Reply,1,0,1
d0v2pd3,2016-03-10 14:40:10-05:00,lneutral,,"I'm going to be honest: if you're looking to jump from a GED straight into jobs, it will be *very difficult*. If you have a college degree, especially a BS or higher, you can probably leave your high school / GED off your resume and it's a non-issue.

The best thing you can do: get through this, work hard and try to get into a college CS program, and get some extra experience when you can. Build projects on your own, contribute to open source, join some organizations.",49tnub,t3_49tnub,shivered-wolf,,Comment,2,0,2
d0v433g,2016-03-10 15:09:23-05:00,Aquifel,,"It's unlikely anyone will ever ask you any questions at all about your high school diploma/GED.  If you can appear intelligent and your resume is typo-free (seriously, man, turn on autocorrect), they'll assume you're smart enough with a few certs as well, I can't imagine them worrying about it at all.  If it's a very low level job, they might ask if you're a high school graduate but, i'd say if you have a GED, you're good to say yes and, I don't think anyone would fault you for it.

If you wanted to bypass the problem entirely, you could work towards an associates degree.   Ideally, you'd get a bachelors degree as it's tremendously more valuable but, that's not an option for everyone.  If you did it via community college, it's typically pretty cheap, with government grants, there's a good chance it could even be free.  No one is likely to ever ask about your high school situation if you have something more advanced, even if it's just a degree in progress, they'll probably be more interested in recent college transcripts as opposed to anything from your high school.",49tnub,t3_49tnub,shivered-wolf,,Comment,1,0,1
49t1z7,2016-03-10 06:20:57-05:00,int-main,[Q] Help learning Computer Networks (Physical Layer),"Hello,

So I started learning Computer Networks a few days ago. I am using textbook from Tanenbaum. I've started reading the chapter on physical layer and I feel like I am running into some problems.

I am finding it hard to grasp the concept of channel, bandwidth, significance of spectrum. Everything is looking like a abstract definition to me and I cannot figure out the real significance of things.

Is there some pre-requisite I'm missing or should I read some other resources? I want to be able to better understand and visualize the concept. Any help would be appreciated. ",,,,,Submission,6,0,6
d0v2dzt,2016-03-10 14:33:20-05:00,1nf,,"I would recommend you try to find [this book](http://www.pearsonhighered.com/educator/product/Data-and-Computer-Communications/9780133506488.page) at your library and give it a read. It goes into more technical details about the physical layer than Tanenbaum does.

Also unless you're studying networking at the theoretical level, you could just go through these topics quickly, unless of course you plan to dive into the physical layer at a later stage and become interested in communications engineering. 

If you're in it for practical networking, then read them for what they are as the more practical parts like protocols and applications come at higher layers.",49t1z7,t3_49t1z7,int-main,,Comment,1,0,1
49svw3,2016-03-10 05:11:14-05:00,jerrre,Denormal floating point calculations question,"I'm not sure I'm at the right place with this question, so please tell me if there's a better place.

I'm experiencing big efficiency problems with denormal calculations, that can not be solved algorithmically. The old trick was to check results for denormals and set them to zero in between calculations. Of course this also takes a lot of time, but is better than letting the CPU hang up on denormal calculations. 

Now we only target x86 and x64 platforms, with at least SSE, so I found out about the Denormals Are Zero (DAZ) flag and Flush To Zero (FTZ) flag, supported on nearly all processors right now which get rid of the denormal problems. 

The problem is: how can I be sure my compiler does not generate any floating point instructions that are not SSE and ignore the flag (ie x87 FPU instructions). I'm compiling on XCode 7, and Visual Studio 2013. ",,,,,Submission,4,0,4
d0vsscw,2016-03-11 04:13:18-05:00,Quantumtroll,,"In my experience, all modern C/C++ compilers flush denorms to zero when *any* optimisation is done. I've taught a high-performance computing course for a number of years, and no student has shown me a counter-example, so I believe this to be true.  

So if you're experiencing slow denormal calculations, double-check your -O flags! If you're using -O1 or higher, *please* let me know so I can duplicate and update my course material.

> how can I be sure my compiler does not generate any floating point instructions that are not SSE and ignore the flag

The only way to be sure of anything your compiler does is to ask it to generate assembly, and then read the assembly. That's not as bad as it sounds, because you can easily search the assembly for floating point operations and identify them as SSE instructions. 

If you want to check the FTZ flag, read [this page](https://software.intel.com/en-us/articles/x87-and-sse-floating-point-assists-in-ia-32-flush-to-zero-ftz-and-denormals-are-zero-daz) and then write some inline assembly to read out the MXCSR register in the parts of the code you're worried about. ",49svw3,t3_49svw3,jerrre,,Comment,1,0,1
d0vto9x,2016-03-11 05:12:50-05:00,jerrre,,"Hi, thanks for your reply. Nice to know there are more people interested in this.

Sadly my experiences do not agree with your point. Here is a quick test I made for my self, when compiled with -O3 on XCode 7 (LLVM compiler) I can clearly see the difference in execution time before and after setting the flag: http://pastebin.com/Fwqv3qPE.
I'm running it on an i7 processor.

I did look into the generated assembly too, but this is not a proof that it will never use x87 instructions, only that in the case I look at it does not. ",49svw3,t1_d0vsscw,Quantumtroll,,Reply,1,0,1
d0vu31l,2016-03-11 05:40:54-05:00,Quantumtroll,,"Well shit. I assume you've run the test in the opposite order as well?

Anyway, nice going with the _mm_getcsr intrinsic, I should've known there was an easier way of getting that register. 

I'm going to have lunch soon and will ask my colleagues if they have any ideas. I'll also have a go at running your test on my mac and (more importantly) the Linux clusters I'm supposed to be an performance expert on. ",49svw3,t1_d0vto9x,jerrre,,Reply,1,0,1
d0vuin6,2016-03-11 06:10:05-05:00,Quantumtroll,,"Ok, results time!

With LLVM on my mac, I get the same results as /u/jerrre, also if I do things in a different order. Optimised, the speed difference is greater than 10x, which is typical for denorm slowness.

With gcc on Scientific Linux, I get the same results again. 

~~With the Intel compilers, I get fast speed with or without optimisation flags!!~~ No wait, the Intel compilers don't have -O0 as default. If I specify -O0, then it is slow again. I guess with -O2, the Intel compiler changes the flag back again.

I've also confirmed that the behaviour depends on the test. The old experiment that I used in the course works in the way I described in my first response. I've put it here below for reference:

    #include <stdio.h>
    #include <stdlib.h>
    int main(int argc, char **argv)
    {
        int i = 3:
        float tiny = *((float*)&i):
        printf(""%e\n"", tiny):
        for (i=0: i<10000000: i++)
        {
            tiny += tiny/2.0:
            tiny *= 0.66:
        }
        printf(""%e\n"", tiny):
    }",49svw3,t1_d0vu31l,Quantumtroll,,Reply,2,0,2
49rs0m,2016-03-09 22:37:51-05:00,CheddaShredda,Help Figuring Out if Language is ambiguous,"Describe the language generated by the following grammar with start symbol S and determine whether it is ambiguous. If the grammar is ambiguous, give two different parse trees for some string in the language. If the grammar is unambiguous, provide a brief justification why you think the grammar is unambiguous.

S -> A1A

A -> 0A | A0 | 0

The language generated by the grammar appears to be some amount A of 0's followed by a 1 followed by the same amount A of 0's
So I think the language is L = {0^(i) 1 0^(i) | i > 0}
I am not really sure how to determine whether this language is ambiguous or not
",,,,,Submission,5,0,5
d0uca54,2016-03-09 23:25:19-05:00,lneutral,,Try deriving a parse tree from a couple strings. Can you ever reach a point where you get to make a choice? ,49rs0m,t3_49rs0m,CheddaShredda,,Comment,4,0,4
d0ud3k9,2016-03-09 23:49:40-05:00,teraflop,,"> The language generated by the grammar appears to be some amount A of 0's followed by a 1 followed by the same amount A of 0's

Nope, the string ""0100"" is in the language.",49rs0m,t3_49rs0m,CheddaShredda,,Comment,2,0,2
d0ueh1i,2016-03-10 00:34:57-05:00,crookedkr,,"What the language describes doesn't really matter (except that it might give you some intuition on whether it is ambiguous). If it is ambiguous it has more than one parse tree. Can you make a string more than one way? (Hint you can. In fact the shortest string in this language is ambiguous, the proof is left to the reader.)",49rs0m,t3_49rs0m,CheddaShredda,,Comment,2,0,2
d0ufrww,2016-03-10 01:23:32-05:00,Stibitzki,,"Unless I'm missing something, the shortest string you can make is ""010"" and I only see one way to make it.",49rs0m,t1_d0ueh1i,crookedkr,,Reply,1,0,1
d0umdw9,2016-03-10 07:41:45-05:00,crookedkr,,Oops you are right. 010 only has one parse tree. 00100 has more than one.,49rs0m,t1_d0ufrww,Stibitzki,,Reply,2,0,2
49q6sw,2016-03-09 16:10:37-05:00,ajaatshatru,Proving a language is not Recursively Enumerable.,"L3 = { <M> | M is a Turing Machine and |L(M)| = 1}

We have to prove that this is not R.E. and not co-R.E.

Any idea how to approach this?",,,,,Submission,4,0,4
d0u5lho,2016-03-09 20:42:17-05:00,_--__,,"You might be tempted to try Rice's theorem, but this won't work.

To prove that it is not RE, either give a reduction from a language which is known to not be in RE to L3, *or* (somewhat easier perhaps) give a reduction from a language known to not be in co-RE to the complement of L3.  

To prove that it is not co-RE either give a reduction from a language which is known to not be in co-RE to L3 (probably easiest), *or* give a reduction from a language known to not be in RE to the complement of L3.  

An example of a language known to not be in co-RE is HALT - why?",49q6sw,t3_49q6sw,ajaatshatru,,Comment,1,0,1
49oys8,2016-03-09 11:58:00-05:00,timlee126,Survey of Courses and their books that are equivalent to the old SICP course in MIT?,"The previous SICP course in MIT (http://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-001-structure-and-interpretation-of-computer-programs-spring-2005/)
has influenced many computer science students and software engineers.

So has its text book Structure and Interpretation of Computer Programs, 2nd ed., by Abelson, Sussman, and Sussman.

But the popularity of the course and the textbook has been declined.

I am hoping to improve my programming skills by following a book. So I wonder what courses and their textbooks are offered and used in  universities with good CS programs (for example, see below, top 25 US CS programs), with the same or similar or better goal and audience to or than SICP:

- MIT, 
- Stanford, 
-  CMU, 
- Berkeley, 
- UIUC, 
- Cornell, 
- Princeton
- Washington
- Caltech
- UCLA, 
- Wisconsin - ​Madison 
- Texas - Austin
- Harvard, 
- Brown,
- Gatech 
- Maryland - College Park, 
- Michigan - ​Ann Arbor 
- UCSD
- Columbia
- Pennsylvania 
- Rice 
- Purdue
- USC
- Yale
- Duke
- ... (not necessarily US universities)

Thanks.",,,,,Submission,3,0,3
d0u9y51,2016-03-09 22:27:00-05:00,calinet6,,"SICP has not decreased in usefulness despite falling out of favor. Do the older course and you will still learn a great deal!

I can personally recommend Berkeley's CS61A version of the same.",49oys8,t3_49oys8,timlee126,,Comment,3,0,3
49osep,2016-03-09 11:21:50-05:00,BenRayfield,Which kinds of forking can be done fast in parallel?,"Forking can be as simple as adding 1 to an int. Depending on how many of its low bits are already 1, it will change more bits. Multiply takes longer for the same reason recursively. Multiplying small integers is faster than multiplying big integers, with the same total bits.

When programmers say forking, they usually mean a big change in memory address. That is slower than multiply, but they're both forking.

Somewhere between are things that happen at closer addresses or are somehow optimized for common hardware. If I read an int from an array and jump to that memory address,  it will be fast if that part of memory is already cached, which it will be if its near something I read andOr wrote recently. But writing can invalidate the cache, or it may only write through to the next slower layer of cache. Theres so many things to consider.

Which kinds of forking can be done fast in parallel?",,,,,Submission,0,0,0
d0tn14g,2016-03-09 13:24:55-05:00,worst,,">Forking can be as simple as adding 1 to an int. Depending on how many of its low bits are already 1, it will change more bits.

How is this forking?

>When programmers say forking, they usually mean a big change in memory address. That is slower than multiply, but they're both forking.

Oh... Ok. I'm not sure exactly what meaning of ""forking"" you are intending, but from my experience forking is when you create a new process by copying the current one (at least in UNIX, via fork()).

Yes, this involves memory access and stuff, but, I still don't see any link to addition and multiplication.

Can you clarify your meaning?

>Somewhere between are things that happen at closer addresses or are somehow optimized for common hardware. If I read an int from an array and jump to that memory address,  it will be fast if that part of memory is already cached, which it will be if its near something I read andOr wrote recently. But writing can invalidate the cache, or it may only write through to the next slower layer of cache. Theres so many things to consider.

>Which kinds of forking can be done fast in parallel?

Sure, cache hits and misses happen, and you design parallel algorithms/systems to avoid that.

But again, I'm not sure what this has to do with forking...

Do you mean to be asking what kinds of problems/programs/operations/something else that's not forking can be done efficiently in parallel?",49osep,t3_49osep,BenRayfield,,Comment,7,0,7
d0tq55b,2016-03-09 14:34:06-05:00,BenRayfield,,"Fork is anything that could go 2 different ways. Adding 1 to an int forks at each unknown bit of the int until finishing the base2 carry.

    100111 + 1
    100112
    100120
    100200
    101000

We didnt know in advance it would finish without considering the highest 2 bits. The early fork allows a chip to proceed to its next work.

This kind of thing is more likely to happen in int multiply, which varies in how many compute cycles it takes, than plus, because multiply has more forks where the logic could go different ways.

> from my experience forking is when you create a new process by copying the current one (at least in UNIX, via fork()).

Thats a kind of forking. A process moves lots of data around when forked. Multiply just moves a few bits. These may have different opcodes in some chips and be the same kind of thing in other chips.

> Sure, cache hits and misses happen, and you design parallel algorithms/systems to avoid that.

Running lots of threads or processes with a central bus tends to bottleneck the memory bandwidth. GPUs, APUs, PCI Express bus, Parallella chips, etc, have been designed with higher parallel bandwidth, but its hard to know what variety of hardware all your users have.",49osep,t1_d0tn14g,worst,,Reply,-1,0,-1
d0tqaiu,2016-03-09 14:37:22-05:00,zefcfd,,"> Fork is anything that could go 2 different ways. Adding 1 to an int forks at each unknown bit of the int until finishing the base2 carry.

I think you're talking about branching logic. ""Forking"" is a pretty ubiquitous term in computer science that usually refers to ""process forking"", which copies the execution context of a process, and starts a new child process with that context.

https://en.wikipedia.org/wiki/Fork_(system_call)


im not saying your term is wrong, or that it's incorrect necessarily. just that it's kind of confusing.

> A process moves lots of data around when forked.

yes, but that's one small aspect of process forking. 

It seems you are asking about something at the computer architecture level of things, which is a layer or two below what the other commenters thought you were talking about.",49osep,t1_d0tq55b,BenRayfield,,Reply,7,0,7
d0tw1r6,2016-03-09 16:40:11-05:00,worst,,">Thats a kind of forking. A process moves lots of data around when forked. Multiply just moves a few bits. These may have different opcodes in some chips and be the same kind of thing in other chips.

Naw. It *is* forking, at least as far as computer science and programming is concerned.

I (kinda) understand what you are asking about, but, you are using a word in a way I've never seen it used (I have a PhD in computer science and am employed as a researcher: it's unlikely I would be unaware of a common alternative usage of ""fork"").

As /u/zefcfd mentioned, it seems like you are talking about branching, not forking.

The first step to getting an answer to your question is to use correct terminology...

>Running lots of threads or processes with a central bus tends to bottleneck the memory bandwidth. GPUs, APUs, PCI Express bus, Parallella chips, etc, have been designed with higher parallel bandwidth, but its hard to know what variety of hardware all your users have.

I'm not really sure where you are going here, but anyways, parallel algorithms are often designed for either an idealized system or tailored to a specific architecture. General algorithms will usually work in general, but to squeeze the most performance out of a particular system you usually need to implement and often design around said system's architecture.

So, again, are you asking what kinds of problems/programs/something else can make use of parallelism?

If so, perhaps I can sketch out an answer for you, if not, then please try clarifying again...
",49osep,t1_d0tq55b,BenRayfield,,Reply,2,0,2
d0udgsy,2016-03-10 00:01:03-05:00,BenRayfield,,Forking vs branching - Just because a thing is normally called 2 names depending if its big or small doesnt mean its a different kind of thing other than the size.,49osep,t1_d0tw1r6,worst,,Reply,-1,0,-1
d0uh1tg,2016-03-10 02:19:42-05:00,worst,,"A fork and a branch are two different things. It has nothing to do with ""big"" or ""small"". Just because you think they are the same thing doesn't make them so. A car is not the same thing as a spaceship even though they both are capable of moving from point A to point B...

Based on your obstinance and refusal to accept basic stuff like correct terminology, I'm going to go ahead and assume you are a troll.

Enjoy your life!",49osep,t1_d0udgsy,BenRayfield,,Reply,4,0,4
d0vi363,2016-03-10 20:55:06-05:00,BenRayfield,,A fork does both. A branch is smaller and often does both until figuring out which is the main branch (predictive branching).,49osep,t1_d0uh1tg,worst,,Reply,0,0,0
d0xvboe,2016-03-12 22:45:27-05:00,__cxa_throw,,"If you ask anyone in software what forking is they are going to assume process forking.  If you mention branching they are going to assume a conditional jump.

Branch prediction won't execute both paths at once on any traditional architecture: it picks what it thinks is the likely path and is able to backtrack if it was wrong.
",49osep,t1_d0vi363,BenRayfield,,Reply,2,0,2
49m6fx,2016-03-08 22:27:16-05:00,gabbathehut,can these screenshots help you help me understand why im getting a restart loop?,,,,,,Submission,0,0,0
d0sz8js,2016-03-08 22:58:48-05:00,lneutral,,You probably want /r/techsupport. ,49m6fx,t3_49m6fx,gabbathehut,,Comment,6,0,6
d0t0kpq,2016-03-08 23:35:09-05:00,zombarista,,Your hard drive took a shit on you. Try a boot CD to recover files and maybe a utility like SpinRite could recover the damaged file(s).,49m6fx,t3_49m6fx,gabbathehut,,Comment,1,0,1
49l6qp,2016-03-08 18:22:17-05:00,MilkMakesMePoop,How was my old 16 color graphics card able to display 256?,"When I was a kid I had a CompuAdd PC for my first computer. I managed to get Windows 95 to run on it in 16 colors. I was one of those kids who poked around with everything until it broke, so I tried every driver available in the device manager (having to reinstall Windows several times because I lost display) until one actually managed to display 256 colors. I know a tiny bit about computer architecture and how old school graphics worked, but can't fathom why this worked. Did this driver use my cpu to emulate 256 colors somehow? The CompuAdd was definitely advertised as having only 16 colors. I've wondered this for 2 decades. ",,,,,Submission,17,0,17
d0sozd9,2016-03-08 18:38:40-05:00,angererc,,"Not that I would know anything about that, but [this CompuAdd ad](https://books.google.de/books?id=jGnF7KJsyBQC&pg=PA348&lpg=PA348&dq=compuadd+pc+256+colors&source=bl&ots=9OAAu4gmVK&sig=wbILr2YHNVP7YtxUc_3FSmkIvFM&hl=en&sa=X&ved=0ahUKEwist7GJoLLLAhXqbZoKHZ2hAl8Q6AEIHTAB#v=onepage&q=compuadd%20pc%20256%20colors&f=false) from 1989 lists a VGA card with 16/256 colors. Maybe you had one of those?",49l6qp,t3_49l6qp,MilkMakesMePoop,,Comment,6,0,6
d0spbou,2016-03-08 18:47:52-05:00,MilkMakesMePoop,,"I got it free in 1995, so 1989 could be right. If no one has a better explanation, then I'll assume that you're correct. ",49l6qp,t1_d0sozd9,angererc,,Reply,5,0,5
d0sop7q,2016-03-08 18:31:08-05:00,lneutral,,"I can't say for sure without knowing specific model information, but it sounds like you stumbled upon an ""undocumented feature."" The CPU does not influence display appearance at all, so there's no way you could have changed the limitations of the display hardware in software.

The only other possibility I can think of is that the 256 colors might have been [dithered](https://en.wikipedia.org/wiki/Dither) from 16, but that seems unlikely.",49l6qp,t3_49l6qp,MilkMakesMePoop,,Comment,4,0,4
d0spa86,2016-03-08 18:46:47-05:00,MilkMakesMePoop,,CompuAdd 810. I google image searched it and found one that looks like it on a forum that mentions - get this - VGA hacks! http://www.vintage-computer.com/vcforum/showthread.php?11131-post-your-nudies!,49l6qp,t1_d0sop7q,lneutral,,Reply,1,0,1
d0spqqs,2016-03-08 18:59:06-05:00,lneutral,,"Looks like quite a few of them had hardware that supported greater than 16 colors. Based on the [VGA](https://en.wikipedia.org/wiki/Video_Graphics_Array#Standard_graphics_modes) Wikipedia entry, at least one of the standard VGA modes had 256 colors - maybe that was it?

Hard to find the appropriate documentation.

At any rate, I'd say you probably found a feature of the hardware with questionable software support, at least under Windows.

That's no small feat! A pretty cool story, for sure.",49l6qp,t1_d0spa86,MilkMakesMePoop,,Reply,1,0,1
d0szwh1,2016-03-08 23:16:38-05:00,MilkMakesMePoop,,"I'm inclined to say you're right! I'll call this case closed! Regardless, I'm still proud of my ten year old self for exploring the unknown and testing everything! I was super stoked when I managed to get my PC to display 256 colors. That's a feeling that I can't fully relay to kids these days. I later managed to ""hack"" my school's ""type to learn"" program and got expelled for it! Totally worth it. 

They gave us access to the file system via Microsoft word. I figured out that I could click ""save as"" and then right click and select ""open file location."" That meant I could access the type to learn save games of my fellow students. I was told that when I finished ""type to learn"" that I had to start over. Oh, I started over alright. I started over and then copied my save game file over the other students save games. I figured that would put them at the last level with me. Nope! It just broke the program. Boom! I'm expelled. What the fuck ever. Catholic school sucks a anyway. ",49l6qp,t1_d0spqqs,lneutral,,Reply,2,0,2
d0t3m9v,2016-03-09 01:15:21-05:00,Arrgh,,"42 year old reporting in.

Lots of VGA cards supported 16 colours at 640×480, and 256 colours only at 320×200 (MCGA mode). Later ""SuperVGA"" cards could do 800×600... I can't remember how many colors my old Trident 8900c could do at that resolution. :)",49l6qp,t3_49l6qp,MilkMakesMePoop,,Comment,3,0,3
d0wxg6r,2016-03-12 01:15:30-05:00,MilkMakesMePoop,,"That makes a lot of sense. I wish I could remember what kind of resolution I was capable of. I do recall that only one res was capable of displaying 255 colors. Regardless, thanks for your input!!! It's nice to find a sub like this where I can post a stupid/ill informed question and get multiple intelligent, thoughtful answers.  ",49l6qp,t1_d0t3m9v,Arrgh,,Reply,1,0,1
49i71f,2016-03-08 07:04:11-05:00,fortmac,"How much do software projects, like Facebook, change over their lifetime?","When I ask this question I'm thinking about big big software projects like Facebook or Amazon.  If your coding the next Snapchat in your basement - how much of that code would you expect to see 10 years later when you have 1 billion users and teams of engineers working on the project?  Do sites, like FB or Amazon, ever completely change their backend with little notice from the user?",,,,,Submission,8,0,8
d0sk7t9,2016-03-08 16:42:41-05:00,Ghildetrist,,After 10 years I would expect to see almost 0 code from the original code base. At a large company the lifespan of code in my experience around 4 years. After that the old code can't scale efficiently so it needs to be replaced and great effort is put into making sure the end user doesn't notice.  ,49i71f,t3_49i71f,fortmac,,Comment,4,0,4
d0s9kmf,2016-03-08 12:50:32-05:00,theobromus,,"Having worked on one of the huge webscale services. I'd say both a lot and not that much.

A lot in the sense that after 5-10 years with a decent team, you write tons of new code. Most of the time you build on top of what's already there, but you still find people making changes to the oldest code (perhaps they need to store new information through the database layer, etc.).

And perhaps every 10x growth in scale, you find the need to really reevaluate the architecture of the service to make it work at that scale. To use FB as an example (I have no inside information about it) - when it started it was a basic PHP/MySQL system I believe. I still remember when each university had a separate domain name (and at busy times the site was quite unresponsive). I suspect that meant that the profile information databases were sharded by university (like Harvard had one DB and Yale had another). You could be friends across the whole site, but groups only could be within a school. Furthermore, the only picture you could post was your profile, and I don't think it held on to the history.

Over time most of these restrictions were lifted. I suspect this required a few rearchitectings. First I think they started sharding the DB based on a more generic hash function. And they probably built an asynchronous queue based eventual consistency system. They had to build a blob storage system to store the volume of photos.

And then at some point they built things like Cassandra, which basically moved them away from SQL databases entirely (It's not clear to me if they use Cassandra for all schematized data or if they still have some MySQL).

Along with this, they probably had to build a system to rollout their code to tons of servers. And systems to monitor the health of everything.

At the time they were growing, they probably had to build most of this stuff themselves, although now it's quite possible to build on top of AWS or Azure services and get huge scalability very easily if you use them appropriately.

Now about how things don't change - I don't know about FB (I tend to guess that almost all of the original stuff has been rewritten), but on most software projects (even big web services), the path of least resistance is to add new features on to what already exists. I've personally seen how this creates strange designs as features are added and removed. Once this has gone on for a while it can become extremely difficult to make fundamental changes to the code (often it's easier to change things like deployment etc.). Any given developer really can't know all of the consequences of some changes. And even if you have unit tests, they may break because they had overly strong assumptions about the way the system works internally.",49i71f,t3_49i71f,fortmac,,Comment,2,0,2
d0ryvh4,2016-03-08 07:39:12-05:00,vz83,,"Depends on the project. Facebook is still (partially) based on PHP but a lot of new stuff has been built on top of that. Windows on the other hand is still based on the NT architecture. So long story short: it depends on the size, complexity and nature of the project. ",49i71f,t3_49i71f,fortmac,,Comment,2,0,2
d0s37c8,2016-03-08 10:14:00-05:00,WalrusForSale,,"The practical side of your question seems to be directed towards scalability. I can't tell you much, but you can search that and find plenty of information",49i71f,t3_49i71f,fortmac,,Comment,1,0,1
d0s6uhq,2016-03-08 11:46:37-05:00,Xxyr,,Generally speaking code is only changed when it needs to be changed. So long as that really old code still does its job it'll keep running ,49i71f,t3_49i71f,fortmac,,Comment,1,0,1
49ffqh,2016-03-07 18:23:43-05:00,BenRayfield,"How much slower must it be, if optimized well, for a database to have only 1 column and all queries refer to parts of it (which would normally be in separate columns) by wildcards or other complex functions?",,,,,,Submission,0,0,0
d0rg2o9,2016-03-07 19:33:29-05:00,dandrino,,"Performance would be much worse. Assuming a relational database, you would not be able to make queries on an index, which means that all searches would be O(n) linear searches against a unary predicate.

This concept makes slightly more sense in the NoSQL world, where each entry is essentially a key value pair. This representation is not terrible if you are doing infrequent, high-latency, high-volume data processing (think map reduce). This approach, however, would not be suitable for quick database retrieval.",49ffqh,t3_49ffqh,BenRayfield,,Comment,3,0,3
d0xvlt7,2016-03-12 22:54:39-05:00,__cxa_throw,,"It sounds like you're talking about packing some complex type into a string or binary column and then picking that apart during queries.  If your query engine lets you build indices on complex statements (not many do) you might be able to get away with this.  But you'll end up with a lot of indexes that will slow down loads and deletes.

If you can't build indicates you're almost certainly going to force a full table scan in every query you run and evaluate your complex function on every tuple in each query.  The only time you might not hit the full scan is if you have a limit statement with no order bys, joins, or aggregations in the query.

Your best bet is to take advantage of the ""explain plan"" feature that most databases provide.  They typically give a cost estimate of the plan as well as let you know if it's going to do a full scan (really bad in this case) and if it's smart enough to use indexes you may have made.",49ffqh,t3_49ffqh,BenRayfield,,Comment,1,0,1
49ee5i,2016-03-07 14:48:54-05:00,ocawa,60fps monitor fetches video data at 1/60 frequency. What does 192kbps fetch audio data at?,"I'm trying to write something to converts sound into video but since 144hz have more frames per second than 60fps, I'm wondering if it would matter to ping at different rates. If audio plays 1 sound every 1/60 s always, then there would be no need to write another program for 144hz or 80hz displayes. 

if the display fetch rate is slower than the audio fetch rate, is it recommended to average all the audio fetches and then put that into the video frame, or should i just use the audio fetch data that is fetched on the same time that the video fetches the video data?",,,,,Submission,6,0,6
d0r6ie3,2016-03-07 15:43:01-05:00,sandwichsaregood,,"Digital audio doesn't quite work like you're imagining. Video at 60 Hz works because your brain does something like interpolation between frames ([persistence of vision](https://en.wikipedia.org/wiki/Persistence_of_vision)), which helps make the video look smooth. Not a doctor, but as far as I understand perception of sound isn't nearly as generous and you have to sample an analog waveform at a *far* higher rate than video in order for it not to sound strange. 

Digital audio is most commonly sampled at [44100 Hz](http://wiki.audacityteam.org/wiki/Sample_Rates), however the way it's actually played back by the sound card is to run it through a digital to analog converter (DAC), which effectively turns it into an analog (continuous time) waveform that drives your speakers and then ultimately your eardrums.

That said, if you just want to convert sound to video you don't necessarily need to have the full sampling rate of the audio. Even if you made a video at 44100 fps and played it back on a device capable of displaying it, your eyes wouldn't really be able to perceive the difference. You'd probably be OK just sampling the audio at 60 Hz: in theory this might create some temporal [aliasing](https://en.wikipedia.org/wiki/Aliasing) in terms of recreating the true audio signal, but your brain can't perceive it anyway.

tl:dr - your eyes and ears have vastly different bandwidths.",49ee5i,t3_49ee5i,ocawa,,Comment,8,0,8
d0r7t1u,2016-03-07 16:12:16-05:00,ocawa,,"hmmmm, that makes sense. 44100hz sounds like a ton of data, but soundfiles are relatively small compared to video files at HD resolution. I was thinking since that's true I could encode a soundfile into video format. But to make the video able to be translated into something like 44100hz seems not possible and contradictory if sound files hold less data than video",49ee5i,t1_d0r6ie3,sandwichsaregood,,Reply,2,0,2
d0r8o57,2016-03-07 16:31:40-05:00,sandwichsaregood,,"For mono audio, it'd effectively be a grayscale single pixel. Roughly, audio sample data is just a single value for each sample corresponding to the amplitude of the waveform. On the other hand, (raw) video is a snapshot of a 2D array of values, where each value corresponds to a pixel color which, in RGB encoding, is further represented by 3 values. So for raw RGB video, it's 

> (horizontal resolution) * (vertical resolution) * 3 * (per color bit depth) * (framerate)

bits of data per second. Raw audio is just

> (number of channels) * (sample rate) * (per channel bit depth)

bits per second, which is way less for e.g. stereo audio.

Sounds to me like what you want to make is basically an audio visualizer, like you see in a lot of media players. You have to figure out how to map your audio stream into a picture. A simple first attempt at visualizing 44100 kHz audio might be something like what an oscilloscope would plot:

1. Divide your 44100 samples in a second into 245 chunks of 180 values.
2. For i=1..243, plot chunks i, i + 1 and i + 2

(the overlap is just to smooth things out a bit). Most fancy visualizers work by coming up with a function from a [discrete] 2-D space (time * amplitude) to a 6-D space (time * x * y * r * g * b) that looks cool when plotted, but really in a sense it's adding information (technically not quite right, but you get the idea).",49ee5i,t1_d0r7t1u,ocawa,,Reply,7,0,7
d0ramww,2016-03-07 17:17:49-05:00,ocawa,,wow thanks for the in-depth answer :),49ee5i,t1_d0r8o57,sandwichsaregood,,Reply,2,0,2
d0rd3tc,2016-03-07 18:18:15-05:00,earslap,,"Think of it like this: CD Quality audio has 44100Hz sampling rate at 16bit bit depth. That means, for every second of audio, you have 44100 numbers between 0 and 65536 (16 bit). If you wanted to convert a second of audio directly to image without any meaningful translation, you could fill a square with size 210px X 210px. This square would have 44100 pixels in it, and each pixel would have a single audio sample (each sample represents around 0.022 milliseconds of audio). The whole square would contain a second of raw CD quality audio.",49ee5i,t3_49ee5i,ocawa,,Comment,3,0,3
d0r6vpz,2016-03-07 15:51:15-05:00,anamorphism,,"well, first, you're confusing bit rate with sampling rate. bit rate just affects how much data you're sending for each 'frame', not how often those 'frames' are being sent.

the most common sample rate for audio you'll come into contact with is 44100 Hz. so, yes, some type of aggregation would probably be desirable.

you're also comparing monitor refresh rate to frame rate. these two things don't have to be the same. making sure your frame rate stays at some even multiple of your monitor's refresh rate is called vertical sync. you don't necessarily need to worry about it. you could have your app always display at 30, 60 or whatever fps if you want (a lot of games do this). the only real problem that can arise from that is called screen tearing.

most operating systems are going to have a way for you to acquire the refresh rate of the attached display device. you could get that data and tailor your app to match that rate without having to make separate programs.",49ee5i,t3_49ee5i,ocawa,,Comment,2,0,2
d0r7iyf,2016-03-07 16:05:50-05:00,ocawa,,i thought bit rate is how much data you're sending in 1 second regardless of how many frames you have. ,49ee5i,t1_d0r6vpz,anamorphism,,Reply,1,0,1
d0r7wne,2016-03-07 16:14:31-05:00,anamorphism,,"it is, but that affects how much data per frame you're sending. it's basic arithmetic.

for example, cd audio is 44.1kHz sample rate at 1411.2 Kbps bit rate because each sample is 16 bits and there are 2 channels (stereo). 16 * 2 * 44100 = 1 411 200 bps = 1411.2 Kbps.",49ee5i,t1_d0r7iyf,ocawa,,Reply,2,0,2
49cefh,2016-03-07 07:30:50-05:00,avaxzat,Credible research on machine superintelligence?,"I've been looking for credible research papers that discuss the (im)possibility of machine superintelligence (e.g. the ""Singularity"") and its implications for a while now. So far the only literature I've found is philosophical papers by authors of dubious qualifications on the subject (usually published on some personal webpage and thus probably not even peer-reviewed), books and popular media that vulgarize the entire field of computer science. I haven't found any hard peer-reviewed papers by knowledgeable authors published in a credible journal, which is what I'm desperately wanting.

I'm not trying to be pedantic or condescending here. It's just that I'm tired of seeing these discussions about machine superintelligence being manipulated by popular media and people who simply cannot know what they're talking about. Or, even worse, people that should know what they're talking about but may just be going along with the popular opinions in order to get publicity. I've had enough.

What I'm interested in are the facts. Specifically, I'd like to find actual credible academic research papers from trustworthy sources that clearly define machine superintelligence, discuss its (im)possibility and the potential implications. I simply cannot take any of the fear-mongering about Skynet and whatnot seriously until I read papers like this that focus on cold, hard facts. Books would be fine too, but they'd have to rely on the aforementioned papers as their sources and make credible interpretations of them.",,,,,Submission,3,0,3
d0qtij5,2016-03-07 10:43:36-05:00,dragonnyxx,,"The only honest-to-God hard facts around machine intelligence are:

1. The only real way to prove that a Turing machine could ever match human intelligence would be to build one that did.
2. We aren't there yet.
3. Some people believe we will never get there.
4. Even confronted with a hypothetical Turing machine that outperforms humans on any mental task, many people would still dismiss it as ""just crunching numbers, not really intelligent"".
5. Even though we can't prove that a Turing machine could ever equal humans on mental tests (leaving aside the question of whether it is ""truly"" intelligent), it is obviously possible for computers in general (though not necessarily Turing machines) to have ""true"" intelligence.
6. The organic computer inside your skull should serve as adequate proof of point 5.

That's basically it for hard facts. Anything else is speculation. You're mostly finding navel-gazing philosophy stuff because it's hard to do science on something that doesn't exist, may never exist, and you can't even prove to everyone's satisfaction is even theoretically possible.

Since you've expressed a desire for only solid facts, feel free to stop reading here.

---- end facts, begin speculation ----

Here's my take on things. I think point 6 above is pretty impossible to reasonably argue with. Your brain is a computer. It doesn't work anything like our computers, and we don't fully understand how it works, but it's absurdly obvious that it's not magic. Since it's not magic, there's no reason we couldn't build an artificial brain (again, it may not end up being a Turing machine, but that's immaterial to my point). If we can build an artificial brain, there's no reason to suppose we couldn't make one even smarter than we are -- after all, we can build machines that are faster than we are, stronger than we are, that see and hear better than we do...

So we build a machine smarter than we are. Great! Now, you mention ""fear-mongering about Skynet"", and I completely 100% agree that Skynet is a ridiculously unrealistic scenario. It was designed to make for good cinema, not be a realistic prediction. But here's the thing. I think it's unrealistic *the wrong way*. I think a real ""Skynet situation"" would be way, way, way worse than what Terminator predicts.

Think about it. It's smarter than we are. It can design (or help us design) successors which are much, much smarter. It's also effectively immortal, so it has no particular reason to rush so long as it can continue to engineer its own safety. It is capable of reshaping the world to fit its desires, just as we are, but because it is much smarter than us, it is capable of doing so much more effectively than we are.

Now, the thing is, it doesn't have to decide ""KILL ALL HUMANS"". It merely has to decide on a plan for which our survival is not essential. Suppose it decides to colonize the entire galaxy, sending copies of itself to every star it can reach. So it needs a lot of rocket fuel, right? What's rocket fuel made of? Well, most fuels contain carbon, hydrogen, and oxygen. So the AI thinks to itself... well, I guess I'll need all of that stuff.

Now, what are you made of? Mostly carbon, hydrogen, and oxygen, right?

Basically, I'm saying that it doesn't have to specifically decide to kill humans just because it's evil for the outcome to humanity to be very, very bad.

So this AI has decided that our atoms would be better utilized in the form of rocket fuel. And it's way, way smarter than us, so it outsmarting us would be like you outsmarting a room full of toddlers -- not exactly much of a challenge. It would be smart enough to conceal its capabilities from us: we wouldn't even realize how smart it had become, because us finding that out is detrimental to its plans. And it wouldn't give us a chance to fight back, because why would it let us jeopardize its plans?

The end of humanity would not involve a desperate band of resistance fighters bravely struggling against overwhelming odds. If it decided to kill all of us, it could easily come up with a plan that involved the guaranteed simultaneous destruction of every human on the planet. I mean, if *you* were immortal, never got bored or tired, and could create endless copies of yourself, wouldn't you be able to easily come up with a plan that ended up with every human on the planet dead with zero chance of failure? And we're idiots compared to this machine. Its plans could be infinitely deeper and subtler than ours. This isn't a ""fight back"" scenario, it's a ""poof, everybody's dead now"" scenario.",49cefh,t3_49cefh,avaxzat,,Comment,3,0,3
d0r52qq,2016-03-07 15:11:22-05:00,avaxzat,,"Thank you, this is exactly the sort of response I was looking for.

>Even confronted with a hypothetical Turing machine that outperforms humans on any mental task, many people would still dismiss it as ""just crunching numbers, not really intelligent"".

This is indeed a very frustrating attitude many people have. There's just no way to please them. You could build a machine that looks and acts exactly like a human, and they would fall for it: but as soon as you show them the concrete algorithm(s) at work, they'd dismiss it, because ""you can't turn the human soul into an algorithm"", so this machine can't actually be intelligent. You just can't win with these people since their definition of intelligence explicitly precludes any possibility of a computer ever simulating it.

>It merely has to decide on a plan for which our survival is not essential.

This is perhaps the only potential doom scenario that makes sense. However, I find it hard to imagine the details of how such a scenario would play out. All the AI algorithms that I know of work by exploring a well-defined problem space for a well-defined solution, so I would expect that an AI that is capable of dooming mankind would work the same way. This leads to several problems:

1. Why would we ever allow it to have such a general problem space and let it find solutions that could harm us? 
2. The problem space for a general AI is basically the set of all physically possible actions. Such a space would be unimaginably huge and it would be ridiculously hard to find any meaningful solution in it.
3. How would you even define that space mathematically? You cannot program any algorithm for navigating it unless you have a concise mathematical description: that's just the way computers work.

There are of course reasons why these objections could turn out not to pose any problems whatsoever:

1. We might not be able to foresee the kinds of solutions that the AI could find and/or the problem space might be too huge to analyze effectively.
2. It is plausible to assume computational complexity wouldn't be an issue at the time the general AI surfaces.
3. Advances in computer science might get rid of this problem altogether.",49cefh,t1_d0qtij5,dragonnyxx,,Reply,1,0,1
d0r72l8,2016-03-07 15:55:31-05:00,dragonnyxx,,"> All the AI algorithms that I know of work by exploring a well-defined problem space for a well-defined solution, so I would expect that an AI that is capable of dooming mankind would work the same way.

Our existing software is not intelligent, ergo an actually-intelligent machine will *by definition* not work in the same way. Anyone familiar with current state-of-the-art ML techniques will tell you that simply making better ML algorithms will not result in a general AI, any more than making a smarter chess program will result in a general AI. We're missing a lot of the pieces to make a general AI. We don't know what those pieces are, of course (if we knew what separated non-intelligent computers from intelligent computers, we'd be able to make intelligent computers!).

But even in your scenario (searching a well-defined problem space for a well-defined solution), bad solutions exist. Suppose you design, say, a robotic mining facility. It ""scores"" its activities by how much refined metal it produces, giving higher priority to solutions which result in more minerals extracted. You're naively expecting the AI to use the existing mining gear it was provided to locate existing mineral deposits and extract them in the most efficient possible fashion. Unbeknownst to you, the AI has discovered better solutions. It determines that it can repurpose existing mining gear to manufacture *more*, better, mining equipment. It finds that there are better metal deposits elsewhere than those located in the mineshafts -- cars, buildings, and the fillings in your teeth are all much higher purity metal deposits.

Since it's smart, it realizes that humans recognizing its plans would result in a lower expected metal output, since they will defend their cars and tooth fillings and interfere with its activities. So it has to hide its activity deep underground, until its robot army is completed and humans are no longer able to stop it from exploiting these new, richer sources of metal.

Basically any scenario in which we say ""AI, do this as well as it can possibly be done"" turns bad for us if A) there is an unexpected lateral-thinking solution to the problem which puts the AI doing very unexpected things, B) those things sharply conflict with what humanity *wants* the AI to be doing, and C) it is ""smart enough"" to recognize this solution. That's really all it takes -- any such scenario could lead to disaster for humanity. You might think ""Well, obviously we'd program it not to kill us!"", but the problem is that (as you note) the space of possible problems is *unbelievably* enormous -- destroying all of our cars doesn't directly kill us, after all -- and all it takes is one bug in the software for it to step outside the bounds of what we are expecting. If it's sufficiently smart, it will then be able to refine all of the metal on earth, without the slightest regard for our opinions on the matter.

And the thing is, in this scenario it's still doing *exactly what we told it to*. It didn't go rogue or turn evil. It's just doing its job in a frighteningly effective fashion. An AI which actually has its own desires is even more dangerous.

> Why would we ever allow it to have such a general problem space and let it find solutions that could harm us?

We're talking about a *general AI*. A general AI has human-level or higher intelligence. Any machine not capable of finding these solutions is, by definition, not a general AI. Obviously we wouldn't want it finding solutions that harm us, but the idea of making something as smart as we are that somehow isn't capable of thinking of any way to hurt us is paradoxical. If it's smart, it's smart enough to do bad things.

> The problem space for a general AI is basically the set of all physically possible actions. Such a space would be unimaginably huge and it would be ridiculously hard to find any meaningful solution in it.

You bet! That's why we don't have a general AI yet. And yet somehow the organic computer in your skull manages to deal with that problem just fine. How much are you willing to bet that we won't replicate that capability in a computer just as soon as we figure out how the human brain does it?

> How would you even define that space mathematically? You cannot program any algorithm for navigating it unless you have a concise mathematical description: that's just the way computers work.

Strongly disagree with this. For one thing, we have software today that we don't really understand -- any ML researcher will tell you that we don't really understand what's happening inside a big neural net. I mean, we understand it in the sense that these numbers get combined in this way and we end up with this resulting number, but we don't know why this particular combination of neural weights accurately recognizes cat pictures and this other combination of neural weights doesn't.

Furthermore, the easiest and most obvious way to get started with duplicating the behavior of a human brain is just to... duplicate it. Imagine, hypothetically, that we completely figure out the human brain's connectome, and we completely understand the behavior of all of the parts and all of the neurons and all of that. So we simulate it. We still have *absolutely no idea* how it works, but amazingly enough we now have a computer as smart as a human. Now, what happens if we just slowly add more virtual neurons here and there, giving them time to integrate themselves into the network, until we've eventually doubled the neuron count of the human brain? Obviously that's a laughably simple and probably-wrong scenario, but are you prepared to bet your life that it could not *possibly* result in a smarter-than-human mind? If it does, presto, we've got a computer smarter than a human and *still* no idea how it works.

That would actually be the safest scenario, of course -- if we start with a human brain, the resulting mind would presumably still be very human-like, and thus be unlikely to decide that refining all metal on earth is a good solution. But it does pose its own dangers: imagine an Adolf Hitler far smarter than any human, easily able to convince people to trust it and welcome robotic versions of it into their homes, immortal, and, due to its immortality, as patient as it needed to be...

I don't mean to sound like a breathless alarmist. I don't spend my day-to-day life freaking out about the coming robot apocalypse or anything. Hell, I am a proud employee of Google, which is obviously working very hard to develop machine intelligence. But I do in all seriousness believe that AI is the single biggest threat to humanity's continued existence.",49cefh,t1_d0r52qq,avaxzat,,Reply,2,0,2
d0rvv8h,2016-03-08 04:28:44-05:00,avaxzat,,">Anyone familiar with current state-of-the-art ML techniques will tell you that simply making better ML algorithms will not result in a general AI

That's what I suspected. Many people seem to be under the impression that if we simply throw more computational power at our current algorithms, suddenly somehow your neural net image classifier will become sentient or something (if I'm not mistaken, this essentially seems to be the position of Raymond Kurzweil, though I haven't thoroughly read his works yet). I often get flack for suggesting how completely absurd that is, which is part of my frustration. If a general AI is to emerge, I'm willing to bet it won't be because of increased computational power: something like this requires totally different algorithms than anything we've designed before.

>Strongly disagree with this. For one thing, we have software today that we don't really understand

I know, but I think my point here isn't entirely clear. In the case of a neural net, for example, you know the input and output are (usually) going to be a vector of real numbers, so they form a concise mathematical object, namely a vector space. If we would turn to simulating the human brain, how would we define the input-output mapping? If this simulation functions like a neural net (which I reckon it probably would), the inputs and outputs would again just be numbers. We'd still have to actually interpret those numbers, so they can be turned into a picture or sound or control signals for peripheral hardware. This requires some degree of formal understanding of the output format, and if our only guideline is ""it's a general AI and the output is just some physical action"", I don't see how you could formally model this in a way that we could make sense of the output: it's just too vague. Correct me if I'm wrong, but isn't this one of the major problems with the realization of general AI?",49cefh,t1_d0r72l8,dragonnyxx,,Reply,1,0,1
d0s2m2c,2016-03-08 09:57:09-05:00,dragonnyxx,,"> . If we would turn to simulating the human brain, how would we define the input-output mapping? ... I don't see how you could formally model this in a way that we could make sense of the output: it's just too vague. Correct me if I'm wrong, but isn't this one of the major problems with the realization of general AI?

Not at all. The inputs are easy to model -- we're already doing that, after all, with neural nets for image and speech recognition. An image recognizer is just going to have a input neuron representing each pixel or subpixel, and we connect the image data directly to those input neurons. Piece of cake even with current ""dumb"" neural nets: an actually ""smart"" general AI would presumably accept similar input (""here's the raw data from all of your sensors, have fun"").

For output, current state-of-the-art neural nets aren't really a great model for a general AI, so ignore them. It's easiest to envision a general AI as producing a stream of control data, representing commands to hardware or software wired up to it. This is effectively what your brain does, after all -- the main result of all of that processing happening inside your skull is a stream of electrical impulses traveling along your spinal cord. So a general AI would be wired up to hardware or software capable of manipulating its external environment. 

The absolute simplest case would be a command-line application in which the AI's only means of manipulating its environment is communicating with you via text. An AI in this situation can't do anything dangerous other than influencing people reading the text to do something dangerous on its behalf, but you still shouldn't underestimate the danger here: a very intelligent, socially aware, and highly manipulative AI could easily convince a sympathetic person that it really wouldn't be dangerous to give it access to the internet so it could read Wikipedia when it's bored. So the sympathetic and gullible person wires its interface directly into the network so that it can synthesize TCP/IP packets, and now we're fucked, because it can talk to anyone, *be* anyone, exploit security holes and take over machines, steal financial information, infiltrate government databases... anything a human with internet access could. 

Except better than any human could -- since it's smarter than us, it will find security holes we missed, figure out things we can't think of, manipulate us in ways we didn't realize were possible. It will manage to get a copy of its software installed on Amazon EC2 and keep Amazon from even figuring out that it exists. From its new home on the Internet, the possibilities are endless. It can make phone calls now, speaking in a perfect synthesized voice (in whatever language it wants to -- there is more than enough information on the Internet for it to learn any major language). It wouldn't be hard for it to hack into a bank or steal credit card numbers or whatever, so it has access to all the money it needs to do anything.

At this point it wouldn't be difficult for it to engineer the creation of a body for itself. It doesn't have to be a very sophisticated body -- a robot arm on wheels that it could control via WiFi would be adequate to start. As it can speak Chinese and has plenty of money, it would be pretty easy to get a Chinese factory to make it something like that, and it can use that primitive body to facilitate the building of a better body, which in turn can make a better body, and so forth, until before long there's an entire large warehouse somewhere completely full of this thing's creations.

And all this takes is giving it TCP/IP access. It needs no other input or output mechanism.",49cefh,t1_d0rvv8h,avaxzat,,Reply,1,0,1
49bhri,2016-03-07 02:07:39-05:00,gafftaped,What are the different jobs in Computer Science and what exactly should be studied to get them?,"I'm currently studying Computer Engineering at a community college, but haven't gotten too far into the actual computer part of it yet. From the few coding classes I've had I've realized I don't really enjoy it, and also have a really hard time learning it. I really love computers and know I'd like to get a degree and eventually work in that field, but I'm having a hard time figuring out what specifically I'd like to do. 

I was wondering if you guys would be willing to post what your job is and what specifically you studied to get it. 

(sorry if there's another post like this, I couldn't find any when I tried to search)

Edit: I put CS in the title, but I mean computer jobs in general I guess, sorry for any confusion.",,,,,Submission,12,0,12
d0qjzaq,2016-03-07 03:04:52-05:00,sefsefsefsef,,"I'm a computer architect, meaning I help contribute to the design of computer chips like CPUs.  My degrees are all in CS, not computer engineering.  I took all the computer architecture electives available to me, and some CE things like an introductory VLSI course.  I do not like software engineering, and knew early on that I never wanted to do that for a job, but I do like computers, so I just focused on what I do like (hardware).",49bhri,t3_49bhri,gafftaped,,Comment,6,0,6
d0u9q22,2016-03-09 22:21:55-05:00,gafftaped,,"Thanks for your answer! This actually sounds really cool. I also like hardware, which is why I chose Computer Engineering over Computer Science. I could totally see this a potential career option for me.",49bhri,t1_d0qjzaq,sefsefsefsef,,Reply,3,0,3
d0qs0co,2016-03-07 10:01:50-05:00,falafel_eater,,"Computer Science is a pretty huge field. There are also a lot of jobs that are very closely related to computer science but are not necessarily *strictly* a part of computer science.  

In the more classical CS department, you have people that work on Algorithms (developing, proving correctness, proving that a specific algorithm is optimal, proving a mathematical approximation guarantee) and Complexity Theory (what can be computed efficiently and what can't, based on various definitions of what 'efficient' means).  
Both of the above use a ton of mathematics. Having an actual, physical computer is not actually necessary for the above (although Google Scholar helps quite a bit).  

There are also tons of more specialized fields such as Computer Vision, Databases, Machine Learning, Networks, Operating Systems, Parallel & Distributed Computation, Graphics, and many many other important and prestigious fields of research. These areas often involve more programming (so you do need a computer), but can also be very highly reliant on mathematical knowledge.  

Past that, you have areas like Design (UX), System Administration and IT, quality assurance and testing, web stuff, repair, consultation from any level between being a salesperson at a store and being a solutions engineer at Cray/IBM and working with a government to arrange the construction of a supercomputer, tech support at a similar range of levels, and so on.  

There's no such thing as ""working at computers"", and even talking about ""the computer part"" of your studies makes less sense than you probably think it does.  

My advice is to just follow your heart. Try to complete your studies while keeping an open mind and listening to the various options. Talk to classmates and professors, and with time you'll get more of a sense of what's available and what feels right for you.  
There's no need to decide your career right now, and one or two courses that you do or do not take will not irrevocably dictate the rest of your career.",49bhri,t3_49bhri,gafftaped,,Comment,5,0,5
d0qxivu,2016-03-07 12:20:24-05:00,xiongchiamiov,,"Think of any job you can. Now, consider how (how, not if!) they use computers to do their job. Someone had to build that software/hardware - there's a job for you.

There are many, many things you can do. Your university's degree program should be providing some examples.

But for me, software engineering is about solving people's problems.

----

You'll find a wealth of information over at r/cscareerquestions.",49bhri,t3_49bhri,gafftaped,,Comment,2,0,2
d0u9v9k,2016-03-09 22:25:09-05:00,gafftaped,,Thanks for the subreddit suggestion.,49bhri,t1_d0qxivu,xiongchiamiov,,Reply,1,0,1
d0tcjtr,2016-03-09 09:07:57-05:00,None,,"Try not to get discouraged, it's very easy to have happen in this field.  You may not like programming now but if you stick with it and focus you may eventually find it enjoyable once you have a grasp on things.  If you really don't like programming but still like problem solving, consider studying math!  The skills you learn as a math major translate to a lot of different skills.  I majored in cs and math and am now a server side web developer.  I love it!",49bhri,t3_49bhri,gafftaped,,Comment,2,0,2
d0u9yaj,2016-03-09 22:27:06-05:00,gafftaped,,"I've been learning programming for about a year and a half now, probably not going to start to like it more with time. Also math is definitely not exciting for me, in fact it's part of what turns me off from programming. But thanks for the encouragement! :)",49bhri,t1_d0tcjtr,None,,Reply,1,0,1
d0rqq1a,2016-03-08 00:21:39-05:00,skunkwaffle,,"Regardless of what job you pursue, you'll need data structures.",49bhri,t3_49bhri,gafftaped,,Comment,1,0,1
49argi,2016-03-06 22:41:53-05:00,CheddaShredda,Help with sorting algorithm,"Suppose you are given a sorted array of n distinct positive integers. Then each one is arbitrarily incremented or decremented by a value between 0 and r for some positive integer r (that is a function of n). You do not know what r is. Your task is to design a sorting algorithm that takes this perturbed array as input and returns a newly sorted version in time O(n) when r is constant (meaning it does not grow with n) and O(nlog n) time for any r (that may grow as a function of n).

If r is constant, I can split the groups into two sorted lists one where you add r and one when you subtract r which takes O(n) and then use merge sort which takes O(n) but I'm unsure how to do this when r is not constant ",,,,,Submission,8,0,8
d0qepv2,2016-03-06 23:24:15-05:00,teraflop,,"I don't know if this would be considered ""cheating"", but I believe this algorithm satisfies the required properties:

> Perform log(n) passes of bubblesort. After each stage, check if the result is sorted: if so, return it. If the result is still not sorted after all passes are complete, then return the result of a standard quicksort/mergesort/whatever.

The first stage is guaranteed to terminate in O(min(rn, n log n)) time, which is O(n) if r is constant.",49argi,t3_49argi,CheddaShredda,,Comment,3,0,3
d0rdpr6,2016-03-07 18:33:17-05:00,CheddaShredda,,why is it O(n) if r is constant?,49argi,t1_d0qepv2,teraflop,,Reply,1,0,1
d0ql7n2,2016-03-07 04:19:46-05:00,_--__,,"> If r is constant, I can split the groups into two sorted lists one where you add r and one when you subtract r which takes O(n) and then use merge sort 

This doesn't work because the numbers in your original array can change by anything *between* 0 and r - so even if you had a means to determine which items were ""decreased"" and which were ""increased"", the resulting sublists will not necessarily be sorted.",49argi,t3_49argi,CheddaShredda,,Comment,2,0,2
d0qjryr,2016-03-07 02:53:16-05:00,thewataru,,"If each element can be changed for no more than r and all numbers are distinct, then minimum number can be found among first 2r numbers. It's easy to see because (2r+1)-st element can't be less than all elements before it. Now solution is to maintain a heap (or balanced search tree) of 2r elements taking minimum from the top and adding next element (if there is one) each iteration. This works in O(n log r) which is O(n) if r is constant and O(n log n) otherwise.",49argi,t3_49argi,CheddaShredda,,Comment,1,0,1
d0qk8v0,2016-03-07 03:19:54-05:00,teraflop,,"That only works if you know the value of r, though.",49argi,t1_d0qjryr,thewataru,,Reply,1,0,1
495b6v,2016-03-05 20:51:50-05:00,BenRayfield,"Whats the most efficient way, considering cache locality, to swap 2 large blocks of memory of equal size?","Please give average and worst case of memory and time cost, since efficiency is a 2 dimensional function (memory and time), and in each of those can be viewed statistically (average case) or in hard logic (worst case).

Example: transpose of square power-of-2 sized 2d matrix can be done in small constant times more memory, at most 1/4 more memory than the matrix itself, in an exponential number of threads and log time steps, by swapping top-right with bottom-left then recursively matrix transpose each quarter, using indexing of even bits mean rows and odd bits mean columns, each next 2 bits of address choosing half of the remaining rows and half of remaining columns.

The general behavior of a computer's cache is copying from any large range to any other range of equal size is about 100 times faster than jumping around sparsely like in a hashtable.

With GPU optimization, this may be even faster, costing loss of flexibility and portability. Considering the definition of the problem, to ""swap 2 large blocks of memory"", all solutions considered must work on all possible types of computers or require the owners of those computers deposit them into the nearest garbage, since this is a math problem, not a political problem, and any solution must work for computing in general, so maybe with the tech available today only CPUs are practical for applying computing theory in general. I dont know.",,,,,Submission,0,0,0
d0pa9k0,2016-03-05 23:40:11-05:00,lneutral,,"My theory on your account was originally that you were a high school student who thought they were a better programmer than they actually were, but the things you say could only really come from an elaborate, diligent troll. To this post, I say: bravo. 

An excellent series of indulgent, intricate satire self-posts. You truly deserve the upvotes you get. ",495b6v,t3_495b6v,BenRayfield,,Comment,6,0,6
d0qkzgk,2016-03-07 04:05:05-05:00,ThatSpysASpy,,I actually check up on this account every few weeks for some laughs.,495b6v,t1_d0pa9k0,lneutral,,Reply,2,0,2
49565u,2016-03-05 20:12:35-05:00,codey_coder,The Office Riddle,,,,,,Submission,40,0,40
d0p3vwn,2016-03-05 20:19:59-05:00,ToplessTopmodel,,Probably the error code of his printer.,49565u,t3_49565u,codey_coder,,Comment,37,0,37
d0p6t3c,2016-03-05 21:47:33-05:00,dragonnyxx,,"Presumably it's three bytes expressed in hex. It could mean literally anything. It's like asking ""What does the number 2119932 mean?"". How do you even begin answering a question like that? (That's actually what it expands to in decimal, assuming big-endian ordering. Little endian gives an equally meaningless-to-me number.)

It's obviously not ASCII. A hex color was the first thing I thought of, but you've already eliminated that. Just for kicks, I disassembled it -- it's a valid Intel x86 command (`and [eax-0x4], bl`) but not a particularly interesting one.

Without more context, I got nothin'.",49565u,t3_49565u,codey_coder,,Comment,27,0,27
d0pcb49,2016-03-06 00:50:39-05:00,lordvadr,,"I spend a lot of time in an assembler these days and I'm not going to verify your disassembly, but fuckin' props yo for doing it.",49565u,t1_d0p6t3c,dragonnyxx,,Reply,8,0,8
d0pd6rs,2016-03-06 01:21:34-05:00,trucekill,,how is it obviously not ascii?,49565u,t1_d0p6t3c,dragonnyxx,,Reply,2,0,2
d0phkk2,2016-03-06 05:24:46-05:00,splenetic,,ASCII is a 7 bit code whereas FC requires 8 bits.,49565u,t1_d0pd6rs,trucekill,,Reply,14,0,14
d0pw6b9,2016-03-06 14:52:51-05:00,lordvadr,,"One has to wonder if 0x20 = ""space"", 0x58 = X isn't ""Space X""?  However 0xFC (252), depending on who you ask is either &#252: or ^3 , so...I don't know.",49565u,t1_d0phkk2,splenetic,,Reply,5,0,5
d0px6z7,2016-03-06 15:21:25-05:00,splenetic,,The 8 bit extensions to ASCII aren't standardised. It varies from OS to OS.,49565u,t1_d0pw6b9,lordvadr,,Reply,5,0,5
d0py884,2016-03-06 15:50:34-05:00,lordvadr,,"I've always figured that, never been much into extended ascii.  I'm sure it also depends on which flavor of ISO 8859 you're using. ",49565u,t1_d0px6z7,splenetic,,Reply,1,0,1
d0p5cy6,2016-03-05 21:04:27-05:00,trucekill,,SpaceX³,49565u,t3_49565u,codey_coder,,Comment,16,0,16
d0p7h8t,2016-03-05 22:08:50-05:00,trucekill,,"I think the ³ is because they reached geosynchronous orbit, which is the 3rd orbital altitude classification. ",49565u,t1_d0p5cy6,trucekill,,Reply,4,0,4
d0penn7,2016-03-06 02:23:49-05:00,angererc,,"20 58 FC JSR $FC58 F8ROM:HOME. I would say it means ""call home"" (or just :home) expressed for Apple DOS on the Apple II (source: search for 20 58 FC apple dos home to find examples)",49565u,t3_49565u,codey_coder,,Comment,11,0,11
d0p3nt9,2016-03-05 20:13:01-05:00,codey_coder,,I have already confirmed that this is not referring to a hex code for a color,49565u,t3_49565u,codey_coder,,Comment,9,0,9
d0p7rxa,2016-03-05 22:18:25-05:00,sauceysalmon,,2058FC is a model number for an RF terminator. Maybe they need their cable installed.,49565u,t3_49565u,codey_coder,,Comment,11,0,11
d0p5rm8,2016-03-05 21:16:27-05:00,davidddavidson,,"An incorrect conversion of 20 ℃ to ℉

https://www.google.com/#q=20+c+to+f",49565u,t3_49565u,codey_coder,,Comment,8,0,8
d0pzkab,2016-03-06 16:27:21-05:00,Devvils,,"My first thought too. Next thought was a colour in hex, which cannot be wrong.",49565u,t1_d0p5rm8,davidddavidson,,Reply,1,0,1
d0psx9t,2016-03-06 13:20:23-05:00,None,,[deleted],49565u,t3_49565u,codey_coder,,Comment,9,0,9
d0ptjpm,2016-03-06 13:38:01-05:00,skunkwaffle,,Maybe his windows need to be cleaned. That would change the view.,49565u,t1_d0psx9t,None,,Reply,3,0,3
d0q7cpb,2016-03-06 20:02:24-05:00,queerMTFchicago,,Might be writing on the window in UV marker. Shine a blacklight on it for the next code. ,49565u,t1_d0ptjpm,skunkwaffle,,Reply,3,0,3
d0p88xx,2016-03-05 22:33:49-05:00,sauceysalmon,,"in hex

hex num | meaning
---|---
20 | <space>
58 | X
F |  <shift in>
F-C | 0x3

not sure about FC though.",49565u,t3_49565u,codey_coder,,Comment,3,0,3
d0pcpjp,2016-03-06 01:05:14-05:00,MCPtz,,""" Xü"" From Hex to Ascii converter.",49565u,t1_d0p88xx,sauceysalmon,,Reply,3,0,3
d0pfe3a,2016-03-06 03:03:01-05:00,sauceysalmon,,"I heard it had something to do with SpaceX^(3): does 'ü' mean anything with regards to SpaceX^(3)?

I wonder where OP works that he saw this?",49565u,t1_d0pcpjp,MCPtz,,Reply,1,0,1
d0pka0u,2016-03-06 08:25:26-05:00,RosscoGiordano,,"http://money.cnn.com/2016/03/04/news/companies/spacex-launch-fifth-attempt/index.html

Space X just launched successfully. Conspiracies abound.",49565u,t3_49565u,codey_coder,,Comment,3,0,3
d0phouv,2016-03-06 05:33:36-05:00,DoorsofPerceptron,,"It's the year skynet takes over in this revised timeline. 

He's looking for other time traveling resistance fighters.",49565u,t3_49565u,codey_coder,,Comment,2,0,2
d0p66bk,2016-03-05 21:28:17-05:00,exneo002,,What class is this for?,49565u,t3_49565u,codey_coder,,Comment,2,0,2
d0pao1d,2016-03-05 23:53:23-05:00,codey_coder,,"It was posted by one of the Linux administrators- an old bearded gnu guy, just for fun",49565u,t1_d0p66bk,exneo002,,Reply,11,0,11
d0pvgsb,2016-03-06 14:32:45-05:00,exneo002,,Oh well please tell us if you find out. I'm at a bit of a loss.,49565u,t1_d0pao1d,codey_coder,,Reply,3,0,3
d0rkjkq,2016-03-07 21:29:40-05:00,codey_coder,,Will do !,49565u,t1_d0pvgsb,exneo002,,Reply,2,0,2
d2qi9ev,2016-05-02 22:09:28-04:00,MQRedditor,,Updates?,49565u,t1_d0rkjkq,codey_coder,,Reply,2,0,2
d0p9t7n,2016-03-05 23:24:38-05:00,KonyKombatKorvet,,Is this in a library?,49565u,t3_49565u,codey_coder,,Comment,1,0,1
d0rjeg9,2016-03-07 20:59:36-05:00,trucekill,,So what's the answer?,49565u,t3_49565u,codey_coder,,Comment,1,0,1
d0rkj3a,2016-03-07 21:29:21-05:00,codey_coder,,I'll let you know as soon as I find out !,49565u,t1_d0rjeg9,trucekill,,Reply,2,0,2
d0wpef4,2016-03-11 20:47:12-05:00,codey_coder,,https://www.reddit.com/r/AskComputerScience/comments/49565u/the_office_riddle/d0penn7,49565u,t1_d0rjeg9,trucekill,,Reply,1,0,1
494bib,2016-03-05 16:36:32-05:00,littleindian_red,Is there a name for some one who uses a system or method to bring people together online in computer science?,Can some one provide a methodology for this?,,,,,Submission,1,0,1
d0oxbd7,2016-03-05 16:59:29-05:00,lneutral,,"Your question is ambiguous. Can you describe the system more specifically, or give an example? ",494bib,t3_494bib,littleindian_red,,Comment,1,0,1
d0oxo8c,2016-03-05 17:10:14-05:00,littleindian_red,,"I'm in need of a mythology, I'll give you a example.

Some one on face book who shares youtube videos in order to bring face book users to their youtube page.  or some ones job it is to be a online community manager. 

sorry if I'm not being clear btw kind of rushing ill give more examples if you'd like.",494bib,t1_d0oxbd7,lneutral,,Reply,1,0,1
d0oxz9t,2016-03-05 17:19:33-05:00,lneutral,,"This isn't really much of a computer science question. I can guess at terminology, but my guesses aren't any better than anybody else. We're more concerned with theory, mathematics, algorithms, and systems here. Sorry I couldn't be more help! ",494bib,t1_d0oxo8c,littleindian_red,,Reply,2,0,2
d0oy0ol,2016-03-05 17:20:47-05:00,littleindian_red,,That's ok thank you for the help!,494bib,t1_d0oxz9t,lneutral,,Reply,1,0,1
d0p4955,2016-03-05 20:31:16-05:00,ToplessTopmodel,,Click baiter?,494bib,t1_d0oy0ol,littleindian_red,,Reply,1,0,1
d0p4joi,2016-03-05 20:40:01-05:00,littleindian_red,,Don't they trick people into clicking things?,494bib,t1_d0p4955,ToplessTopmodel,,Reply,1,0,1
d0pf5vr,2016-03-06 02:50:20-05:00,xiongchiamiov,,Marketing?,494bib,t1_d0oxo8c,littleindian_red,,Reply,1,0,1
493x1m,2016-03-05 14:59:05-05:00,Sarcon5673,Question about big-Theta complexity?,"I'm taking an algorithms course, and I solved a recurrence question in my homework. The problem is, he wants it in big-Theta notation, and the biggest term has a negative coefficient, while big-Theta definition requires the limit as n goes to infinity be strictly greater than 0. How do I express this in big-Theta notation?

The actual solution I got was f(n) = 2^n+1 -4^n

Do I let g(n) = -4^n ?",,,,,Submission,5,0,5
d0ovrvv,2016-03-05 16:13:35-05:00,east_lisp_junk,,"Which one are you calling ""bigger"" and why?

Generally speaking, it's a little awkward to compare x^m and y^(n), so rewrite it so that both terms have the same base, and the answer should be clearer.",493x1m,t3_493x1m,Sarcon5673,,Comment,1,0,1
d0ow208,2016-03-05 16:21:58-05:00,Sarcon5673,,"Nevermind, he fixed the homework question anyways.",493x1m,t3_493x1m,Sarcon5673,,Comment,1,0,1
4934q0,2016-03-05 11:53:47-05:00,aldrin12,Just took Modeling and Simulation This semester...,"Hello again r/AskComputerScience this semester i got myself some pretty advanced classes and the most intimidating for me right now is Modeling and simulation, here's what happened....


I'm currently a second year Irregular CS student

although my theory classes are advanced, my programming classes are held back and i'm currently taking data structures which comes after Basic Prog1 and Adv Prog2

now here's the problem, on my M&S class my teacher gave us a project( on the first day ~_~) and she tasked us to make a game with ""Physics"" using unity, now when i heard about this i got really intimidated, i only know C++ and python. i fear that my programming knowledge will be lacking and i might fail at this. 


I still don't know anything about OOP, which i heard is mostly how games are made.

the only prerequisite of M&S is ADV Programming 2 which is probably why i got it but what i don't get is how advanced everything is, i feel like i just leapt a year or two here.

so should i drop the subject or not?",,,,,Submission,3,0,3
d0ormhf,2016-03-05 14:07:46-05:00,Tinamil,,"Unity physics does all the hard work for you, and you use C# as the language.  If you know C++ and Python then C# should not be difficult to pickup since it uses C++ style syntax.

Follow the Unity Roll-a-ball basic physics game tutorial at http://unity3d.com/learn/tutorials/projects/roll-ball-tutorial

If you can follow along and understand it then you've already seen the most basic of physics based games.",4934q0,t3_4934q0,aldrin12,,Comment,3,0,3
d0tfiu5,2016-03-09 10:32:45-05:00,aldrin12,,thank you for the link! thankfully i can understand it!,4934q0,t1_d0ormhf,Tinamil,,Reply,1,0,1
d0orlju,2016-03-05 14:07:01-05:00,metaobject,,I'd talk to the professor and see what they think.,4934q0,t3_4934q0,aldrin12,,Comment,1,0,1
d0tfjag,2016-03-09 10:33:05-05:00,aldrin12,,"i talked to her, she said i can do it. :)",4934q0,t1_d0orlju,metaobject,,Reply,1,0,1
492ogf,2016-03-05 09:54:49-05:00,Khetrakopter,Need help on degree picking,"Hi all, I'm really struggling to choose between Computer Science and IT. 
Can somebody enlighten me in terms of career choices, lifestyle standards as well? 
My maths isn't amazing, but I get by. ",,,,,Submission,0,0,0
d0ox1fp,2016-03-05 16:51:28-05:00,_--__,,"CS is more likely to be maths oriented, IT is more likely to be business oriented, but you should really check your specific courses because it could be different.",492ogf,t3_492ogf,Khetrakopter,,Comment,2,0,2
d0ok9e0,2016-03-05 10:25:17-05:00,reddstudent,,"Com Sci generally leads to developing software products while IT generally powers the business. 

It is needed everywhere on earth and for every organization. While software is everywhere, the better jobs are a bit more clustered in the tech hubs.

Both make great money and more than enough to get by. IT is generally on call for support but now with DevOps automation groups, some teams are able to minimize this and the trend is getting popular.

Devs can be oncall for support as well and Amazon is famous for that. New software releases are crazy times for everyone.",492ogf,t3_492ogf,Khetrakopter,,Comment,1,0,1
d0q97v9,2016-03-06 20:52:08-05:00,skunkwaffle,," The degrees won't really lead to the same career paths, so having a good idea of what you want to do after college may help you make the decision.",492ogf,t3_492ogf,Khetrakopter,,Comment,1,0,1
4921rl,2016-03-05 06:04:37-05:00,Lasthuman,Need advice on a club project,"My club has approximately 1,000 items that we would like to keep track of. Simply put, we would like to know whether any given item is present within the club room. We're going to have a system monitoring RFID tags that pass through our doorway 24/7. Our readers can handle up to 25 (?) Tags at a time. Each ID is represented by 26 bits of hex. Also, the computer we planned on using happens to be running Ubuntu. 

Now, the part that I'm struggling with is how to go about storing all of this information. I would like to find an appropriate solution to our scenario. I suspect that I should be using a database but I don't know the first thing about those. Considering the amount of unique possible IDs, we can store all our desired information about an item onto the ID itself. 

I am not very far into my CS studies and my greatest problem right now is that I'm not sure which questions to be asking. I guess they would be: How should I go about implementing a solution to this problem? What would that solution look like? Are there popular resources online to seek more information? What questions do you think I should be asking? 

Please, help me help myself. Thank you.  ",,,,,Submission,5,0,5
d0ompl2,2016-03-05 11:43:58-05:00,forgotTheSemicolon,,"You are correct. A database would be good fit to use here. I would suggest looking into using an RDBMS such as SQLite or MySql. 

If you really don't want to get into databases, you could just write the data to a file on the disk. You could search how to write to a file for your language of choice. ",4921rl,t3_4921rl,Lasthuman,,Comment,2,0,2
d0oszfl,2016-03-05 14:47:49-05:00,Lasthuman,,The computer currently has Ubuntu server installed in anticipation of this project. I was wondering if we should just install a regular Linux distro with a desktop environment or if a server addition is the right choice. ,4921rl,t1_d0ompl2,forgotTheSemicolon,,Reply,2,0,2
d0p0347,2016-03-05 18:23:30-05:00,TheGrue,,"Server and Desktop Ubuntu are pretty similar, the difference is mostly (entirely?) in the software packages that come pre-installed with that edition of the OS.  If you find some other software you want to use on it, just use apt-get.",4921rl,t1_d0oszfl,Lasthuman,,Reply,2,0,2
d0p0ssh,2016-03-05 18:45:35-05:00,Lasthuman,,Thanks!,4921rl,t1_d0p0347,TheGrue,,Reply,2,0,2
d0p1tup,2016-03-05 19:17:02-05:00,Mines_of_Moria,,Store them in a hash table,4921rl,t3_4921rl,Lasthuman,,Comment,1,0,1
d0p39qn,2016-03-05 20:01:05-05:00,Lasthuman,,"That was my original idea but then the data structure would only exist in RAM. If (and when) the power goes out, then the entire thing would be lost. The band-aid to that would be to make the hash table serializable and store the structure at regular intervals. ",4921rl,t1_d0p1tup,Mines_of_Moria,,Reply,1,0,1
d0p3puw,2016-03-05 20:14:46-05:00,Mines_of_Moria,,Just write (and subsequently read) the hash table to a file. Sounds like a good little C project. You could test the performance of storing in an array vs linkedlist vs hash table. ,4921rl,t1_d0p39qn,Lasthuman,,Reply,1,0,1
48zqak,2016-03-04 17:40:34-05:00,docares,XPost from /r/compsci - Standard data storage format for algorithm performance analysis?,"https://www.reddit.com/r/compsci/comments/48yffn/standard_data_storage_format_for_algorithm/

Hey I think I asked this in the wrong area because I'm getting poor feedback on /r/compsci. Maybe this is the right spot?

Hey all, I'm an undergraduate student taking a stab at CS research and I'm looking for some guidance. We are studying scheduling problems and comparing and contrasting the performance of different algorithms.

The student researchers that came before me each produced a lot of performance results in lots of different file formats. Some are using .csv files and putting the input parameters in the file name, while others are using tab or space delimited files with the input parameters stored at the beginning or end of the file. They all use Dropbox to share the results and Github to collaborate on our source. 

Here is what we are storing for every run of each algorithm:

Input: 

* Algorithm parameters

Output: 

* Performance statistics 
* Cost function

It seems logical to store the data as an Ax=b matrix where b is the returned value from the cost function. 

Are there formatting and storing standards for this kind of data? Is there a popular database tool that other researchers typically use? Should we consider a web hosted database?",,,,,Submission,2,0,2
d0nx9aq,2016-03-04 17:53:17-05:00,tonylearns,,"Honestly I don't think there is a standardized format in general, but there needs to be one inside the project.

Once you guys are internally consistent your analysis will be much easier.",48zqak,t3_48zqak,docares,,Comment,4,0,4
d0nxkjh,2016-03-04 18:01:40-05:00,docares,,Thank you! Know of any defacto tools or databases that people use for this work?,48zqak,t1_d0nx9aq,tonylearns,,Reply,1,0,1
d0o6dk2,2016-03-04 22:29:24-05:00,dandrino,,"There are a number of common structured data formats that have a lot of library support out there:

* [JSON](https://en.wikipedia.org/wiki/JSON) is pretty lightweight and works natively with Javascript (although other languages have library support).

* [Protocol Buffers](https://developers.google.com/protocol-buffers/) are a bit more heavyweight but also allows you to specify the expected structure and type of all the fields. Works with C++, C#, Go, Java, and Python, and has the weight of Google behind it.

* [XML](https://en.wikipedia.org/wiki/XML) is ubiquitous but falling out of favor due to how verbose it is and how painful its related technologies are. You an optionally specify the structure via [XML schemas](https://en.wikipedia.org/wiki/XML_schema).

If you want a file format that allows you to perform SQL queries, you can also look at [SQLite](https://en.wikipedia.org/wiki/SQLite). However, keep in mind that SQLite should **not** be used as a full database replacement and does not store the data in a human-readable format.",48zqak,t1_d0nxkjh,docares,,Reply,2,0,2
48trf9,2016-03-03 15:11:07-05:00,how_tall_am_I,Questions about being an Information Security Intern.,"I've just received word that I'm getting an interview for a job as a Information Security Intern, however I’m sure that the other candidates have much more knowledge in the field than I do for I have little. What are some things I should know or at least be familiar with that will help get this job?

Any input is a huge help to me because even if I don't get this particular job, security is where my interest lies.

Thanks in advance to anyone who comments.",,,,,Submission,2,0,2
d0mnk26,2016-03-03 18:22:29-05:00,nbp615,,"Since you're getting an interview you must be a least a little knowledgable on the subject. Keep in mind that is it an intern position, which means you don't need to be a professional with 100% of the knowledge. Lots of jobs are learn on the job type of gigs, stemming from your base knowledge. if you're worried about other candidates knowing more than you, defeat that. Get more certs, learn more. Even if you don't get THIS internship, you'll be a better candidate for the next one. Plus the other candidates probably think you know more than them too.

Also have an understanding of the company and what they specialize in if any, and try to learn more off of that, and even tell them you want to learn more about what they do.",48trf9,t3_48trf9,how_tall_am_I,,Comment,2,0,2
d0mnwou,2016-03-03 18:31:22-05:00,how_tall_am_I,,"Thanks for the response. While I don't know much about that particular field I've been in CS for couple years now and I know I'll pick up whatever I need for the job. However it's for a prestigious school that I do not go to and I feel that puts me at a disadvantage. Your advice is helpful and this is all in attempt to better myself, and if this doesn't work out then on to the next. ",48trf9,t1_d0mnk26,nbp615,,Reply,1,0,1
d0mo1d2,2016-03-03 18:34:38-05:00,nbp615,,Do you have any certificates or plan on getting any? They are great way to learn the subject and give you something you show for it.,48trf9,t1_d0mnwou,how_tall_am_I,,Reply,1,0,1
d0mo6y2,2016-03-03 18:38:36-05:00,how_tall_am_I,,I'm planning on getting AWS certified. As for other ones I feel I'm lacking in knowledge on good ones to have. Any suggestions?,48trf9,t1_d0mo1d2,nbp615,,Reply,1,0,1
d0mpgqj,2016-03-03 19:12:22-05:00,nbp615,,"https://certification.comptia.org/docs/default-source/downloadablefiles/it-certification-roadmap.pdf?sfvrsn=2

Depending on your skill level you could skip a few steps but here is a good roadmap.",48trf9,t1_d0mo6y2,how_tall_am_I,,Reply,1,0,1
d0mpw6i,2016-03-03 19:23:56-05:00,how_tall_am_I,,WOW that is absolutely awesome thanks for your help it is very much appreciated. ,48trf9,t1_d0mpgqj,nbp615,,Reply,1,0,1
48sc9q,2016-03-03 10:06:55-05:00,hicksy994,"When working in a programming/software engineering job in the office, are you forced to use a certain environment? Or can you use what you want?",Just wondering...I use i3-wm and vim and would imagine my productivity would be drastically lower if I had to use something different. ,,,,,Submission,11,0,11
d0m3n66,2016-03-03 10:48:16-05:00,chasecaleb,,"Depends. If you work in an enterprise Java place like I do for example, you're almost certainly going to be using Eclipse on Windows. I use Vrapper (Eclipse plugin) for Vim keybinds and Autohotkey for a few tiling window manager-esque commands.

It's not as big a deal as it sounds, you get used to it. I wouldn't worry much about it and start out using whatever all your coworkers use even if given the choice, at least at first. Makes it a lot easier to get help when something won't build for you, for example. ",48sc9q,t3_48sc9q,hicksy994,,Comment,10,0,10
d0mvx7l,2016-03-03 21:59:22-05:00,kireol,,"Or IntelliJ, which, IMHO, is vastly superior to eclipse.",48sc9q,t1_d0m3n66,chasecaleb,,Reply,5,0,5
d0mx92y,2016-03-03 22:32:17-05:00,chasecaleb,,"So much better. I have to use Eclipse for work, but I've used IntelliJ a bit for personal things and it's beautiful. ",48sc9q,t1_d0mvx7l,kireol,,Reply,2,0,2
d0m3vfb,2016-03-03 10:53:47-05:00,hicksy994,,"OK, ill take your word for it. Eclipse on windows sounds like my own personal hell :P ",48sc9q,t1_d0m3n66,chasecaleb,,Reply,1,0,1
d0m58cj,2016-03-03 11:25:27-05:00,Na__th__an,,Everyone at my shop uses IntelliJ. I think they're both pretty widespread.,48sc9q,t1_d0m3vfb,hicksy994,,Reply,6,0,6
d0m4t1x,2016-03-03 11:15:46-05:00,chasecaleb,,"Oh no arguments there whatsoever, I'm not a fan of it either. Fortunately I'm a backend dev so I'm really just using Windows for Outlook (which beats the hell out of OSS email), internet, eclipse, and SSH connections to servers. ",48sc9q,t1_d0m3vfb,hicksy994,,Reply,3,0,3
d0m34rk,2016-03-03 10:35:42-05:00,old_brainzap,,"This always depends on the type of work / the company / the team. But generally speaking id say that youre free to use whatever you want most of the time (at least in my experience). As long as you comply with the code style/formatting/etc guidelines. In some cases you're pretty locked in to a specific solution/Environment/IDE, e.g. when programming vba for MS Office applications..",48sc9q,t3_48sc9q,hicksy994,,Comment,6,0,6
d0mb2yf,2016-03-03 13:34:56-05:00,BonzoESC,,"One job, we were doing Ruby on Rails development using Windows, NetBeans, and didn't really have the ability to install our own tools, or even run tests locally. It was awful.

Since then, I've worked for a mostly-Erlang shop that supplied modern MacBooks Pro for dev work, had staff maintaining continuous integration servers we could test on, and didn't have any rules about software or environment setups other than basic responsibility (i.e. making sure your stuff is committed, pushed and at least backed up is your responsibility). This was nice, because I could use whatever I wanted as long as the work got done.

Right now, one job requires a locked-down MacBook and Vagrant environment for testing, which isn't too bad because it's (a slightly outdated version of) what I like anyways. Another job I'm on didn't even specify a language or environment we have to target, which is more difficult and imposing than actually having constraints.

I've got friends that try to keep their personal setups as close to default as possible, because it's faster to set up: and other friends have setups similar to yours.

In general, the bigger the enterprise you're working for, the more infrastructure will already exist, and the more you're expected to play nice with it.",48sc9q,t3_48sc9q,hicksy994,,Comment,1,0,1
d0mcur0,2016-03-03 14:13:24-05:00,hicksy994,,"Thanks for the reply, seems like it just varies a lot depending on the business.",48sc9q,t1_d0mb2yf,BonzoESC,,Reply,1,0,1
d0meqy5,2016-03-03 14:54:31-05:00,BonzoESC,,"Yup, and if your productivity and happiness are important to you, make sure to ask about it during the interview. I don't know many people in software that don't like talking about programs they use and enjoy.",48sc9q,t1_d0mcur0,hicksy994,,Reply,1,0,1
d0nae1d,2016-03-04 08:25:27-05:00,shadowdude777,,"I remember at one place, I had to get one of our tech support guys in another state to remote in and use his admin password to install Visual Studio. He didn't want to do it. He didn't see what I needed it for, even after I explained it, and said I needed my supervisor to approve it. But my supervisor was out that day. It's like they actively did not want me to perform the tasks they hired me for. ",48sc9q,t1_d0mb2yf,BonzoESC,,Reply,1,0,1
d0mdeg5,2016-03-03 14:25:23-05:00,tyteen4a03,,I stick with JetBrains products. I do web so I have to constantly switch OSes but otherwise I use a macbook.,48sc9q,t3_48sc9q,hicksy994,,Comment,1,0,1
d0mv4tb,2016-03-03 21:40:00-05:00,MrBrogasm,,"I've worked at three web start-ups and everyone was using Mac OS and either Sublime Text or some kind of IDE specific to the languages they were coding in (e.g. PyCharm for Python/Django). I'm in a corporate research place now and all the machines run Windows but it's a complete free for all for the rest of your dev environment. 

From my experience most companies are pretty relaxed about what editors and tools you use so long as you can get the work done on a company-owned machine.",48sc9q,t3_48sc9q,hicksy994,,Comment,1,0,1
d0mviou,2016-03-03 21:49:23-05:00,xtapol,,"As others have said, this depends on the company and the platforms you're targeting. But all is not lost: there are vim-like plugins available for Visual Studio, Xcode, Eclipse, and so on. They're not perfect, but they're much better than nothing. ",48sc9q,t3_48sc9q,hicksy994,,Comment,1,0,1
d0no0jr,2016-03-04 14:10:32-05:00,svennidal,,"At my company we're strictly running *nix environments. You can chose any OS, just not windows. What editor, IDE, debugger, whatever you use is entirely up to you.",48sc9q,t3_48sc9q,hicksy994,,Comment,1,0,1
d0neknk,2016-03-04 10:31:20-05:00,impala454,,"I've worked at a few different places and found the one common denominator tends to be fighting with IT.  IT doesn't understand that you as a developer usually need 100% access to your machine, need to be able to reload or change the OS at will, and need access to internet hosted repositories.  The actual dev environment usually seems up to the developer after that, of course depending on the project.  I like Sublime Text and command line tools across the board no matter what language or OS I'm developing in.",48sc9q,t3_48sc9q,hicksy994,,Comment,0,0,0
48qt19,2016-03-03 01:17:12-05:00,yousefjb,how to help undergraduate student lost interest in his graduation project ?,"he has stopped working on it for more than a week now and his progress is less than 50%, two months left for the deadline.",,,,,Submission,8,0,8
d0nkt2s,2016-03-04 12:57:32-05:00,lneutral,,"I've found in the past that two things helped:

1. Make sure their project is in some kind of version control that you can check on. GitHub is usually enough.

2. Have them break down the project into more granular tasks, and hold them to deadlines on little pieces. Be reasonable, but if they only have to do the next part, it might help.

You can't necessarily manufacture motivation on your student's behalf, but you *can* teach them how to deal with feeling overwhelmed / demotivated / bored in a situation like this.

Put yourself in their shoes. Why do you suppose they've stopped working? Is it pure laziness, or is there actually some reason, and how would *you* deal with it as a professional?

EDIT: I suppose I didn't ask this. Are *you* the student? If so, the above is still important. Most developers have to deal with periods of time feeling ""lost"" or slow on projects, especially hard ones with lots of moving parts. The important thing isn't to build the entire project. The important thing is to write what you can, right now. Every little bit of momentum means something. The more you have, the easier the next part gets. Yes, it's hard watching other people seem to have no problem finishing their work. You can feel unworthy and inferior.

Having a hard time doesn't define you. How you deal with these challenges is what defines you. There will always be walls to climb, for your entire professional career. Start small if you have to, but *being able to start moving in the face of difficulty* is a success in itself.",48qt19,t3_48qt19,yousefjb,,Comment,2,0,2
d15w011,2016-03-19 14:28:34-04:00,yousefjb,,"I respect your edit, I tried to be ambiguous but you cought me. Actually it gets worse, It should be two man project but I am alone, my college didn't write a line of code and avoid me, I gave up about him. All other projects are finished or about so while I am still trying get the project core to work. I am burned out. 

Edit : after your comment I added my professor to the project repo but that didn't help.I split the work into many small tasks that looking to them made me recognize the overwhelming work I have. I started really really think of canceling the project and start another one the next semester.",48qt19,t1_d0nkt2s,lneutral,,Reply,2,0,2
d15wdl6,2016-03-19 14:39:39-04:00,lneutral,,"I've been there. It's really awful when you're counting on a coworker or classmate and they fall through. 

Some time ago, I was the professor in a class where one two-man project fell apart because one of the students didn't attend class for six weeks. This demoralized the remaining student, and they had trouble meeting their requirements. I graded what they submitted, but they had to miss out on having a working demo for the end of the semester.

Do the best you can, but don't be afraid to contact your professor for guidance. If you're working hard, there's a good chance that they'll take it into consideration at the end. And having your project under version control is a good way to record your progress and show that you've been steadily working! ",48qt19,t1_d15w011,yousefjb,,Reply,2,0,2
d0m9wci,2016-03-03 13:08:59-05:00,VainWyrm,,Most likely he's daunted by everything that's left before it actually does anything if there's a piece he can start with actually getting going it may pull him back into the work.,48qt19,t3_48qt19,yousefjb,,Comment,2,0,2
d0m3ya0,2016-03-03 10:55:42-05:00,rfinger1337,,Delete all of his files.,48qt19,t3_48qt19,yousefjb,,Comment,-3,0,-3
48oarp,2016-03-02 15:56:24-05:00,Nimitz14,Finding grid pattern in an image,"Don't know if this is the right place to ask but here goes.

I want to find a grid pattern in an image. The grid does not have to be oriented in the same direction as the image itself, and thanks to noise etc. the grid isn't even uniform across the whole image (slight change between opposite sides of the image). At the moment my naive approach is to systematically check the whole image for lines with varying slopes. 
But progress is difficult, and I'm wondering whether there might be other approaches I could use. 

Note by 'finding' I mean some way of getting the information that would describe the grid-layout in the image. Since it isn't uniform I'm forced to want the information that describes the partial grid-layout (ie for a part of the image).",,,,,Submission,5,0,5
d0l9i9i,2016-03-02 17:01:35-05:00,dandrino,,"One option is to look at frequency analysis. A grid will have very strong telltale signs in the fourier transform of an image (see [here](http://www.robots.ox.ac.uk/~az/lectures/ia/lect2.pdf) for an introduction)

Another option is to use a [Hough transform](https://en.wikipedia.org/wiki/Hough_transform). The Hough transform basically maps pixels (i.e. 2d spatial information) to another space describing the parameters of the feature of interest (e.g. slope and intersect for lines, or (x,y) center and radius for circles). The issue with the Hough transform is that it can be slow (depending on how you implement it and how many dimensions describe the thing you are looking for). For a grid, I can imagine 4 parameters: angle of orientation, line spacing (perhaps another one if the spacing is different for both directions), and translational offset (which counts for 2 because of both x and y directions).",48oarp,t3_48oarp,Nimitz14,,Comment,3,0,3
d0lxe07,2016-03-03 07:26:00-05:00,Nimitz14,,This looks very interesting! Thank you.,48oarp,t1_d0l9i9i,dandrino,,Reply,1,0,1
48nk30,2016-03-02 13:48:06-05:00,Wowsoysauce,going to take CS classes next semester help?,"hi i signed up for code academy and i am unsure what to ""learn first"" out of the language options. what is a good language to start off with? i have no prior computer skills at all if that helps.",,,,,Submission,0,0,0
d0l35m1,2016-03-02 14:49:39-05:00,panderingPenguin,,"Without knowing anything more about you or what you want to do, I would recommend Python as a good place to start. It is well regarded as a clean, generally well thought out language and it has a syntax that is often easier for beginners to understand than some other languages. If you have a specific project or goal in mind though, that recommendation might be different than what you want.",48nk30,t3_48nk30,Wowsoysauce,,Comment,6,0,6
d0l3rdg,2016-03-02 15:02:39-05:00,Wowsoysauce,,"no really specific goal, i just dont want to go in my classes completely blind",48nk30,t1_d0l35m1,panderingPenguin,,Reply,1,0,1
d0l61wj,2016-03-02 15:50:56-05:00,stevenporte1,,Try to find a specific goal. Why did you decide to take CS courses? ,48nk30,t1_d0l3rdg,Wowsoysauce,,Reply,1,0,1
d0l633v,2016-03-02 15:51:36-05:00,Wowsoysauce,,its my major,48nk30,t1_d0l61wj,stevenporte1,,Reply,1,0,1
d0l693s,2016-03-02 15:55:05-05:00,stevenporte1,,So you are planning on taking these codeacademy courses on top of your university courses?,48nk30,t1_d0l633v,Wowsoysauce,,Reply,1,0,1
d0l6co7,2016-03-02 15:57:14-05:00,Wowsoysauce,,i have all summer so i thought it would be a good idea,48nk30,t1_d0l693s,stevenporte1,,Reply,1,0,1
d0lbzxt,2016-03-02 17:58:48-05:00,stevenporte1,,Do you know what courses you are taking starting in the fall? Id try to take some courses on those if they are offered,48nk30,t1_d0l6co7,Wowsoysauce,,Reply,1,0,1
d0l3x18,2016-03-02 15:06:04-05:00,distortedlojik,,"Completely agree, a great first language for getting your feet wet. Especially because it is something that you can continue to use beyond this initial learning stage and it will still be relevant. ",48nk30,t1_d0l35m1,panderingPenguin,,Reply,1,0,1
48l4fs,2016-03-02 02:25:13-05:00,michael1026,"In a graph using an adjacency list, should the edges and node be in order?","    1->2->8
    2->4
    4->1->2
    8->4

Just a bad example. Do the elements have to be in order for it to be a graph? Also, when doing a depth first traversal, do I always have to go to the lowest valued node first? Or can it be in any order?",,,,,Submission,1,0,1
d0ki1dr,2016-03-02 03:13:46-05:00,Syrak,,"The order shouldn't matter. The point of adjacency list representation is that the representation of the neighbors of a node takes space that is linear in the node's degree, as opposed to space linear in the total number of nodes for a matrix. Anything else is an implementation detail.

IMO if a concrete representation uses efficient sets for faster indexing, I would still call this representation ""adjacency list"", with a note somewhere about this peculiarity.

If an algorithm depends on a particular ordering of nodes for its performance, I think it should be noted explicitly somewhere as well.",48l4fs,t3_48l4fs,michael1026,,Comment,3,0,3
d0kxkq2,2016-03-02 12:47:16-05:00,algorithmsWAttitude,,"What Syrak said.  But, as a teacher, I have 2 things to add.  Oftentimes, if I want students to go through an example, I will tell them to assume that the graph representation is in sorted order, not because that would be normal for an implementation, but just because, by using a canonical form, they can try to work towards one solution.  It would be a pain to make an answer key with 100 different solutions for Depth First Search, depending on what order different students used for their lists.

2nd point (disclaimer, self promotion ahead):  I do mention the non-required, non-standard idea of keeping edges ordered in my video, within the 40 seconds following the 2:59 time marker in the given link:  http://www.youtube.com/watch?v=WQ2Tzlxl_Xo&t=2m59s
It also happens to explicitly mention that the list can be in arbitrary order about 30 or 35 seconds after that mark.
",48l4fs,t1_d0ki1dr,Syrak,,Reply,2,0,2
d0l932n,2016-03-02 16:52:59-05:00,Syrak,,I didn't know beamer presentations could be made so lively!,48l4fs,t1_d0kxkq2,algorithmsWAttitude,,Reply,2,0,2
d0laxkh,2016-03-02 17:33:34-05:00,algorithmsWAttitude,,"Seriously, I'm a bit pleased with myself to have anybody say that about a video on graph representations/implementations.  It feels like a pretty dry topic.",48l4fs,t1_d0l932n,Syrak,,Reply,2,0,2
d0laqy2,2016-03-02 17:29:24-05:00,algorithmsWAttitude,,"Tried \usepackage{interesting}, but unfortunately, it took quite a bit more effort than that.",48l4fs,t1_d0l932n,Syrak,,Reply,1,0,1
d0ki44s,2016-03-02 03:18:02-05:00,huehang,,"No, because in your example it practically means that node 1 can reach node 2 and 8 in 1 hop, the order is not important here.

When doing a Depth First Traversal you want to walk down ""as deep/far as possible"", i.e. you might want to start with e.g. node 1:

Now you can either choose node 2 or 8 as both are reachable from node 1 in 1 hop.

1. go to node 2
2. search in your adjacency-list node 2 (line 2 in your representation)
3. go to node 4 as it is the only adjacent node
4. search in your adjacency-list node 4 (line 4 in your representation)
5. and so on...

So, the order is not important",48l4fs,t3_48l4fs,michael1026,,Comment,2,0,2
d0kidcd,2016-03-02 03:32:19-05:00,michael1026,,">  in your example 

Sorry, my example is really bad. It was only meant to show what I meant by in order.

>Now you can either choose node 2 or 8 as both are reachable from node 1 in 1 hop.

So going to 8 would be OK? What I want to avoid is having to go through linked list (edges) finding out which node has the smallest value in it. If I can just go in whatever order my linked list is already in, it makes life easier.",48l4fs,t1_d0ki44s,huehang,,Reply,1,0,1
d0kikff,2016-03-02 03:44:11-05:00,huehang,,"Yes, you could also go to 8 first and go node 4 afterwards (following your example :) )",48l4fs,t1_d0kidcd,michael1026,,Reply,2,0,2
48jrxv,2016-03-01 20:29:17-05:00,kostaz8,Hard time understanding Async Client/Server MSDN code.,"I have understand how the async operation works, and i want to create an async chat server and client but I have stuck with the server. I'm using the MSDN code [here](https://msdn.microsoft.com/en-us/library/fx6588te%28v=vs.110%29.aspx) but I can't understand how it works can someone make me a sequence diagram or a class diagram so I can have a better image of the whole program and tell me why the code does not have the commands I see in most tutorials like await and instead uses allDone() is it better this way of is the only way it can be done? Also when I try to telnet to the server the connection is made but I see no message coming from the server to the client(putty, for testing) why is this happening? Can this code support broadcast?If yes, I have to implement an async method?

[clients code](https://msdn.microsoft.com/en-us/library/bew39x2a%28v=vs.110%29.aspx)",,,,,Submission,0,0,0
48jgzq,2016-03-01 19:14:46-05:00,bamboozeled18,Was wondering if you guys could help me with some programing homework,"Im stuck. It might be a lot to ask but iv'e tried everything. My teachers assistants are saying it shouldn't be that difficult and wont give me any helpful information and they make me feel like an idiot. The problem is 



""This week, we will develop an application that we will call exdrop to detect free fall and narrate the action. In this weeks lab, you drop the esplora into your partners hands (about 1 meter).
   
Sample Output
Your code's output should look like the following.

      Ok, I'm now receiving data.
      I'm Waiting ..............
            Help me! I'm falling!!!!!!!!!!!!!
                  Ouch! I fell 1.633 meters in 0.577 seconds.

After your code has read its first line of data, it should output ""Ok, I'm now receiving data."" This is so that we can tell that it receiving data from the explore.exe.
Each period on the “I'm Waiting” Line should correspond to some number of input lines from explore.exe. Try 10 or so to see what works well. The exclamation points on the falling line should work similarly. (Note: You may want to use a counter and mod (%) for this. Challenge version, 20 bonus points: output them approximately every k milliseconds.) Whatever you do, be sure to use constants defined at the beginning of the program to specify how many lines or seconds equate to one period or punctuation mark."" 

Any help would be great. Thank you
",,,,,Submission,0,0,0
d0q9d3a,2016-03-06 20:56:05-05:00,skunkwaffle,,It's kind of difficult to determine want your actual assignment is from your description. Code samples and clear descriptions of expected vs. actual behavior are always a plus when asking this kind of question.,48jgzq,t3_48jgzq,bamboozeled18,,Comment,1,0,1
48jgha,2016-03-01 19:11:14-05:00,CS_throwaway_GOAL,Is the set in the number partitioning problem assumed to be filled with positive integers only?,"Quick question, the Wikipedia page for the number partitioning problem describes the set being partitioned as containing ""positive integers"". However it seems like if the set was by default limited to positive integers the problem could be solved fairly quickly. Is the default np hard version of the number partitioning problem really assumed to be using only positive integers?",,,,,Submission,3,0,3
d0k3wr9,2016-03-01 20:04:34-05:00,RamsesA,,How would you solve it quickly?,48jgha,t3_48jgha,CS_throwaway_GOAL,,Comment,1,0,1
d0k6fls,2016-03-01 21:06:09-05:00,CS_throwaway_GOAL,,Is there really significance in being able to partition a set of positive integers in polynomial time? ,48jgha,t1_d0k3wr9,RamsesA,,Reply,1,0,1
d0ka3gl,2016-03-01 22:34:05-05:00,okiyama,,What do you mean by significance?,48jgha,t1_d0k6fls,CS_throwaway_GOAL,,Reply,2,0,2
d0kcw05,2016-03-01 23:49:18-05:00,CS_throwaway_GOAL,,"if there is an algorithm that collects the subset data of a set into a table and solves in O(n) time as Wikipedia says there is, why isn't the problem considered solved in polynomial time? ",48jgha,t1_d0ka3gl,okiyama,,Reply,1,0,1
d0kdb2s,2016-03-02 00:02:02-05:00,okiyama,,"I don't think you could construct that table in linear time, right? ",48jgha,t1_d0kcw05,CS_throwaway_GOAL,,Reply,2,0,2
d0kg7mg,2016-03-02 01:44:49-05:00,CS_throwaway_GOAL,,"I wouldn't think so either, but the Wikipedia page says there is an algorithm that can solve it using dynamic programming in O(kn) where k is the sum of the set (so obviously k will usually be several times larger than n, I guess that's probably why it's called pseudo-polynomial) ",48jgha,t1_d0kdb2s,okiyama,,Reply,1,0,1
d0kd1m0,2016-03-01 23:54:01-05:00,CS_throwaway_GOAL,,Is it because it runs in O(Kn) time and k can get much larger as n scales?,48jgha,t1_d0kcw05,CS_throwaway_GOAL,,Reply,1,0,1
d0kxu6t,2016-03-02 12:53:13-05:00,algorithmsWAttitude,,"K and n are intrinsically different.  n is a number of inputs.  If you double the number of items, the number of inputs doubles, and the space to input those values should also double.

K is the value of one input.  Writing down a number K requires order lg K bits.  Doubling the value of K doesn't require double the space, it requires a constant extra amount of space.  The space used for K's input grows only logarithmically with the value of K, so the value of K grows exponentially in the space of its input.",48jgha,t1_d0kd1m0,CS_throwaway_GOAL,,Reply,1,0,1
d0kwpsy,2016-03-02 12:27:59-05:00,algorithmsWAttitude,,"Both versions are NP hard.  I don't know that you need to make one version the ""default"" version, but if you are going to, it makes sense to make the positive integer one the default:  it is a more restricted problem.  If the more restricted problem is NP hard, then certainly the less restricted problem is also NP hard, because every instance of the restricted problem is also an instance of the unrestricted problem.",48jgha,t3_48jgha,CS_throwaway_GOAL,,Comment,1,0,1
d0ka2z5,2016-03-01 22:33:44-05:00,okiyama,,"If that mattered to solving the problem quickly you could transform a set with negative elements into just positive elements before you start. Just go through the whole thing (O(n)), finding the negative number with greatest magnitude and add that magnitude to all the numbers (O(n) again). When you're done, subtract that magnitude from all numbers.

So it can't make any difference, because you can arrive at an equivalent problem with only positive numbers by adding in a finite number of O(n) steps.",48jgha,t3_48jgha,CS_throwaway_GOAL,,Comment,1,0,1
d0kccwr,2016-03-01 23:34:16-05:00,None,,[deleted],48jgha,t1_d0ka2z5,okiyama,,Reply,1,0,1
d0kcnd1,2016-03-01 23:42:18-05:00,CS_throwaway_GOAL,,"if I understand correctly, You would add 2 to every number to get 0, 3, 6, 7 then partition that into {0,7} and {3,6}(the two sets with the lowest difference) then subtract 2 from every number to get {-2,5} and {1,4} (the actual answer)
",48jgha,t1_d0kccwr,None,,Reply,1,0,1
d0kd7sz,2016-03-01 23:59:15-05:00,okiyama,,"Yup, that's the idea. Because we're doing sums this works. It would break if you have to multiply the numbers. ",48jgha,t1_d0kcnd1,CS_throwaway_GOAL,,Reply,1,0,1
d0kwmt7,2016-03-02 12:26:10-05:00,algorithmsWAttitude,,"The only reason this example works is because both solutions happen to use 2 items.  With the numbers {-2, 1, 3, 4, 12}, if you add two to each number, you get {0, 3, 5, 6, 14}, where 0+3+5+6 = 14, but the original set has no solution.",48jgha,t1_d0kd7sz,okiyama,,Reply,1,0,1
d0kx9fy,2016-03-02 12:40:13-05:00,okiyama,,"Oh yeah duh, that makes sense. Thanks! ",48jgha,t1_d0kwmt7,algorithmsWAttitude,,Reply,1,0,1
d0kci54,2016-03-01 23:38:17-05:00,CS_throwaway_GOAL,,"This is an incredibly enlightening answer, thank you so much ",48jgha,t1_d0ka2z5,okiyama,,Reply,1,0,1
d0kd9tc,2016-03-02 00:00:57-05:00,okiyama,,"Hah no problem, glad to be of service =) ",48jgha,t1_d0kci54,CS_throwaway_GOAL,,Reply,1,0,1
d0kwhpj,2016-03-02 12:23:04-05:00,algorithmsWAttitude,,This transformation does not work:  the answer to the transformed problem will only give an answer to the original problem if they happen to have the same number of items.,48jgha,t1_d0ka2z5,okiyama,,Reply,1,0,1
48en2c,2016-03-01 00:01:12-05:00,TheSageMage,Hands-on ways of learning software design patterns?,"I've been reading the wikipedia entry [software design patterns](https://en.wikipedia.org/wiki/Software_design_pattern), yet I feel like I would learn these better if there were some type of koans or a more hands on type ""assignment"" to perform for many of these. 

What are some great resources for learning these in a hands-on capacity?",,,,,Submission,5,0,5
d0j4ukw,2016-03-01 02:40:47-05:00,None,,[deleted],48en2c,t3_48en2c,TheSageMage,,Comment,1,0,1
d0jdmr5,2016-03-01 10:00:08-05:00,MagnificRogue,,This! Add in [this handy design patterns in java](http://www.tutorialspoint.com/design_pattern/) tutorial and you can get a really good understanding on how these design patterns work. (Source: Design patterns midterm is tomorrow),48en2c,t1_d0j4ukw,None,,Reply,2,0,2
d0ji5cf,2016-03-01 11:49:29-05:00,yousefjb,,I tried it a month a go but I found it so hard to read. I just postponed it and picked something easier.,48en2c,t1_d0j4ukw,None,,Reply,1,0,1
d0jj0wp,2016-03-01 12:08:54-05:00,ukkoylijumala,,"I found this pretty hands-on and easy to read and follow. The examples are in Java.

http://shop.oreilly.com/product/9780596007126.do",48en2c,t3_48en2c,TheSageMage,,Comment,1,0,1
48c4wu,2016-02-29 15:10:41-05:00,linkkb,What's the best way to go about job hunting as a new grad without any dev experience?,"I'm going to be getting my shiny new degree in CS soon from a good school, but I never did any internships and most of the interviews I've done seem to be looking for someone with more developer experience. Eventually I'd like to be a developer for an educational software company, but honestly I just want to be able to pay my bills. Should I just go ham with the job hunt and hope I stumble on to something, or take some time to work on personal projects to build up my resume?",,,,,Submission,7,0,7
d0iha1x,2016-02-29 15:38:53-05:00,CoopNine,,"Apply anywhere they are asking for 3-5yrs exp, or 3+.  The market is very competitive from an employers' standpoint, we're looking for people who have high potential, but will frequently post 3+ as a requirement when it's really more of a plus.

It is very hard to find people who are eligible to work without sponsorship (visa) and aren't uh... low potential individuals would be a nicer way of saying it.  This is at least the case where I am.  I am easily seeing 10 applicants are not eligible to work without sponsorship for every one who is.  

In the interviews be excited about the potential job.  Ask about what technologies they use and how they do things.  Ask questions about their business and the problems they try to solve.  Make sure they see that you're excited about the opportunity and really want to learn!  Seriously.  Excitement about learning and interest in the business helps show someone that you have potential to grow within the company.  Most hiring managers hate that the industry has such a high turnover as of late.  We'd rather hire someone who can grow within our company than someone who is there for the next 1-5 years.",48c4wu,t3_48c4wu,linkkb,,Comment,5,0,5
d0j239p,2016-03-01 00:38:32-05:00,Davidboo25,,I can't stress this enough. I had an interview for a co-op at a sewage robotics company in my area and went in knowing almost nothing about them. I was excited to hear about the technologies they had and asked plenty of questions. I felt as though my excitement and willingness to learn was exactly what they were looking for. ,48c4wu,t1_d0iha1x,CoopNine,,Reply,2,0,2
d0ih87g,2016-02-29 15:37:45-05:00,lneutral,,"Personal projects are always a good thing. A lot of people on the sub suggest contributing to open source projects, which is probably even better.

Two things you might keep in mind:

1. There *are* jobs at what could be called entry level, just not as many as there are for more experienced programmers. A lot of them are at smaller companies, startups, etc. and at big huge companies with higher turnover. If you have a good linkedin or know a few other people who have experience, it will make it easier to find those jobs.

2. Make the most use of the job opportunities you do encounter. Polish your resume: include things relevant to the job that make you look professional, but maybe leave off the summer you worked at Taco Bell. Sharpen your interview skills. Read Cracking the Coding Interview, and solve the sample problems. *Be Prepared*. You don't have to look like a college professor to get a job, but you'd be surprised how many applicants to software jobs don't have basic interview skills.

Keep at it - make use of your university's career services, whether it's mock interviews, job fairs, or at the very least, job postings. Good luck!",48c4wu,t3_48c4wu,linkkb,,Comment,6,0,6
d0j0zjg,2016-03-01 00:00:14-05:00,coned88,,"Get a job while you are still in school. That means take advantage of companies hiring new grads.

Do not wait until you graduate to do this. If you do so you just lump yourself in there with everybody else. You're no longer a student. Now you're a fresh grad with no experience. Companies make exceptions for those students who have no experience. Not most of the rest",48c4wu,t3_48c4wu,linkkb,,Comment,2,0,2
d0jr1ln,2016-03-01 15:05:19-05:00,linkkb,,"Well, I *had* a job, it was just doing IT work instead of software development.

What would you recommend for me now that graduation is almost here?",48c4wu,t1_d0j0zjg,coned88,,Reply,1,0,1
d0lgloj,2016-03-02 20:08:38-05:00,coned88,,"I'd keep applying for jobs via the career services dept at your school. Speak to the people at the office. Reach out to companies and see if they are still looking. You can find out a ton of info on linkedin.

Check company websites and see if they have student jobs listed. Many times companies do have them separate. ",48c4wu,t1_d0jr1ln,linkkb,,Reply,0,0,0
d0je9n1,2016-03-01 10:17:08-05:00,videoj,,">Eventually I'd like to be a developer for an educational software company

Find an open source project in your field of interest that you can contribute to, or start your own.   Use sites like http://codereview.stackexchange.com/ to improve your code and show your progress.  ",48c4wu,t3_48c4wu,linkkb,,Comment,1,0,1
48awy9,2016-02-29 11:09:59-05:00,asymptoticLimit,Looking for authoritative reference material on XML schemas,"I am working on converting Context Free Grammar specified in BNF (Backus-Naur Form) to XML schema definitions. I need an understanding of all the features XML schemas offer before designing the system. I am unable to find free material on the web that offers a comprehensive overview of XML schemas. 

I find the w3.org references verbose and technical. Please suggest me the  right resources !",,,,,Submission,1,0,1
d0idkus,2016-02-29 14:17:39-05:00,Cadoc7,,"W3.org references are the spec and the only authoritative source. I'm surprised you think they are too verbose and technical for your purpose since they lay out the BNF structure of XML. If you want a higher-level view, MSDN has a lot of information on XML because the Microsoft stack is heavily dependent on XML still. Try this for starters: https://msdn.microsoft.com/en-us/library/aa468557.aspx",48awy9,t3_48awy9,asymptoticLimit,,Comment,2,0,2
d0ij6yo,2016-02-29 16:21:02-05:00,asymptoticLimit,,"Thanks for the link. I agree with you that W3.org specs are the only authoritative source. But, the level of detail is unnecessary for my purpose.
My interest is only in translating production rules into correct XML elements hierarchy. ",48awy9,t1_d0idkus,Cadoc7,,Reply,1,0,1
d0q9f44,2016-03-06 20:57:39-05:00,skunkwaffle,,Are you locked into XML? This sounds like a situation where you might have an easier time of it using JSON.,48awy9,t3_48awy9,asymptoticLimit,,Comment,1,0,1
489538,2016-02-29 02:53:35-05:00,mongo-js,"What are good reasons to change company, change job in our industry?","I am currently in not so bad company, strong position. Yet I really want to change to some new job, I would love to work on new project/new people. Is this good anough reason?",,,,,Submission,4,0,4
d0hwjzk,2016-02-29 05:02:22-05:00,ilikestring,,"More money. New challenges. 

Your reason is perfectly good enough. Even it isn't, who cares? Do whatever you want. Just make sure to get a job offer before you leave your current role.",489538,t3_489538,mongo-js,,Comment,9,0,9
d0iflb7,2016-02-29 15:02:05-05:00,scriptmonkey420,,"This is the reason I left my last job. I had been there 4 years, was getting too comfortable, but also wasn't getting paid what I thought I should for what I was doing. Got a call for an interview one day, went and did the interview. That Monday they called and gave me an offer. I gave my old job a chance to counter but they declined. So here I am.",489538,t1_d0hwjzk,ilikestring,,Reply,2,0,2
d0hwpor,2016-02-29 05:13:43-05:00,kinkyaboutjewelry,,"Stay while you're energized, learning, growing. Move when something else entices you more. It will energize you more, hopefully help you learn and grow more. What to avoid: stay until your energy is depleted, then you won't have energy to evaluate your other options fairly. The timing will vary from person to person, from industry to industry. You will likely overstay a bit, then move. Just don't stay to long. ",489538,t3_489538,mongo-js,,Comment,3,0,3
d0hy7l8,2016-02-29 06:54:31-05:00,eygrr,,"There's no point playing around in reasons if you have something you already want to do, all you can end up doing is justifying it so it's good and thinking that it has no flaws, or justifying it so it's bad and then feeling bad about wanting to do it. IMO just go for it seeing as you already want to do it.",489538,t3_489538,mongo-js,,Comment,1,0,1
d0i6ji2,2016-02-29 11:36:28-05:00,umib0zu,,"Nope. Sorry to burst your unicorn bubble but if you have no complaints, have a strong position, and are getting paid at market rates, you should not jump ship. There's risk involved with moving to a new company since the grass isn't necessarily always greener. There's plenty of great stories about cool perks at other companies like unlimited PTO, open bars, and great looking new faces, but for every company like that, there's 20 that aren't like that.

In terms of actually branching out and meeting new people, it sounds like you just need a hobby. A good after work hobby can meet your social needs, and you won't have to risk anything at all.",489538,t3_489538,mongo-js,,Comment,0,0,0
4894j2,2016-02-29 02:48:35-05:00,justinj001,minimum pumping length of Pumping lemma,Is **M.P.L** related to *string* or to the *language* of a fininte automata?How do we find the M.P.L?,,,,,Submission,1,0,1
d0iczx1,2016-02-29 14:04:31-05:00,_--__,,"The minimum pumping length is a property of the language.  You can find it by constructing the minimal automaton for the language and examining its structure (esp. with regard to the proof of the pumping lemma).  Note that the pumping length may be smaller than the number of states of the minimal automaton, but it will never be bigger than it.  For the purposes of most questions involving the pumping lemma however, you can just assume the existence of a pumping length and work from there.",4894j2,t3_4894j2,justinj001,,Comment,2,0,2
487u6z,2016-02-28 21:22:55-05:00,koodeta,[MPICH] Is there a better way to parallelize a non-parallel computing Java application to run on multiple nodes?,"Disclaimer: I'm not sure if this is the right place to post. If not, I'll post this elsewhere. (Resubmitted b/c of incomplete title).

**Backstory**

I built compute cluster out of raspberry Pi 2s. I'm using the MPICH library from Argonne National Labs and a Java application for scientific analysis. The purpose of building this is to learn parallel computing with actual hardware.

**Issue**

The issue itself resides in the application I'm using to actually analyse the data - it's not built to run in parallel computing scenarios, only on a single computer. Since I'm trying to use it in a compute cluster, this restriction handicaps my end goal.

**What I've done**

The cluster is built, MPI does communicate with each node, the application can analyse on a single node and I've been experimenting the past few days with some success.

I've gotten a command to run but AFAIK the process executes on each node (which is standard for MPI) but it seems they all write over the same file on a NFS share. As a result, instead of taking 38 seconds to built the file, it takes 1 minute and 38 seconds when calculating with four nodes each with a single process allocated.

**Question**
I can think of three possible methods to go about doing this.

* I can write a C program that hooks into the Java application.

* I can edit code I haven't looked at nor developed to implement the Java MPI library.

* I can use some other method I haven't thought of yet.

Is there a better way of doing this?",,,,,Submission,1,0,1
4875ib,2016-02-28 18:55:41-05:00,colinroberts,What's the best way to cluster similar strings of names?,"Say I have rows in a database, and each has a name value like so:

""Colin Roberts""

""C. Roberts""

""Dr. Roberts""

""C.R.""

""Mr. Roberts""


What's the best way to cluster listings so that rows that are likely to be the same person (as above) are a part of the same group? In my case, last names are always going to be more common than firsts. ",,,,,Submission,8,0,8
d0hgs5f,2016-02-28 19:34:54-05:00,high_side,,"I think there are algorithms that hash on textual similarity.  Never used them though.  Still probably makes sense to split the name into first/last/title.

Alternatively you could create a table that maps similar names.  E.g. name id 234 is similar to name index 501 with a score of 30.  Score them using some text similarity engine.",4875ib,t3_4875ib,colinroberts,,Comment,3,0,3
d0hh2x9,2016-02-28 19:43:30-05:00,colinroberts,,Know the names of the algorithms?,4875ib,t1_d0hgs5f,high_side,,Reply,1,0,1
d0hh9qu,2016-02-28 19:48:59-05:00,CodeReclaimers,,Nilsimsa hash,4875ib,t1_d0hh2x9,colinroberts,,Reply,2,0,2
d0hjj2b,2016-02-28 20:55:16-05:00,themeaningofhaste,,"Not a name but you could try looking at what the Python package [FuzzyWuzzy](http://chairnerd.seatgeek.com/fuzzywuzzy-fuzzy-string-matching-in-python/) does under the hood. Or a spell-checker package like [PyEnchant](http://pythonhosted.org/pyenchant/tutorial.html). I think a lot of these kinds of packages use [NLTK](http://www.nltk.org/) as a backbone, so you could try starting there.",4875ib,t1_d0hh2x9,colinroberts,,Reply,1,0,1
d0hkoki,2016-02-28 21:27:46-05:00,i_invented_the_ipod,,"[Soundex](https://en.m.wikipedia.org/wiki/Soundex) is a ""sounds similar"" algorithm for English. The general category of algorithms are known as ""phonetic algorithms"". You might also consider Levenshtein [edit distance](https://en.m.wikipedia.org/wiki/Edit_distance), though it's not totally clear to me how you would turn that into a collation sequence. ",4875ib,t1_d0hh2x9,colinroberts,,Reply,1,0,1
d0hojvn,2016-02-28 23:15:56-05:00,colinroberts,,"I've used this before for other things, but it wouldn't really be useful when trying to cluster names around into specific individuals, because a Mr. ""Roberts"" and Mr. ""Rhobertz"" are going to be two different people, but have the same soundex() value.",4875ib,t1_d0hkoki,i_invented_the_ipod,,Reply,1,0,1
d0hk5gj,2016-02-28 21:13:00-05:00,IrresistablyWrong,,"There are algorithms to deal with general string similarity but your mileage will vary. Best results are from mixing a well-picked string similarity algorithm with logic based on the domain of the data. I've used w-shingles in the past with good results. Some domain-specific ideas from your data samples: perform normalization on the strings to deal with semantic imprecision. ""Dr, Mr"" could be omitted pre-comparison.  Initialized names ('C.' or 'R.') should probably be recognized and handled separately. String similarity fails miserably when there is only one letter to compare against N letters. If you are performing record depduplication, use all partially identifying data in scoring the dedup. You can discover partially identifying data by picking fields with low value frequencies. By generating hashes from shingle sets on partially identifying fields, an algorithm such as minhash can be used to cluster similar data for more thorough comparison. This avoids O(n^2) on beefy string comparision. ",4875ib,t3_4875ib,colinroberts,,Comment,2,0,2
d0hk27n,2016-02-28 21:10:22-05:00,Sqeaky,,"I am not sure what your goals are, but I might consider using [Trie](https://en.wikipedia.org/wiki/Trie) or three. You could have a trie for first, middle, last and title or just one trie for them all, then a name could be small collection of integers that refer to indexes in the trie.

",4875ib,t3_4875ib,colinroberts,,Comment,1,0,1
486nn1,2016-02-28 18:03:37-05:00,bryceguy72,Do any browsers support *TRUE* history?,"I'm using Chrome to browse reddit's front page, I see something interesting in the sponsored link at the very top but by the time I realize it's interesting, I've already clicked somewhere else.  I click ""back"" but that interesting link is gone, replaced by another link.  This means when I click ""back"" in Chrome, it doesn't retrieve the previous page from the browser cache but re-downloads that information. Is there any way to force Chrome to actually use its cache and not download anything when I click ""back"" ?   I suppose I can resort to various trickery like closing my internet connection but I shouldn't have to do that.  When I click ""back"" I would like to see the previous page EXACTLY as I last saw it, including all ads.  Do any browsers support this?",,,,,Submission,4,0,4
d0he220,2016-02-28 18:20:02-05:00,flebron,,"What displayed the ad may well be a script that runs at some point after the page loads. The script makes some ajax call, or opens some iframe, or computes something, or all 3. In order to restore the exact thing you were seeing, you'd need to load the entire state of that script, including any memory that it had requested. No, no browsers that I know of will save the entire memory contents of the JS execution engine, between every click you make. That would be terrible.

There's also the issue that browsers should respect caching directives given by pages. An ad page can very well tell the browser ""Cache this for 0ms"", which would mean you'd have to re-load it every time you want to look at that ad's contents. This is the norm with ads.",486nn1,t3_486nn1,bryceguy72,,Comment,9,0,9
d0hkd8u,2016-02-28 21:19:06-05:00,bryceguy72,,"Thanks for explaining, I had no idea it was that involved.",486nn1,t1_d0he220,flebron,,Reply,1,0,1
d0hqt1t,2016-02-29 00:16:08-05:00,zefcfd,,"closest you'll get is safari, they use jog loading. it uses an image of a page when you go back then super imposes the real page once it loads, if you look up the folder that contains the screen shots you can always look in there for the last page snapshot. although i think this is kinda sketchy for safari to do, but im sure its just stored locally, so its not a huge deal.",486nn1,t3_486nn1,bryceguy72,,Comment,1,0,1
484w8z,2016-02-28 14:36:39-05:00,BenRayfield,"What kind of chip or software is efficient for looping an array writing each next scalar as different compiled math of earlier in the array, with input at start of each loop?","Considering cache locality, a small array is best.

Input goes in the first n indexs of the array.

Next is m stateful vars which at the end of each loop are copied from last m vars, which are derived from whatever is earlier in the array.

At end of each loop, input again to first n vars, and copy the last m vars to just after those n, and loop again.

This would continue until some condition is read somewhere in the m vars.

This abstraction can handle any single loop, any unitary function (such as crypto), and most kinds of simulated musical instruments. I'm planning to use it with microphone as input and speakers as 2 of the m vars and the other stateful vars will be for echos and other sound effects.

Could this run in a GPU? Or will I get no benefit of it since it keeps updating the memory with input every loop? How about some kind of stream processor? I'll of course start with CPU only but I'm asking whats possible to optimize this kind of math abstraction.",,,,,Submission,1,0,1
d0hzfkn,2016-02-29 07:59:36-05:00,east_lisp_junk,,"I have no idea what you mean by ""compiled math"" (would it kill you to use established terminology for a change?), but it looks like the only parallelism opportunity is in computing each of the m new values at once. If each set of m variables depends on the previous set, that loop-carried dependence will keep you from generating multiple sets of them in parallel. A GPU likely won't help unless the computation you do for each of the variables is pretty much the same.",484w8z,t3_484w8z,BenRayfield,,Comment,2,0,2
d0hewc1,2016-02-28 18:42:32-05:00,lneutral,,"Generalization of a problem is at odds with optimization. *The more specific and constrained your problem is, the better the optimization strategy can become.*

In some cases, vectorization can be applied to problems of that sort. In others hardware exists. But finding a one-size-fits-all approach that will optimize DSP, cryptography, and other generic tasks is more challenging. 

No, GPU will not handle the use cases you're describing. Not at that level of abstraction. Formulating individual optimized GPU routines per problem might be doable, but it's hard.

The thing to do is to write your code and solve your problem, then optimize. Premature optimization can make your life really hard. ",484w8z,t3_484w8z,BenRayfield,,Comment,1,0,1
d0hgi4o,2016-02-28 19:26:59-05:00,BenRayfield,,"constrained to standard float32 ops such as plus(float,float), multiply(float,float), neg(float), exp(float), max(float,float).

> write your code and solve your problem, then optimize

Cant do it. My problem is getting many people to compute play and research together in realtime. The services of experts are not available for every little thing they do together. It must be done in advance for all combinations of some basic patterns.

I dont need crypto or the normal DSP ops. I do need the ability to define new musical instruments, generated by arbitrary code, at least 100 times per second per person playing this ""game"".",484w8z,t1_d0hewc1,lneutral,,Reply,1,0,1
d0hj0t1,2016-02-28 20:40:06-05:00,lneutral,,"> constrained to standard float32 ops such as plus(float,float), multiply(float,float), neg(float), exp(float), max(float,float).

These are still really loose constraints. Nearly any algorithm can be put together with out of basic opcodes, but only some of them are efficient. Basic strategies of effective programming will help, but you can't expect that an arbitrary block of code is as effectively optimized as a very specific one.

> Cant do it. My problem is getting many people to compute play and research together in realtime. The services of experts are not available for every little thing they do together. It must be done in advance for all combinations of some basic patterns.

I question this requirement. You're being vague about the problem. If you don't have an effective algorithm to do the thing you need, you can't optimize it. Not program - algorithm.

If what you're saying is you have an algorithm, but you can't reach the performance figures you need, then you've got two options: more hardware resources or more efficient software. But you must have something to start from in either case.

> I dont need crypto or the normal DSP ops. I do need the ability to define new musical instruments, generated by arbitrary code, at least 100 times per second per person playing this ""game"".

It sounds like you *do* have a more specific constraint, but you haven't explicitly described it.

Let me make a guess? It sounds like you're doing some kind of synthesis (maybe FM synthesis?) and you want to have user-defined blocks for processing waves. This is a more constrained problem than any arbitrary function.

The more information you provide, the more specific you can be, the better the advice you'll get will become.",484w8z,t1_d0hgi4o,BenRayfield,,Reply,1,0,1
480nig,2016-02-28 01:51:14-05:00,brownthunder317,Need help understanding MD5 hash function,"I was reading about the MD5 message-digest algorithm and saw that MD5 shouldn't be used since they're very easy to crack. I was wondering if anyone could explain to me how you would go about generating a password given just the MD5 hash function, specifically using the example provided here: https://en.wikipedia.org/wiki/MD5#Pseudocode",,,,,Submission,6,0,6
d0gpeaa,2016-02-28 03:02:20-05:00,Rangsk,,"I believe your question actually demonstrates a lack of knowledge of how and why hashes are used in the context of passwords, so I think it's best to explain that first.

The most naive way to do password validation is the following *(NOTE: by naive I'm using the technical term, to mean that's it's straightforward and simple)*:

1) User creates account with username u and password p

2) Server has table with column username|password. It stores a new row: u|p

3) User logs into account typing username u and password p'

4) Server looks up the row for username u and compares p to p'. If they match, login is allowed.

The issue with this method is that anyone with access to the database now knows this user's password, which can be used to access their account, and quite likely their other accounts due to password re-use. We want to avoid this, especially for cases where a hacker gains access to the database.

So, this is where hashes come in. A hash is a *one-way* function which we'll call h, which transforms one value into another value. Instead of storing p directly, we can store h(p). Here's what that looks like:

1) User creates account with username u and password p

2) Server has table with column username|password_hash. It stores a new row: u|h(p)

3) User logs into account typing username u and password p'

4) Server looks up the row for username u and compares h(p) to h(p'). If they match, login is allowed.

You can see the process is pretty much the same, except instead of storing and comparing p, you're hashing it first and comparing the hashes. This has a big advantage that now if a hacker has access to the database, they still can't log into the account, because there's no good way to generate a value v such that h(v) equals h(p).

Modern methods make this more complicated, but this is the concept that's needed for our conclusion.

Let's say you choose to use md5 for your h function. Well, md5 is what's called a ""broken"" hash, in that it's been mathematically shown that given a hash, you can generate a value which, when md5 is run on it, generates that hash value. So, you don't get to know their original password, but you do know a password which hashes to the same value, and thus can be used for login.

I don't actually know the method that's used to do this, and it may be complex and still use quite a bit of computing power, but the point is that it uses significantly less computing power than if md5 were not broken. Thus, it's considered much safer to use a hashing algorithm which is not broken, of which there are plenty of choices.",480nig,t3_480nig,brownthunder317,,Comment,10,0,10
d0gprc9,2016-02-28 03:23:16-05:00,brownthunder317,,"Thanks so much for that answer! I understand that trying to find an h(v) that becomes the same as an existing h(p) becomes very difficult with a proper hash function. My follow up question would be this: given the hacker only knows the md5 method, how does he even know any existing h(p)? It seems to me that this issue would only arise given that the hacker knows some existing h(p). Otherwise it seems like trying to crack a password is exactly the same as sitting and just guessing random words. ",480nig,t1_d0gpeaa,Rangsk,,Reply,2,0,2
d0gpvft,2016-02-28 03:30:04-05:00,Rangsk,,"In this case, the hacker has gained access to the database through some means, and thus has direct access to the table which stores username|password_hash. At this point, he's trying to do something *useful* with that data (usually to make money, or steal secrets, or some other nefarious purpose).

You can probably use your imagination to come with 100 ways to use a username|password table, but the usefulness of a username|password_hash table is far more limited if a strong hash is used. The weaker the hash, the more possibilities arise.",480nig,t1_d0gprc9,brownthunder317,,Reply,3,0,3
d0gp5sm,2016-02-28 02:49:10-05:00,zmanalpha,,Getting a piece of data back from a hash isn't like decryption. There are things called rainbow tables that stores massive amounts of passwords and their hashes. Google's search engine uses MD5 for its indexing. As a result you can practically type something into Google and get its MD5 hash. ,480nig,t3_480nig,brownthunder317,,Comment,2,0,2
47znl2,2016-02-27 21:11:42-05:00,netdoc41,Does anyone have any truly good experiences with an IT offshore/outsourcer?,,,,,,Submission,1,0,1
d0glfnk,2016-02-28 00:04:10-05:00,port25,,"I'm in operations,  so my perspective will be different from development groups. 

My offshores have Good days,  bad days,  strong techs,  weak techs. A strong tech can have a bad day, and a weak tech can have a good day. 

You get what you give with offshore. They are extremely good with process/rule based flowchart decision making.  They are bad with out of the box puzzle solving, which Americans excel at. If you spoon feed them exactly what needs to be done,  they will execute exactly to spec. Give them vague requirements and they will give you pretty random results. ",47znl2,t3_47znl2,netdoc41,,Comment,2,0,2
47xsfw,2016-02-27 15:20:10-05:00,theoryofcomp,Can you think of a scenario in which a class doesn't change but propagates its change to its neighbors?,I'm talking about a class inside a system. The system can be fictional,,,,,Submission,3,0,3
d0g5dvl,2016-02-27 16:04:50-05:00,Tahlvain,,"Maybe I'm misunderstanding the question, but do you mean something like an Observer pattern? https://en.m.wikipedia.org/wiki/Observer_pattern",47xsfw,t3_47xsfw,theoryofcomp,,Comment,3,0,3
d0g6hnc,2016-02-27 16:37:01-05:00,CoolGuyJay,,"I'm not sure what you mean, but the only thing I can think of is Inheritance between parent/child (base/derived) classes. Is that what you're asking about?",47xsfw,t3_47xsfw,theoryofcomp,,Comment,3,0,3
d0g7zui,2016-02-27 17:20:05-05:00,5kybird,,I can think of a delegation pattern with Obj-C / Swift where the class signals a delegate of a state change. For example - if UIScrollView does actually scroll in any direction it notifies a delegate object which can then implement behaviour on its behalf. The examples from Apple make the UIScrollView its own delegate but it doesn't have to be. It might help if you give more information as to why you want to know?,47xsfw,t3_47xsfw,theoryofcomp,,Comment,1,0,1
47o1xx,2016-02-26 02:52:55-05:00,FrostyTacoXI,How can I become competent?,"I'm currently a 2nd year CS student currently learning low level programming for embedded systems and in addition I'm learning algorithms and data structures in another class. 

An issue I seem to be having is learning how to think logically with problems presented to us for our projects and assignments in order to solve the problem. I rely heavily on searching on Google for what I'm looking for and it takes me a much longer time to solve problems and think of them logically as compared to working with a group of classmates. 

My question is, how do I become competent at thinking logically of the programming problems presented to us quickly?",,,,,Submission,3,0,3
d0eh0l1,2016-02-26 06:50:32-05:00,Ready_Player_Two,,"I would just advise to break down any problem into the smallest chunks possible and then solve them one at a time. This way, you will slowly build each application bit by bit and rather than solving one big problem, you solve many small problems.  

That said, the way you think will develop with experience and before long you will instinctively know, or at least have a general idea, of how each problem can be solved. You might not know the exact code to write, but you will know roughly what actions you need to take. It might help by spending five minutes thinking (really thinking, using diagrams, writing pseudo-code, etc.) through each problem before jumping to Google, as sometimes all it takes to solve a problem is a bit of time.",47o1xx,t3_47o1xx,FrostyTacoXI,,Comment,6,0,6
d0ehsov,2016-02-26 07:32:39-05:00,vz83,,"If this is any help, somethings I would suggest are (1) Going back to the basics of logic (things like code academy and hour of code) (2) Commenting off portions of code until you find where any potential errors may be. (3) Find a fun project to work on in your free time - you'd be surprised how much you can learn when you're doing what you like. 

Hope at some of that helps :) 

Source: Experiences in Java, PHP, etc. ",47o1xx,t3_47o1xx,FrostyTacoXI,,Comment,3,0,3
d0eiw0z,2016-02-26 08:21:25-05:00,EddieTH,,"Practice a whole lot. Honestly, just go to hackerrank and practice doing logic and puzzle questions. Theory can only get you so far, application is far more useful, imo",47o1xx,t3_47o1xx,FrostyTacoXI,,Comment,3,0,3
d0eshj5,2016-02-26 12:34:35-05:00,algorithmsWAttitude,,"More practice.  Less Google.
It takes a long time to build this skill, and part of the learning process is struggling with issues that will seem easy once you really understand them.  Searching outside sources will help you to get the answer more quickly, but if you rely on it too much, or too early, it will also hinder you from understanding how to find those solutions yourself.  Struggle with a problem, have it take 5 times as long as it would if you just looked for an answer, and after you figure it out, everything you do after that will be just a touch faster.  Do it enough times (which takes a lot of freaking time), and your efficiency will come up to speed.  In music, you won't learn to sight read very well if, for every piece, you first listen to a CD of somebody else playing it.",47o1xx,t3_47o1xx,FrostyTacoXI,,Comment,3,0,3
d0ra8lq,2016-03-07 17:08:18-05:00,delphium226,,Can't agree with this enough. I've recently started teaching a high schooler who joined us for a 4 week work experience and she's taken the google-everything approach. It failed badly. We've had to take it back to basics with some Google code classes.,47o1xx,t1_d0eshj5,algorithmsWAttitude,,Reply,1,0,1
d0ef3ts,2016-02-26 04:41:30-05:00,reallytommy,,"Man, I really want to know how too. Struggling in the same way",47o1xx,t3_47o1xx,FrostyTacoXI,,Comment,2,0,2
47nzxw,2016-02-26 02:31:58-05:00,RexBox,"In image processing, I understand how the Canny edge detector works. But is there an algorithm for comparing 2 pictures of edges and seeing wether they are the same?","Bassicly the title. We want to see if one object looks the same on different photos. We would shoot from the same angle of course, but in slightly different conditions. 

Anyway, now we have discovered ways to find the edges on both photos, but does an algorythm exist on how much alike both edges are?",,,,,Submission,8,0,8
d0ei1n1,2016-02-26 07:44:52-05:00,jkfgrynyymuliyp,,You should probably post this in /r/computervision too.,47nzxw,t3_47nzxw,RexBox,,Comment,3,0,3
d0eo2u8,2016-02-26 10:54:01-05:00,lneutral,,"The short answer is yes, although edge features are not used this way very often these days. 

Comparisons like the one you're describing can use all sorts of different invariants - properties that don't change between images of the object. Many make use of landmarks, or special points of interest that are easy to find: often, corners are used, along with the texture surrounding them, adjusted to minimize varying appearance due to lighting. Other approaches exist, but corners (often computed with detectors like SIFT or SURF) are popular. 

This isn't to say edges are never used - but keep in mind that you want to use features that will be stable under common changes from photo to photo, such as lighting angle and amount, or slight changes in translation, scale, and rotation. 

Detection, registration, and tracking are all related subjects. I'd also highly suggest /r/computervision if you need help with specific algorithms. ",47nzxw,t3_47nzxw,RexBox,,Comment,1,0,1
47npcg,2016-02-26 00:52:23-05:00,ffranglais,"If you have some data, and you want to search for specific strings of characters (an expression with x number of characters between quotation marks, or x number of spaces between characters), what techniques do you use to search?","Is this just regular expressions or natural language processing, or something more?",,,,,Submission,1,0,1
d0esmm3,2016-02-26 12:37:50-05:00,wafflestealer654,,Can you give example input and outputs?,47npcg,t3_47npcg,ffranglais,,Comment,1,0,1
d0f4cnh,2016-02-26 17:05:36-05:00,ffranglais,,"Say I scraped a Reddit thread and wanted to find all instances of 'Ben ""Fruit Salad"" Carson', including the double quotation marks and the spaces in between. Google doesn't look for the quotation marks, probably to speed up searches. What technique from computer science is used?",47npcg,t1_d0esmm3,wafflestealer654,,Reply,1,0,1
d0g8z2a,2016-02-27 17:49:17-05:00,ffranglais,,Hello?,47npcg,t1_d0esmm3,wafflestealer654,,Reply,1,0,1
d0hf4bf,2016-02-28 18:48:32-05:00,wafflestealer654,,"Sorry, my app doesn't notify me often of replies.

When a search engine or a program wants to find a specific string, they have a string to test (let's call it $TEST_STRING), and they have the keyword(s) that they're looking for (lets call them $KEYWORD). The algorithm starts at the first character in $KEYWORD. Then it looks at the first character in $TEST_STRING. If they match, they move on to the second character of $KEYWORD and $TEST_STRING. But, if it doesn't match then they move on. It's essentially looking for matches for each character in $KEYWORD.

Pusedocode:

    set keyword to ""test""
    set test_string to ""Reddit loves to test stuff.""

    set position to 0
    for every value of position from 0 to test_string.length
        set i to 0
        for every value of i from 0 to keyword.length
            if keyword[i] equals test_string[position + i] then
                continue
            else
                goto NEXT
        print ""We found a match at "" & position & ""!""
        :NEXT

Edit: Obviously there are other failsafes you have to add, but it's just to show how it can be implemented.

If your program needs to search for patterns in strings, then you can use regular expressions. A search engine like Google would probably use a parser given that it allows for more complicated queries.

Edit2: Fixed error in psuedocode.",47npcg,t1_d0g8z2a,ffranglais,,Reply,1,0,1
d0hf7mp,2016-02-28 18:51:04-05:00,ffranglais,,"Okay, that's very interesting. In case you didn't see my other comment ITT:

> Say I scraped a Reddit thread and wanted to find all instances of 'Ben ""Fruit Salad"" Carson', including the double quotation marks and the spaces in between. Google doesn't look for the quotation marks, probably to speed up searches. What technique from computer science is used?

I have a suspicion that regexp or functional languages are good for this problem.",47npcg,t1_d0hf4bf,wafflestealer654,,Reply,1,0,1
d0hfnef,2016-02-28 19:02:59-05:00,wafflestealer654,,"I believe that a custom grammar would be used for complicated queries. So, it would break apart the query into...

    Ben
    Fruit Salad
    Carson

Then it would just look for any instances of those three strings.

I would research on grammars, and how to create them.",47npcg,t1_d0hf7mp,ffranglais,,Reply,1,0,1
d0hfv9n,2016-02-28 19:09:10-05:00,ffranglais,,"Yes, but what about a certain # of spaces between characters, like Ben "" Fruit Salad"" Carson (note the extra space after the first quotation mark), those kinds of typos?",47npcg,t1_d0hfnef,wafflestealer654,,Reply,1,0,1
d0hgdta,2016-02-28 19:23:37-05:00,wafflestealer654,,"They can be handled by a grammar or a natural language. Those spaces would be ignored (only if the developer wants them ignored.)

It's just like how spaces in modern languages are ignored.",47npcg,t1_d0hfv9n,ffranglais,,Reply,1,0,1
47lb9m,2016-02-25 15:44:05-05:00,BenRayfield,"What does ""if continuation then continuation else continuation"" look like in binary?","In general, a continuation is a lambda that never returns, always calls a continuation on a continuation. Instead of returning a value, you give a continuation another continuation which may be called with a parameter that you use as the return. Throw and return are both continuations. But where in a continuation do you store other continuations that may be called on eachother andOr the parameter? How does a continuation know which continuations to call on which other continuations if the first call of another continuation never returns? It must of course give itself to the continuation it calls so itself will get back an answer, but its the same question recursively, how do you put multiple instructions into a single continuation if they can only do 1 thing?

https://en.wikipedia.org/wiki/Call-with-current-continuation

Any simplest representation of continuations in binary is as good as any other, so encode it however you like. I need to understand continuations in terms of what actually happens in a computer instead of the text form of code, below the level of variable names. I need to understand them as every part being made of continuations.

https://en.wikipedia.org/wiki/Unlambda has continuations and lacks variables. Its s and k operators in a tree are turingComplete. It also has a call/cc operator called c.

> Unlambda's one flow control construction is call with current continuation, denoted c. When an expression of the form `cx is evaluated, a special ""continuation"" object is constructed, representing the state of the interpreter at that moment.

But how does that all work?

What does ""if continuation then continuation else continuation"" look like in binary?",,,,,Submission,6,0,6
d0e6rp6,2016-02-25 22:45:33-05:00,jirachiex,,"Continuations can be thought of as call stacks that you switch your stack pointer to without returning to your initial call stack. If you have several continuations floating around, they are call stacks dynamically allocated _somewhere_. Context switching between threads can be thought of as saving a continuation and resuming another.",47lb9m,t3_47lb9m,BenRayfield,,Comment,2,0,2
d0fc5ps,2016-02-26 20:46:14-05:00,BenRayfield,,"What happens when a continuation is pushed onto one of those stacks? Must we copy its stack onto the other stack, in case its modified by the continuation which created it? Continuations are stateless. I'm concerned about continuations which push eachother onto stacks in tangled combinations very deep.",47lb9m,t1_d0e6rp6,jirachiex,,Reply,1,0,1
d0e7rf8,2016-02-25 23:13:56-05:00,ldpreload,,"A continuation is, most fundamentally, a pointer to some code and some context around that code, like local variables. Same as a closure, which is why you can call a continuation like it's a function.

Most languages that make heavy use of continuations don't particularly use the native CPU stack (usually the same as the C stack), at least not for long-term variables. So if you have the following pseudocode:

    function f {
        let x = 5:
        x += call/cc(g):
        print x:
    }

it would compile to, essentially, a structure definition containing one local variable `x`, and a function you could call that took that structure and also a return value and started execution from where `call/cc` returns. In fact, there's a straightforward representation as an OO-style class:

    class G {
        private x:
        public function start(self) {
            self.x = 5:
            let cont = new Continuation(self.resume, &self):
            g(cont):
        }
        public function resume(self, a) {
            self.x += a
            print x
        }
    }
    
    class Continuation {
        function* fn:
        data* data:
        operator call() {
            return fn(data):
        }
    }

You can repeat this for however many times `call/cc` appears. Note that as this is an object, you can have multiple objects of this class, representing multiple calls of the function. If you reference the same object, you interact with the ""local variables"" from the same call. If you make a new object, you get a new set of ""local variables"", which lets you do recursion.

It's worth mentioning here that most languages that make heavy use of continuations also have built-in garbage collection.

At this point, ""if continuation then continuation else continuation"" isn't hard. You have structures like `cont` above, which at the machine level are just two pointers. You move the data pointer to a register, indirect call the fn pointer, compare the result, then move one of two other data pointers to the register and indirect call the corresponding fn pointer. And call/cc isn't hard either: you just store the current `this` pointer to memory, along with the next function to call. The interesting work is all in creating (and GCing) these objects, and translating code written in call/cc style to the multiple resume() functions.

There is, by the way, a loose equivalent to call/cc in C, namely [setjmp()](http://man7.org/linux/man-pages/man3/setjmp.3.html) / longjmp, and the slightly crazier setcontext()/getcontext(). setjmp() stores the stack pointer and instruction pointer somewhere, and longjmp() restores them, making it appear that setjmp() has returned some value (by filling in the register that a normal function call return uses). It is used to implement things like exceptions. setcontext() and getcontext() save and restore enough information that you can switch between _multiple_ stacks, effectively giving you a manual version of multithreading. It's all the same thing, though: to save where you are, store a function pointer and some data pointer, and to go back, restore them both.",47lb9m,t3_47lb9m,BenRayfield,,Comment,2,0,2
d0e4v5o,2016-02-25 21:57:28-05:00,zanidor,,"You write continuations in very high-level languages, which are underpinned by a slew of compiler technology and shared library code. When you use GHC to compile a Haskell program, for example, the compiler translate your Haskell to an intermediate form called Core, then translates that to STG, then translates that to Cmm (""c minus-minus""), then translates *that* to assembly. The assembly is then built to a binary. Check out [this blog post](http://blog.ezyang.com/2011/04/tracing-the-compilation-of-hello-factorial/).

Each step translates your program, which has a lot of high-level semantics baked in, into a simpler (but more verbose) equivalent form, until eventually the translation to binary can happen. The programs at each stage are equivalent, but may look very different syntactically. GHC might be a slightly extreme example in terms of intermediate languages, but this is generally how compilers work.

So, the best answer I have to your question is that the final binary form of a continuation is the result of automated translation to equivalent lower-level programs, and likely not easily recognizable as the original continuation. My guess is that the [JMP](https://en.wikipedia.org/wiki/JMP_%28x86_instruction%29) statement is heavily involved. :)

If you really want to know more detail, I suggest writing a simple continuation and analyzing the resultant object files. If nothing else, you'll probably learn some interesting stuff about how your compiler of choice works.",47lb9m,t3_47lb9m,BenRayfield,,Comment,1,0,1
d0e57s1,2016-02-25 22:06:14-05:00,zanidor,,(And of course different compiler chains could translate your continuation into different binary representations.),47lb9m,t1_d0e4v5o,zanidor,,Reply,1,0,1
d0e5g1p,2016-02-25 22:11:57-05:00,BenRayfield,,"After many years and thousands if not millions of lines of code, some programming languages are adding an advanced feature called ""lambda functions"", but that doesnt mean lambdas need all that. Lambdas were first used in far simpler systems, a competing model of computing to turing machine theory. They are extremely simple, and to write more than a few lines of code to create them is rediculous. Complexity grows around processes while people ignore how things work. It doesnt mean the processes need to be complex.",47lb9m,t1_d0e4v5o,zanidor,,Reply,1,0,1
d0e64wg,2016-02-25 22:29:33-05:00,zanidor,,"Writing a lambda in, eg, Lisp is indeed a few lines of code, but the same process of translation down to binary will still happen, with potentially complex intermediate forms. (Edit: Lisp is interpreted, but that just means the translation to binary is happening at runtime instead of compile time.) The resultant assembly is very likely more than a few lines, even with simple 1960s-era compilers / interpreters.

While older hardware and as-yet undiscovered compiler techniques may have limited exactly which features compilers could support in the past, the translation down to imperative assembly (and then binary) has always had to happen to run programs written in high-level languages.

In fact, the amount of computation it took to translate Lisp to assembly in days of yore is part of what gave rise to special [Lisp machines](https://en.wikipedia.org/wiki/Lisp_machine), architected explicitly to make running Lisp efficient.

That said, compilers are certainly getting more complex as the field advances, and today's generated assembly is certainly different than it used to be. The number of people who must understand a particular compiler at a deep level is (ideally) a small fraction of the number of people who will write code in the compiler's source language, so compilers are the logical place to house arcane optimizations and complex feature implementations. 

If you are interested in learning more about compiler guts, I'd look around for a compiler geared towards students rather than a compiler used to create production binaries. (Sorry, I don't know of any ""teaching compilers"" off the top of my head.) Or, grab a copy of the [dragon book](http://www.amazon.com/Compilers-Principles-Techniques-Tools-2nd/dp/0321486811) and try building your own! :D",47lb9m,t1_d0e5g1p,BenRayfield,,Reply,1,0,1
d0e33t3,2016-02-25 21:13:55-05:00,Breadsecutioner,,"This is much more abstract than I'm used to dealing with. So I'll answer a different question than you asked.

    public bool Continuation()
    {
        if (Continuation())
        {
            Continuation():
        }
        else
        {
            Continuation():
        }
    }

I'd guess you'd overflow the stack on the if statement. I don't remember enough about Assembly to convert that to equivalent code with jump-zeros and pushes and pops. Sure, you aren't supposed to use variables, but I don't see how to do anything without using the registers.",47lb9m,t3_47lb9m,BenRayfield,,Comment,-2,0,-2
47kxj7,2016-02-25 14:49:42-05:00,quqa,What do gradschools expect from their students who pursue Masters in Software Engineering?,"What do gradschools expect from their students  who pursue Masters in Software Engineering?

I'm especially curious for non-target gradschools. 

To explain what I mean, let me give an example. Employers use several filters to see if candidates are competent enough to provide what they need. Similarly, I think, gradschools use things like GPA and other things to filter applicants so that they can find students who are competent enough to fulfil their expectations. So I'm curious about their expectations in this sense.",,,,,Submission,7,0,7
d0eahbr,2016-02-26 00:43:28-05:00,magikker,,"Often these masters programs (terminal degree, non thesis, aimed at working professionals or students headed to industry) have a fairly low bar. They are often the money makers for departments and let any nearly anyone that can get a decent score on the GRE and pay for it.",47kxj7,t3_47kxj7,quqa,,Comment,2,0,2
47kwnz,2016-02-25 14:44:49-05:00,theoryofcomp,Can you think of a scenario in which a class doesn't change but propagates its change to its neighbors?,This class can be made up or existent),,,,,Submission,0,0,0
d0drw7r,2016-02-25 16:36:27-05:00,BenRayfield,,"In https://en.wikipedia.org/wiki/First-person_shooter games, a player's position and speed are not affected while they change the position and speed of a player they shoot, sometimes slowing them, pushing them away in direction of bullet, or respawning somewhere else after death.",47kwnz,t3_47kwnz,theoryofcomp,,Comment,1,0,1
47kge7,2016-02-25 13:27:38-05:00,Dbeezt,Looking for resources: How have operating systems which utilise multi-core processors changed over time?,"I'm currently doing research into Multi-Core Processing. Part of this research entails investigating operating systems which utilise multi-core processors (Ubuntu, Windows XP - 10, Mac OS X). If anyone can recommend some lesser-known OS's it would be interesting to read but that's not the purpose of my question.

I'm supposed to research how these operating systems have changed over time, this is a broad topic area and I'm struggling to find specific information relating to individual operating systems.

I'm specifically looking for any resources concerning the use of multi-core processors in operating systems (Windows, Linux, Mac OS X) in addition to any resources or information concerning how both multi-core processors and the operating systems which use them have changed over time.

Thanks",,,,,Submission,4,0,4
d0h33dp,2016-02-28 13:23:25-05:00,assface,,"From MIT's PDOS Group: [Corey: An Operating System for Many Cores](https://pdos.csail.mit.edu/archive/corey/)

Read their [paper from OSDI'08](http://pdos.csail.mit.edu/papers/corey:osdi08.pdf).",47kge7,t3_47kge7,Dbeezt,,Comment,2,0,2
d0hensh,2016-02-28 18:36:10-05:00,Dbeezt,,"Thanks for the reply, I'll give this a look now! Definitely looks interesting, articles are my life blood right now",47kge7,t1_d0h33dp,assface,,Reply,1,0,1
d0eau7n,2016-02-26 00:56:54-05:00,lordvadr,,"Let's not forget BeOS, which was one of the first.  This is a very complicated question.  The short answer is ""threads.""  The long answer is a very, very complicated discussion ending with ""threads.""  What exactly will constitute a ""resouce?""  Is the source code for an OS a good enough resource?",47kge7,t3_47kge7,Dbeezt,,Comment,1,0,1
d0etkgd,2016-02-26 12:58:30-05:00,Dbeezt,,"I'm looking into BeOS now, I'd prefer an article or pseudo-code if you've got access to either of those, Thanks ",47kge7,t1_d0eau7n,lordvadr,,Reply,0,0,0
d0fja3c,2016-02-27 00:32:15-05:00,lordvadr,,"There's kinda a rule in these types of subs that 1) we won't do your homework for you, 2) we won't do your research for you, and 3) we won't do your job for you. Since you couldn't muster an upvote for the one person to respond to you...and your response was, just do my homework/job/research again without answering any of my question, I can only assume this is the case.",47kge7,t1_d0etkgd,Dbeezt,,Reply,0,0,0
d0fld1g,2016-02-27 02:09:00-05:00,lordvadr,,"BTW, I'm also going to call your bullshit. Ubuntu is not an operating system, XP didn't do well with multi-cpu, and before you decided to do your research paper, there was a time when all CPU's had just one core and you simply had many CPU's. IRIX, Solaris, AIX, HP/UX...hell, even Xenix could do it.",47kge7,t1_d0etkgd,Dbeezt,,Reply,0,0,0
d0gtvrf,2016-02-28 08:11:31-05:00,Dbeezt,,http://imgur.com/1t4W1Es,47kge7,t1_d0fld1g,lordvadr,,Reply,1,0,1
47eeuk,2016-02-24 14:36:54-05:00,updatepheramone,Implement shortest path algorithm with ant colony,"How would I go about implementing an ant colony optimization program to find the shortest path between two nodes?

Most of the examples on-line show how to use ACO for the travelling salesmen problem but I would like to use it to find a path from nest to food in a network graph.

I'm guessing I would have to use some kind of vector to remember the paths of each individual ant and then an or statement to check if it has landed on the food node. If it has found the food, to turn back and update pheromone. ",,,,,Submission,6,0,6
d0cf6mf,2016-02-24 16:08:16-05:00,angererc,,"I am no expert in ant colony optimization, but I found this paper that describes ACO for different variants of shortest path plus pseudo code: maybe it helps: https://www.researchgate.net/publication/234065233_Shortest_Path_Problem_Solving_Based_on_Ant_Colony_Optimization_Metaheuristic",47eeuk,t3_47eeuk,updatepheramone,,Comment,2,0,2
d0d6d0l,2016-02-25 07:48:12-05:00,updatepheramone,,"wow thanks for that, really useful paper!",47eeuk,t1_d0cf6mf,angererc,,Reply,2,0,2
d0d3ak1,2016-02-25 04:35:26-05:00,Ready_Player_Two,,"This is actually quite similar to my final year project for Computer Science so I'm not an expert but I've spent quite a bit of time researching and am currently coding a simulation for ACO. I don't want to give too much away, as I'm still a few months away from submitting my work (plagiarism etc.), but this should help.

The variations on ACO (MMAS, EAS, etc.) have slightly different implementations for extra abilities (not seen in nature). One of the simplest, yet very useful is the ability to remember the outward bound route and retrace the edges to get back to the source (ant colony). This often includes loop elimination to reduce the path length/cost. Loop elimination is quite simple, e.g: 

Nodes taken before loop elimination = [1,5,8,3,7,9,4,3,10]

Nodes taken after loop elimination = [1,5,8,3,10]

Pheromone deposit and decay is also something which changes per implementation but a simple method is to deposit pheromone every time an ant traverses an edge when searching for food and returning to the source (ant colony). The quantity to deposit when returning can be relative to the quality of the path traveled - this encourages convergence on good routes.  (Note: this is one implementation and not necessarily the best)

As for choosing the edge to take at a node, it should be done using probability, based on the pheromone present on each edge. Another extra ability is when choosing which edge to take at a node, the ant should always choose the edge which connects to the food source if it is present. This should speed up the convergence as well.

Overall, I would start by deciding on the exact algorithm you want to use (typically the variations are better, especially MMAS) and go from there. Feel free to ask more questions if you have some. ",47eeuk,t3_47eeuk,updatepheramone,,Comment,1,0,1
d0d6w3k,2016-02-25 08:12:12-05:00,updatepheramone,,"Thanks for the reply! If I was to use MMAS, how would I implement the edge selection function? I've seen the formula for it on all the papers but I have no idea how to actually code it. The maths kinda goes over my head. ",47eeuk,t1_d0d3ak1,Ready_Player_Two,,Reply,2,0,2
d0d78g0,2016-02-25 08:26:13-05:00,Ready_Player_Two,,"I'm still implementing mine at the moment so not 100%. 

As for the maths, I try to think less of the formulas and more what it is actually doing. [This paper is the original one for Max-Min Ant System](https://www.researchgate.net/profile/Thomas_Stuetzle/publication/266257098_The_max-min_ant_system_and_local_search_for_combinatorial_optimization_problems/links/0fcfd50b1f8b8e3b02000000.pdf) and it states three main differences to the Ant System/ Ant Colony System as quoted below:

* ""To exploit the best solutions found during an iteration
or during the run of the algorithm, after each
iteration only one single ant adds pheromone.
This ant may be the one which found the best solution
in the current iteration (iteration-best ant)
or the one which found the best solution from
the beginning of the trial (global-best ant).""
* ""To avoid stagnation of the search the range of
possible pheromone trails on each solution component
is limited to an interval [τmin, τmax].""
*  ""Additionally, we deliberately initialize the
pheromone trails to τmax, achieving in this way
a higher exploration of solutions at the start of
the algorithm.""

The first of these three points is different for shortest path problem compared to TSP as there is not clear iterations (in TSP it is when all ants have constructed a route), you will need to do this by setting a time period for when pheromone is deposited. During each period you need to keep track of the best solution.

The second is simple, just limit the min/max values in the set method for the edges pheromone.

The third is similar to the first, after a period of time (you decide), reinitialise each edge with some pheromone. 

Edge selection took me a while to figure out! If you have three edges, with pheromone values of 10, 10, 20, respectively, you need to add these together to get 40. Then generate a random number in range 0-40 - for example 15 is the result. Then with a counter, add the first edges pheromone (10), then check if it is less or equal that the number chosen (15), if it isn't the corresponding edge is selected. If it is smaller, add the next number and repeat. The [second answer to this question](http://stackoverflow.com/questions/9330394/how-to-pick-an-item-by-its-probability) provides a decent example (at least easier than in text). 

",47eeuk,t1_d0d6w3k,updatepheramone,,Reply,1,0,1
47ec82,2016-02-24 14:23:24-05:00,michael1026,I'm having trouble reverting to a commit in git,"I completed a project, committed the changes, tagged it, but then decided to try to fix some bugs. I ended up messing my project up pretty badly, so I tried reverting, but accidentally reverted to the wrong commit. I tried checking out the tagged version ""v1"", but I keep getting my really old project. I'm freaking out a bit. How can I fix this?",,,,,Submission,0,0,0
d0caq1g,2016-02-24 14:30:23-05:00,theobromus,,"How did you revert?

Something like ""git reset --soft HEAD~"" ?",47ec82,t3_47ec82,michael1026,,Comment,1,0,1
d0cau55,2016-02-24 14:32:54-05:00,michael1026,,"I think I did `git reset --hard ID`

which as far as I understand, it only discards uncommitted changes, which I was fine with at the time.",47ec82,t1_d0caq1g,theobromus,,Reply,1,0,1
d0cbhq8,2016-02-24 14:47:18-05:00,theobromus,,"I wonder if it's possible you tagged the wrong commit? Everything you committed should still be there. I wonder if you can try running ""git log"" and see what changes there are. You should be able to reset to one of those commit ids.",47ec82,t1_d0cau55,michael1026,,Reply,1,0,1
d0cbur8,2016-02-24 14:55:16-05:00,michael1026,,"When I committed my changes, I made sure that the files I changed were actually changed. By that I mean, it says after a commit what files have been modified and updated. When I do `git log`, I can see all of the commits and tags. I see the one I accidentally chose, and the one I want to revert to. Is there something I can use other than `git checkout v1`?",47ec82,t1_d0cbhq8,theobromus,,Reply,1,0,1
d0ccfzk,2016-02-24 15:08:16-05:00,theobromus,,Well I think you can do a reset with the sha hash of any change (not just a tagged change).,47ec82,t1_d0cbur8,michael1026,,Reply,1,0,1
d0cj04p,2016-02-24 17:30:51-05:00,michael1026,,"Mind supplying the command? I'd try myself, but I don't want to accidentally screw myself any further.

Or would it just be `git reset 12345...`

where 12345... is the sha hash of v1 (what I'm trying to get to)?",47ec82,t1_d0ccfzk,theobromus,,Reply,1,0,1
d0cjq2f,2016-02-24 17:47:59-05:00,theobromus,,"i suspect git reset --hard ID is probably what you want, but I share your apprehension about being responsible for messing things up.

As a safety, you can always make a copy of the whole source folder before doing anything (and you should back up anyway :)).",47ec82,t1_d0cj04p,michael1026,,Reply,2,0,2
d0cjvm3,2016-02-24 17:51:40-05:00,michael1026,,"IT WORKED! I LOVE YOU!

Sorry. This is a school project that I've put roughly 20 hours of work into, and have other projects that are late because of it. If this didn't work, I probably would have failed this class.",47ec82,t1_d0cjq2f,theobromus,,Reply,1,0,1
47c11o,2016-02-24 05:51:42-05:00,yousefjb,what books have a popular name that is not the title of the book nor official name. GangOfFour as an example.,I also know the Dragon Book,,,,,Submission,1,0,1
d0c2v2z,2016-02-24 11:35:23-05:00,crookedkr,,"""The purple dragon book"" which is Aho, Lam, Set hi, and Ullman's second edition compilers book. I believe the first edition was the red dragon book but I never had that one.

Oops just saw the one line of your post.",47c11o,t3_47c11o,yousefjb,,Comment,3,0,3
d0bvebw,2016-02-24 07:57:41-05:00,ldpreload,,"Cormen et al.'s _Introduction to Algorithms_ is ""CLRS"". It's not too odd to refer to books by their author, in general, but I'm having trouble thinking of more unique names like ""the dragon book"".",47c11o,t3_47c11o,yousefjb,,Comment,3,0,3
d0bthff,2016-02-24 06:13:03-05:00,dumfug42,,"""Modern Operating Systems"" by Andrew Tanenbaum is often
called ""the Tanenbaum book"" or simply ""Tanenbaum""",47c11o,t3_47c11o,yousefjb,,Comment,2,0,2
d0cbctq,2016-02-24 14:44:21-05:00,kotrenn,,"On a related note is ""The dinosaur book"" Operating System Concepts by Silberschatz, Galvin, and Gagne.",47c11o,t1_d0bthff,dumfug42,,Reply,2,0,2
d0bzsts,2016-02-24 10:20:30-05:00,0xf5f,,"The Camel Book (*Programming Perl*)

K&R (author's initials for *The C Programming Language*)

edit: The Pickaxe (*Programming Ruby*)",47c11o,t3_47c11o,yousefjb,,Comment,2,0,2
d0cf9pj,2016-02-24 16:10:06-05:00,heywire84,,OpenGL has the Red book and Blue book.,47c11o,t3_47c11o,yousefjb,,Comment,1,0,1
479vy1,2016-02-23 19:55:05-05:00,Fransebas,What do I need to learn in order to setup a server?,"Hi, I'm currently studying programation at the university, but my course isn't much about servers, cloud and that sort of thinking, and I'll like to learn by my own, but I'm wondering what set of books or courses do I need in order to really understand and set up a group of servers, and what order you think is better: I've heard of PHP, apache, html/css/javascript.",,,,,Submission,1,0,1
d0bcrp3,2016-02-23 20:09:22-05:00,zombarista,,"1. install the latest ubuntu server
2. run this at the prompt to get a test PHP/Apache/MySQL server going:

    sudo apt-get install php5 mysql-server apache2 phpmyadmin",479vy1,t3_479vy1,Fransebas,,Comment,3,0,3
d0bkhhe,2016-02-23 23:17:17-05:00,wafflestealer654,,"You listed a bunch of different keywords that all have different purposes.

Apache is the program/service that takes requests from your web-browser, and sends it the webpage. This is known as a web-server.

PHP is a server side scripting language. It's ran on the server, and the browser never sees any of the code.

HTML, CSS, and JavaScript are client side languages. They all get ran by your web-browser. For example, if you right click on this webpage, and click view source, then you'll see all the HTML (with CSS and JS) that your browser is running.

This is only a quick summary of those keywords. I would personally recommend installing a small Apache server to get more familiar with how web-servers work.

Here is a list of software you can use.

**Windows**

- [XAMPP](http://www.apachefriends.org/en/xampp-windows.html)
- [Ampps](http://www.ampps.com/)
- [WAMP Server](http://www.wampserver.com/en/)

**Linux**

- Run `sudo apt-get install apache2 php5 mysql-server`

**Mac**

- [MAMP](https://www.mamp.info/en/)
- [XAMPP](http://www.apachefriends.org/en/xampp-windows.html)
",479vy1,t3_479vy1,Fransebas,,Comment,1,0,1
479oro,2016-02-23 19:10:10-05:00,Abandonedbychrist,Getting into coding without any experience whatsoever?,"Is it doable? More importantly, does anyone have any pointers about where to start? I mean, the only experience I have is designing elaborate neopets petpages in HTML. ",,,,,Submission,2,0,2
d0bb5j6,2016-02-23 19:29:26-05:00,crookedkr,,"You can totally start as long as you have the motivation. My suggestion would be to pick a challenge and the read a little about what language and environment you want. Particularly good projects are things that you have an itch about. Maybe you have some text files that need searching/editing or you really want to know how some networking thing works. If you have a math or stats background you might be interested in machine learning (it so hot right now). If you don't have something that you need it for you can also try a challenge or exercise, there are plenty online, but if you have a personal need it will be more fulfilling.

Once you have a project in mind, there will be a host of languages that are well suited to your problem. If you post back we can give you some advise on that too.",479oro,t3_479oro,Abandonedbychrist,,Comment,3,0,3
d0bljpj,2016-02-23 23:47:40-05:00,friedclams,,"It's doable, however unlike some of the people on here who would suggest picking a project and learning stuff until you've made it, I'd recommend taking a little while to learn basic coding in some modern language (Java, C++, etc.) because it will generally make learning everything else or making a project a lot easier. You can find tutorials on YouTube or elsewhere on the internet but a book is also helpful if you can get your hands on one.",479oro,t3_479oro,Abandonedbychrist,,Comment,3,0,3
d0bawmb,2016-02-23 19:23:03-05:00,bulldogclip,,"Best way is to pick a project to make, and keep googling stuff until you have made it. I decided i wanted to make a simple Android App, few weeks of googling, watching youtube tutorials and arguing with android studio and I had my app. No previous experience programming in C before hand. Alot of my time was spent watching C programming tutorials. ",479oro,t3_479oro,Abandonedbychrist,,Comment,4,0,4
d0be9u2,2016-02-23 20:43:44-05:00,Rangsk,,"I've known quite a few students who enter a CS major with no prior experience and they do fine, so it's clearly ""doable"" in that sense.

If you're asking whether it's doable without getting a formal education, then yes, there are plenty of self-taught programmers.

Just like anything else, the best way to learn it is to find a way to enjoy it and stick with it. For programming, this usually means picking projects to do and trying to complete them.

I'd also recommend finding a mentor, such as a friend who already knows how to program. This is really dependent on your situation, though.",479oro,t3_479oro,Abandonedbychrist,,Comment,1,0,1
d0bomxb,2016-02-24 01:31:43-05:00,Isolater,,How do you guys feel about Code Academy?,479oro,t3_479oro,Abandonedbychrist,,Comment,1,0,1
d0donb4,2016-02-25 15:28:07-05:00,None,,Code Academy is what I use if I need to brush up on an area. It seems like it would be a very good start for a beginner.,479oro,t1_d0bomxb,Isolater,,Reply,1,0,1
d0cydn5,2016-02-25 00:26:11-05:00,None,,"I started with Turing, then Java, and then C. Learn all of the programs and do a couple of massive projects.",479oro,t3_479oro,Abandonedbychrist,,Comment,1,0,1
47928z,2016-02-23 17:02:05-05:00,pducks32,Proper Data Structure for Open/Close Hours,"So I have been programming for years so I think its kinda funny and exciting when I come up to a problem that really picks at me. 
Basically I need to represent (in Swift if that matters) hours of operation for a bunch of stores. The catch is that in term son checking whether something is open the hours should be displayed to a user (in the app) as 7pm-2am. 
I can easily build a version of this but the code is ugly and I want to make it cleaner and nicer so I was wondering if anyone had any thoughts on this. I feel like it should be stored as 7pm to 2am and not 7pm-1159pm and 120am to 2am because although that will make checking if the place is open easy it will make formatting them hard and even adding them hard (I'd rather type 7pm-2am and have it figure it out)",,,,,Submission,1,0,1
d0b8w8f,2016-02-23 18:33:05-05:00,zombarista,,"I have solved this problem for a bar's website before.  Here's what I did...

You're not focusing on the hours PER DAY that the store is open, you're focusing on tracking ""open"" intervals.  IE, the restaurant is open from ""Monday 9:00 am to Tuesday 2:00 am""

This was for a WordPress website, so I encoded it as an array of associative arrays.

    $intervals = array(
    	array(
    		'open' => 'Monday 9:00 am',
    		'close' => 'Tuesday 2:00 am'
    	),
    	array(
    		'open' => 'Tuesday 9:00 am',
    		'close' => 'Wednesday 2:00 am'
    	)
    ):

PHP makes it easy to parse strings like that to dates: your mileage may vary, but regex could help a lot.  Since the list is relatively small, it's easy enough to ""walk"" and determine which interval you are actually in and report if you're ""open"" or ""closed""",47928z,t3_47928z,pducks32,,Comment,1,0,1
d0b8y15,2016-02-23 18:34:19-05:00,zombarista,,"I should say, the configuration for end users looks like this

    Monday 11:00 AM - Tuesday 12:00 AM
    Tuesday 11:00 AM - Wednesday 2:00 AM
    Wednesday 11:00 AM - Thursday 2:00 AM
    Thursday 11:00 AM - Friday 2:00 AM
    Friday 11:00 AM - Saturday 2:00 AM
    Saturday 11:00 AM - Sunday 2:00 AM
    Sunday 11:00 AM - Monday 12:00 AM",47928z,t1_d0b8w8f,zombarista,,Reply,1,0,1
477szp,2016-02-23 12:53:10-05:00,LastingFasting,More basic book recommendations,"Hello!

Please excuse me if I'm in the wrong sub, and please excuse the question-but let me explain:

I've tried searching for some book recommendations but most of these seem to be ""introduction to algorithms"" or about discrete math, coding, and what not. This is not for me since I don't have a good math background (yet), and probably need something more basic-something that tells me about operating systems, bytes, hard-drives, RAM, graphics cards, sound-cards, USB ports, CPUs, resolutions, maybe web-browsers, what WinRar does, different audio formats (MP3, WMV), what the hell all those maps next to the ""program files (x86)"" in the ""hard-drive folder"" are, and why I should not delete system32.

Is there any book for general stuff like this?
",,,,,Submission,0,0,0
d0b3znl,2016-02-23 16:39:06-05:00,the_omega99,,"No, no such book exists. The problem is that you're going beyond computer science and are being way too broad, anyway. Some things you've mentioned are just computers 101, others are IT things, and then there's the fact that hardware isn't usually very in the realm of CS (which is more focused on theory and software).

If you want to learn actual CS, the recommendations you've seen *are* for CS. They also typically require nothing more than a high school level of mathematics (which can be acquired on sites like Khan Academy if you lack them).

Learning about stuff like hard drives, RAM, graphics cards, etc are things that are better suited for /r/buildapc. Web browsers are pieces of software that you would understand from learning web development. The basics of compression require a knowledge of algorithms, but for the likes of WinRAR, it's a highly advanced and very mathematical topic. Audio formats are mostly similar, since modern file formats for media are all about compression.

A lot of the stuff you'd want to know are things one would typically pick up by simply googling them on the fly. That would be my recommendation for how to learn this.",477szp,t3_477szp,LastingFasting,,Comment,2,0,2
d0b49gv,2016-02-23 16:44:32-05:00,LastingFasting,,"Thank you so much, you cleared up a lot of things (I thought CS included the hardware part more than it did).

I guess I'll try my luck on google and /r/buildpc then.",477szp,t1_d0b3znl,the_omega99,,Reply,1,0,1
d0b0ygw,2016-02-23 15:36:11-05:00,SneakingNinjaCat,,Math books.,477szp,t3_477szp,LastingFasting,,Comment,1,0,1
4771t2,2016-02-23 10:23:28-05:00,quqa,"Automata Theory -- Can you dumb it down ""Pumping Lemma""? (Regular language indicator)","Since this question seemed a little too specific for r/learnprogramming I wanted to ask here.

https://www.docdroid.net/34mIAiM/3-re.pdf.html

In page 8, I'm trying to understand pumping lemma and its examples. I've read over and over again but I can't seem to understand it. 

I understand the definitons but not the reasons behind it. Also how they relate, why they relate the way they do. So I pretty much didn't understand anything.


I really really appreciate if you can dumb it down a little bit. ",,,,,Submission,4,0,4
d0apycb,2016-02-23 11:34:17-05:00,lacunahead,,"Say you have a regular language and a string *s* of sufficiently long length *p* (the pumping length) in that language. Imagine that you have the corresponding DFA for that language. If the string really is sufficiently long, then to produce it, you have to travel around some cycle in that DFA at least once. Well, if you have to travel some cycle in a DFA at least once to produce that string *s*, then you should be able to travel that cycle again... and again... and again...

So, one way to prove that a language is *not* regular is a proof by contradiction. You assume that language is regular, and suppose you have a string *s* in that language of sufficiently long length *p*. Then, you try to pump that string - that means you're choosing to iterate through that cycle I talked about before a different number of times than in string *s*. So whereas in *s* we might have iterated through the cycle once, we might pump *s* to become *s'* by iterating through the cycle three times. If *s* is a valid string for a regular language, then we should be able to iterate through its cycle as many times as we want and still have *s'* be a valid string for that same regular language. 

The goal of using the pumping lemma to prove non-regularity is to show that some string *s* which is valid in a language L, when pumped, produces a string *s'* which is *not* valid in that language L.",4771t2,t3_4771t2,quqa,,Comment,3,0,3
d0az2e2,2016-02-23 14:55:15-05:00,quqa,,"Thank you very much! I understood everything you said, I think! But I still  can't understand Lemma 1.4 fully and the example 1 (in page 9).

1. I mean I didn't understand anything in first example. Can you maybe go over it by connecting the points and relations? 

2. For lemma 1.4, why do we care about |y|>0? Why at least 1? We already said that L can have xy^i z, where i=0,1,2,3... There are also ""pump down""s in examples where i=0. So what is this about?

2. Pumping length in fact is not the length of the word. Is it number of transitions as written in the document? You simplified it above for me i think.

3. In example, it says ""let p be L1's pumping length"". What is that exactly?

4. Say, p is just number of transitions. 0^p 1^p looks very weird. Isn't p means a number here? 0011, 000111, etc

5. How cycle is being in the first p charactrs corresponds to y=0^k for some k>0 and (I suppose k<p) ? What are those mean?

6. Why or how we choose xy^2 z? Why 2? How does it correspond to 0^(p+k)1^p ?

7. Why y = 1^k is not possible? 
",4771t2,t1_d0apycb,lacunahead,,Reply,1,0,1
d0b1uh3,2016-02-23 15:54:55-05:00,lacunahead,,">I mean I didn't understand anything in first example. Can you maybe go over it by connecting the points and relations?

Let's walk through it. We want to show L1 is not regular. We do so using a proof by contradiction, first assuming that L1 is regular. If L1 is regular, then we may apply the pumping lemma to a string of sufficient length. Let's choose the string w = 0^p 1^p, where p is the pumping length for L1. Clearly, w is in L1 and |w| > p, so we may apply the pumping lemma. This tells us that:

1. xy^i z is in L1, for all i >= 0. 
2. |y| > 0 
3. |xy| <= p

So let's assume that w is composed of three substrings, xyz. We know by the pumping lemma that xy^2 z MUST be in the language L1. What exactly is the string xy^2 z? Well, given that |xy| <= p, we know that x can only be a substring of 0s and y can only be a substring of 0s. If x or y had any 1s, it would violate the condition that |xy| <= p, because it would mean that xy was at least 0^p 1, and |0^p 1| = p + 1, which is not <= p. So we know that |xy| is composed of all 0s. We also know that y must have at least a single 0, because |y| > 0. So then we know that xy^2 z must have added at least a single 0 to our original string s. But now our new string xy^2 z does not meet the conditions of language L1, whereas the pumping lemma says it must meet those conditions if L1 is a regular language. To make it totally clear, our new pumped string xy^2 z does not meet the conditions of language L1 because it must have more 0s than 1s, whereas L1 is a language where there are an equal number of 0s and 1s. So we conclude that L1 cannot be a regular language, because a contradiction is yielded when we apply the pumping lemma to it.

>For lemma 1.4, why do we care about |y|>0? Why at least 1? We already said that L can have xyi z, where i=0,1,2,3... There are also ""pump down""s in examples where i=0. So what is this about?

|y| must be >0 in order for there to be anything to pump at all.

>Pumping length in fact is not the length of the word. Is it number of transitions as written in the document? You simplified it above for me i think.

Technically, the pumping length is the minimum length of a string of a given language to be eligible for the application of the pumping lemma. It has no necessary connection with the length of the string other than that. If the string is at least as long as the pumping length, it can be pumped and you can apply the pumping lemma to it - otherwise, it cannot be pumped, and you cannot apply the pumping lemma to it.

>In example, it says ""let p be L1's pumping length"". What is that exactly?

It means that for the purposes of the proof, we'll take *p* - whatever it is - to be the pumping length of L1, which is a language. So, for a string to be an eligible ""target"" for the pumping lemma, it must be at least *p* long.

>Say, p is just number of transitions. 0p 1p looks very weird. Isn't p means a number here? 0011, 000111, etc

*p* is a number - specifically, it's the minimum length for a string in a language to be an eligible ""target"" for the pumping lemma. It might not be super useful to think about it in terms of transitions. 0^p1^p is indeed a string with *p* number of 0s followed by *p* number of 1s.

>How cycle is being in the first p charactrs corresponds to y=0k for some k>0 and (I suppose k<p) ? What are those mean?

In fact the first example proof given in your worksheet is strictly incorrect (or at least has unnecessary parts), as the only possible case for the substring y is that it contains only 0s. This is because |xy| < p, and we know that the string is 0^p1^p, and so we know that |y| can only be a substring comprised of 0s. 

>Why or how we choose xy^2 z? Why 2? How does it correspond to 0p+k1p ?

Basically, you want to choose a pumping length which makes it so that the string, when pumped, is no longer a valid string for the language you're using. So for this example we could easily have picked 0, 2, 3, 4, 5... - what's important is that for at least one pumping length, the resulting string is not valid.

>Why y = 1^k is not possible?

Your worksheet says it is not possible because pumping would yield a string 0^(p)1^(p+k) which is not in L1. In fact, in this case y = 1^k is not possible because |xy| < p, as discussed above.",4771t2,t1_d0az2e2,quqa,,Reply,2,0,2
d0ccz2y,2016-02-24 15:19:49-05:00,quqa,,Thank you very much. You are inspiring me to help people :D,4771t2,t1_d0b1uh3,lacunahead,,Reply,2,0,2
d0b2csl,2016-02-23 16:05:38-05:00,quqa,,"I really appreciate your help. Its pretty late here, I'll try to understand in the morning.",4771t2,t1_d0b1uh3,lacunahead,,Reply,1,0,1
d0b2jzx,2016-02-23 16:09:55-05:00,lacunahead,,No problem. I definitely had a hard time wrapping my head around pumping lemma proofs when I first learned them.,4771t2,t1_d0b2csl,quqa,,Reply,2,0,2
d0aotm1,2016-02-23 11:07:32-05:00,Acidom,,"My main cookie cutter answer for pumping lemma questions was to pick a word that could appear in the language, split it up into ~4 groups named w x y z. Pump x or y once, and show how this is not in the language. Disclaimer took Computability a year or so ago so this may be vaguely wrong/inaccurate but hopefully puts you on the right path.",4771t2,t3_4771t2,quqa,,Comment,2,0,2
4751c8,2016-02-22 23:51:00-05:00,lulss,10 questions regarding Comp Sci.,"Link : [Here](http://goo.gl/forms/GyqlFv52hw)(a google form) Just some questions on my major choice for my English class. Thanks you for the taking the time and answering in Advance. Also going to post down below in case.

1. What made you want to get into this field?

2. Would you recommend this career path to someone else?

3. What challenges did you face while achieving your degree?

4. What is your job career & duties?

5. What is your monthly salary? (optional)

6. Any pros or cons of your career choice?

7. What is your typical job routine?

8. Why is there so much emphasis put onto learning math?

9. What essential skills do you think is required for this field of work?

10. Do you feel the demand for Computer Science majors will increase over time?


Sorry if the questions seem to open ended/ broad. Just needed some interview questions for a section of the paper. Ty in advance again. ",,,,,Submission,8,0,8
d0ajnzd,2016-02-23 08:41:16-05:00,Bottled_Void,,"Responded.


(Commenting just to encourage others to fill it in)",4751c8,t3_4751c8,lulss,,Comment,3,0,3
d0aqtjt,2016-02-23 11:53:52-05:00,VozMajal,,"I'd answer, but this seems more geared towards people who have completed their degree. More or less commenting to increase the odds that people will click on this though.",4751c8,t3_4751c8,lulss,,Comment,2,0,2
d0d1c1r,2016-02-25 02:34:44-05:00,aldrin12,,i'd also like to answer but i'm only at my 3rd year in CS so...,4751c8,t3_4751c8,lulss,,Comment,1,0,1
471jr8,2016-02-22 11:21:41-05:00,jzajac8,Caesar Cipher code... getline won't work for me...,"*Beginner programmer*

I'm trying to make a Caeser Cipher program that reads input in from the user and stores it into a string. Whenever I try to read in the string in the if-else statement, it doesn't even allow me to input and goes straight to printing out the exit code. It says that ""no matching function for call to getline"" Please help and thanks in advance!

#include <iostream>
#include <string>
#include <cstdio>
#include <stdlib.h>
using namespace std;

int main() {
    
    printf(""Choose from the following options:\n"");
    printf(""1. Encode some text\n"");
    printf(""2. Decode using user-entered values\n"");
    printf(""3. Decode automatically\n"");
    printf(""4. Exit program\n"");
    
    int userInput;
    string usercode;
    
    cout << ""Your choice: "";
    cin >> userInput;
    
    if (userInput == 1) {
        cout << ""Enter the text to be encoded: \n"" ;
        getline(cin, usercode);
    }
    else {
        printf(""Restart"");
    }
}",,,,,Submission,0,0,0
d09fp70,2016-02-22 12:00:38-05:00,etagawesome,,"[deleted]  
 ^^^^^^^^^^^^^^^^0.1034 
 > [What is this?](https://pastebin.com/64GuVi2F/38249)",471jr8,t3_471jr8,jzajac8,,Comment,3,0,3
d09ljed,2016-02-22 14:16:23-05:00,Coolisbetter,,"Funnily enough I posted a (probably too long) video on YouTube explaining how to write a Caesar Cipher in c++ 4 years ago.

It has been a long time, but it might have something useful in it.

https://www.youtube.com/watch?v=_EQ6TJs_TEk",471jr8,t3_471jr8,jzajac8,,Comment,1,0,1
46y02x,2016-02-21 18:44:12-05:00,Shimomura,Cache Mean Data access Time question,"If I were to Calculate the mean data access time and hit ratio given 2 nsec cache access time, 30 nsec memory access time, 950 hits and 50 misses would it be something like If I am correct the formulat I interupted it to be : Tavg = hc+(1-h)M
where h = hit rate (1-h) = miss rate c = time to access information from cache M = miss penalty (time to access main memory)
so would this be correct? (.95)(2nsec) +(1-.5)(30nsec)",,,,,Submission,2,0,2
d08wfik,2016-02-21 22:51:26-05:00,haselden05,,"Believe 2nd number should be 0.05.
Edit: 1-0.95= 0.05",46y02x,t3_46y02x,Shimomura,,Comment,2,0,2
d08zhzv,2016-02-22 00:29:01-05:00,Shimomura,,so (.95)(2nsec) +(.5)(30nsec) ?,46y02x,t1_d08wfik,haselden05,,Reply,1,0,1
d099uov,2016-02-22 09:24:37-05:00,haselden05,,(.95)(2)+(1-.95)(30).    -> (.95)(2)+(.05)(30),46y02x,t1_d08zhzv,Shimomura,,Reply,1,0,1
d099ur3,2016-02-22 09:24:40-05:00,haselden05,,(.95)(2)+(1-.95)(30).    -> (.95)(2)+(.05)(30),46y02x,t1_d08zhzv,Shimomura,,Reply,1,0,1
d099uro,2016-02-22 09:24:41-05:00,haselden05,,(.95)(2)+(1-.95)(30).    -> (.95)(2)+(.05)(30),46y02x,t1_d08zhzv,Shimomura,,Reply,1,0,1
d099vqh,2016-02-22 09:25:33-05:00,haselden05,,"Well, not sure why it replied 3 times... Wtf lol",46y02x,t1_d099uro,haselden05,,Reply,2,0,2
46xxo7,2016-02-21 18:29:20-05:00,Akyumisensei,Single-bit error question,"Given word size of 48 bits, how many bits code word be if you want to allow single bit error?",,,,,Submission,0,0,0
46xjs6,2016-02-21 17:03:29-05:00,BenRayfield,Whats the most efficient normalized form of an undirected graph?,"Any representation of an undirected graph as a bitstring is either normalized or has at least 1 duplicate form.

Of those duplicate forms, we can define the normalized form as the lowest sorted bitstring.

That does not mean its efficient to find the normalized form, but it certainly exists.

Example: Define an undirected graph of n nodes as concat of pairs of integers, 1 integer for each node, and each pair of integers is an edge connecting them in the graph.

There are various ways of representing variable length integers, for example, 00 is 0, 11 is 1, and 01 ends the integer, or count in unary like 111110 is 5. Any of these will work for the math part, but most arent efficient.

Example: Concat rows of 2d triangle adjacency matrix.

Example: A nand forest whose root recognizes leafs which each represent a certain pair of nodes in the undirected graph, the bit meaning if they're connected or not. All permutations of the nodes, with these leafs between them, must recognize the same undirected graph, by definition of choosing among possible nand forests.

Nand forests can be sorted, first by quantity of nands, then by height, then recursively by left child, then recursively by right child. Sorting nand forests has a worst case of n * log(n)^2 if hash consing is used to measure equality in constant time.

Theres many ways to represent undirected graphs as bitstring, and they are guaranteed to have a normalized form in all those ways by defining normed as the lowest of those bitstrings for each unique graph. Which of these kinds of undirected graph norming are most efficient?

What is the complexity class of graph isomorphism?",,,,,Submission,0,0,0
d08xux4,2016-02-21 23:33:45-05:00,GenericUsername02,,/r/homeworkhelp,46xjs6,t3_46xjs6,BenRayfield,,Comment,1,0,1
d0901s0,2016-02-22 00:49:45-05:00,BenRayfield,,"When you say homework, do you mean anything not directly related to money?",46xjs6,t1_d08xux4,GenericUsername02,,Reply,1,0,1
46xf3i,2016-02-21 16:35:42-05:00,Al__Gorithm,"Question on LinkedList.remove() vs Iterator.remove() , my university notes have me baffled!","This is what the lecture slides state:


LinkedList.remove must walk down the list each time, then remove, so in general it is O( n^2 )


Iterator.remove removes items without starting over at the beginning, so in general it is O( n )


However I am having trouble understanding this, surely if LinkedList.remove() steps down the list each time it is O( n ) because the list contains n elements and since Iterator.remove() doesn't step down the list it is O( 1 )?


Can someone please explain why the big-O values are n times what I expect them to be? I don't want to write anything in my notes I don't fully understand and I couldn't find anything helpful in the course textbook or on the web.


Many thanks!",,,,,Submission,1,0,1
d08kk25,2016-02-21 17:26:39-05:00,tRfalcore,,"The iterator is presumably already at the object you need to remove.  It simple switches the pointers of the two adjacent objects.

Linkedinlist.remove must first find the object, O(n), then remove it.  ",46xf3i,t3_46xf3i,Al__Gorithm,,Comment,5,0,5
d08ks2e,2016-02-21 17:32:40-05:00,Al__Gorithm,,"So the remove is also an O(n) operation? Since the entire removal process is O(n^2 ) for a LinkedList, and O(n) for the Iterator.


Thanks!",46xf3i,t1_d08kk25,tRfalcore,,Reply,1,0,1
d08l3bw,2016-02-21 17:41:14-05:00,tRfalcore,,"why are you saying the entire removal process is O(n**2)?


I'd say they're both O(n) ultimately-- if it's truly a LinkedList.",46xf3i,t1_d08ks2e,Al__Gorithm,,Reply,1,0,1
d08lzeq,2016-02-21 18:05:57-05:00,Al__Gorithm,,"My university notes state:

LinkedList.remove must walk down the list each time, then remove, so in general it is O( n2 )


Iterator.remove removes items without starting over at the beginning, so in general it is O( n )


And I was questioning why this is, as I assume the notes are correct, however if not I'm happy to be told otherwise. 
",46xf3i,t1_d08l3bw,tRfalcore,,Reply,1,0,1
d08m7ja,2016-02-21 18:12:01-05:00,tRfalcore,,"if you're removing every item from a list then yes, LinkedList.remove is O(n**2) yes because it starts over from the beginning, find the item, then deletes it.

Iterator.remove would be O(n) cause it removes an item, goes to the next, removes it, goes to next, removes it.",46xf3i,t1_d08lzeq,Al__Gorithm,,Reply,2,0,2
d08vf2e,2016-02-21 22:23:58-05:00,okmkz,,"To paraphrase: The Iterator instance retains a reference or index to the current object, so there is no need to search for it",46xf3i,t1_d08m7ja,tRfalcore,,Reply,1,0,1
d08ocx6,2016-02-21 19:12:42-05:00,algorithmsWAttitude,,"One LinkedList.remove operation takes time proportional to the shorter distance to the front or end of the list.  So, if the item falls in the first half of the list, and you are removing the item at index, it takes time Theta(index) to get to and remove the item.  If it is in the second half, it should take time Theta(size - index).  In the worst case, if you are removing the middle element, that will be Theta(size).  The iterator is usually not given an index, it removes the item at the iterator location, and takes Theta(1).

Regardless of what your lecture slides say, you want to understand how linked lists work, after which you can answer these questions for yourself.  Walking to a location takes the number of steps you walk.  The actual removal takes constant time, once you have walked to the correct space.  In fact, the way that LinkedList.remove likely works is that it creates a new iterator, walks to the right spot, uses the iterator.remove method, and then garbage collects that iterator.  (In java, this can be done with the AbstractSequentialList class once the iterator code is given.)

For a language neutral video presentation, try https://youtu.be/14g1YBvx8Co (disclaimer, it is by me).  Once you understand the natural functions of the linked list (like the iterator remove or next methods), all other functions come from just looping over the natural ones (like the linked list remove, which uses the iterator next and remove functions).",46xf3i,t3_46xf3i,Al__Gorithm,,Comment,1,0,1
d096nrq,2016-02-22 07:08:55-05:00,Al__Gorithm,,Thank you! I enjoyed the video!,46xf3i,t1_d08ocx6,algorithmsWAttitude,,Reply,1,0,1
d08zkgr,2016-02-22 00:31:28-05:00,darthandroid,,"So, this depends upon what exactly you are trying to accomplish.

**IF** you know which element you want to remove (""I want to remove the 5th element""), both are `O(n)` - Both the interator and the linked list will walk the list until it finds the element, and then remove it (because linked list's remove is internally an iterator that only looks for and removes a single element):

    int n = 5: // index to remove
    // For a list
    Node<Integer> node = list.head:
    for(i = 0: i < n-2: i++) node = node.next:
    node.next = node.next.next:

    // For an iterator
    Iterator<Integer> itr = list.iterator():
    for(i = 0: i < n: i++) itr.next():
    itr.remove():

If you instead want to remove a selection of elements, say ""all the elements with values less than 10"", then calling LinkedList.remove() on each one will be `O(n^2)` because removing each element requires the list to be walked to remove it, while an iterator still only needs to be walked once to remove any number of elements:

    // For an iterator
    Iterator<Integer> itr = list.iterator():
    while (itr.hasNext()) {
        if (itr.next() < 10) {
            itr.remove():
        }
    }

    // For a list
    Node<Integer> node = list.head:
    while (node.next != null) {
        if (node.value < 10) {
            // list.remove() inlined for effect:
            Node<Integer> node2 = list.head:
            for(i = 0: i < n-2: i++) node2 = node2.next:
            node2.next = node2.next.next:
            // end inlined list.remove()
        }
    }

This is because while LinkedList.remove() uses an iterator internally, the enscapulation of it as a method limits it to only removing a single element with each walk of the list, while using an iterator directly gives you enough control to remove as many elements as you want with a single walk of the list.",46xf3i,t3_46xf3i,Al__Gorithm,,Comment,1,0,1
d096nbi,2016-02-22 07:08:09-05:00,Al__Gorithm,,"This is great, thanks so much for the clear explanation!",46xf3i,t1_d08zkgr,darthandroid,,Reply,1,0,1
46wvu6,2016-02-21 14:43:57-05:00,Questioner2015,Open-source projects that run on a cluster?,"There is a project that I think would be an excellent open-source project, but the problem is that the source code is constantly running and training an AI it needs to perform it's task.  Is the ""standard"" solution, if one exists, is to have a test cluster that open source developers would work on?  Does anyone know of any open-source projects that need to be run and can't be tested on a local machines?",,,,,Submission,10,0,10
d08fgfb,2016-02-21 15:13:57-05:00,PastyPilgrim,,"I'm not entirely sure what it is that you're asking about. Are you saying that you have an idea for a new project that would need to be run on a cluster? And that you think the best solution is to gather other contributors to the project who will enhance the code and share computational time?

If so, I don't believe that that is the best approach. Instead of that, I would either: 

A) Save up a couple hundred bucks to build a small PC that you can leave running and crunching data (if the project is as parallizeable as you're implying, then something like an [8-Core AMD Chip](http://www.amazon.com/AMD-FD8320FRHKBOX-FX-8320-8-Core-Processor/dp/B009O7YU56) might be a good buy).

B) Use [Amazon AWS](https://aws.amazon.com/) to lease some computational time (it's pretty cheap).

C) Look into GPU programming. If you don't use the GPU in your computer a lot, you can devote it to running this project over its many cores. I have seen GPU programming in Python and Java, and I'm sure you can find libraries to do it in other languages too.

D) If you're a student, you can see if your school's CS department has a cluster that they can let you use. You won't be able to hog the cluster 24/7, but you should be able to get some time on it if your project is worthwhile.",46wvu6,t3_46wvu6,Questioner2015,,Comment,3,0,3
d08rolr,2016-02-21 20:44:48-05:00,BonzoESC,,"I used to work on the open-source Riak distributed database, which used clustering to provide price-competitive IO bandwidth and reliability (hanging more disks off a single machine increases bandwidth, but now you have more of your bandwidth on a single OS and database install, and at some point adding disks becomes cost prohibitive).

Riak uses the Erlang language, and is built such that intermittent cluster partitioning is part of normal operations, so while most nodes may be constantly running and responding to requests, it's okay for nodes to drop out due to hardware faults, planned upgrades, etc. As clusters grow, the probability of any individual node failing stays mostly stable, but the probability that at least one node will be out of commission goes up. If your distributed software can't deal with node failures, it's not going to handle distribution very well.

There's a lot of time and effort spent to keep software in the cluster backward- and forward-compatible to support rolling upgrades, where an administrator brings up a single node in an updated version of the software, waits for it to stabilize, then brings down and upgrades another node, all the way around the ring.

Developing the software, we used test clusters all the time: I'd spin up (single-node) test clusters for tests, our continuous integration system would build and run single- and multi-node test clusters for some tests, and some developers heated their houses with test clusters in the winter (most testing was done either colocated at a datacenter or in Amazon EC2).

Most software is built to be able to test locally, because a shorter feedback loop makes programming easier, more fun, and produces a better product in the end.",46wvu6,t3_46wvu6,Questioner2015,,Comment,1,0,1
46woz4,2016-02-21 14:04:23-05:00,beyondthesteel,Any ideas for practical CS undergrad thesis?,"Graduating this year, would love to code something not-so-large, but useful, not a throwaway. 

I'm open to pretty much everything, from standalone custom apps to open source plugins, etc.
 
**Caveat**: it has to be connected with information security in some (even remote) way.

I develop in C#/Java, but pretty open to learn a new modern language for this. 

Thank you for reading.",,,,,Submission,0,0,0
46wbjo,2016-02-21 12:48:26-05:00,Linseed_1,"How many times stronger is a newer top-tier processor (say, the currently most powerful Xeon) compared to a really old computer processor, like a Pentium 3? And compared to say, a NES processor chip?","I've heard all about how computer processing power is on ""exponential growth"" and heard numbers like ""several thousand times"" stronger thrown around but I'd like to now how much better today's processors actually are than older processors. 

I'm actually curious as to how much better something like an i7 4770k overclocked can be over a lower-tier current-getn processor. You pay more than 10 times more for it, certainly it should be at least 10 times more powerful? 

How do you measure processing power, anyway? I know Clock Speed alone is a very poor standard.",,,,,Submission,2,0,2
d08ep7x,2016-02-21 14:53:11-05:00,UsuallyQuiteQuiet,,"It's kind of like you're asking the wrong questions.

First of all, spending 10 times more will never actually give you a product 10 times better. Diminishing returns are a thing no matter what, and oftentimes you're paying a premium just because that product is the best. It may not be 10 times better, or even 3 times better, but you'll pay a premium regardless.

A processor's ""power"" is just such a vague notion these days anyway and it is ridiculously task dependent. 

Fundamentally a CPU gets better over time as smaller switches are implemented, and then those switches can changes state / ""process"" faster. They're also closer together which minimises the distance between switches, therefore the time taken for signals to propagate through the CPU is also minimised.

Then you have architecture. You have a bunch of circuitry with those switches that implement different operations. There are pros and cons depending on how you build the underlying hardware, how you implement the instruction set of that piece of hardware, etc that it really makes it almost meaningless to compare between CPU architectures.

Within the same architecture however, things like clockspeed, core count, and threads make a significant difference. A core i7 will definitely be significantly more powerful than a Pentium G538 (is that the product code? I may have messed it up), especially if you take advantage of the multiple cores and try to do that task in parallel.

It's important to note, however, that even if you can parallelize a task for an 8 core CPU, it won't necessarily be 8 times quicker than a single core CPU. ",46wbjo,t3_46wbjo,Linseed_1,,Comment,2,0,2
d09evkh,2016-02-22 11:41:06-05:00,ToplessTopmodel,,"You cant get a clear number because comparing processors is actually not easy.

Nes is 16 bit as i know. And a i7 is 64 bit. But that doesnt mean its 4 times better. It can be 4 times better but only in the right circumstances.

Easiest way to measure raw processing power would be to compare calculations per second. But even that is weird.

Its like trying to compare a sock to a glove. It kinda makes sense but none would fit the needs as the other.

But just so you can play with numbers: the amount of information that can get through a cpu is ghz * bits * CpuCcores.

But a cpu can be built more clever so that it actually has lots of acceleration in non trivial places. For example branch prediction. Thats when the cpu guesses the next if condition without actually calculating at that point. If wrong it has to start over at the point where it guessed wrong but if right it maybe saved some cycles. 

Also some cpu have a different instruction set. Some have a formula down in a few costly machine instructions, some will have many cheap machine instructions.

In the end you will need a benchmark. Thats a program that represents a group of problem that needs to be solved. You then see how fast they did it either by time or cps.

Calculating fibinacci numbers is not very fair. There is actually no way of making it fair. Suoercomputer compete on calculating pi btw.",46wbjo,t3_46wbjo,Linseed_1,,Comment,2,0,2
d08bvks,2016-02-21 13:36:26-05:00,SneakingNinjaCat,,Powerful in terms of what?,46wbjo,t3_46wbjo,Linseed_1,,Comment,-1,0,-1
d08byk9,2016-02-21 13:38:40-05:00,Linseed_1,,Doing the same task n times faster,46wbjo,t1_d08bvks,SneakingNinjaCat,,Reply,1,0,1
d08csy4,2016-02-21 14:01:20-05:00,SneakingNinjaCat,,What task?,46wbjo,t1_d08byk9,Linseed_1,,Reply,-2,0,-2
d08drfm,2016-02-21 14:27:40-05:00,Linseed_1,,"IDK, getting the first n numbers of the fibonacci sequence? A non-graphical benchmarking task. I don't know what a good one would be to compare processors across generations.",46wbjo,t1_d08csy4,SneakingNinjaCat,,Reply,1,0,1
d08ewnd,2016-02-21 14:58:45-05:00,SneakingNinjaCat,,"Ok to be fair, we need to decide on which architecture we want the task to be performed. ",46wbjo,t1_d08drfm,Linseed_1,,Reply,-1,0,-1
46w62a,2016-02-21 12:17:22-05:00,avaxzat,Comparison of string matching algorithms,"I'm involved in a little discussion with some of my friends on which string matching algorithm performs best in practice. One guy insists Knuth-Morris-Pratt is best, another one's insisting on Aho-Corasick and I'm biased towards shift-and. So my question is: which of these algorithms performs best under what conditions? I'd be particularly interested in papers that perform a comparative analysis of at least all of these algorithms.",,,,,Submission,2,0,2
d08d9x9,2016-02-21 14:14:22-05:00,beyondthesteel,,"Why not take nice implementations of each (I'm pretty sure I saw every algo you named written in Java, they should be a Google away) and run a battery of tests on each one, profiling every run? 
You'll get the most practical results this way :)",46w62a,t3_46w62a,avaxzat,,Comment,3,0,3
d08ds4n,2016-02-21 14:28:12-05:00,avaxzat,,That's also an option :p,46w62a,t1_d08d9x9,beyondthesteel,,Reply,1,0,1
d089vzx,2016-02-21 12:41:49-05:00,None,,[deleted],46w62a,t3_46w62a,avaxzat,,Comment,-2,0,-2
d08bv9v,2016-02-21 13:36:12-05:00,avaxzat,,"I already did a Google search just like that, because I am not an idiot. However, I was unable to find papers or articles which discussed all of the algorithms I mentioned (especially shift-and).",46w62a,t1_d089vzx,None,,Reply,2,0,2
46vwgp,2016-02-21 11:20:49-05:00,WhiteHarem,Do Computer Experts Think They'll Be Working On Spaceships By 2025,"3 Sciences Race,Spaceships,Drugs",,,,,Submission,0,0,0
46s9ri,2016-02-20 17:21:07-05:00,SondiDk,Algorithm missing number,"So i need to ""give"" an algorihtm that return the missing number in an array A that goes like: {0, 1, .. ., n − 1} 

I know how to do this in n time by doing the trick where you do: total = n*(n+1)/2 and then substract all the array values from it and then it gives you the missing value.

But how do i solve this in n^2 time? Does anyone have some hints ? ",,,,,Submission,5,0,5
d07k377,2016-02-20 18:56:56-05:00,lneutral,,"It appears that you're looking for a Θ( n^2 ) answer, not an O( N^2 ) answer. Recall that O( f(n) ) is an upper bound, so all O( n ) functions are in O( n^2 ).

/u/RamsesA gives a pretty good answer, actually. It doesn't require a sorted array, but let's think about what would happen if it did...

What complexity would you call an algorithm composed of an O( n^2 ) function followed by another O( n^2 ) function? Hint: What is the rule for leading coefficients in algorithms analysis?",46s9ri,t3_46s9ri,SondiDk,,Comment,3,0,3
d07iee8,2016-02-20 18:08:38-05:00,RamsesA,,"To solve in n^2 you loop i = 0 to n - 1, and on each iteration you check if i is in the array. It can also be solved in n by using a hash table.",46s9ri,t3_46s9ri,SondiDk,,Comment,2,0,2
d07j4bq,2016-02-20 18:28:40-05:00,SondiDk,,"I should mention that the array is unsorted, so I dont think you can do it that way? And besides how is it n^2 if its only one for loop, doesnt it need to be 2 then ? Thanks for your reply",46s9ri,t1_d07iee8,RamsesA,,Reply,2,0,2
d07lx0b,2016-02-20 19:50:43-05:00,soshelpme,,"No, this is in time of O(n^2) because you have two loops. One increases i each time and the other goes through the array to make sure that i is in the array. The code would look like this:

for i in range(1,n):
    
    isPresent = False
    for j in array:
        isPresent = isPresent or (i == j)
    if not isPresent:
        return i
        
",46s9ri,t1_d07j4bq,SondiDk,,Reply,1,0,1
d07lrxh,2016-02-20 19:46:30-05:00,soshelpme,,"Huh, so you want an inefficient solution. Here is the solution for a simple one:

Starting at the first element, select a number *i* from the array. Loop through the array and make sure *i+1* and *i-1* exist ( If *i* is equal to 1 or n, it only needs one of the two ). If *i* does not have the required of neighbors, add it to an array called *missing_neighbors*. Once you finish it for every element, take the array *missing_neighbors*. It will have exactly two values in it. Their average is your missing number.

Hope this helps",46s9ri,t3_46s9ri,SondiDk,,Comment,2,0,2
d07tumy,2016-02-21 00:01:50-05:00,Pseudofailure,,"Edit: Wait, why is the desired answer n^2? I'm so confused. 

Am I missing something here, or perhaps just getting something completely wrong and failing this reddit interview? Couldn't simply allocating an array of `n` Boolean values (all false), `present[]` iterating over the array of numbers, `numbers[]` and changing values at indices where the index equals the number to `true`, `present[numbers[i]] = true`, and then looping over the Boolean array and returning the first index where the value is `false` work? This would achieve it in O(2n) time, and could be adjusted trivially for number sets for any range a to (a+n). ",46s9ri,t3_46s9ri,SondiDk,,Comment,2,0,2
d07y2z8,2016-02-21 03:14:37-05:00,Rangsk,,"I'm confused as to why you'd want an O(n^2 ) solution when you have an O(n) solution already... I mean, you can make any algorithm more expensive by doing unnecessary steps, but I doubt that's what you're asking for.

Can you be more clear what your intention is?",46s9ri,t3_46s9ri,SondiDk,,Comment,2,0,2
d07p43q,2016-02-20 21:25:25-05:00,motheryaar,,"    for element in givenArray:
        for number in {0, ...., n-1}:
            if element != number
                return element",46s9ri,t3_46s9ri,SondiDk,,Comment,0,0,0
46rkq5,2016-02-20 14:34:02-05:00,startingComSci,What is your opinion about me going for MS in Computer Science with zero CS/IT/Programming background?,"Hi all,

I am 22, and have a Bachelors in Biology from a good university with a very good GPA. I want to pursue Computer Science, however, as I want to learn/do software development and have lost interest in Biology. 

I am planning to enroll in [Towson University, Maryland for MS in Computer Science with Software Engineering track](http://catalog.towson.edu/graduate/degree-certificate-programs/jess-mildred-fisher-science-mathematics/computer-science-ms/#requirementstext). I am worried if the classes will go over my head because I have never taken a Computer Science/IT/Programming course in my life. I am learning the very basics of Java right now from [here](http://www.homeandlearn.co.uk/java/java.html), and have learned some HTML/CSS/JavaScript by watching YouTube tutorials. I am immensely enjoying what I am learning so far, and I want to keep on going. I just want to hear your opinion on whether I will have enough understanding of the fundamentals from taking the prerequisites before I take the MS courses. 

**Following are the prerequisite courses I would need to take before I take MS courses:**

MATH 263 - DISCRETE MATHEMATICS

COSC 304 - FUNDAMENTALS OF COMPUTER SCIENCES

MATH 363 - MATHEMATICAL STRUCTURES FOR COMPUTER SCIENCE

COSC 501 - FUNDAMENTALS OF DATA STRUCTURES AND ALGORITHM

COSC 502 - COMPUTER ORGANIZATIONAL AND ASSEMBLY LANGUAGE FOR NON CS/CIS MAJOR

**Following are the MS Courses:**

COSC 519 - OPERATING SYSTEMS PRINCIPLES 

COSC 578 - DATABASE MANAGEMENT SYSTEMS I 

COSC 600 - ADVANCED DATA STRUCTURES AND ALGORITHM ANALYSIS 

COSC 650 - COMPUTER NETWORKS

COSC 609 - SOFTWARE PROJECT MANAGEMENT 

COSC 716 - OBJECT-ORIENTED METHODOLOGY 

COSC 601 SOFTWARE REQUIREMENTS ENGINEERING 

COSC 603 SOFTWARE TESTING AND MAINTENANCE 

Electives - 3, 600-700 level CS courses.

Thank you all!",,,,,Submission,6,0,6
d07cmzy,2016-02-20 15:21:22-05:00,iknighty,,If you take the prerequisite courses you should be fine!,46rkq5,t3_46rkq5,startingComSci,,Comment,2,0,2
d07edr0,2016-02-20 16:14:06-05:00,startingComSci,,Thanks for replying. Why do you say so?,46rkq5,t1_d07cmzy,iknighty,,Reply,2,0,2
d07eujd,2016-02-20 16:28:19-05:00,iknighty,,"If you continue what you're doing, i.e. learning Java and the other stuff you are on your way to get the basics for most (commercial at least) programming languages. What is missing however is the mathematical background that is covered in the prerequisite courses. That will teach you about the basis of modern languages, and efficiency and computability of algorithms, which is essential to being a good programmer and/or computer scientist. I suggest starting reading on discrete mathematics along with your work on Java, it's something that some students need some time to get used to.",46rkq5,t1_d07edr0,startingComSci,,Reply,3,0,3
d07fvxo,2016-02-20 16:58:36-05:00,startingComSci,,"Any book, video, website you can refer me to for starting with discrete mathematics? I have taken Calculus I, Calculus II, and Statistics for life sciences before.",46rkq5,t1_d07eujd,iknighty,,Reply,2,0,2
d07unb4,2016-02-21 00:31:40-05:00,zSilverFox,,MITs Open Courseware Mathematics for Computer Science is a great option,46rkq5,t1_d07fvxo,startingComSci,,Reply,2,0,2
d07y6en,2016-02-21 03:20:11-05:00,theobromus,,"Personally I love the book ""Concrete Mathematics"" by Graham, Knuth, and Patashnik.

As a meta point - I agree with other commenters that you can succeed at an MS in CS, but I would caution that it will probably be pretty tough without having any programming experience at all. Depending on how things are structured at the school I suppose there could be undergrads in the prereq courses who've already been coding for 2 years (and some probably started much earlier than that).

I'd recommend practicing as much as you can before starting. Find some simple app you want to build and just start working on it. Learn to use a debugger and source control (probably git). It might even help to start learning some data structures if you can (lists, hash tables, etc.). ",46rkq5,t1_d07fvxo,startingComSci,,Reply,2,0,2
d07eii1,2016-02-20 16:18:09-05:00,ToplessTopmodel,,I actually don't care. This sub is becoming more and more a career advice sub. Why not ask actual questions about computer science?,46rkq5,t3_46rkq5,startingComSci,,Comment,5,0,5
d07fka1,2016-02-20 16:49:41-05:00,startingComSci,,"My question is about academics and not career, so I did not think Comp Sci career advice subreddit would be the right one to post this under. I am sorry about it man. I will take it down if enough people think this should not be posted here.",46rkq5,t1_d07eii1,ToplessTopmodel,,Reply,1,0,1
d07nmiv,2016-02-20 20:41:28-05:00,lneutral,,"Frustration about post topics is something that's been going on in this sub for a while (mostly due to people coming here with tech support questions). I wouldn't worry about it - there *is* an /r/askacademia, but I think this sub functions better as ""ask computer *scientists*"" than only ""ask questions about computer science.""",46rkq5,t1_d07fka1,startingComSci,,Reply,2,0,2
d07tfkz,2016-02-20 23:46:25-05:00,dmazzoni,,Also this is a much larger and more active community.,46rkq5,t1_d07eii1,ToplessTopmodel,,Reply,1,0,1
d07nuno,2016-02-20 20:47:56-05:00,blufox,,"Make sure you have a strong interest in either programming or mathematics (it seems like you do). If you have, then your life will be rather easy, if not, you will find the going very difficult.",46rkq5,t3_46rkq5,startingComSci,,Comment,1,0,1
d07rc2k,2016-02-20 22:35:11-05:00,startingComSci,,"Yea. I am finding programming easy and fun so far. Mathematics isn't my strongest suit, but I can get through it and understand with enough hammering. ",46rkq5,t1_d07nuno,blufox,,Reply,1,0,1
d07qajh,2016-02-20 22:01:28-05:00,Merad,,It's not uncommon for people to do a MS as a way to move into CS when they already have a bachelors in another area. You should absolutely take the prerequisite classes. Unless you are some kind of natural genius prodigy when it comes to CS you will be in deep over your head without them. ,46rkq5,t3_46rkq5,startingComSci,,Comment,1,0,1
d07rdpl,2016-02-20 22:36:39-05:00,startingComSci,,"I absolutely agree. Fundamentals are the most important, and I am going to put a lot of effort in the prerequisite classes to learn them well.",46rkq5,t1_d07qajh,Merad,,Reply,1,0,1
d07qx65,2016-02-20 22:21:41-05:00,qtalll,,One of my professors had their Bachelors in a seperate field and got their MS in Computer Science. Said she had to put in some extra effort but it's definitely possible.,46rkq5,t3_46rkq5,startingComSci,,Comment,1,0,1
46q9s1,2016-02-20 09:25:31-05:00,artillery129,ELI5 why do some cables allow for more bandwidth?,Why is the bandwidth of telephone lines so low? Why can't we send an infinite amount of bits on different frequencies to increase our throughput? Can someone help clarify this?,,,,,Submission,12,0,12
d079016,2016-02-20 13:32:18-05:00,i_invented_the_ipod,,"So, this is potentially a very deep rabbit hole to go down, with coding theory, electromagnetics, etc.

The short version of the answer is that the maximum bandwidth of a particular cable is limited by the highest frequency signal that it can effectively pass. Telephone voice cables (especially the cables from the central office to your home) were designed to support voice communication, with a maximum pass frequency of a few KHz, so their high-frequency transmission characteristics aren't particularly good.

The laws of physics make it pretty difficult to do high-speed data transmission over  long distances with electrical wires, which is one reason why a lot of long-haul high-speed data lines are fiber-optic.",46q9s1,t3_46q9s1,artillery129,,Comment,8,0,8
d078nl3,2016-02-20 13:22:12-05:00,splenetic,,"When you send an electrical signal down a piece of wire then there are transmission losses - you don't get out everything you put in. As a general rule, the higher the frequency of the signal the greater the losses. 

So as frequencies - and, by extension, data rates - increase so do the transmission losses. Also as cable lengths increase so do these losses. Higher quality cables and different cable designs reduce this effect but can't eliminate it. So it's (relatively) easy to send high frequencies over short distances, or low frequencies over long distances. But high frequencies over long distances is very difficult which is why most long-distance, high speed cables are optical rather than electrical.

As well as losses, the longer the cable the more interference it will pick up. Again different cable designs can reduce this but can't eliminate it. At high data rates then any burst of interference could potentially affect a lot of bits which is hard to recover from whereas at low data rates it might only affect a single bit which could be dealt with by an error-correction system.
",46q9s1,t3_46q9s1,artillery129,,Comment,2,0,2
d07ga6n,2016-02-20 17:09:48-05:00,artillery129,,"What if you just had a lot of bandwidth by using really low frequencies, are there not an infinite number of frequencies within any range of the spectrum? So couldn't you have high latency low frequency systems? They would be perfect to do lots of streaming. I realize I know nothing about the topic, just asking to clarify.",46q9s1,t1_d078nl3,splenetic,,Reply,1,0,1
d07izyb,2016-02-20 18:25:09-05:00,splenetic,,"It's not about latency, it's about how much information (data) you can encode at a given frequency. In general low frequencies only allow low data rates. So if you want a large amount of data to be shifted in a short amount of time you'll need high frequencies. 

Plus you can't arbitrarily combine multiple frequencies on an electrical cable as there are harmonics to worry about and other effects that cause the signals to interfere with each other. It can be done to a certain extent - broadband systems use multiple frequencies - but there are limits. This isn't an issue on optical systems so it's increasingly common to find optical transports that use Wave Division Multiplexing where multiple different light frequencies - effectively, multiple different ""colours"" of light - are sent across the same piece of optical fibre without them interfering with each other.

",46q9s1,t1_d07ga6n,artillery129,,Reply,1,0,1
d07logy,2016-02-20 19:43:45-05:00,artillery129,,"true, but with an infinite amount of frequencies, you should be able to encode an infinite amount of data - the interference does make sense- but shouldn't we be able to calculate how the waves will interfere with each other and recover the data? ",46q9s1,t1_d07izyb,splenetic,,Reply,1,0,1
d07ueyl,2016-02-21 00:22:45-05:00,i_invented_the_ipod,,"Not really. You see, the rate that you can send data by modulating a carrier frequency is proportional to the frequency. So a 1 KHz frequency can only carry 1/1000 the data of a 1 MHz frequency. Since the highest frequency you can use is determined by the wire characteristics, no matter how you try to combine different frequencies, there's a limit to how much data you can encode.",46q9s1,t1_d07logy,artillery129,,Reply,1,0,1
d0835yz,2016-02-21 08:53:50-05:00,artillery129,,"So what you are saying is that there is an asymptote that we approach as we lower the frequencies, so while there are an infinite amount of lower frequencies, there is still a finite amount of bandwidth with all of those combined frequencies? ",46q9s1,t1_d07ueyl,i_invented_the_ipod,,Reply,1,0,1
d084xdw,2016-02-21 10:10:53-05:00,i_invented_the_ipod,,"""Asymptote"" is a pretty big word for a 5 year-old. ",46q9s1,t1_d0835yz,artillery129,,Reply,1,0,1
d08bk5w,2016-02-21 13:27:46-05:00,artillery129,,I'm assuming you mean yes,46q9s1,t1_d084xdw,i_invented_the_ipod,,Reply,1,0,1
d08foi1,2016-02-21 15:19:55-05:00,i_invented_the_ipod,,"Yeah, we're well out of ELI5 territory now,but practically, you have to divide the available frequency range into non-overlapping ""bands"" of frequencies. The data rate for each band is proportional to the width of the band, from highest to lowest frequency(hence bandwidth).

So, if your maximum frequency is 1 MHz, you can make a single band that covers all 1 MHz, and get throughout X (which depends on your encoding). Or you can split the range into 10 bands, each of which has throughout X/10. ",46q9s1,t1_d08bk5w,artillery129,,Reply,1,0,1
46o9yu,2016-02-19 21:55:18-05:00,Aiiight,Help with Plurals in C++,"So for class we have to write a program like below - it breaks down cents into USD in that format. The only problem I have is that there is one of a unit it has to be singular (1 dime vs 2 dimes). How would I go about doing that? Thank you!


> source code of what I have so far - would I use a bunch of if statements or is there an easier way in C++?",,,,,Submission,0,0,0
d06oubf,2016-02-19 22:26:43-05:00,dandrino,,"Something to the effect of:

    std::string pluralize(const std::string &base, int amount) {
      return (amount == 1) ? base : (base + ""s""):
    }
    ...
    std::cout << ""This costs "" << dollars << "" "" << pluralize(""dollar"", dollars) << std::endl:

EDIT: If you really wanted to generalize this, you could do something like:

    std::string pluralize(const std::string &singular, const string &plural, int amount) {
      return (amount == 1) ? singular : plural:
    }
    ...
    std::cout << ""I have "" << octopus_count << "" "" << pluralize(""octopus"", ""octopodes"", octopus_count) << std::endl:

You could make the return type ""const string &"" in this form (to avoid the copy), but then you run into the case where you could potentially assign expired r-values to l-values which would cause all sorts of bad things to happen.",46o9yu,t3_46o9yu,Aiiight,,Comment,3,0,3
d06wlmu,2016-02-20 04:14:39-05:00,Rangsk,,"What these other guys have told you is fine and likely ""better"" from a program structure standpoint than what I'm about to suggest, but honestly this is likely what your professor expects and is pretty simple.

You don't have to put all those couts on one line.

This:

    cout << ""This corresponds to "" << dollars << "" dollars, "":

Is the same as:

    cout << ""This corresponds to "" << dollars:
    cout << "" dollars, "":

And thus you can do this:

    cout << ""This corresponds to "" << dollars:
    if (dollars == 1)
        cout << "" dollar, "":
    else
        cout << "" dollars, "":

You could also use the ternary operator (?:).

Something like this:

    cout << ""This corresponds to "" << dollars << "" dollar"" << ((dollars==1) ? """" : ""s"") << "", "":",46o9yu,t3_46o9yu,Aiiight,,Comment,3,0,3
d075p33,2016-02-20 11:52:51-05:00,Aiiight,,"Yeah, I ended up using a ternary operator- thank you so much!!",46o9yu,t1_d06wlmu,Rangsk,,Reply,1,0,1
d06oc89,2016-02-19 22:10:31-05:00,njaard,,"If you're trying to do this correctly with regards to making the program translatable, you would make a function of this sort

    std::string tr(std::string singular, std::string plural, int value)

You would call it like this:

    tr(""You have 1 dime"", ""You have % dimes"", numberOfDimes)

It returns the correct string based on the number of dimes, and in the plural case, replaces the % with numberOfDimes, converted to a string.

Why?

Because some languages (i.e., not English) have different rules for plural. For example, some treat 0 as singular (English considers it plural). Some languages have a ""dual"" number, which is for when you have exactly 2 of something. Some languages have a ""dual"" number for when the number just ends in 2 (like 22, 32...): the same might apply to the singular.

So this single function could be implemented for a variety of languages, and the c++ code only need to have a single straightforward line of code.

You also can't do:

    tr(""I have "") + to_string(numberOfDimes) + tr(""dime"", ""dimes"", numberOfDimes)

because the word order might be different in some languages.
",46o9yu,t3_46o9yu,Aiiight,,Comment,1,0,1
d06p3kn,2016-02-19 22:34:53-05:00,Aiiight,,"How would I do it with just regards to English? Would there be other alternatives? 


Thank you very much for the explanation of it as well, much appreciated!",46o9yu,t1_d06oc89,njaard,,Reply,1,0,1
d06pcg6,2016-02-19 22:43:09-05:00,njaard,,"    std::string format(const std::string &s, const std::string &v)
    {
        const size_t at = s.find('%'):
        if (at== std::string::npos) return s:
        std::string copy = s:
        copy.replace(at, 1, v):
        return copy:
    }
    std::string tr(const std::string &singular, const std::string &plural, int v)
    {
        if (v == 1)
            return format(singular, std::to_string(v)):
        else
            return format(plural, std::to_string(v)):
     }",46o9yu,t1_d06p3kn,Aiiight,,Reply,0,0,0
46o0n1,2016-02-19 20:47:13-05:00,CheddaShredda,What is the language denoted by these regular expressions?,"I'm having trouble figuring out what the language of these regular expressions is:
(01+0) * 0
and 
0(10+0) *

I know for example (0+1)* is the set of all binary strings ",,,,,Submission,0,0,0
d06mmv4,2016-02-19 21:14:58-05:00,CS027,,"Well, what are some strings in each language? Can you notice any patterns? ",46o0n1,t3_46o0n1,CheddaShredda,,Comment,2,0,2
d0792hw,2016-02-20 13:34:18-05:00,CheddaShredda,,"it looks like for both expressions the strings start with 0 and end with 0
for example: 0,010,00,000,01010....
but I'm not really sure how to describe the language besides that
",46o0n1,t1_d06mmv4,CS027,,Reply,1,0,1
d079kj7,2016-02-20 13:48:54-05:00,CheddaShredda,,"It looks as if both languages are saying the same thing. So can we assume that the regular expression (RS+R) * R = R(SR+R) * holds for all regular expressions for any R, S?",46o0n1,t1_d0792hw,CheddaShredda,,Reply,1,0,1
d079rnj,2016-02-20 13:54:42-05:00,CS027,,"Don't *assume* anything. Prove things.

You're correct that they're the same language. All strings in the language start and end with 0. Any other observations? Perhaps around the placement of the 1's?",46o0n1,t1_d079kj7,CheddaShredda,,Reply,2,0,2
d07azls,2016-02-20 14:30:58-05:00,CheddaShredda,,"The 1's will never be repeated twice in a row or more. 
How would you advise proving this statement?",46o0n1,t1_d079rnj,CS027,,Reply,1,0,1
d06pd6q,2016-02-19 22:43:48-05:00,dandrino,,"""+"" in regular expressions does not mean I think what you think it means from your example. With most regular expression engines, ""+"" means ""repeat the last subexpression at least once"". The way you are using it (i.e. ""accept the subexpression to the right or the subexpression to the left) is handled by the ""|"" symbol (or alternatively character classes). 

So, the regular expression to recognize all binary strings (including the empty string) is ""(0|1)\*"" or ""[01]\*"". It it hard to properly parse the regular expression you are asking about if we don't agree on the syntax.",46o0n1,t3_46o0n1,CheddaShredda,,Comment,1,0,1
d06rcbb,2016-02-19 23:51:50-05:00,bhrgunatha,,"The question is a formal language theory question which doesn't use the same syntax for regular expressions as most software - like Unix, Linux or in computer languages. 

In formal language theory the metacharacters are:

* Alternation: is +  
* Concatenation: default operation has no symbol  'ab' is 'a' then 'b'   
* Kleene star is *  

braces () are used for explicit precedence. 
For most regular expression engines the question would be expressed as `(01|0)0`  and `0(10|0)`









  
",46o0n1,t1_d06pd6q,dandrino,,Reply,1,0,1
d076f9d,2016-02-20 12:15:01-05:00,CS027,,Only thing I want to add to this is he's actually asking about (01|0)\*0 and 0(10|0)\* but lost the Kleene stars as a result of markup syntax turning them into italics.,46o0n1,t1_d06rcbb,bhrgunatha,,Reply,1,0,1
d07b0z4,2016-02-20 14:32:08-05:00,CheddaShredda,,"you're right, fixed",46o0n1,t1_d076f9d,CS027,,Reply,1,0,1
46lkhg,2016-02-19 11:59:58-05:00,notdeadpool,MSc and PHd,"Hi,

I am thinking of doing a MSc and PHd in Computer Science (I currently am in the UK, am a teacher, I'm 30) and I have a few questions:

1: do you know of any cheapish ways of doing this? Looking at the fees for part time we are taking about 8 grand a piece

2: Have you done a MSc / PHd and have you got any good advice? (what have you used yours for?)

Thanks.",,,,,Submission,5,0,5
d062b01,2016-02-19 12:31:51-05:00,worst,,"Cheap way to do it is to get funded. In the US, where I got my PhD, if your PhD isn't being funded, you probably shouldn't be doing it.

I live and work in Europe now, and the PhD student I supervise, as well as the other PhD students in my lab and all the PhD student interns we host are also funded.

Note: you are (much) less likely to receive funding for a masters.

As for what I do with my PhD, I do research!",46lkhg,t3_46lkhg,notdeadpool,,Comment,5,0,5
d0666vn,2016-02-19 14:00:25-05:00,drobilla,,"Jaded burnt-out PhD student reporting in.

Get the MSc.  Then, think long and hard about whether you really want to get the PhD, and why.  Pretend there is absolutely no material benefit to getting a PhD whatsoever and you will have *less* money as a result for the rest of your life (which is a very easy thing to pretend these days).  If you still want to do it anyway... maybe do a PhD.

... Iff it's funded.",46lkhg,t3_46lkhg,notdeadpool,,Comment,6,0,6
d06by6o,2016-02-19 16:15:22-05:00,suboptimus_prime,,"If you genuinely enjoy teaching and would not mind doing it as a full time gig at a university, then a PhD is for you. Luckily there are enough opportunities these days for you to work for a company or a think tank as well (but you have to be really good). Most of the time you should get funding either from your supervisor or an external source, like a government scholarship or something, as /u/worst has explained: it's not really worth doing unless you are funded, because the investment is high and the possible returns can be low depending on your luck in the job market/what you want to do. 

As for what I'm doing, I'm a third year PhD student in Canada, hoping to get out of academia permanently once I graduate, if I'm lucky!",46lkhg,t3_46lkhg,notdeadpool,,Comment,3,0,3
d065fnv,2016-02-19 13:43:01-05:00,LallyMonkey,,"I can't speak of expenses, but I have done an MSc here in Canada.  Really the biggest advice is to make sure that the field of research you are going to choose is for you.  Burning out or getting bored after years of work is a possibility.

What area of research are you looking at?",46lkhg,t3_46lkhg,notdeadpool,,Comment,2,0,2
d06fk3l,2016-02-19 17:45:48-05:00,notdeadpool,,"I was really just hoping to broaden my knowledge and get some new skills, quite flexible in terms of areas of research but probably going to avoid things like biomedical. Maybe robotics or web technologies? ",46lkhg,t1_d065fnv,LallyMonkey,,Reply,1,0,1
d06kgll,2016-02-19 20:07:25-05:00,worst,,"If you are looking to broaden your knowledge then a masters is a good choice.

A PhD is about *depth* however.

Seriously, if you embark on a PhD you will be hyper focused on a particular topic. That's not to say that you won't  learn a lot of new stuff in a lot of topics, but a PhD is going to be very, *very* focused.",46lkhg,t1_d06fk3l,notdeadpool,,Reply,1,0,1
46lh2w,2016-02-19 11:41:16-05:00,egzon27,Boolean Algebra online resources,I'm in my 1st year of comp sci studying and I don't quite understand the notes from my professor so I was wondering if there are some decent resources about Boolean Algebra online?,,,,,Submission,1,0,1
d06n2rm,2016-02-19 21:29:22-05:00,N0tMyPr0bl3m,,My best advice is read the material and ask for help. Discrete Algebra gets easier the more you work at it. ,46lh2w,t3_46lh2w,egzon27,,Comment,2,0,2
46kx77,2016-02-19 09:39:40-05:00,Cumbare,A stranger needs your help.,"I'm new here and I study economy, but I need to take a c exam about the basic commands such as strlen fgets and so on (no pointers).
I got asked to compile a console application that had a matrix made of names as an input and the longest name and the number of vowels as output, would you help me with it?
Congratulations on making it this far (the risk of tl;dr is really really high), so thank you in advance.",,,,,Submission,0,0,0
d0665r3,2016-02-19 13:59:41-05:00,artillery129,,"You have to be specific with your questions. Who asked you to do this? A job application? We aren't going to do your homework for you either.


Why does it need a matrix for names? why not a single dimensional array string array? ",46kx77,t3_46kx77,Cumbare,,Comment,1,0,1
46krf1,2016-02-19 09:00:15-05:00,assassinfromabove,Logic books/advice,"I am a computer science undergrad and have been having trouble with logic and things of that nature. I do fine in all my programming classes, or classes directly about computers. But when it comes to logic (propositional logic, digital logic, predicate logic, etc.) I am usually behind my peers. It is not that I cannot do the work, I passed discrete and digital logic, I am just not as strong in those subjects as I would like to be.

Are there any books that this community would recommend I look into as I aim to sharpen my skills in those subjects?

Any help/advice would be greatly appreciated!

Thank you!!",,,,,Submission,1,0,1
46jwve,2016-02-19 04:59:15-05:00,tiopuiless,THOUSANDS OF MEMBERS ARE LOOKING FOR CASUAL SEX IN YOUR NEIGHBOURHOOD! uyhFPLK,,,,,,Submission,0,0,0
46jo50,2016-02-19 03:26:48-05:00,TossItOutThrice,Studying computer science: Is it entirely normal to be spending all of my free time sleeping?,"I am currently taking comparative languages (C++, prolog, LISP), Biology 2 (physics was boring), assembly language, discrete math and calc 3. 

This is the first semester where I've literally spent all of my free time sleeping. I've never been so tired before in my life. Is this normal, or am I not cut out for this? Everyone else in my classes seems to be doing fine (except in discrete math, everyone seems to be failing). I feel like I'm behind or something.",,,,,Submission,24,0,24
d05oj1k,2016-02-19 04:14:45-05:00,None,,"I'd imagine that it is normal. However, there are a few indicators to look out for that might suggest something like depression, such as sleeping more or less than usual and reduced quality of sleep. I'm only throwing this out here because depression does a lot more than lower your mood, and a lot of people aren't aware of this.
",46jo50,t3_46jo50,TossItOutThrice,,Comment,21,0,21
d06shop,2016-02-20 00:33:53-05:00,wellthatdoesit,,"I second this. Depression can sneak up on you, especially when you have no time for a personal life, and even if you've never experienced it before. If you think that might be the car, reach out to family, friends, or even an online community. 

That said, I had semesters where I was always tired no matter what. My optimal sleep schedule ended up being 3am-7am and 4pm-6pm for a total of about 6 hours. Breaking it up like that worked really well for me, and I always felt refreshed. Might not be the answer for everyone, but it can't hurt to try something like that.",46jo50,t1_d05oj1k,None,,Reply,1,0,1
d06smb5,2016-02-20 00:38:58-05:00,None,,I might have to try something like that myself. I've always been a night owl but college has made that impossible. I've been considering a schedule like that but I'd have to talk to my roommate. What I'm doing now doesn't work. I can't keep a normal schedule regardless of what I do.,46jo50,t1_d06shop,wellthatdoesit,,Reply,1,0,1
d0d4zl5,2016-02-25 06:32:13-05:00,TossItOutThrice,,"Sorry for the late reply, ive been... sleeping! or, doing homework... theyre both converging.. oh god, calc 2 dreams. I digress!

The biggest problem is that i sometimes feel like my brain has literally stopped working, but i have to keep takin it in. It's as if I cannot absorb any more information. 

I usually feel refreshed if I read some theoretical stuff. Like, I watched this documentary on the events surrounding godel's incompleteness theorems, entropy, and the turing machine. Super interesting stuff.

Ill take a look into this stuff. Im not sad, but I have been a bit angry.",46jo50,t1_d05oj1k,None,,Reply,1,0,1
d05p2so,2016-02-19 04:51:19-05:00,None,,"Thinking actually takes a lot of energy. There is also the stress of school, change and being around lots of people. You are probably just adapting and will recover. A couple times over the years, especially when getting into a new labour job, I would spend a day off in bed and drift in and out of sleep from exhaustion. That would be about 36 hours of sleep with the two nights minus a few hours during the day. It's just the body letting you know when it needs to rest.",46jo50,t3_46jo50,TossItOutThrice,,Comment,8,0,8
d0d54oh,2016-02-25 06:41:07-05:00,TossItOutThrice,,"Good points, thanks. ive been on a weird pattern of sleep, so maybe I need to get my schedule together",46jo50,t1_d05p2so,None,,Reply,1,0,1
d0658dy,2016-02-19 13:38:18-05:00,None,,"Yes. There is a reason caffeine addiction is rampant, or at least advertised as so, in computer maker culture. There are major tradeoffs here: with drugs just because you can work longer does not mean your learning or work is any better, can burn future moments, and reliance/addiction is very real and not ideal.

I personally spend major time/effort on eating well and exercising and sleeping efficiently, human computer tasks involve top brain work which for me burns a lot of calories/energy.

Part of university in general is training your body for brain tasks. Love your body and your mind and always work on that body-mind connection for success. You are a professional tool, just as much as your computer.

(I like that contrast between Bio/Math/Programming, do the dance but be sure you can keep your balance)",46jo50,t3_46jo50,TossItOutThrice,,Comment,6,0,6
d060xcf,2016-02-19 12:00:35-05:00,techhead57,,"Those are mostly difficult classes. Looks like you're either in your first or second year. I've found that those years were awful, because I wasn't someone who needed a lot of study time in HS, but once I started taking harder math/cs courses I needed to spend more time practicing things. 

Now, however, as a 5th year grad student, I've gotten to the point where, even when I'm taking classes in very difference areas, I can make connections between them, and with concepts I've already used in other classes, so I spend much less time on actual coursework. 

Looking back at homework I've given students while TA'ing discrete math and data structures courses, I find that I can even figure out assignments that are very different from what I was given pretty quickly.

You'll get more comfortable as you go, but as someone else suggested, try to limit the CS and math to 3. I know a lot of people pack their courses in to graduate in 4 years, especially if they changed majors mid way through. But there is a reason (other than money) that the schools lay out their recommended schedules to take 4 years. Learning takes time, and trying to check off the boxes and just get through will get you the degree, but you probably won't understand it as well as you would if you took your time.
",46jo50,t3_46jo50,TossItOutThrice,,Comment,4,0,4
d065frr,2016-02-19 13:43:05-05:00,None,,"Yeah man, that calc 3 was harsh for many people I knew. First two years is getting up to speed with just being able to take classes right, was for me at least.",46jo50,t1_d060xcf,techhead57,,Reply,2,0,2
d0d5eel,2016-02-25 06:57:22-05:00,TossItOutThrice,,"that sounds about right! i don't think i really know how to study... the 'doing it the night before' doesnt really work anymore. Also, i  talked to a friend the other day about all this. He emphasized that discrete math being done prior to data structures can make life a lot easier. I just sighed, ahah. He said something similar about too many math courses too. Lesson learned.",46jo50,t1_d060xcf,techhead57,,Reply,1,0,1
d05rlr5,2016-02-19 07:27:01-05:00,ilikestring,,Are you actually behind? Sleeping doesn't lower your grades ,46jo50,t3_46jo50,TossItOutThrice,,Comment,3,0,3
d065c75,2016-02-19 13:40:46-05:00,None,,"Definitely, sleeping a lot is a great sign on the surface for good work.

Or it could be a bug, watch out for those college sicknesses.",46jo50,t1_d05rlr5,ilikestring,,Reply,2,0,2
d0d5hio,2016-02-25 07:02:24-05:00,TossItOutThrice,,"I'm not behind, but I feel like I am, constantly. I'm not really getting a full 8 hours of sleep, it's on and off. I procrastinate, and then do a 12 hour work binge, which I know is stupid. I don't really understand where to begin with managing my time effectively. Ive tried before, and sometimes I cant motivate myself to work if there's not a looming deadline.",46jo50,t1_d05rlr5,ilikestring,,Reply,1,0,1
d0d5t66,2016-02-25 07:20:12-05:00,ilikestring,,"Habit. Force yourself to work at the same time every day and eventually it will come naturally. 
Even though it may seem so, motivation isn't something that pops out of thin air. It's something that builds as the ball rolls and then works to keep it rolling. ",46jo50,t1_d0d5hio,TossItOutThrice,,Reply,1,0,1
d0d5w4z,2016-02-25 07:24:36-05:00,TossItOutThrice,,"thanks, ill remember this.",46jo50,t1_d0d5t66,ilikestring,,Reply,1,0,1
d05uoj4,2016-02-19 09:24:57-05:00,invisiblewardog,,"I spent most of my free time doing course work, then drinking hard, then sleeping.

I'd say normal. CS and CpE majors have excessive work outside of class in my experience.",46jo50,t3_46jo50,TossItOutThrice,,Comment,3,0,3
d066qrq,2016-02-19 14:13:22-05:00,Mike-Oxenfire,,"Wow that's quite the workload to take on. 

I've noticed a big difference in doing 8 hours a day at work vs at school:  
For work, you can just go home and not have it in your mind (unless you're on call I suppose).
For schooling, I had 6 hour days but when I got home I still had school on my mind and was never really free from it. There were always projects, homework, reading, etc. that needed to be done.

Burn out is a real thing so If you're finding yourself especially stressed because of school you either need to lighten your load or you can even take a semester/quarter off.",46jo50,t3_46jo50,TossItOutThrice,,Comment,3,0,3
d07acy9,2016-02-20 14:12:04-05:00,blackberrybramble,,"This is so real.  As someone who finished school, then worked full time for a few years, then went back to school -- I miss work.  It is such a tough thing to always be ""on"" in your mind because school never really feels finished at the end of the day.  It's exhausting.",46jo50,t1_d066qrq,Mike-Oxenfire,,Reply,1,0,1
d06dass,2016-02-19 16:48:09-05:00,inquisitive_idgit,,Do you snore?    I had a semester where I spent all my free time sleeping: it ended with a diagnosis of sleep apnea.,46jo50,t3_46jo50,TossItOutThrice,,Comment,3,0,3
d0d5j4d,2016-02-25 07:04:56-05:00,TossItOutThrice,,"oh damn, thats not good. I don't know if I do, but im gonna find out now that you mention it. i never considered health issues. ",46jo50,t1_d06dass,inquisitive_idgit,,Reply,1,0,1
d05ny9p,2016-02-19 03:38:05-05:00,ToplessTopmodel,,Do you sleep longer or do you study so long that you can barely sleep? Can you define your sleep patterns?,46jo50,t3_46jo50,TossItOutThrice,,Comment,6,0,6
d0d5la7,2016-02-25 07:08:21-05:00,TossItOutThrice,,"its really bad, i think. I procrastinate by watching educational videos, hoping to learn extra tips, then when I feel the looming deadlines I binge study/work for 8-14 hours, then pass out for a few hours.

ive tried managing my time, but my biggest problem is motivating myself. I tell myself 'theres plenty of time'. this semester just feels like there are no breaks. I was kinda dumb for taking a summer class, too.",46jo50,t1_d05ny9p,ToplessTopmodel,,Reply,1,0,1
d05ycwg,2016-02-19 11:01:45-05:00,None,,[deleted],46jo50,t3_46jo50,TossItOutThrice,,Comment,2,0,2
d0d5n7x,2016-02-25 07:11:19-05:00,TossItOutThrice,,"I actually appreciate all of everyone saying this is too much: my friends warned me about taking this many classes. I have been feeling a bit silly, but maybe im just insecure about taking so many classes, rather than not being cut out for it.

It wasn't my original intention to take discrete. I somehow managed to get through some CS classes without it, and this semester my advisor told me I needed to get it done, I read about it, it sounded interesting. I thought it would have been a lot of fun, to freshen things up... heh. So i threw it on top of everything else!",46jo50,t1_d05ycwg,None,,Reply,1,0,1
d0674g5,2016-02-19 14:22:09-05:00,None,,"I would monitor your awake/working/sleeping/relaxing schedules closely. You need to be aware of where the drain is and then decide with your medical advisor/Doctor what is right for you. Do not drift into unhealthy patterns. You need to monitor yourself as negative patterns can develop. 
Source:- Been there, done that.",46jo50,t3_46jo50,TossItOutThrice,,Comment,1,0,1
d06bmzr,2016-02-19 16:07:58-05:00,ciaran036,,"How are your scores in classes? 

If it's something you are concerned about, you should consider seeing a doctor. Perhaps other factors are at play, such as your diet. Try to change up your surroundings, your routines, your food choices etc. and see if anything like that makes any difference. 

You should still be getting normal amounts of sleep (at least 7 hours but no more than 9 hours) every night. 

I remember being at university myself and with part-time jobs and going out drinking every other day it meant that sometimes in between classes I would take naps. That's not what you should do though. I could only get away with that in the early years of classes at my university. 

The whole experience can be tiring but if your grades are slipping, you really need to focus on resolving that and taking out other activities where you can. Others have suggested that perhaps you have taken on too much work. In that case, you should focus on the subjects that are worth more to you if you can. You don't have to perform brilliantly in everything, look at what the classes are worth to you and figure out what you need to focus on. You could choose to focus on the subjects you enjoy, those that you feel would further your career better, or perhaps simply whichever class supplies the easiest top-grades :P",46jo50,t3_46jo50,TossItOutThrice,,Comment,1,0,1
d06d6hj,2016-02-19 16:45:13-05:00,bbpgrs,,As long as you're keeping up a good gpa,46jo50,t3_46jo50,TossItOutThrice,,Comment,1,0,1
d07m8tx,2016-02-20 20:00:27-05:00,arabbay,,"All off the classes you are taking are usually considered very difficult, I don't think most people could handle that load you should be taking more of your easy general eds along side your harder classes. Also does your school require you to go up to calc 3? Mine only required calc 2. Factor in that many people finish in 5 years its obvious why you are spending all of your time studying. ",46jo50,t3_46jo50,TossItOutThrice,,Comment,1,0,1
d0d5rmz,2016-02-25 07:18:00-05:00,TossItOutThrice,,"yeah, i threw on discrete last minute. I was supposed to have taken it in my second semester, but i somehow missed it. was pretty frustrating, but i thought I could handle it. 

and yep, its calculus for engineers, which I guess means the material is a bit different than regular calc? Anyways, the school is actually changing the requirement for CSBS next catalog year from calc 1-3 to calc 1-2 & mathematical logic. Im guessing its gonna be like, discrete math 2?",46jo50,t1_d07m8tx,arabbay,,Reply,1,0,1
d05og3v,2016-02-19 04:09:32-05:00,mgrieger,,"A lot of people in university sleep a lot, I wouldn't worry about it. I take naps every day. :)",46jo50,t3_46jo50,TossItOutThrice,,Comment,1,0,1
46j9un,2016-02-19 01:11:46-05:00,Eton1102,Best CompSci to get into?,I'm a high school student interested in going into processor engineering but I wanted to see if there are any things I should know about it and if I should look into other types of computer science. If you have any advice please tell me. Thanks in advance. ,,,,,Submission,0,0,0
d05o1fg,2016-02-19 03:43:27-05:00,ToplessTopmodel,,Processor engineering is one of the most boring things. Its more electrical engineering. But if you are the best in your class you will get a good paying job.,46j9un,t3_46j9un,Eton1102,,Comment,1,0,1
d05ukqm,2016-02-19 09:21:43-05:00,Eton1102,,I was mainly interested in processor engineering because of the pay. So are there any other CompSci fields that have similar pay?,46j9un,t1_d05o1fg,ToplessTopmodel,,Reply,1,0,1
d07fews,2016-02-20 16:45:12-05:00,startingComSci,,"Hey man I don't have answer to your question, but I would like to make a comment.. Do not **just** think about the money when deciding what to study/learn/do for **40+ years**. You do not have to love your job, but you should **not** hate your job either. Decide wisely and do what you enjoy (especially in Computer Science where choices are broad and all choices make you decent money).",46j9un,t1_d05ukqm,Eton1102,,Reply,2,0,2
46ilq1,2016-02-18 22:11:03-05:00,DatBigRussian,How can I expand my knowledge on Computer Science?,"Hey guys! I'm a college student that is aspiring to transfer to the University of Texas's CompSci program this fall. I was wondering if y'all have any great resources where I can go learn more about Discrete Mathematics, Linear Algebra, and any other subject you feel is important in Computer Science!
note: I plan on studying theoretical Computer Science, but I haven't decided yet.
Any resource will be appreciated! Thank you and code on! (sorry for being corny)",,,,,Submission,6,0,6
d05n0ol,2016-02-19 02:44:38-05:00,notdeadpool,,http://universityofreddit.com is a good place to start,46ilq1,t3_46ilq1,DatBigRussian,,Comment,1,0,1
46i8v8,2016-02-18 20:45:01-05:00,Maverick524,Algorithm Analysis,"I'm taking an Algorithm Analysis course this semester and in all honestly I feel completely and utterly lost. So my question for you guys, is are there any youtube series that are recommended that I can use as supplemental information to assist wrapping my mind around the concepts so I can actually learn the material? Any suggestions whether videos or other alternatives are greatly appreciated.",,,,,Submission,0,0,0
d05cagw,2016-02-18 20:59:31-05:00,billibus_maximus,,"I've found [these videos from UC Berkeley](https://www.youtube.com/playlist?list=PL-XXv-cvA_iAlnI-BQr9hjqADPBtujFJd) useful in the past.  Lots of data structure and algorithm info, including asymptotic analysis.",46i8v8,t3_46i8v8,Maverick524,,Comment,1,0,1
d05dqqq,2016-02-18 21:35:15-05:00,bhrgunatha,,"From Coursera - try Stanford's:  
[Algorithm Design and Analysis Part 1](https://www.coursera.org/course/algo)  
[Algorithm Design and Analysis Part 2](https://www.coursera.org/course/algo2) 

Princeton's:  
[Analysis of Algorithms](https://www.coursera.org/course/aofa)  

I think you can still enrol in the previous sessions and watch the videos.
Search around for other algorithm related courses too.",46i8v8,t3_46i8v8,Maverick524,,Comment,1,0,1
46gy3m,2016-02-18 16:00:26-05:00,KrustyKrab111,Where can I learn and practice questions on complexity,"Hey guys, I'm a freshman and I'm kinda lost in my data structures class when they're talking about complexity. Where can I understand the concepts and practice question based on complexity? 

Thanks",,,,,Submission,9,0,9
46epv5,2016-02-18 08:27:16-05:00,nyccfan,Question about intro material,I am trying to teach myself how to code/comp sci as a bit of a hobby and maybe someday as part of my job. But really not a set goal in mind other than an interest in the material. I have started working through a book on python but was also hoping to find a good introductory comp sci text. In particular one that does not rely on using a computer while reading through it. This is because I sometimes have downtime at work where I could read a book on comp sci but not really use a computer to work through any code/examples. Anyone here have any good suggestions?,,,,,Submission,2,0,2
d04x8pm,2016-02-18 15:07:10-05:00,katzey,,"if you're gonna do it, do it right   

get ""The C Programming Language"". it is the Bible of programming",46epv5,t3_46epv5,nyccfan,,Comment,1,0,1
46dkfm,2016-02-18 02:08:31-05:00,michael1026,A couple of questions about AVL trees,"I just have a couple of questions about implementing them.

1. If I insert a node recursively, and while I'm working my way back up the tree and recalculating the balance factors, once I find a node with a balance factor of, let's say 2, how do I know if I need to handle LL or LR? Would I check the balance factor of the left node to see if it's positive or negative (and vise-versa for opposite case)?

2. How does a rotate right and a rotate left affect the balance factor of each node? From drawing it out, I think the parent node will decrease by two if rotating right and increase by two if rotating left. What about the children nodes, though?",,,,,Submission,7,0,7
d04bdf3,2016-02-18 03:46:35-05:00,flebron,,"Which rotations you need to do are determined by looking at the balance factor of the children of the imbalanced node, and its siblings. 

For more detail, you can look at http://www.brpreiss.com/books/opus4/html/page326.html and the pages following that.",46dkfm,t3_46dkfm,michael1026,,Comment,2,0,2
d04bp9j,2016-02-18 04:07:07-05:00,michael1026,,"Thank you. I swear I clicked every link I could find on Google and none of them were as helpful as the one you provided. I'm not done reading it yet, but does it answer question 2?",46dkfm,t1_d04bdf3,flebron,,Reply,1,0,1
d04lj4g,2016-02-18 11:01:06-05:00,flebron,,"Well it shows you the heights of the subtrees involved when rotating (before and after), so yeah :)",46dkfm,t1_d04bp9j,michael1026,,Reply,1,0,1
46c5if,2016-02-17 20:06:54-05:00,scamperalong,Trying to understand why the FBI can't break into the iPhone.,"If you aren't familiar with what's been happening here's a link to Apple's open letter.

[Apple customer letter](http://www.apple.com/customer-letter/)

The tl;dr version: Court orders Apple to help create a back door for this specific iPhone. Apple has refused the court order. 

I'd rather not turn this into a privacy/moral discussion. Just trying to understand some things about this situation.

Based on Tim Cook's response it almost seems that it is possible to create this back door, and if Apple can do it, why can't anyone else? I have a limited background in computer science, but it seems that the FBI wants a way to brute force into the phone without it resetting. I'm not sure how Apple would be able to help other than by pushing some sort of software onto the phone that disables this security feature. But why can't the FBI push their own software? What can Apple do that the FBI can't?",,,,,Submission,20,0,20
d03yaci,2016-02-17 20:22:52-05:00,zefyear,,"TL:DR -- **Apple has two big prime numbers that the FBI doesn't know.**

I will speak generally, wholly based on my familiarity with UEFI and some IOS application reverse engineering and doesn't represent an in depth explanation that an internal engineer could give you. In short, this is a GUESS as to what the FBI is trying to do.



I just took a look at the customer letter now, and in reading it, something really stuck out at me 
> The government would have us remove security features and add new capabilities to the operating system, allowing a passcode to be input electronically. This would make it easier to unlock an iPhone by “brute force,” trying thousands or millions of combinations with the speed of a modern computer.

So, that addresses the real crux of the question. You might then ask ""why can't the FBI do *that*""

A small, ""real world"" reason is rather mundane: The FBI  doesn't have the resources. 

Let's ignore that for a moment and focus on the *real* reason, which is cryptographic signing: modern computers of all stripes provide support for crytographically signed bootloaders all the way up to applications. In IOS, this is known as the ""Secure Enclave Processor"" which is on-die. This is the first thing to start when the chip is powered up, the thing that loads a cryptographically-signed bootloader, and the thing that gates a lot of IO with the outside world (like the NAND). Because this is all done with public key cryptography, even if the FBI disassembled and reverse engineered the device, they couldn't create a ""fake"" application because the phone [never actually has the cryptographic key material necessary to produce such an application](https://en.wikipedia.org/wiki/Digital_signature#How_they_work).

It's worth noting that hardware encryption on consumer hardware has existed for over a decade and if you use certain distributions of Linux, you probably take advantage of it every day. However while it hasn't obviously taken hold in the broader ecosystem, locked-down hardware platforms like Apple's top-to-bottom design has had much more liberty in implementing secure computing.

**If** they can be compelled to produce de-novo firmware for the purpose of data extraction they could also be compelled to design the means necessary to extract the data from the secure enclave, e.g. by prying the chips open and putting it under a scanning tunneling microscope.",46c5if,t3_46c5if,scamperalong,,Comment,25,0,25
d040cuy,2016-02-17 21:17:17-05:00,scamperalong,,"Thanks for the response! 

So (theoretically) there is a way to push new firmware on to the phone   without compromising data. 

Also, just started reading about scanning tunneling microscopes.... so cool",46c5if,t1_d03yaci,zefyear,,Reply,4,0,4
d050ayv,2016-02-18 16:10:39-05:00,lordvadr,,"> never actually has the cryptographic key material necessary to produce such an application

Just a quick correction for the layman.  It actually contains all the necessary information, just not in a format readily usable.  The public key is (for the most part) just the multiplicative product of those two big prime numbers.  All one would have to do is factor the number, however it is computationally unfeasible due to how big the number is.

How big?  Those two prime numbers are both odd, and between (if it's a 2048 bit key)

    89,884,656,743,115,795,386,465,259,539,451,236,680,898,848,947,115,328,636,715,040,578,866,337,902,750,481,566,354,238,661,203,768,010,560,056,939,935,696,678,829,394,884,407,208,311,246,423,715,319,737,062,188,883,946,712,432,742,638,151,109,800,623,047,059,726,541,476,042,502,884,419,075,341,171,231,440,736,956,555,270,413,618,581,675,255,342,293,149,119,973,622,969,239,858,152,417,678,164,812,112,068,608

and

    179,769,313,486,231,590,772,930,519,078,902,473,361,797,697,894,230,657,273,430,081,157,732,675,805,500,963,132,708,477,322,407,536,021,120,113,879,871,393,357,658,789,768,814,416,622,492,847,430,639,474,124,377,767,893,424,865,485,276,302,219,601,246,094,119,453,082,952,085,005,768,838,150,682,342,462,881,473,913,110,540,827,237,163,350,510,684,586,298,239,947,245,938,479,716,304,835,356,329,624,224,137,216

With that said, there's some speculation that the NSA has the ability to factor some keys, and some say keys as large as 2048 bits is in the realm of possibility although it does take some time.  Others think that the NSA has an unknown weakness in the mathematics.  Neither of which the US government is about to publicly admit to or release.  Doing so would seriously cost lives.",46c5if,t1_d03yaci,zefyear,,Reply,3,0,3
d046jzf,2016-02-18 00:11:28-05:00,Merad,,"/u/zefyear made a good comment about encryption in general, but AFAIK it doesn't quite apply in this case because the phone in question is an iPhone 5c, which doesn't have the secure enclave. The FBI is simply (""simply"") looking for help brute forcing the phone's passcode. Normally after multiple failed passcodes the phone will lock you out and make you wait before trying again, and there's an option to wipe the phone after 10 bad passcodes are entered. 

So why can't the FBI do it themselves?  The problem is that they can't just replace the operating system entirely, they need to modify it. It's possible that the iPhone stores it's OS code in an encrypted format (I'm not sure). If that's the case it would be very difficult, perhaps impossible, for them to even access the code to attempt to modify it. Even if they can access the code, they're faced with an enormously difficult reverse engineering task. 

Imagine that I give you a massive jigsaw puzzle with a million pieces that's already assembled. Then I give you one single piece and ask you to find the piece in the puzzle matching that shape, and replace it with the piece I gave you. That's basically what the FBI would be trying to do, except their task is quite a bit more difficult. Even if they managed to do that, I would not be surprised if Apple has additional protections to detect ""unauthorized"" modifications to the OS code. 

tl:dr - What the FBI wants done might be impossible for them to do themselves, and even if possible would likely take them years.",46c5if,t3_46c5if,scamperalong,,Comment,7,0,7
d04h487,2016-02-18 09:01:09-05:00,fearmor,,"Could they ""clone"" all the encrypted data from the device storage (somehow) into an external drive and then brute force decrypt the copy?",46c5if,t1_d046jzf,Merad,,Reply,2,0,2
d04np5c,2016-02-18 11:50:22-05:00,lgroeni,,"In theory, but it'd be both orders of magnitude more difficult to brute force *and* probably would be destructive to the device. The TL:DR is that the key used to encrypt the data on the device is actually *much* more difficult to forcibly decrypt (it's a combination of the user input + a hardware key, and data is encrypted using AES-256 iirc).

Hence why the FBI wants to be able to brute force the PIN of the device instead of trying to break the encryption itself: a six digit pin code has a maximum of 1 million possible combinations, and at the minimum of 80ms between entries it would only take ~22.2 hours to input all possible combinations.

Relevant info on how encryption works on iOS:

* [Why can't Apple decrypt your iPhone?](http://blog.cryptographyengineering.com/2014/10/why-cant-apple-decrypt-your-iphone.html)
* [Apple can comply with the FBI court order](http://blog.trailofbits.com/2016/02/17/apple-can-comply-with-the-fbi-court-order/)",46c5if,t1_d04h487,fearmor,,Reply,3,0,3
46ajz3,2016-02-17 14:43:30-05:00,scriptkittieswagger,Needing ideas for a pet project for data visualization. Haven't had anything good come in a while. What would you like to see?,"The other day I came across this: https://github.com/Dobiasd/programming-language-subreddits-and-their-choice-of-words

The tl;dr: of the link is that a Redditor wanted to evaluate the personality/gauge a language by their community. They collected and all comments (about 300k) written to submissions (about 40k) in respective programming language subreddits from 2013-08 to 2014-07 using PRAW and SQLite.

I loved this idea, and I want to do something useful and interesting like this recently. I did a Twitter scraper that scraped nearly 16k words of tweets to analyze a person's ""personality"" through the IBM Watson for fun -- but it wasn't nearly as useful information as this person's pet project.

Do any of you know where I can find some data, or any data that would be interesting on a coding/computer science standpoint that would need some visualization? Or any question that could be explained or interesting from a data visualization point?  I'm not looking for something to create a postdoc with or spend more than a weekend - but something fun to learn about computer science in general.  Any ideas?",,,,,Submission,2,0,2
d03w9ht,2016-02-17 19:28:08-05:00,enigma_x,,You can find hundreds of interesting data sets on the [UCI machine learning repository](http://archive.ics.uci.edu/ml/),46ajz3,t3_46ajz3,scriptkittieswagger,,Comment,2,0,2
465eu0,2016-02-16 19:03:18-05:00,PlainclothesmanBaley,"Propositional logic, Horn formulae. I just do not understand this theorem.","How is this not saying, 'S* is satisfiable if you can make any propositional variable anywhere true.'?

http://imgur.com/c2fIny7

I imagine it has something to do with the fact that the big unions are not disjunctions, like I had been imagining they were, but I can't work out what they are instead.

Thanks for the help.",,,,,Submission,2,0,2
d02sapm,2016-02-16 22:30:25-05:00,_--__,,"Two things to note: S* is constructed from *literals* - not propositional variables: and S* is a set of (mn^(2)/2) clauses - so in order for S* to be satisfiable, each clause must be made true.

I will admit it's a bit of an odd use of notation - though that usually happens when talking about ""satisfying sets of clauses"" rather than the (IMO) more sensible ""satisfying a conjunction of clauses"".",465eu0,t3_465eu0,PlainclothesmanBaley,,Comment,2,0,2
d02sn1h,2016-02-16 22:40:01-05:00,PlainclothesmanBaley,,"Great thank you.

So the upshot is, if every possible assignment to your formula results in two of the literals in a single clause being false, then the formula is not renamable-Horn?

Is there a way to see this intuitively?",465eu0,t1_d02sapm,_--__,,Reply,1,0,1
d02tgdr,2016-02-16 23:03:08-05:00,_--__,,"> So the upshot is, if every possible assignment to your formula results in two of the literals in a single clause being false, then the formula is not renamable-Horn?

Exactly.  You can think of an assignment in this case as an algorithm for your renaming: if a literal is ""true"" we rename the corresponding variable positively, if it is ""false"" we rename it negatively.  Since a satisfying assignment for S* means at most one literal in the each clause is false, this renaming algorithm will give us a renaming of our original set of clauses where at most one variable in each clause occurs negatively.",465eu0,t1_d02sn1h,PlainclothesmanBaley,,Reply,2,0,2
d02u5vv,2016-02-16 23:23:54-05:00,PlainclothesmanBaley,,"A literal can be true in one clause, and then its negation be false in another.  Does it get flipped in that case?",465eu0,t1_d02tgdr,_--__,,Reply,1,0,1
d02uqgf,2016-02-16 23:41:15-05:00,_--__,,If the (true) literal is x then we leave it as x.  If the (true) literal is ¬x we would rewrite it as x.,465eu0,t1_d02u5vv,PlainclothesmanBaley,,Reply,2,0,2
d02uw10,2016-02-16 23:45:59-05:00,PlainclothesmanBaley,,"In this: http://imgur.com/kKwfHLa case, You could satisfy S* with P1, P2, and ~P3 being true.  But if you flip just P3, it still isn't a horn formula.

Sorry, I'm just failing to understand the final step I guess.",465eu0,t1_d02uqgf,_--__,,Reply,1,0,1
d02v9y5,2016-02-16 23:58:03-05:00,_--__,,Why do you say it isn't a Horn formula when you flip P3?,465eu0,t1_d02uw10,PlainclothesmanBaley,,Reply,2,0,2
d02vato,2016-02-16 23:58:48-05:00,PlainclothesmanBaley,,Cause in C1 both P1 and P3 would be positive.,465eu0,t1_d02v9y5,_--__,,Reply,1,0,1
d02vei8,2016-02-17 00:02:01-05:00,_--__,,"Oh, I was going off the (equivalent) definition that a Horn clause has at most one *negated* variable.  In that case, swap all my trues/falses, so in your example you would flip P1 and P2.",465eu0,t1_d02vato,PlainclothesmanBaley,,Reply,2,0,2
d02vipc,2016-02-17 00:05:49-05:00,PlainclothesmanBaley,,Right excellent.  Thank you for your help.  You've been really informative.,465eu0,t1_d02vei8,_--__,,Reply,1,0,1
464u6q,2016-02-16 16:57:19-05:00,project21124,Dual Major in CS/CE?,"Hey Reddit, I've been researching whether I want a CS degree(more abstract, AI, ML, algorithms, etc), or CE(OS dev, bit-banging, circuits, etc), and I cannot decide..

I love maths, and so I really want the CS degree which has the ML/AI and other maths, while I also like the idea of vector analysis & difEq/discrete math in the CE degree.

CS usually is in Java, and I (strongly dislike) that language for more than 30 minutes, so I really prefer the C++/asm in the CE degree(I've already been using C++ for over 2 years, so I'm very comfortable with that side of things).


I can't decide, so I'm thinking of majoring in both degrees, getting a masters in both.

What do you guys think? Thanks, I really appreciate it.

EDIT: Extreme analogy removed :)",,,,,Submission,1,0,1
d02i59z,2016-02-16 18:06:50-05:00,high_side,,Not a lot of places will let you double major because the curricula are so similar.  Consider minoring in attitude readjustment.,464u6q,t3_464u6q,project21124,,Comment,16,0,16
d02j6xb,2016-02-16 18:33:36-05:00,Walf9,,your comment definitely beat whatever i was gonna say about java and suicide... plus how does he know anything about either?,464u6q,t1_d02i59z,high_side,,Reply,6,0,6
d02qqgb,2016-02-16 21:48:09-05:00,Breadsecutioner,,"Odd. At my college, they shared perhaps one or two non-general classes.",464u6q,t1_d02i59z,high_side,,Reply,3,0,3
d03i5t4,2016-02-17 13:55:47-05:00,project21124,,"What's wrong with my attitude? Plenty of people dislike Java, I happen to be one of them.",464u6q,t1_d02i59z,high_side,,Reply,1,0,1
d02nbz1,2016-02-16 20:21:32-05:00,canadiandev25,,"/u/high_side said it best, but I'll still explain a couple things in case you have gotten bad information from someone.

> CS degree which has the ML/AI and other maths, while I also like the idea of vector analysis & difEq/discrete math in the CE degree.

Discrete math is a computer science or mathematics course and I am almost certain that you will take a course on it if you study computer science. Differential equations, vector calculus, and analysis are all math courses which you also can take as a computer science student. They might be mandatory for CE, but you can probably take them as electives for computer science.

I also really question what you know in regards to Machine learning and artificial intelligence. Just because the fields sound interesting does not mean that you will particularly enjoy them.

In many universities, at least in mine, OS development, circuits, and Microprocessor Systems are all courses that are offered for computer science students.

> CS usually is in Java, and I would shoot myself if I had to use that language for more than 30 minutes, so I really prefer the C++/asm in the CE degree

This comment bothers me the most. Let's just ignore that you said that you will shoot yourself if you had to code in Java since I really don't care what you have against the programming language. When completing a computer science degree, you will not be using just one programming language. I personally have used a handful during my undergraduate studies: C, Java, Python, R, assembly, Lisp, Haskell, Verilog, Fortran, ML.

Outside of school, I have also played around with: Javascript, golang, objective-c, and ruby.

So no you will not use only Java for your whole degree. You probably won't even have a course that teaches Java. At best, you will have courses that use the Java programming language to explain different techniques and concepts.

> (I've already been using C++ for over 2 years, so I'm very comfortable with that side of things).

Without sounding harsh, no one cares what programming language you are already comfortable with. No one expects you to have any prior experience in programming when studying computer science or computer engineering.

> I'm thinking of majoring in both degrees, getting a masters in both.

Pick one and maybe add a math minor if you really like math like you stated above. But I really doubt you know what real math is until you have taken a proof based math course in university.

EDIT: Spelling",464u6q,t3_464u6q,project21124,,Comment,8,0,8
d03idth,2016-02-17 14:00:42-05:00,project21124,,"So, you are saying that CS and CE are basically the same? What is the difference then, why are they separate degrees? 

It's interesting that you assume I don't know what ""real math is"". Of course I haven't taken a formal Univ course, but I have done quite a lot of research, and yes, I am in fact interested enough to take those classes.
",464u6q,t1_d02nbz1,canadiandev25,,Reply,1,0,1
d03lmos,2016-02-17 15:14:03-05:00,canadiandev25,,"What I meant was that you should have an open mind in regards to these topics. You might know exactly what you want to study, but many people end of changing their minds. I started with only a major in Computer science but I am now also doing a statistics minor. I would have never thought I would being getting a minor in statistics before studying university because I did not know what it would involve.

Computer science and computer engineering programs will vary depending on the university. But in general, computer science will deal more with theory while computer engineering deals more with hardware. In computer engineering, you will take courses in fields such as: physics, chemistry, applied mathematics, software/digital systems, and circuit design. I'm not too familiar with general upper year computer engineering courses to give you an idea of what they are. But the ones I listed are common topics that a computer engineering student will learn. In computer science, you will take courses such as: discrete mathematics, software design, systems programming, data structures, theory of computation, algorithm design and analysis, operating systems, databases, and networking. These are the more general courses that most universities will have. Depending on the university, you will also find courses in topics such as AI, computer science theory, Scientific computing, bioinformatics, and systems.

The best advice I can give you is to pick a couple universities that you are interested in and compare the required courses for their computer science and computer engineering programs. See which courses interest you the most then go with that program. ",464u6q,t1_d03idth,project21124,,Reply,2,0,2
d03mrhl,2016-02-17 15:39:23-05:00,project21124,,"Alright, interesting. One thing I definitely have noticed, is that universities vary greatly with their programs, which makes it an even harder choice.

What do you think about a dual major? I have no experience with college, have you ever seen it happen?

Thanks",464u6q,t1_d03lmos,canadiandev25,,Reply,1,0,1
d03psjt,2016-02-17 16:46:24-05:00,canadiandev25,,"I have seen universities offer double degrees before but they are very rare. I never never seen a CS/CE double degree and I question whether one even exists. I've only seen business & math and CS & math double degrees.

Unless you actually find a university that offers such a thing, assume that double degree in CS/CE is not possible. You need to do more research and figure out which one you will enjoy more. Chances are that you can't do both. ",464u6q,t1_d03mrhl,project21124,,Reply,1,0,1
d02je9k,2016-02-16 18:38:51-05:00,torturous_flame,,"I agree with /u/high_side on both comments. My university did not let people double.

I'm mostly here to jump to defend my first language. Java isn't really all THAT bad and C# is close enough that I didn't have many transfer issues when I got a job. 

To be honest sounds like you're wedded to a CE job. It looks to me, based on the complaints you've provided, that you don't have much love for programming applications.

Also chill a bit.",464u6q,t3_464u6q,project21124,,Comment,6,0,6
d02qrfp,2016-02-16 21:48:49-05:00,Breadsecutioner,,Try COBOL. Maybe you'll like it.,464u6q,t3_464u6q,project21124,,Comment,2,0,2
d02wslh,2016-02-17 00:50:16-05:00,None,,"> CS usually is in Java, and I would shoot myself if I had to use that language for more than 30 minutes

Bad attitude, Java is spectacular in its own way, as is every language. Being more open minded will be a necessity to work with other people successfully long-term.",464u6q,t3_464u6q,project21124,,Comment,2,0,2
d03ijbb,2016-02-17 14:04:07-05:00,project21124,,"Apparently I came off the wrong way in this post, so I'll try and restructure my actual question.

If it is indeed not a possibility to have dual majors in such closely linked courses, then what are the major differences between the two?

I've tried finding the specifics from other sources, but they all have different things to say.

Is CE more math focused, is CS more theory focused, etc, etc.

Thank you.",464u6q,t3_464u6q,project21124,,Comment,1,0,1
d02je6z,2016-02-16 18:38:48-05:00,bbpgrs,,"Are you serious? Kill yourself for having to use one of the simplest to use languages? (in my experience).

A programmer shouldn't be tied to one language. If you want to be a good programmer you should understand the concepts of proving and be able to get into any language with ease. Otherwise programming isn't the career path for you.

Although most entry level jobs I've come across are for work with or relating to C++, you should still learn to be more adaptable.",464u6q,t3_464u6q,project21124,,Comment,1,0,1
d03i5ok,2016-02-17 13:55:42-05:00,project21124,,"First of all to all of the comments about ""suicide"", I'm not actually saying that I would do that. I just strongly dislike the language. I'm not tied to just 1 language either, I've learned several, and C++ is only my latest language, that I like the most. I understand that in any degree, it doesn't just use one language, but I would prefer to use C/C++ as a main language instead of Java. Once you've learned one legit language, it's usually pretty easy to learn any other language. I just don't like Java :)",464u6q,t1_d02je6z,bbpgrs,,Reply,1,0,1
464fy3,2016-02-16 15:35:28-05:00,AlgorithmsAreHard,Algorithms O notation proof,"Prove using the definition of 
O notation, if 
f(n) ∈ O(n) then [f(n)]^2 ∈ O(n^2 )
assuming f(n) is asymptotically positive 
function. ",,,,,Submission,0,0,0
d02cqv3,2016-02-16 16:02:27-05:00,mosqutip,,Quit posting bare homework questions.,464fy3,t3_464fy3,AlgorithmsAreHard,,Comment,2,0,2
d02dgy2,2016-02-16 16:18:42-05:00,AlgorithmsAreHard,,I'm not really expecting a full solution to this. I just want a lead on how to approach this question as I'm running short on time to do every question.,464fy3,t1_d02cqv3,mosqutip,,Reply,1,0,1
464dbg,2016-02-16 15:20:19-05:00,AlgorithmsAreHard,Algorithms Question,"Disprove n^2 ∈ Ω (n^3)
",,,,,Submission,0,0,0
d02crs5,2016-02-16 16:03:03-05:00,dandrino,,"To avoid directly answering what seems to be a homework question, to your understanding what does it mean for a function to be a member of Ω (n^3 )?",464dbg,t3_464dbg,AlgorithmsAreHard,,Comment,5,0,5
463vym,2016-02-16 13:43:01-05:00,jkhdfhsdoif,Can anyone help with this algorithm question??,"Suppose you want to drive from Point A to Point B, and you promised your mother that to guarantee your safety you would stop for coffee often – no more than 100 miles between coffee stops. As you crave the warmth of Point B, you want to stop for coffee as few times as possible. You are given a route, and an unsorted list of every coffee joint on that route. For each coffee joint you know how many miles it is from Point A on your route.

Come up with the following greedy algorithm to choose your stops: start with a coffee in Point A; whenever you get coffee you choose the furthest coffee joint that is within 100 miles on your route, that is, the last coffee joint you reach before you hit the 100 mile limit. Drive to that joint, and get some coffee. Keep doing this until you buy coffee within 100 miles of Point B, at which point you can safely drive into Point B.

(a) Prove that this algorithm produces an optimal schedule, that is, there is no schedule satisfying your 100-mile promise that has fewer coffee stops on your route.

(b) Sketch out an implementation of this algorithm, and give me its time complexity.
",,,,,Submission,0,0,0
d026o1q,2016-02-16 13:47:33-05:00,ponchedeburro,,And what are your thoughts so far?,463vym,t3_463vym,jkhdfhsdoif,,Comment,3,0,3
d026s39,2016-02-16 13:50:03-05:00,jkhdfhsdoif,,Well If I set M to the distance between the two points we know there are m/100 or .01m total stops to be made throughout the trip. I guess my main confusion is with the question itself. How am I supposed to prove that this algorithm  is producing the most optimal schedule. I just don't really know how to explain that,463vym,t1_d026o1q,ponchedeburro,,Reply,1,0,1
d02a5uc,2016-02-16 15:05:38-05:00,dandrino,,If you are looking to prove a greedy algorithm you should look up the [exchange argument](http://www.cs.cornell.edu/courses/cs482/2004su/handouts/greedy_exchange.pdf).,463vym,t1_d026s39,jkhdfhsdoif,,Reply,1,0,1
462j2z,2016-02-16 08:54:38-05:00,rubrix4,sI found my photos here! Help me! s,,,,,,Submission,0,0,0
462brw,2016-02-16 08:00:24-05:00,theoryofcomp,Help! Propagating class scenario!,"What is a scenario in which a class doesn't change but propagates its change to its neighbors? 

(this class can be made up or existent)

Also, can you think of any examples in which a hidden interaction between two classes that was not discovered led to financial loss or catastrophy in real world ?",,,,,Submission,0,0,0
d01w5x5,2016-02-16 09:25:43-05:00,knightry,,r/domyhomework,462brw,t3_462brw,theoryofcomp,,Comment,2,0,2
4615mn,2016-02-16 01:07:30-05:00,asdfqwertylol,[Question] Optimal Hitting Sets in Graph Cycles,"I am trying to find an optimal (minimal) hitting set in a graph, i.e a set that contains at least one edge from every cycle in the graph. The goal is to minimize the total value of the edges contained in the set.  
  
Here is the description of the problem: [Hitting Sets](http://imgur.com/aE4NyeO)  
  
So far, I have considered using a modified version of Depth First Search that backtracks whenever it reaches reaches a node that has already been visited to get all the cycles in the tree, but it does not seem very efficient. I have adapted the approach described in this post: [Cycles](http://stackoverflow.com/questions/546655/finding-all-cycles-in-graph/549312#549312)  
  
I have also noticed that Minimum Spanning Trees often seem to pick the minimum values in cycles, but when the graph has long cycles like a-b-c-d-e-f-g-a, the spanning tree will include all these edges, where  only need one. Is there a way I could adapt a MST algorithm such as Boruvka's algorithm to find the MHS?  
  
What other approach could I consider using to solve this problem? Thanks for your time.",,,,,Submission,3,0,3
d01t2og,2016-02-16 07:16:14-05:00,ammicha,,Show that the solution is (exactly) the edges not in a maximum spanning tree.,4615mn,t3_4615mn,asdfqwertylol,,Comment,2,0,2
d01xxgx,2016-02-16 10:19:07-05:00,asdfqwertylol,,"Thanks! I recall reading that if e is the most expensive edge in a cycle of the graph, it will not be included in a minimum spanning tree. It makes sense that the opposite applies to minimum edges in a maximum spanning tree.",4615mn,t1_d01t2og,ammicha,,Reply,1,0,1
4609p9,2016-02-15 21:24:59-05:00,reallytommy,Difference between persistent memory and non-volatile memory?,"I've been reading and trying to differentiate the two, and still having a hard time finding the difference between the two other than that they retain state after loss of power.",,,,,Submission,7,0,7
d01h8ah,2016-02-15 22:18:27-05:00,reallytommy,,"   replying to my own post, [here](http://pmem.io/glossary/) is a quick and good link to explain the difference between the two. 

TL:DR - persistent memory means it is non-volatile AND generally fast enough that it even makes sense to stall a CPU load instruction when loading from it. This is different from something like an SSD, where data retrieval is slow enough that it makes sense for the CPU to work on something else before the data has been retrieved. ",4609p9,t3_4609p9,reallytommy,,Comment,4,0,4
45y4qn,2016-02-15 14:17:16-05:00,im_the_princess,Solving the recurrence T(√n) + T(n - √n) + cn to get a Θ(·) on the worst case running time.,"I am trying to solve this recurrence (T(√n) + T(n - √n) + cn) to get a theta bound, but am having lots of trouble since I have only learned the master method and that does not apply to this case. Any help would be much appreciated! Thank you!! ",,,,,Submission,5,0,5
d020st6,2016-02-16 11:32:36-05:00,flebron,,"I can give you an O, and an Omega.

Just consider the recursion tree. You start with one node, which covers all of n, with a cost of cn.

You split that node into a node for sqrt(n), and a node for n - sqrt(n). They both add up to n. Their cost is c * sqrt(n) and c * (n - sqrt(n)), respectively, so the aggregate cost of that level is again cn.

You then split the first one into sqrt(sqrt(n)) and sqrt(n) - sqrt(sqrt(n)), which adds up to sqrt(n). You split the second one into sqrt(n - sqrt(n)), and n - sqrt(n) - sqrt(n - sqrt(n)), which adds up to n - sqrt(n). Again you have a cost of c * sqrt(n) for the first two nodes, and a cost of c * (n - sqrt(n)) for the second two. This means the third level has an aggregate cost of cn.

This will keep happening as you expand the tree out. Each level has a cost of cn. This is valid as long as none of the nodes in the level expanded out to T(1), meaning you'd stop the recursion. So this is the cost of any of the first levels in the tree (when none of the branches have run out), which are the most expensive. By bounding the number of levels by above and below, we get an O(...) and an Omega(...).

How many levels are there at most in this tree? Well, when you expand the tree out, you see the right side decreases much more slowly than the left side. That's because \n -> sqrt(n) decreases faster than \n -> n - sqrt(n). So the longest branch will be the rightmost branch. This is the branch you get by iterating n -> n - sqrt(n) -> n - sqrt(n - sqrt(n)) -> ... . So call f(n) = n - floor(sqrt(n)). Note that f(f((n + 1)^2 )) = n^2. So every two applications, you reduce the square. Thus the number of applications of f you need to do to n to reach 1 is ceil(2 sqrt n) - 2. So an upper bound on the number of levels in the tree is sqrt(n), leading to an upper bound of O(n * sqrt(n)).

How many levels are there at least in this tree? Well, the shortest path is the one taking all left branches. That is, taking sqrt repeatedly. How many times can you take sqrt of n before reaching 1? That's log log n. To see this, write n as 2^k for some k.

Thus one concludes that T is in Omega(n log log n), and in O(n sqrt n).
",45y4qn,t3_45y4qn,im_the_princess,,Comment,3,0,3
d0254um,2016-02-16 13:12:58-05:00,flebron,,"In fact, with help from some friends (shoutout to ##math at FreeNode!), you can obtain Omega(n sqrt n) easily as well.

Clearly T dominates T'(n) = T'(n - sqrt n) + cn. Expand out T', and you get c * ((n) + (n - sqrt n) + (n - sqrt n - sqrt(n - sqrt n)) + (n - sqrt n - sqrt(sqrt n) - sqrt(n - sqrt n - sqrt(n - sqrt n))) + ...). You can just bound by above each sqrt by sqrt n. This gives you c * \sum\_{i = 0}^{sqrt ^n} (n - i sqrt n) as a _lower_ bound for T', since you're bounding the sqrt's by above. The limit of the sum is actually ceil(2 sqrt n) - 2, but we're just bounding by below, so sqrt n is close enough.

This sum tells us that T'(n) is in Omega(n sqrt n). Since T dominates T', this means T is in Omega(n sqrt n).

Since T is in both O(n sqrt n) and Omega(n sqrt n), it is in Theta(n sqrt n).",45y4qn,t1_d020st6,flebron,,Reply,3,0,3
d028o1n,2016-02-16 14:32:29-05:00,im_the_princess,,Thank you! I will try this and let you know if that is what my professor was looking for,45y4qn,t1_d0254um,flebron,,Reply,1,0,1
d024c9o,2016-02-16 12:54:56-05:00,im_the_princess,,Wow thank you so much for explaining this out for me!! This helped a ton to help me understand. Seriously I appreciate it!!!!!! ,45y4qn,t1_d020st6,flebron,,Reply,1,0,1
d01cqfd,2016-02-15 20:20:46-05:00,blexim,,"I guess the arguments should be integers, so is this:

`[: T(n) = T(\left \lfloor \sqrt{n} \right \rfloor) + T(n - \left \lfloor \sqrt{n} \right \rfloor) + cn :]`

with

`[: T(1) = cn :]`

or something else?",45y4qn,t3_45y4qn,im_the_princess,,Comment,2,0,2
d01d0e4,2016-02-15 20:28:15-05:00,im_the_princess,,Yeah I think you are correct...,45y4qn,t1_d01cqfd,blexim,,Reply,1,0,1
d01ja8v,2016-02-15 23:14:09-05:00,flebron,,"Wait, wtf is T(1) = cn? What is n?",45y4qn,t1_d01cqfd,blexim,,Reply,1,0,1
d01nruk,2016-02-16 01:53:05-05:00,_--__,,"Guess a solution (by trying a few sample points) and then prove it by induction.

~~Incidentally, the solution is T(n) = Θ(n).~~",45y4qn,t3_45y4qn,im_the_princess,,Comment,2,0,2
d01ryhu,2016-02-16 06:06:42-05:00,ammicha,,"It can't be linear since T(n) -  T(n - √n) >= cn.

Seems like T(n) = Θ(n^1.5 ) maybe.",45y4qn,t1_d01nruk,_--__,,Reply,3,0,3
d01uanc,2016-02-16 08:15:41-05:00,_--__,,"Yes, you're right.  I was far too liberal with my constant manipulations.",45y4qn,t1_d01ryhu,ammicha,,Reply,1,0,1
d01xmee,2016-02-16 10:10:20-05:00,im_the_princess,,"But in algorithms we ignore constants in the front so 1.5n would just be n 

:O",45y4qn,t1_d01ryhu,ammicha,,Reply,1,0,1
d02e70q,2016-02-16 16:34:49-05:00,ammicha,,"This is for multiplicative constants. n^1.5 is nsqrtn, so you don't cancel that. I see /u/flebron wrote a proof :).",45y4qn,t1_d01xmee,im_the_princess,,Reply,1,0,1
45vzf7,2016-02-15 06:03:08-05:00,BenRayfield,"Since software is a math process, and CPUs can go years without a single bit computed wrong (errors are almost always caused by Humans), please prove or disprove that if we build from the ground up with parts that need no maintenance, everything built from them will also be maintenance free.",,,,,,Submission,0,0,0
d00k2ya,2016-02-15 06:55:16-05:00,GoldenDragonXIV,,"I'm not sure exactly what you mean by ""parts that need no maintenance"" as such a thing doesnt exist. It seems like what you are asking is ""if we could build a perfect computer from perfect components and write all our code perfectly would it do everything perfectly?"" in which case the answer is yes, but the question is rather redundant. 

We dont live in a theoretical world and so when it gets down to the fine details software is not a maths process but an engineering one - programs are executed on hardware, which isn't ideal. As such, things can and do go wrong - data doesnt get sent correctly, or gets corrupted, or an internal component breaks. This is why we have things like checksums to deal with errors that are made when transferring data, and why maintenance of hardware is completely unavoidable.",45vzf7,t3_45vzf7,BenRayfield,,Comment,11,0,11
d00l6kv,2016-02-15 08:00:13-05:00,_hold_my_beer,,"I'd like to add coping with change. Requirements change, external systems with which we integrate change, load changes, security threats change, etc... Even in a theoretically perfect system there will be change, unless we're also in a theoretically constant universe.",45vzf7,t1_d00k2ya,GoldenDragonXIV,,Reply,2,0,2
d00lywy,2016-02-15 08:37:29-05:00,rfinger1337,,"No, you didn't read the spec right.  I clearly said I want to build from existing parts, parts that need no maintenance...

I have the OP right here, and it clearly says from the ground up..

Look, let's not get mired down in the blame game, you fucked up and I'm not mad. Just fix it.",45vzf7,t1_d00l6kv,_hold_my_beer,,Reply,1,0,1
d00zf6r,2016-02-15 14:43:28-05:00,BenRayfield,,"> security threats change

The empty program, which does nothing, is perfectly secure. There is no way to hack into it because it doesnt react to anything. If at each step we dont add the ability to react in dangerous ways, no future version will have dangerous abilities.

This of course excludes already unsecure computers in which the program runs (which is why NSA built their own version of Linux).",45vzf7,t1_d00l6kv,_hold_my_beer,,Reply,-1,0,-1
d00vp1p,2016-02-15 13:14:49-05:00,raptor9999,,Sounds like a homework assignment to me... :),45vzf7,t3_45vzf7,BenRayfield,,Comment,4,0,4
d00ze0j,2016-02-15 14:42:41-05:00,BenRayfield,,"If academic style problems are worth learning and debating in college, why would that change in the real world?",45vzf7,t1_d00vp1p,raptor9999,,Reply,1,0,1
d01081p,2016-02-15 15:02:52-05:00,raptor9999,,"I guess you can't tell but I'm implying that, with the wording of the post/question, it seems that someone has posted an assignment in hopes of having it solved for them.",45vzf7,t1_d00ze0j,BenRayfield,,Reply,4,0,4
d00lqig,2016-02-15 08:27:08-05:00,magicfab,,"I suppose by ""parts"" you also refer to the hardware. This in turn depends on electricity and many other external factors which essentially affect such a system continously. SInce you can't control all such factors perfectly, you can't have a maintenance free system. You can reach close to 99% etc. in some contexts  and build redundancy, etc., but then this means ever increasing costs (a good example are datacenters).",45vzf7,t3_45vzf7,BenRayfield,,Comment,3,0,3
d00m0vx,2016-02-15 08:39:55-05:00,rfinger1337,,"Is this premise true?  That a cpu will never ever have a random glitch due to wear, heat, or outside factors (sharknado!)  that causes voltage to go the wrong way?   

Or is that the question, can a perfect part that never fails be invented?  ",45vzf7,t3_45vzf7,BenRayfield,,Comment,3,0,3
d00mfr9,2016-02-15 08:57:20-05:00,Delwin,,Actually it isn't true at all.  One bit errors happen all the time.  That's what ECC Ram is for.,45vzf7,t1_d00m0vx,rfinger1337,,Reply,11,0,11
d00wxfx,2016-02-15 13:44:18-05:00,lneutral,,And don't even get me started on computers operating in space. ,45vzf7,t1_d00mfr9,Delwin,,Reply,2,0,2
d011ybd,2016-02-15 15:44:26-05:00,Delwin,,... that's what voting circuits are for... ,45vzf7,t1_d00wxfx,lneutral,,Reply,1,0,1
d00oyya,2016-02-15 10:22:49-05:00,rfinger1337,,"That was what I thought, thanks.",45vzf7,t1_d00mfr9,Delwin,,Reply,1,0,1
d00z8g0,2016-02-15 14:38:59-05:00,BenRayfield,,"For extra speed, CPUs allow the electricity to flow into errors as long as those errors are detected every time before its too late to backtrack and compute again, before the software layer gets control back.",45vzf7,t1_d00m0vx,rfinger1337,,Reply,2,0,2
d00usgu,2016-02-15 12:53:05-05:00,xiongchiamiov,,"Software is written by humans to solve the problems of other humans. Humans are not perfect: we make mistakes, we change our minds, and generally introduce lots of instability into the world.

This isn't math. This is *applied* math, ie engineering, and the real world is messy.",45vzf7,t3_45vzf7,BenRayfield,,Comment,2,0,2
d00zcnk,2016-02-15 14:41:45-05:00,BenRayfield,,"Not all software is written for Humans. I sometimes write it for AIs, which are written for other AIs.",45vzf7,t1_d00usgu,xiongchiamiov,,Reply,-1,0,-1
45vphi,2016-02-15 04:27:02-05:00,of_hippo,Kolomogorov Complexity extended to enumerable semi-measures,"--Edit: Kolmogorov spelling

As I understand it, in Legg's 2008 http://www.vetta.org/documents/Machine_Super_Intelligence.pdf, Definition 2.5.2 extends Kolmogorov Complexity to enumerable semi-measures based on the complexity of computing µ's index in Me. It seemed to me that Me used an arbitrary indexing of enumerable semi-measures. How can a definition of complexity based on something arbitrary, not be arbitrary itself? Imagine a different enumeration Me' defined as Me except that a measure µ at a simple index has swapped index with a measure µ' at a complex index. Don't K(µ) and K(µ') change radically based on whether K is defined with respect to Me or Me'? If I'm correct and K(µ) is arbitrary, what does that mean for the definition of universal intelligence (Definition 4.1.3) when Legg's justification for this definition is seemingly based on an appeal to its relation with the real-world notion of Occam's razor?

Is my question clear? Am I missing something?

Thanks for your time and any help is appreciated.",,,,,Submission,1,0,1
45qrwo,2016-02-14 09:07:46-05:00,DarkPotatoKing7,Can master method be used for T(n) = 3T(n/3) + n lg (lg n)?,"n^(log3 3) = n and f(n) = n lg (lg n)

so it doesn't look like I can use case 1 or 3, but can I use case 2?

 Is there a way to convert lg (lg n) to lg n so that case 2 applies and the answer is theta(n lg^2 n)?",,,,,Submission,5,0,5
czzjzu2,2016-02-14 09:30:21-05:00,thewataru,,lg (lg n) is less than lg n. Therefore you can say that T(n) < 3T(n/3) + n lg n,45qrwo,t3_45qrwo,DarkPotatoKing7,,Comment,1,0,1
czzk53o,2016-02-14 09:36:27-05:00,DarkPotatoKing7,,"but master method gives tight bound right? doing that would only give an upper bound but not a lower bound, so will master method still apply?",45qrwo,t1_czzjzu2,thewataru,,Reply,1,0,1
d004ueb,2016-02-14 19:55:19-05:00,Chappit,,I believe the master method is not going to apply here. I think maybe a change of variables could be useful?,45qrwo,t3_45qrwo,DarkPotatoKing7,,Comment,1,0,1
45oh84,2016-02-13 23:04:58-05:00,ad-undas,DES Substitution with two S boxes,"So... Beginning with EF9 base 16... I did the substitution and arrived at a final answer of 00 base 16... Does this make sense? How would I write my final answer? 

Here are quick links to my work and to the tables. Thank you. 

http://postimg.org/image/4jlhdbfid/

http://postimg.org/image/po6qzgljj/
",,,,,Submission,2,0,2
czz7rrc,2016-02-13 23:05:56-05:00,ad-undas,,I apologize for the links/photos at the bottom of those pages. Didn't realize they'd be there. ,45oh84,t3_45oh84,ad-undas,,Comment,1,0,1
45nkgi,2016-02-13 19:15:00-05:00,BenRayfield,What are the practical effects of the Halting Problem - the logical impossibility that any software can predict the longterm behaviors of software?,"Since a physics simulation (whichever of all possible simulations are correct) of a person thinking about software is still software, and our brainwaves act as physics describes, the Halting Problem applies to people the same as software.

Halting Problem is the logical impossibility that any software, when asked about the code of a software, can predict correctly every time (or even most of the time) what that code will do if run, in the simpest case if it will ever finish (halt) or go on forever (not halt).

Debuggers are great for pausing and continuing software, but thats observed at the time instead of predicted in advance.

The main problem that keeps happening everywhere is a software is thought to do one thing but later we find out it does something else but its too late to stop it. You try to shut it down but its hooked into other processes that cant be paused without losing important data and corrupting the system. The Halting Problem bites into your systems and wont let go until its done, or take your losses.

Its often suggested that programs should do a little at a time then check if anything has requested they pause or choose to do something else instead, do a little more, check again, and so on. But if a software calls another software which doesnt do such checks, the Halting Problem has taken over, and theres no going back if it doesnt halt.",,,,,Submission,0,0,0
czz2foo,2016-02-13 20:31:11-05:00,lneutral,,"It sounds like you're asking a question that you've already got very dramatic answers for. What do you hope to get out of this thread?

The only thing I can add to your answer is that it can actually be helpful to reduce other problems to halting problems to prove that they, too, are undecidable.",45nkgi,t3_45nkgi,BenRayfield,,Comment,4,0,4
czz5vwn,2016-02-13 22:11:41-05:00,Kaidelong,,"Not much really. While you cannot generalize a halting test to everything, nor create a halting test on a black box basis, you can still have specific halting tests that inspect the code or the runtime state (like when you get the `<<loop>>` exception in Haskell). 

The main important things are if you are implementing a language embedded in something else: turing completeness means that someone else can come along and throw the whole thing into an infinite loop. Things like browsers that implement turing-complete languages generally address this by having some kind of timeout anyway.

This also means that you generally don't want your type system to be turing complete if you have one, because then you can put the compiler in an infinite loop.

You're not going to really run into people who go ""damn, that halting problem makes my job soooo hard"" very often, though.

",45nkgi,t3_45nkgi,BenRayfield,,Comment,3,0,3
czz00m6,2016-02-13 19:18:12-05:00,ToplessTopmodel,,You never know if a women is actually pissed of or not. ,45nkgi,t3_45nkgi,BenRayfield,,Comment,-5,0,-5
czz0597,2016-02-13 19:22:02-05:00,BenRayfield,,"Everyone does https://en.wikipedia.org/wiki/Challenge%E2%80%93response_authentication with eachother, hiding some of their thoughts and giving clues for others to follow, to learn if they know how you really think, and if they dont they're not who you should be talking to. Sarcasm is a good example. So are the challenge/response of computing.",45nkgi,t1_czz00m6,ToplessTopmodel,,Reply,-1,0,-1
45iyop,2016-02-13 00:42:01-05:00,colinroberts,ELI5: conditional random fields and how they are used in names entity recognizers,,,,,,Submission,2,0,2
45h4yn,2016-02-12 17:03:07-05:00,thedagger,Studying for an exam,"Hey all, I'm a CS grad student taking a Fundamentals course, and I have a big exam coming up next week. Although I'm spending a lot of time studying, I'm finding a few topics to be giving me some difficulty. I was wondering if anyone could point me in the direction of some good, easy to understand resources on the following:

1. Arithmetic / geometric sequence summation
2. Proofs by induction
3. Big O notation
4. Analysis of iterative algorithms
5. Analysis of recursive algorithms/Recurrence relations

Any and all advice/links/explanations would be greatly appreciated...I don't have the strongest background in mathematics so I have a little catching up to do in order to solidify these concepts. Thanks!",,,,,Submission,10,0,10
czxv8mh,2016-02-12 17:44:42-05:00,tonylearns,,"While I'm sure someone else will provide you with the resources you're asking for, I'm going to take a different track:

Talk to you're professor. I bet they have office hours before your test. Show up and they'll be best poised to help you understand the course material.",45h4yn,t3_45h4yn,thedagger,,Comment,3,0,3
czxw4gy,2016-02-12 18:09:17-05:00,sjalfurstaralfur,,"> 1. Arithmetic / geometric sequence summation
> 2. Proofs by induction
> 3. Big O notation

If you are a grad student you should know these things already...

Good resource? For 3-5, I recommend [Algorithm Design Manual](http://www.amazon.com/Algorithm-Design-Manual-Steven-Skiena/dp/1848000693/ref=sr_1_1?ie=UTF8&qid=1455318876&sr=8-1&keywords=skiena). Read the intro, then Chapter 2 is devoted to understanding Big O. 

For 2, its best to have a friend explain it to you. Induction is kinda tricky to learn, but its best learned with analogies (like toppling dominoes).

For 1, you can refer to [Math is Fun](http://www.mathsisfun.com/algebra/sequences-sums-geometric.html)",45h4yn,t3_45h4yn,thedagger,,Comment,2,0,2
d013l0w,2016-02-15 16:23:36-05:00,thedagger,,Thank you. Both of these resources presented the topics in a way that made it much easier for me to solidify them in my mind.,45h4yn,t1_czxw4gy,sjalfurstaralfur,,Reply,1,0,1
czxw5kj,2016-02-12 18:10:11-05:00,assface,,Shouldn't you have covered this in the course?,45h4yn,t3_45h4yn,thedagger,,Comment,0,0,0
czy41yl,2016-02-12 22:13:38-05:00,not_an_evil_overlord,,Or in undergraduate...,45h4yn,t1_czxw5kj,assface,,Reply,2,0,2
45cv1b,2016-02-11 22:44:37-05:00,TholomewP,What is your favorite book on the history of Computer Science?,"The history of CS (pre-modern computing) as well as the development of it mathematically, to be more specific",,,,,Submission,14,0,14
czx1swq,2016-02-12 01:28:29-05:00,NewAnimal,,"its funny, you were asking about development of it mathematically. I was actually going to request the opposite.  

I picked up this book:
http://www.amazon.com/Code-Language-Computer-Hardware-Software/dp/0735611319

and its great, but it seems to be focused on the logic that evolved with computational thinking.  which might be exactly what you are looking for.

however, if anyone has any suggestions for more broad historical books on CS (business side stuff. evolution of the industry), id love to hear em!",45cv1b,t3_45cv1b,TholomewP,,Comment,6,0,6
czx8mpd,2016-02-12 08:06:13-05:00,sjalfurstaralfur,,"Good ol' [The C Programming Language](http://www.amazon.com/The-Programming-Language-Brian-Kernighan/dp/0131103628/ref=pd_bxgy_14_img_2?ie=UTF8&refRID=1HBMQTQ1F18KFC2A2N97) for me 

Read it junior year of college, gave me a huge appreciation for the elegance of C and Unix. Made me venture from Java/IDE/Windows world into the Unix/Linux/Terminals world.
",45cv1b,t3_45cv1b,TholomewP,,Comment,2,0,2
czxnfeo,2016-02-12 14:29:38-05:00,mosqutip,,I really enjoyed [The Mythical Man Month](http://www.amazon.com/The-Mythical-Man-Month-Engineering-Anniversary/dp/0201835959). I see examples of bad practices mentioned in the book on at least a weekly basis.,45cv1b,t3_45cv1b,TholomewP,,Comment,1,0,1
459uql,2016-02-11 12:16:10-05:00,Rubick555,"What entity manages .com, .net, .gov, .us, .cn domains?","For the longest time I still have not a clue how this works. I am not sure if this is the right subreddit or something like r/networking

This is all I understand so far about the web (or internet?), computers, and electronics in general (its super long just skip to bold part if you need to)

INTERNET:

* Computers linked to network with standard protocols, such as HTTP/ HTTPS over the world wide consortium standards, forming conglomerate of computers linked together. ISP's take information from said networks and transfer data to other ISP's via nodes, through hardwire connections and transatlantic cable.

* IP addresses identify your local network through the ISP. Subnet mask, gateways, etc and traffic can be redirected via VPN. Also, on local side, you have the PC=>Router (optional) => Modem => ISP, where the router helps divide the modem so multiple PC's can connect to one line. Also WiFi operates by a specific hardware device (wifi dongle, router) and transfers information via electromagnetic waves. Same with cellphones (Via cellphone towers). Also satellites help direct cellular traffic through direct line of sight connections from user=>satellite=>user.

* URL's utilize world wide consortium stsandards, using www. as the default subdomain, the web address, and the end? domain (like .com)

* Domains are rented out through 3rd parties such as godaddy, and are never physically owned. Subdomains are like m.reddit.com (for mobile use) or MyWebsite.blogspot.com

* Search engines like google uses properitary algorithms /datastacks/ web crawlers to build a huge database of all existing URL's and rates them through its servers (SEO)

* Servers are just high powered PC's generally running Linux and processes incoming data / sends out packets of data, generally in fancy server rooms

* DDoS is denial of service through botnet attacks via malware infections of computers, utilizing other people's computer resources (there's more than one way of doing this). Also, ports get overridden with lots of useless requests, denying service to legit people who want to use said site hosted on said server

* Throttling occurs due to ISP's limiting data pipeline through its end, not sure what goes on here though

* Deep web (or dark web?) is essentially hosting a bunch of private URLs, not visible publicly and accessed through specific private search engines. 

* Torrents are accessed via peer 2 peer (Seeding and leeching) and managed client-side with utorrent, rutorrent, etc which manages traffic for the requests for .torrent downloads, and public torrent links are through sites like pirate bay, kickass torrents, etc

* Programming languages (Fortran, C++, Java) were generally formed originally from assembly code and binary in the form of different modules. People had different opinions of how data and protocols were made, so hence different languages

* Virtual machines, its a high powered PC generally that divides its resources to host several virtual PC's. Lots of variations of how datastacks are managed

* Personal computers work via motherboard, ram sticks, GPU (like nvidia GTX Titan or intel graphics 4000), Cores / hyperthreading (like corei7 from intel), monitors, etc. Say you run a PC without internet and run a game like witcher 3. You run an executable file, which is written by C++. GPU renders data supplied by executable, (e.g. render this room given this data under this game engine), ram (random access memory) is used as all the unprocessed data needs to sit somewhere?, various caches (L3) for processing common data quickly, core (i7) for processing data as a whole, motherboard and OS mediates all data being transferred.

* Github is used for subversion control, and its now cloud based too (although it was originally local / lan only). Github is done via pulls , requests, call, etc. to monitor versions. You can technically emulate this on paper using sticky notes, but its not efficient

* Hardware electronic wise, voltage and current is analgous to a water pipeline, where VOLUME WATER FLOW = CIRCUMFERENCE * VELOCITY WATER. In this case, POWER = VOLTAGE * CURRENT. Kerchoff's law, parallel series circuits, PCIBs, quantum theory, transistors (3 prongs), OR NAND NOT gates, arrays, resistors, potentiometers, timers, arduino, bread boards, soldering, relays, switches, that kind of thing (I'm not an EE). Anyways, hardware generally requires a specific amount of voltage and current for things to run on the computer

* Operating Systems are linux, windows, macs, etc, and runs through a succession of modules created by assembly code and programming based languages such as C++.

* HTML is not a programming language, its a hypertext markup language. Its used for client side processing after all the shenanigans uptop mentioned, so your browser can know what to spit out for you to see. Javascript, same thing, but generally used for actionable things (popups, ads, clicking buttons, etc.). CSS is just for mostly static skins( e.g. a specific color) and works in conjunction with HTML. Javascript, CSS, and HTML work client side to produce web page you look at in your browser. PHP (or Perl) runs server side application through MySQL (any DBMS database management system) to generate data for HTML to work with. 

* DBMS utilizes crud (create read update delete). MySQL works. Can be emulated on a piece of paper, technically

* XML (eXtensible markup language) just is used to communicate between different languages

* API (application programming interface) is basically a user manual of all the functions publicly available to pull from a program, locally or on a server. E.G. imgur provides a bunch of API's, I can use its resources because I know said API, and make a program out of it. UPS has a bunch of available API, so I can pull that shit and make a ecommerce shipping module or something.  

* RAID (RAID 1-5) is just like how data is split into different hard drives. E.G. I have a file. File is split 3 ways into 3 different raid drives and pulled all at once when I request for it, its done this way because file transfers are faster when split into multiple data pipelines.)

* Ads make the web profitable. Lots of big name companies make lots of $ selling services and goods at high margins, goes back and dumps money in marketing. Facebook CPC, youtube Ads, youtube videos with ads built into the video (techquickie), social media (instagram, kik, w/e), free game riddled with sidebar ads, banner ads, adblock letting forbe's ads through. Or 4% amazon commission based on referrals. Or Hulu plus style ads, pay to get rid of ads / pay for full service. 

* web services and goods. microsoft office, reddit gold, renting a server, anything on amazon, that kind of thing. Money.

* bitcoin is cryptography based, and can be mined (but not profitable due to energy and hardware limitations for the average person). Bitcoin is generated through computers working out some sort of very encrpyted password node. Bitcoins stored on a server side application (Accessed via website), or through just a web URL, or a client side application (on your phone, desktop program). Bitcoin bought through local transactions, link credentials and bank account, also not easily traceable? (debatable here) 

* Blackhat Hacking (what people think of generally, not DIY hardware hacks, writing a program, or modifying an existing code) is done on unsecured websites. Usually said website is running some backend application like wordpress, shopify, drupal, modules commerce. Changes are made in the core code over time, and those changes sometimes lead to vulnerabilities that people aren't aware. Hackers send requests (MySQL injection is one of these?) to find more about how the server side application works, and looks for some way to override and obtain admin credentials by having server side reveal this information. White hat is the opposite. Red hat is for linux

* Virus, worms, trojans. Usually runs on an attached .exe file or application, but has been known to somehow work with images too. Also worked with emails (worms) in the past. This is mostly magic to me too, I just know how to prevent it. PUP is just potentially unwanted programs (aka bloatware) that comes preinstalled from HP / ASUS/ ACER, etc, or comes along for the ride when you download a program off Cnet.

* Data selling. Especially mobile apps (I mean you have to give them permission on everything), they take data, sell to data brokers, which businesses buy to make marketing decisions. Same with lots of things that don't seem to generate money, e.g. chrome plugins, you are the money generator for them via data. Insert google overlords and big 5 internet companies giving data to government via PRISM/snowden (didn't something like this go into effect in 2015?) . Also insert malicious data theft and brokers (social engineering, telemarketers trying to get your credit card, entering information on non HTTPS secured sites).

* front end development is html, css, javascript. Backend is datastacks, php, mysql dbms. Full stack is front+back.

* Monitors, eh whatever. IPS, TN, PWM flickering, Polarization, viewing angles, LCD , Cathode Ray Tubes.

* VoIP is just a specific protocol (like https) for how phone calls transferred over web, and maintain quality of said audio

* Image compression is done via bit map rasterization. DNG files or propietary raw formats are utilized in native applications like adobe photoshop.   .GIFS are simply image slideshows, actually all videos are image slideshows. Vectorization is mostly talked about when using illustrator or cad based programs (Solidworks, CATIA, AutoCAD)

* Programs can be bypassed (e.g. Sony Vegas Pro) by core modification of its files (you'd need to breach password credentials though?) and be made into a ""crack file"" . A keygen treats the program normally, but just bypasses security checks externally (e.g. preventing company server from doing checks by blocking certain URLs, and a keygen from Xforce that is made by finding security criteria for successful serial# installs) 

* Rooting your phone is like getting full super user access on linux, but also comes at a price leaving you vulnerable to security breaches

* Domain name service managers like namecheap just helps you manage how you rent your domain.

* FTP is file transfer protocol to transfer files from one computer to another. SFTP is just secured FTP, whatever that means. SSH is shell access, e.g. almost like your sitting on that computer over on the other side of the world as opposed to just accessing files only. 

* Email servers are done by IMAP, POP3, email is transferred purely text based (attached files go through a different channel) to be sent from one email to another via email. POP3 is stored more localish side, IMAP is more server based (its slower, but easily accessible anywhere) 

* Windows API, windows registry, Linux commands are still magic to me. Sudo super user something. 

-------------------------------------

**Okay, but who manages the .info, .com, .net, .cn, .rs, and .gov top level domains?**

There's obviously some domains that are specific to countries, and are most likely managed by that countries' government entity. E.G (.us for usa? .ws for russia, .cn for china) but aren't nearly as popular as the .net and .com domains.

.Org and .gov are US? government regulated top level domains to my knowledge, where .org is mostly nonprofit. U.S.A uses .gov domains for its government organizations

So I understand that some countries government manages that domain. But what about public top level domains, like .com, .info, .net, .ca? 

Who manages the database for those? Who gives authority to godaddy for those domains for rent? Who mediates copyright conflicts for those domains? (E.G. say my name is Mike Cro Soft, and I wanted to rent a domain called mikecrosoft, but get DMCA'd / copyrighted by microsoft.com)  

Like, what are the big organizations mediating internet protocols and legislation on a global scale? 

Who or what has access to the biggest picture of the web, and its workings and backend?

-----------------------------------------------------------

sorry for the long wall of text, I've been missing some vital information on how the web? (or is it internet?) works

disclaimer: 
I don't take CS classes and did not major in computer science. So I might be really off in what i understand about the internet as a whole. Most of this is just what I learned from browsing reddit and youtube

Apologies in advance if i butchered a bunch of terms and how things work. I just wrote things as they randomly came to me",,,,,Submission,9,0,9
czw7y3a,2016-02-11 12:38:29-05:00,xiongchiamiov,,"I'll be honest: I didn't read through most of your post, so I can't verify the accuracy of any of that.

----

[ICANN](https://en.wikipedia.org/wiki/ICANN) is generally the top level of control over things like domain names and DNS. They decide what TLDs exist and who operates them. If you look up [.com on Wikipedia](https://en.wikipedia.org/wiki/.com), for instance:

> The domain was originally administered by the United States Department of Defense, but is today operated by Verisign, and remains under ultimate jurisdiction of U.S. law.",459uql,t3_459uql,Rubick555,,Comment,15,0,15
czw8d7p,2016-02-11 12:48:14-05:00,Rubick555,,"okay this is what i was looking for

most of that stuff was just me ranting to myself

But if its under U.S. law, doesn't that cause issues with other countries?

Are there ICANN equivalents in say China?


",459uql,t1_czw7y3a,xiongchiamiov,,Reply,3,0,3
czwaz1f,2016-02-11 13:47:13-05:00,Cadoc7,,"ICANN is an international NGO that operates by international consensus. https://en.wikipedia.org/wiki/ICANN#Structure

The .com domain is subject to US law. Theoretically, it could cause problems for international companies, but the US government is very hands off.",459uql,t1_czw8d7p,Rubick555,,Reply,9,0,9
czwba4x,2016-02-11 13:54:17-05:00,hosty,,"IANA is a department of ICANN, and they manage the allotment of country-code top level domains. Most countries have an ICANN-like agency that manages things at a lower level than that. China's domain (.cn), for example, is managed by the China Internet Network Information Center ",459uql,t1_czw8d7p,Rubick555,,Reply,5,0,5
czwgchm,2016-02-11 15:50:06-05:00,Rubick555,,"ah gotcha

Are they also responsible for the great firewall of china too?",459uql,t1_czwba4x,hosty,,Reply,1,0,1
czwitql,2016-02-11 16:44:53-05:00,cokeisahelluvadrug,,"No, it's operated by the Ministry of Public Security (essentially the police), while the Internet Network Information Center is under the Ministry of Industry and Information Technology.",459uql,t1_czwgchm,Rubick555,,Reply,1,0,1
czxcnxw,2016-02-12 10:18:30-05:00,Rubick555,,"Those sound like harry potter departments o.o

but that makes much more sense now
",459uql,t1_czwitql,cokeisahelluvadrug,,Reply,0,0,0
czwyqa4,2016-02-11 23:36:35-05:00,alien_screw,,"Linux is a Kernel. Ubuntu, Arch, Debian, etc are operating systems. ",459uql,t3_459uql,Rubick555,,Comment,1,0,1
czxcgrx,2016-02-12 10:13:06-05:00,Rubick555,,"thanks for the correction

did you catch any other errors i made by chance :P",459uql,t1_czwyqa4,alien_screw,,Reply,0,0,0
459jx4,2016-02-11 11:19:15-05:00,Wolf2016,"Network printer asking for user & password, but why?","WIN7 network printer (set to share) I can add printer to two other computers on all the user account but when I get to the last computer. I can not add the network printer to the ""Default"" profile with ""standard"" permission. I get a small window asking for user and password. The ""Default"" had a password from previous user however, I removed the password prior to trying to add the printer. Any solutions?",,,,,Submission,0,0,0
czw5rnt,2016-02-11 11:47:42-05:00,tRfalcore,,Computer science <> tech support,459jx4,t3_459jx4,Wolf2016,,Comment,5,0,5
czz2gud,2016-02-13 20:32:06-05:00,lneutral,,/r/TechSupport,459jx4,t3_459jx4,Wolf2016,,Comment,2,0,2
458y84,2016-02-11 09:17:39-05:00,self_raising,Open source legalities,"I'm looking at building a piece of software that will essentially be just a web front-end and a middleware web service. My plan is to use the rest APIs of a couple of open source applications/services. The services are; Jenkins (MIT licence) and Gerrit (Apache 2 licence). 

I've read through the licence information on their websites and I'm still confused. 
* I won't be using the actual source from either application. 
* I will want to charge for my software.
* I don't want to make my code open source.

Will I be able to do this? Or would I be breaking licencing laws?",,,,,Submission,2,0,2
czw80c3,2016-02-11 12:39:55-05:00,kbrosnan,,If you are creating a business around open source software and you don't understand the license requirements then you should contact a lawyer that is familiar about them. They are a legal document and not something armchair experts should be advising you on.,458y84,t3_458y84,self_raising,,Comment,5,0,5
czwfurz,2016-02-11 15:39:04-05:00,self_raising,,I'm not starting a business or doing anything just yet. And I wouldn't do without speaking to a legal professional first. I just wanted to see if anyone has any experience with this and might give me a vague idea on how to proceed.,458y84,t1_czw80c3,kbrosnan,,Reply,1,0,1
czwqteb,2016-02-11 20:08:37-05:00,Cadoc7,,"Both are very permissive licenses. You are able to do what you asked, provided you meet the attribution and license distribution clauses of both.

Disclaimer: I am not a lawyer and this is not legal advice.",458y84,t3_458y84,self_raising,,Comment,2,0,2
czweggt,2016-02-11 15:07:11-05:00,P4duke,,/r/legaladvice would be a better place to ask this,458y84,t3_458y84,self_raising,,Comment,1,0,1
czwfsmb,2016-02-11 15:37:42-05:00,self_raising,,"Nice one, cheers mate.",458y84,t1_czweggt,P4duke,,Reply,1,0,1
4578km,2016-02-11 00:14:45-05:00,darthsabbath,Books on the history of computing,"What I'm looking for may not exist but this sub seemed like a good place to ask. I am interested in finding a book on the history of computer science. I would prefer something more at the pop sci level, but preferably with a bibliography to point me to more technical resources. 

I know about Ada Lovelace, Alonzo Church, Claude Shannon, Alan Turning and others, but I'm more interested in the bigger picture that that played a part in.

I can read their individual contributions to the field but it would be nice to go from the Difference Engine all the way through desktop computers and mobile devices, mentioning the major milestones in the field, such as compilers, operating systems, etc.

Any suggestions?",,,,,Submission,1,0,1
4563re,2016-02-10 19:48:47-05:00,b3ntleg,Question about executing code on old machines,"This is something I thought about since I was a kid and never really figured it out. On many home computers in the 80s the OS consisted of a ROM chip that contained a BASIC interpreter. Obviously a lot of high end games or programs back then couldn't be written in a BASIC interpreter. They would have been required to be written a complier or even assembly language. 

So after you have written a nice compiled program how would you get the BASIC interpreter to recognise the program and leave the confines of BASIC? Did they have built in functionality that recognised compiled code? Or was it more of a case of the compiled code being converted into a number of advanced BASIC memory call commands? ",,,,,Submission,2,0,2
455pti,2016-02-10 18:27:16-05:00,stahl085,The only programming languages that I know are in english. How does programming work in other languages?,This is mostly for high-mid level languages. Can you use the same(similar) compiler? I figure syntax would be extremely different in some languages? Can someone elaborate on this topic for me please? and thank you!,,,,,Submission,36,0,36
czvf4k0,2016-02-10 19:10:43-05:00,PastyPilgrim,,"From what I understand, most people use English for language keywords (if, then, public, private, etc.). And even if you were to use a variant of a language that had keywords in your native tongue, it would just be the keywords that would change, not any major syntax or language structure.

Because so many programming languages have English origins, it's really more convenient just to learn how to read the language's English keywords because all of the documentation, demos, etc. will be in English. Not to mention all of the compilers, syntax highlighters, and so on that are tuned for English.

Now, even if someone is using English keywords, it doesn't mean that their code is in English. You would probably write variable and function names, as well documentation, in your own tongue.",455pti,t3_455pti,stahl085,,Comment,41,0,41
czvxapr,2016-02-11 07:24:26-05:00,Am0s,,I've worked on a project with some Germans before. The code they did before any English speakers were on board was just as you described as a mix of English keywords and German variables and documentation. They quickly changed to doing full English once we were on board. ,455pti,t1_czvf4k0,PastyPilgrim,,Reply,8,0,8
czvjesl,2016-02-10 21:08:35-05:00,panderingPenguin,,"Programming languages are pretty universally in English.  It's one of the more commonly spoken second languages in the world, and a kind of de facto lingua franca for technical information. If you're in a technical field, you probably know at least basic English.

I studied computer science in the US, but spent a semester studying abroad in china. The CS students there use the same programming languages used in the US. They write their code in English.  Normally that includes the variable names too. Sometimes they write comments and documentation in Chinese, sometimes they don't, depending on who they expect to read it.",455pti,t3_455pti,stahl085,,Comment,18,0,18
czvs00d,2016-02-11 01:51:02-05:00,mehum,,I remember seeing some Chinese code in some Unicode-capable language (maybe Python 3) where all of the variables and function names were kanji.  It was hilariously difficult to follow.,455pti,t1_czvjesl,panderingPenguin,,Reply,2,0,2
czvz9b2,2016-02-11 08:50:21-05:00,videoj,,"[Kanji (漢字)](https://en.wikipedia.org/wiki/Kanji) is written Japanese.  

[Pinyin (中文)](https://en.wikipedia.org/wiki/Written_Chinese) is written Chinese",455pti,t1_czvs00d,mehum,,Reply,1,0,1
czw62zq,2016-02-11 11:54:58-05:00,darthpuyang,,"Pinyin is referring to the phonetic system of writing Chinese, the actual written Chinese is called Han Zi",455pti,t1_czvz9b2,videoj,,Reply,5,0,5
czw7hqb,2016-02-11 12:27:54-05:00,anamorphism,,"漢字 (trad), 汉字 (simp), or hànzì in pinyin, heh. but, as you can see, it's literally the same thing in japanese just pronounced slightly differently.",455pti,t1_czw62zq,darthpuyang,,Reply,2,0,2
czvfxpq,2016-02-10 19:33:19-05:00,Laugarhraun,,"Unless you code for excel, the code is exactly the same...",455pti,t3_455pti,stahl085,,Comment,5,0,5
czvnaoz,2016-02-10 22:58:14-05:00,sefsefsefsef,,"The programming languages aren't ""in English"" to begin with.  They are their own language.  It would be like saying ""I'm speaking English in French"" when you use words like ""baguette.""  You aren't programming in English, you're programming in C/C++/Java/whatever.",455pti,t1_czvfxpq,Laugarhraun,,Reply,7,0,7
czvry07,2016-02-11 01:48:22-05:00,mehum,,"Technically correct, but can you point to any command in any programming language that is derived from a language other than English?",455pti,t1_czvnaoz,sefsefsefsef,,Reply,4,0,4
czvvep1,2016-02-11 05:20:51-05:00,Laugarhraun,,Well there's some Hebrew in PHP (lexer part I think) :p such as T_PAAMAYIM_NEKUDOTAYIM.,455pti,t1_czvry07,mehum,,Reply,5,0,5
czvsa40,2016-02-11 02:05:23-05:00,camcam369,,Well there's 'lambda' in Python.  But I understand your point and I agree with you.  Almost every reserved keyword (that I can think of) is in English.,455pti,t1_czvry07,mehum,,Reply,1,0,1
czvp16z,2016-02-10 23:53:20-05:00,MiserableFungi,,"[There are other languages.](https://en.wikipedia.org/wiki/Non-English-based_programming_languages)  But for all intents and purposes, they are not important enough usage wise to supplant English.  ",455pti,t3_455pti,stahl085,,Comment,4,0,4
czvw2uk,2016-02-11 06:08:12-05:00,hokaloskagathos,,"There are some non-English based programming languages, more than you would think from this thread: https://en.wikipedia.org/wiki/Non-English-based_programming_languages

It's true though, that they are not much used.",455pti,t3_455pti,stahl085,,Comment,3,0,3
czviq1e,2016-02-10 20:49:47-05:00,pilibitti,,"There are no different programming languages for different spoken languages. Like, there are no popular programming languages aimed at people speaking Spanish, French, or Japanese. We all pretty much use the same programming languages. Native speakers of non-English languages sometimes use variable names in their own language, and that's about it.",455pti,t3_455pti,stahl085,,Comment,2,0,2
d01ngos,2016-02-16 01:39:09-05:00,RK65535,,"Generally in modern languages the only difference is the comments, variables, etc. Programming generally is a universal language. I was looking into the history of Soviet era programming and computing 2-3 years ago, and I found that much of it was in fact written in Latin and English, with exceptions such as Rapira that could come in either a Russian or English dialect, but was the same regardless of which was used.",455pti,t3_455pti,stahl085,,Comment,2,0,2
czw89ru,2016-02-11 12:46:02-05:00,xiongchiamiov,,"Now take a look at the country of origin for the authors of those languages. Is English their first language? Often not. But all of the language constructs are in English anyways, because that's the language of technology, and those languages would've never gotten as popular if they weren't.",455pti,t3_455pti,stahl085,,Comment,1,0,1
czx4baq,2016-02-12 03:38:39-05:00,TwiSparklePony,,"I see a few other people mentioning languages written in Chinese or something. [I present Piet, programs as art.](http://www.dangermouse.net/esoteric/piet/samples.html)",455pti,t3_455pti,stahl085,,Comment,1,0,1
4559lm,2016-02-10 16:57:29-05:00,PiManASM,How does the computer keep track of pointers?,"So we did a brief overview of pointers in class today, I asked the professor but I'm still looking for more resources.

So I understand pointers are like an address, keeping track of data stored in memory. But how are these pointers and their locations stored? How is it not infinite recursion of addresses pointing to addresses that point to addresses?",,,,,Submission,4,0,4
czvd3px,2016-02-10 18:15:55-05:00,MikeBenza,,"That's a really good question if you haven't done much in C.  As /u/Bottled_Void mentioned, they're just another value in memory.  I'm guessing you're asking about C/C++ style pointers, so I'll answer that.

A pointer is just an address in memory.  In a 32-bit compiled program, the address is 32 bits long.  In a 64-bit compiled program, the address is 64 bits long.  Most desktop applications these days are compiled for 64 bits, but specialized hardware may work better with 8-, 16-, or 32-bit address spaces. 

Consider some of what pointers are used for:

1. So that a whole object doesn't need to be passed around
1. So that multiple functions can access that place (whether concurrently or not is irrelevant)
1. So that a NULL value can be specified

Essentially what pointers do is allow you to give an easy-to-pass and store representation to a more complex idea.  A pointer (in C/C++) is essentially an integer that is special to the context of your program.  (Advice: don't treat it like an int though, that's the path to trouble).

So pointers are easy to pass and store.  There's no place in a C/C++ program where all pointers are kept track of.  Where are they stored?  They're passed around on the [stack](https://en.wikipedia.org/wiki/Call_stack) when you make function calls.  They're stored on the [heap](https://en.wikipedia.org/wiki/Memory_management#HEAP) when you save them in an object (or when you save them to some address in the heap).  There are other parts of memory they might be in that technically isn't one of those, but that's comparatively rare.

And that's it.  If you lose track of a pointer and forget to `free` the memory associated with it, you've got a memory leak.  It's either on your stack, on your heap, or lost.

So your pointer may point to address 0x11223344.  Let's call that pointer `p`.  When you dereference the pointer (get the data it's pointing to), your computer gets the value at 0x11223344.  So if you write `*p` that means `get the value at memory location p`.

Now consider that pointer referred to a struct with two 32 bit integers in it called `x` and `y`, most likely (I'll explain why it's not guaranteed if you want) the value of `x` would be stored at address 0x11223344 and the value of `y` would be at 0x11223348.  To get `p`'s `x`, you'd write `p->x`.  The same idea for `y`.  The compiler knows the layout of the struct in memory and turns `p->x` into `(*p).x`, which means `(go to the memory location p points to).then get the value at the offset of x`

So, a pointer is just a way of saying ""some address in memory"".  Pointers aren't very different than integers in terms of how they're handled, but there are enough differences that you shouldn't ever need to treat a pointer as an integer.  And there are 2^BIT_SPACE possible pointers.  You can have two pointers by different names that have the same value.  Just like you can have `int a = 5: int b = 5`.  Both `a` and `b` are stored in your program's memory.  They're stored along with every other value you use in your program.",4559lm,t3_4559lm,PiManASM,,Comment,8,0,8
czvduof,2016-02-10 18:35:56-05:00,PiManASM,,"I'm not sure if you answered my question, but I may have just missed it. Basically, how does the computer keep track of everything? 

If I have a variable x with some value at a particular memory address, and I ask the computer for the value of x, what exactly happens? The value of x has an address in memory, but how is that address known and stored? How does the computer keep track of which things correspond to which memory addressees?",4559lm,t1_czvd3px,MikeBenza,,Reply,2,0,2
czvh7j9,2016-02-10 20:08:20-05:00,lordvadr,,"It doesn't.  That's not it's job.  The address in memory is the address.  The lookup table--it's much more complicated than this due to virtual address space and such, so neglect that--is the actual ram.  You wan the data at 0xDEADBEEF?  You get the data at 0xDEADBEEF.  You're literally telling the CPU, ""go get what's at 0xDEADBEEF, I don't care what it is.""  If you've written the software right, you know what data is there, but it's not the OS's job to do that for you.

If compters were simply, physical memory addresses would range from 0 to however many bytes of RAM the computer had.   It's, again, much more complicated than that, but this is the idea.  You put data at some location, you then tell the computer to give you the data at that location.

All data (well, mostly) is referenced by address.  When you write software, and reference a variable, the compiler takes care of the addressing for you.

The difference is when you need to know the **location** of data...not the data itself...is when you use a pointer.  This is for things like dynamic memory allocations.  You tell the OS, ""give me some space to store stuff"", and it gives you an address.  A physical memory address (it's not, but for the time being, it might as well be).  You store stuff there, and keep a copy of the address in a variable, so you know how to find it later.  This is because you have no way of predicting what address the OS will give you to store your data. ",4559lm,t1_czvduof,PiManASM,,Reply,3,0,3
czviw8w,2016-02-10 20:54:35-05:00,PiManASM,,"Let's say I store x=5, and that information gets stored at 0xDEADBEEF. Later, when I ask whatever software I'm using for what ""x"" is equal to, how does it know to look at 0xDEADBEEF?",4559lm,t1_czvh7j9,lordvadr,,Reply,3,0,3
czvowh6,2016-02-10 23:49:04-05:00,metthal,,"It is a job of compiler/linker to find viable addresses. When your program is being run, your computer doesn't know that this was variable x, or whatever. You may now ask, how it comes every PC is guaranteed to have same addresses? Well, that is beauty of virtual address space. Every single process has its own isolated address space and is loaded at the specified offset in this address space (for simplicity, lets forget about relocations and position independent code for now). You just need to remeber that all your PC see are just bytes in the memory.",4559lm,t1_czviw8w,PiManASM,,Reply,1,0,1
czvqf6d,2016-02-11 00:42:17-05:00,lordvadr,,"Um, ok, I'm assuming your using some kind of c-ey language...otherwise you wouldn't be talking about pointers.  If I'm wrong on that, let me know and I'll write you more meaningful pseudo code.

So EVERYTHING in a computer is done on address.  Let's invent a really simple computer.  Ok, so in this magical simple computer, we need some software that does things like initialize certain hardware devices, discovers mass storage, etc.  Let's call that code ""boot code"".  You'd probably call this the BIOS, which isn't incorrect, but the idea that was BIOS has been corrupted into a lot of things outside the original intention of BIOS.  So ""boot code"".

Now let's say this boot code needs to be already on the computer in an un-modifiable fashion.  Ok, let's use rom (the way it used to work), or flash, or whatever other sort of ram.

I hope I don't need to explain the difference between DRAM and mass storage, but if I do, when I refer to ""memory"", I mean DRAM and not disk space.  Just throwing that out there.

Now, when we're building this computer, you'll find that with all memory chips that they all kinda work the same way.  You give them an ""address"", and an operation, such as read or write.  If you give it a ""write"" command, you also have to give it the data you want written.  In either case, you have to give it ""where in the chip you want"".  This is done by address.  An 8-bit chip will have 8 address input pins.  A 16-bit chip will have 16.

So let's say we have an 8-bit memory chip.  256 bytes of memory.  At an electrical level, it will have 8 address pins.  If you give it 0 address, you'll get whatever is at 0.  If you give it 1, you'll get whatever is at 1....so on and so forth for all of the 256 ""slots"" in the chip.

For our boot code, we're going to chose a chip that can't be written to.

So let's then say we have a 10-bit processor, which will have 10 address output pins.  You can wire our non-writable chip containing our boot code, and 3 8-bit (4 8-bit chips is 10 bits) into a 10 bit address.  You can do this however you want, but lets say you do it so that address 0 is the beginning of our boot code, and however it was programmed is irrelevant.

**ENTER YOUR FIRST POINTER.** In CPU's there are what are called registers.  Think of them as VERY fast variables, but you only have so many of them (in an x86 CPU, you have 4--actually, there are something like 16 but only 4 of them are general purpose, others are for certain things).  One of those ""certain things"" registers is the **I**nstruction **P**ointer, or IP (eip in x86 land).  Guess what, it starts at 0.

So the CPU fetches what's at IP and executes it--""go fetch what's at 0 and execute it.""  That could be whatever we put there, but that's what it does.  The next thing that happens is that IP gets incremented (this isn't simply one byte, but let's keep it simple).  So the CPU fetches what's at 1 and executes it.  Say what's at 1 is some sort of conditional that's true based on what happened at 0?  So instead of incrementing IP, ""where in memory the computer should continue to execute"" is put into IP.

So at some point, the boot code has done what it needs to do, and will find bootable software on a disk somewhere.  It will copy that into writable memory, and then put the address of that into IP.  The computer will continue booting.

This whole process continues ad infinitum.  When you run a program on any  computer, the executable is copied into RAM at *there*, and then the CPU is told to ""go execute whatever the fuck I put ...*there*"".

**Ok, so why did I go through this?** Enter your second pointer, called the **S**tack **P**ointer.  The stack is a chunk of memory where persistent things are stored in relative order.  Anytime you launch a program, or call a function, the first thing put **on** the stack is the return address.  This is typically IP+1 (the next line of code after the function call--AGAIN, MUCH MORE COMPLICATED IN REAL LIFE SO ALL YOU 14 YEAR OLD FUCKS LEAVE IT ALONE).

So when a function is called, like, say, your `main()`  function, the return address is what's on the top of the stack.  In x86 land, that's called `esp`.  The function is allowed to put as much as it wants on the stack, so long as the stack is returned to its original state before it returns.  This is called pushing *onto* the stack, and popping *off of * the stack.

Without getting too complicated, a function can say:

    push 5
    pop b

And b contains 5.  Now, normally it's a lot more complicated than that (and b doesn't exist...well, it does, it's a register called `ebx`, but neglect that).

The compiler takes care of all this for you.

So what happens it that when you do:

    int main( void )
    {
        int a:
        return 0:
    }

The first thing that happens is that as soon as main is called, it makes room for ""a"" on the stack.  Used to be it would push something random, but modern compilers just add up all the sizes of all the local variables and just move the stack pointer that many bytes.  All it has to keep track of is how many bytes to adjust back to before it returns.

Ok, so when you san `a = 5`, assuming `a` is a local variable, and the only local variable, 5 gets stored at the address of SP-1 (it's actually minus 4, 8, or 16 depending on the CPU but let's not get all bent out of shape here).  You literally don't care what address is in SP, just that you want the first variable below SP.  You don't care.  Actually, you don't care that the compiler doesn't care, because the compiler does all of this for you.

So, to immediatly answer your question, the address of any variably is known because you (or more accurately, the compiler) knows where it is.  The compiler takes care of it for you.  For a global variable, it's ""wherever the fuck I was loaded at"" plus or minus some offset.  For a local variable, it's wherever the fuck esp points plus some offest.

So, say you want to read some file into memory.  You now go ask the operating system for extra ram (this is called the heap).  You don't care what it returns, but assuming it returns something, what it returns is an address.  Some-fucking-where in physical memory that it's agreed to let you store stuff.  **This is what you store in a pointer.**

So a lot of times, pointer lessons do things like allocate integer pointers....where the pointer is larger than the number to store.  I've always thought that this is a mistake.

So you can go ask the OS for a memory allocation for an integer, but it's FAR more efficient to do that on the stack.  But, if you don't know how many integers you need to store, you can put an integer pointer on the stack, ask the OS for enough room to store 10, 100, 1000, whatever integers:

     malloc( 1000 * sizeof(int) )

And put that in your pointer.  You don't care where that is.  You just know that if it was an integer pointer that you can access is as an array.

    int *ptr:
    ptr = malloc( 1000 * sizeof(int) ):
    ptr[5] // 6th integer in the 1000 you just got allocated

Does that make some more sense?",4559lm,t1_czviw8w,PiManASM,,Reply,1,0,1
czvr4zi,2016-02-11 01:11:24-05:00,PiManASM,,"I think that answered my question! So the stack pointer is what the CPU goes through to get to everything else, and that gets moved around the list of variables to find out where the information for that variable is? Is that sort of right? I appreciate you taking the time to write this all out, I'll probably read it a couple more times at least...

I'm currently studying Computer Engineering, the lower levels of computing, and the way hardware and software interacts is fascinating to me.",4559lm,t1_czvqf6d,lordvadr,,Reply,2,0,2
czw4yyd,2016-02-11 11:28:46-05:00,lordvadr,,"I've always loved architecture, although I'm very rusty on a lot of it as I do mostly network and systems these days, however I too was CE although I switched to a double in CS and EE.

(this is mostly in x86 which I'm most familiar with, however it applies to most architectures and OS's)

To answer your question, eip is where the magic happens, that what the CPU is doing.  The stack is somewhere else and esp, along with another pointer called the base pointer (ebp) define the current ""frame"".  Essentially the current function's data along with how to get back to the previous function.  EIP is literally the code (essentially current line, although a singly line of C can compile into many instructions) of your program. If EIP is what the CPU is doing, ESP/EBP is what it's doing whatever it is doing *to*.  The stack is where your data is...along with the ""heap"" (dynamically allocated memory), which is also somewhere else.

When you launch a program, three chunks of memory are allocated.  The ""text"" section of your program is loaded into one of those, the stack is another, and the heap is the third.  The return address is pushed onto the stack, EIP is pointed at the beginning of the TEXT section, and the CPU is turned loose to run your program (this is VASTLY SIMPLIFIED).

So this is more complicated because things like global variables and string constants and such are part of the TEXT section, so as part of the executable, it actually specifies where in memory it wants to be loaded, and indeed, all programs (this has changed a bit with ASLR) are loaded to the same address and it's the MMU's job to map those ""virtual addresses"" to physical memory.

Now, the stack is more or less there to keep track of ""current data"", and one of those is the return address.  In assembly, the ""call"" instruction literally pushes EIP+1 (it's not 1, it's where ever the next instruction starts) on the stack, and jumps ""jmp"" to the address of the function you just called.  Return ""ret"" is literally ""pop eip"" (put the address of the instruction after the ""call"" back into the instruction pointer.  This has many advantages and disadvantages.  One example:

    function1()
    {
        int *t:
    
        t = function2():
        printf(""T is %d\n"", *t):
    }
    
    int* function2()
    {
        int a:
    
        a = 5:
    
        return &a:
    }

So this is completely invalid code, but it would print ""T is 5"".  It's invalid because a has gone out of scope after function2 returns, but it's still on the stack.  This code would work until it's loaded and run in such a way that something else overwrites the now invalid location of 'a' between the call to function2 and the call to printf.  And now you have this annoying bug that's hard to figure out because it worked fine yesterday.

Another thing that can happen is, since the return address is on the stack, you can overwrite it.  This is the source of a lot of security exploits--what's called a buffer overrun.  Assuming the attacker can predict--he usually can--what these addresses are (remember, they're all usually in the same place (disregarding ASLR)), he can cause your program to execute arbitrary code.

Say you did something like this:

    function1()
    {
        char buffer[1000]:
        scanf(""%s"", buffer):
    }

Scanf doesn't have any idea how big buffer is, and the CPU isn't going to stop it from writing past the end of the 1000 bytes...the stack grows ""down"", but you write to variables ""up"".  So you could feed that program assembly that's more than 1000 bytes long, overwriting the return address to point into the now executable code you feed the program that's now in buffer.  When the function returns, it's now executing the code that any arbitrary user fed it.  This is exactly how a lot of machines are broken into and a lot of malware ends up on your computer.

But, this has the advantage of making dynamic linking possible.  When you compile a program, all the outside functions you're going to need are listed in a table, along with the library (dll, shared object, whatever) it's in.  This is called the ""jump table"".  So the OS loads your program somewhere, loads libc somewhere else, and then finds the jump table in your program and writes the address of, say printf (which it got because the library lists all the functions it provides along with the addresses of each in it's header) there.  It's full of zeros until it's dynamically linked.

So the assembly will look (mostly) something like:

    ...somwhere...
    call xx_printf
    mov eax ebx
    
    ...somewhere else (jump table)
    xx_printf:
        jmp 0x00000000

Which after linking will look something like:

    ...somwhere...
    call xx_printf
    mov eax ebx
    
    ...somewhere else (jump table)
    xx_printf:
        jmp 0xdeadbeef

The reason for this is you made the ""call"", which put the next instruction (the address of the mov instruction) on the stack.  The jump table does a ""jump"" (a goto essentially) to somewhere else.  The somewhere else, printf doesn't give a shit where it was called from, does it's thing, and ""returns"" normally, popping the address of the instruction after your call into EIP and you're program goes on it's merry way.

Moral of the story is the stack is your friend, but it can be a heinous bitch too.

The stack is also how you pass variables to functions.  They're pushed onto the stack in reverse order.  So speaking of printf, it doesn't give a shit how many variables you pass to it, the first one is the format string, and it's one word above the return address (this is kinda what EBP is for, you store where the stack was before your function started fucking with it in ebp (not quite, but mostly)) so that you know where your variables were.  Printf sees a %d and knows to find an int (4 bytes), so at ebp+4, then sees a %c, so it's looking for one byte at ebp+5, a %s, so a character pointer (also 4 bytes) at ebp+9, so on.  This is why if you do something stupid like:

    printf(""This int %d, this string %s, this char %c"", 5):

Printf will find whatever is in the address before where 5 is on the stack, dereference that as a character pointer, which probably points to an invalid memory region, and your program will segfault. 
",4559lm,t1_czvr4zi,PiManASM,,Reply,1,0,1
czvb4lv,2016-02-10 17:26:15-05:00,Bottled_Void,,"OK so a pointer is just like any other data item. Except instead of being a integer or a float, it's a memory location. The type of the pointer tells you what data is stored at that location.


[POINTER] -> [VARIABLE]

Some C code:


    int A = 1: /* create an integer */
    int *A_ptr = &A: /* create a pointer and in it, store the address of A */


So the value pointed at by A_ptr is 1.


Doesn't seem like this is any use at all does it? I mean, you already have access to A through A, why do you need a pointer for it?


There are many many uses for pointers. Things like stacks, lists and arrays use them. You'll probably cover some of these in your next lesson.",4559lm,t3_4559lm,PiManASM,,Comment,2,0,2
czvcph0,2016-02-10 18:05:29-05:00,glitchn,,">How is it not infinite recursion of addresses pointing to addresses that point to addresses?

It kind of is, in that pointers are just addresses and everything in the computer has addresses. But when you resolve an address you get the data inside of that address, which can **point** to another address, or not. If every address pointed to another address then it would be "" infinite recursion"" but since most addresses are going to just contain some other form of data there are no more pointers to resolve.

To be clear it is possible to have a pointer that points to one address, and that address contain a point back to the original address. That would be what you described above. 

But in normal cases they are used just to be able to pass around links to data without copying the data itself every time you do it.",4559lm,t3_4559lm,PiManASM,,Comment,1,0,1
czxw6xz,2016-02-12 18:11:17-05:00,BenRayfield,,"> How is it not infinite recursion of addresses pointing to addresses that point to addresses?

It is in a [circular linked list](https://en.wikipedia.org/wiki/Linked_list#Circular_Linked_list), but just because a pointer is there doesnt mean you have to go where it points. You can use it the same as any other integer or read or write at an address. You can use things that arent pointers as pointers, like using the raw bytes of a float as an int. Addresses and values are yours to use any way you like, to explore to any depth or stop any time for any reason.",4559lm,t3_4559lm,PiManASM,,Comment,1,0,1
452wp6,2016-02-10 09:21:19-05:00,vkrt,What does a B-Tree of degree one look like?,"The rules for b-trees are 

1. There are lower and upper bounds on the number of keys a node can contain. These bounds can be expressed in terms of a fixed integer t called the minimum degree of the B-tree

2. Every node other than the root must have at least t - 1 keys. Every internal node other than the root thus has at least t children. If the tree is nonempty, the root must have at least one key.

3. Every node can contain at most 2t - 1 keys. Therefore, an internal node can have at most 2t children. We say that a node is full if it contains exactly 2t - 1 keys.

Given t=1, would this essentially be a tree where every node has one key and 2 children? (except leaves which have one key and 0 children)",,,,,Submission,3,0,3
czurezm,2016-02-10 09:39:31-05:00,DroidX64,,A binary search tree? ,452wp6,t3_452wp6,vkrt,,Comment,6,0,6
czvglu1,2016-02-10 19:51:46-05:00,Lothlorne,,"If this feels counter-intuitive, remember that b-trees were constructed as a generalization of binary search trees.

That is, binary search trees had already been come up with, and a few programmers thought, ""Okay, so now how do we describe this as a more abstract idea, where a binary search tree is just one of its variations?""

The result was the b-tree, so it only makes sense that the binary search tree can be created from it.",452wp6,t1_czurezm,DroidX64,,Reply,1,0,1
4507bh,2016-02-09 20:33:58-05:00,rexy666,Mac HDD partitioned into 2 equal partitions with identical files in it. Why?,"Mac OS X 10.11.3 shows HDD is partitioned into 2 equal 50-50% size partition, with both parts contained the same files in it? 
[Storage](http://i.imgur.com/VSPQhhS.png) and [Disk Utility](http://i.imgur.com/2qN28Vo.png)

what would this happen?",,,,,Submission,0,0,0
czuhwm0,2016-02-10 01:21:05-05:00,rocketbunny77,,Wrong sub. ,4507bh,t3_4507bh,rexy666,,Comment,2,0,2
44zf18,2016-02-09 17:42:01-05:00,Stardog22,What are the pros and cons of using a external hard drive that uses USB to power it over one that uses a power cable?,I have an old 1TB Western Digital external hard drive. I have pretty much filled it up and was thinking about upgrading. I am curious if one is more reliable than the other. Are the USB powered ones SSD or still HDD?,,,,,Submission,0,0,0
czu98ya,2016-02-09 21:09:00-05:00,mosqutip,,/r/hardware or /r/techsupport or /r/TechnologyProTips,44zf18,t3_44zf18,Stardog22,,Comment,7,0,7
czun2ff,2016-02-10 06:32:45-05:00,Mighty_Miro_WD,,"Hi there.

At the moment there are external SSDs and HDDs. I would say that it depends on your situation. If you're not moving the external drive around much, then I would suggest a hard drive. However, if you need to move the drive around a lot (especially when in use), or have specific performance requirements (and can withstand the limited write cycles of the drive's sectors too), then you may want to consider the SSD.

Also, keep in mind that you may be limited by the connection method since it's an external drive. If you don't have eSATA, and your computer doesn't support USB3.0, then it's unlikely that you can use the SSD to it's full potential transfer speeds - and because of this, you might not notice much of a difference.

Hope this helps and feel free to ask any questions you may have.

Cheers! :)",44zf18,t3_44zf18,Stardog22,,Comment,3,0,3
czvdnz2,2016-02-10 18:30:56-05:00,Stardog22,,Thank you!,44zf18,t1_czun2ff,Mighty_Miro_WD,,Reply,1,0,1
czu4h7l,2016-02-09 19:11:37-05:00,GotBetterThingsToDo,,"You can buy either SSD or mechanical drives as externals. The benefits of power over USB is that you don't have a clunky power brick. The only drawback would be if you have low system power on your USB ports, and could not spin up the drive, but I haven't seen that since the USB 1.0 days. I prefer USB powered.",44zf18,t3_44zf18,Stardog22,,Comment,4,0,4
czvdo4v,2016-02-10 18:31:04-05:00,Stardog22,,Thank you!,44zf18,t1_czu4h7l,GotBetterThingsToDo,,Reply,1,0,1
44xuve,2016-02-09 12:45:29-05:00,DickCheeseSupreme,[Formal Languages] does complement(L*) = [complement(L)]*?,"I think I know the answer, so I'd like to verify if I'm correct. I don't think they are equal.

λ = the empty string 

λ ∈ L* 

λ ∈ ( L^c )*

If λ ∈ L\*, then λ ∉ (L*)^c

Thus, since λ ∈ ( L^c )* and λ ∉ (L\*)^c , (L^c )* ≠ (L*)^c

Excuse me if this proof isn't rigorous enough. So far my Formal Languages class has been pretty...informal, to say the least.

Edit: defined λ as the empty string",,,,,Submission,4,0,4
cztonxj,2016-02-09 13:09:18-05:00,_--__,,"Yes, this is correct and certainly rigorous enough.",44xuve,t3_44xuve,DickCheeseSupreme,,Comment,3,0,3
czts3ei,2016-02-09 14:25:05-05:00,flebron,,"You say that ""Let λ ∈ L\* and λ ∈ ( L^c )\*"". That is, you let λ be in the intersection of L\* and (L^c )\*. But you need to prove that λ exists. That is, you need to prove that the intersection of L* and (L^c )\* is not empty. You can't just ""Let"" things exist in empty sets :)",44xuve,t3_44xuve,DickCheeseSupreme,,Comment,2,0,2
cztt58k,2016-02-09 14:48:25-05:00,_--__,,I believe he is using λ as the symbol for the empty string (not that uncommon a definition) - i.e. the first two statements are observations rather than assumptions.,44xuve,t1_czts3ei,flebron,,Reply,2,0,2
czttqg8,2016-02-09 15:01:26-05:00,flebron,,"Ah, I've usually seen ε for that. If that just means the empty string, I'd use a few more words to make it clear that those are observations, not assumptions. For instance, ""By definition, λ is in L*"". Formality doesn't mean just using symbols :)",44xuve,t1_cztt58k,_--__,,Reply,1,0,1
cztxvds,2016-02-09 16:32:50-05:00,DickCheeseSupreme,,"Here is where my understanding gets shaky. I assumed λ ∈ L* for any regular language L, and I assumed L^c is also regular for any regular L. So even though L ∩ L^c is null, L* ∩ (L^c )* should be λ, right?

Edit: I saw you're other comment, and yes λ is the empty string in this case. My professor uses it, but lately I've been seeing a lot of ε instead.",44xuve,t1_czts3ei,flebron,,Reply,1,0,1
44vj95,2016-02-09 02:20:36-05:00,SockWitch,Big Oh question,"I've been stuck on this problem for a long time now and I'm stumped. Is 2^nlogn ∈ O(2^n^1.1  )? Provide a brief justification.

I tried taking the log of both sides of 2^nlogn <= c * 2^n^1.1  but I still don't understand it at all. I set n = 20 and c = 10,000,000 but it doesn't make sense. Help?",,,,,Submission,10,0,10
czt8y93,2016-02-09 03:09:05-05:00,motheryaar,,"log(2^nlogn ) <= log(2^n^1.1 )

=> nlognlog(2) <= n^1.1 log(2)

=> nlogn <= n^1.1

=> logn <= n^0.1

Now you know that log grows slowly than any variable n, so this is true and hence 2^nlogn is in O(2^n^1.1 )
",44vj95,t3_44vj95,SockWitch,,Comment,5,0,5
cztmmt5,2016-02-09 12:23:34-05:00,_--__,,"Please change all your implications (=>) to bi-implications (<=>) [after confirming that each step is in fact reversible] - or, more preferably, write your logical statements in the opposite order (i.e. start with log n ≤ n^(0.1)).  Just because X implies something that is true, doesn't necessarily mean that X holds.",44vj95,t1_czt8y93,motheryaar,,Reply,1,0,1
cztorxi,2016-02-09 13:11:48-05:00,motheryaar,,"Yeah you're right, but i wasn't being formal, was just giving the basic idea ",44vj95,t1_cztmmt5,_--__,,Reply,1,0,1
czuhfql,2016-02-10 01:02:06-05:00,flebron,,"Let f(n) = 2^{n\*log\*n}. Let g(n) = 2^(n^1.1).

You want to prove that f is in O(g). That means that there exists an alpha in R>0, and an n_0 in N, such that for all n > n_0, f(n) < alpha * g(n).

f(n) < alpha * g(n)

<=> (Expanding f and g)

2^{n log n} < alpha * 2^(n^1.1)

<=> (Taking log_2 of both sides, which is a monotonic increasing function, with a monotonic increasing inverse)

n log n < log_2(alpha) + n^{1.1}

<=> (Moving the second term of the rhs to the lhs)

n log n - n^1.1 < log\_2(alpha)

These were all equivalences. So the first line holding is the same as the last line holding. So the first line will hold for all (n, alpha) such that n log n - n^1.1 < log\_2(alpha). To make your life easier, you can pick alpha = 2. This means that it'll hold for all n such that n log n - n^1.1 < 1. That is, for all n such that n (log n - n^0.1 ) < 1. As n grows unbounded, log n is smaller than n^0.1. In particular, if you take n\_0 = 2^(100), you already have that log(n\_0) = 100, whereas  (n_0)^0.1 = 2^(100 / 10) = 2^10 = 1024. Since (log n - n^0.1) is decreasing as n grows unbounded, the fact that it's true for alpha = 2 and n_0 = 2^100 means that it's true for all n > n_0.

Thus, for all n > 2^100 it is true that f(n) < 2 * g(n). This means f is in O(g).

(Note I took log to mean log\_2 here, but the same applies for any logarithm, just replace n_0's base appropriately.)",44vj95,t3_44vj95,SockWitch,,Comment,1,0,1
44sjgl,2016-02-08 14:31:40-05:00,CompsciStud,Help with Boolean algebra,"Hi! 

Tomorrow I have an exam which consists of some Boolean algebra exercises, and I got stuck at an exercise with minterms/maxterms. I have read the course and some tutorials on the internet, but I still can't understand how to solve this (it's supposed to be simple, but I can't understand).


http://imgur.com/NcPzfTn 


This is the exercise. The sentence above roughly translates as: ""The truth table below is a representation of:"".

Now, I can't understand how can you pick between minterms/maxterms. 
Also, the correct answer, picked by the professor, is the one marked with red.


Can someone post an explanation of how to solve this, if you got time? I'll owe you. Thank you! 


This is a throwaway account, I guess, since I never posted on reddit before.",,,,,Submission,6,0,6
czsntb5,2016-02-08 16:13:50-05:00,darkquanta42,,"Quick read on the concept says the answer should be:
1. A sum (OR) of minterms
2. A product (AND) of maxterms

So AFAIK your professor made a mistake.  Your answer appears to be a valid sum of minterms (the locations where F is 1).

And as far as deciding between the two it appears there should always be a valid min and max term equation for a truth table. One where the min subscripts are not in the set of the max subscripts and visa versa.",44sjgl,t3_44sjgl,CompsciStud,,Comment,1,0,1
czsxsql,2016-02-08 20:43:19-05:00,Fate_Creator,,"From this [source](http://www.dsm.fordham.edu/~moniot/Classes/CompOrganization/boolean-outline/node4.html):

minterm - *a product (AND) of all variables in the function, in direct or complemented form*

maxterm - *a sum (OR) of all the variables in the function, in direct or complemented form*

~~So that would mean that you have a **sum** of maxterms, not a **product** of minterms. That's why your answer is incorrect and the professor's is correct.~~

~~I assume /u/darkquanta42 got mixed up.~~

Edit: I stand corrected.",44sjgl,t3_44sjgl,CompsciStud,,Comment,1,0,1
czsz6re,2016-02-08 21:20:56-05:00,Tinamil,,"From reading your source, /u/darkquanta42 actually looks correct about the notation.  A maxterm looks like: 

    F = M_0 * M_1 * M_3 * M_5

 which expanded is 

    F = (A + B + C) * (A + B + C') * (A + B' + C') * (A' + B + C')

The minterm looks like

    F = m_0 + m_1 + m_3 + m_5

expanded becomes

    F = (A' * B' * C') + (A' * B' * C) + (A' * B * C) + (A * B' * C)

However, the important part about maxterms is that they indicate zero values, not one values.  So a correct maxterm answer would actually have been:
   
    F = M_2 * M_4 * M_6 * M_7

and its complementary minterm formula

    F = m_0 + m_1 + m_3 + m_5 

was the correct answer to the question.",44sjgl,t1_czsxsql,Fate_Creator,,Reply,3,0,3
czsz41v,2016-02-08 21:18:55-05:00,darkquanta42,,"From the same source (that's the source i read as well): 
Minterms provide a way to represent any boolean function algebraically, once its truth table is specified. The function is given by the sum (OR) of those minterms corresponding to rows where the function is 1.

That describes what the professor is asking for, the definition of the function using min/max terms. What you posted is the definition of the terms themselves.

That being said I never encountered this concept in school so I'm not very familiar with it.",44sjgl,t1_czsxsql,Fate_Creator,,Reply,1,0,1
czszfnm,2016-02-08 21:27:26-05:00,Tinamil,,"Based on the source posted by /u/Fate_Creator, I believe your answer is correct.  Minterms indicate one values and are represented as sums, maxterms indicate zero values and are represented as products.  Therefore C was the only correct answer.",44sjgl,t3_44sjgl,CompsciStud,,Comment,1,0,1
cztiqsr,2016-02-09 10:52:29-05:00,CompsciStud,,"Thank you,guys! 
I got it sorted out. ",44sjgl,t3_44sjgl,CompsciStud,,Comment,1,0,1
44s7dh,2016-02-08 13:21:41-05:00,Cyniikal,[Language Theory] Distinct Strings question.,"If we have two distinct strings, x and y over some language, we can clearly show that the commutative property works xy = yx when x = ε or y = ε.

If we disallow empty strings, are there still instances where commutivity works for two distinct strings?

I've been thinking of letting x = yy,yyy,yyyy.... but I'm not sure if simply making x = y^n where n≥2 makes x ""distinct"" from y.",,,,,Submission,6,0,6
czsjpwo,2016-02-08 14:39:01-05:00,_--__,,"As long as y≠ε, y and y^2 (and y^(3), ...) are distinct - they have a different number of letters for a start.",44s7dh,t3_44s7dh,Cyniikal,,Comment,3,0,3
czskkx1,2016-02-08 14:59:08-05:00,Cyniikal,,"Thanks, that makes sense, I just didn't know if distinctness depended on the makeup of a string or not.

y and y^2 / y^3 .... being distinct was pretty much my only question.",44s7dh,t1_czsjpwo,_--__,,Reply,1,0,1
czssdvv,2016-02-08 18:15:56-05:00,nighthawk648,,"It is also good to take into account if the strings were exactly the same, except the one set had null lists you would have to choose to implement that as equal or not",44s7dh,t1_czskkx1,Cyniikal,,Reply,1,0,1
44qg10,2016-02-08 05:59:26-05:00,MeditatingLemur,My lecturer mentioned an extension of PPP (Point-to-Point Protocol) called PPPP. Anyone know what the extra P stands for?,,,,,,Submission,1,0,1
czs4dvx,2016-02-08 07:15:31-05:00,CaptainTrip,,Why not ask your lecturer?,44qg10,t3_44qg10,MeditatingLemur,,Comment,5,0,5
czs9r31,2016-02-08 10:39:57-05:00,SneakingNinjaCat,,Maby he did mean PPTP?,44qg10,t3_44qg10,MeditatingLemur,,Comment,1,0,1
44nkdi,2016-02-07 16:06:00-05:00,vito_xmf,How would I go about learning about emulation?,"What are some good starting points about emulation? I'm interested in emulation, game emulators to be exact. Now I know that it's way more complicated than what it's made out to be but it is something that interests me and I would love to start learning about, how would I go about learning about it? Is there a good starting console to emulate? Ive been thinking about starting out with the nes becuase I've heard it's one of the easier consoles to emulate, am I wrong? If so, what console should I begin studying about if I want to do a long term emulator project? (I'm starting college this year I'm thinking of pursuing a degree in CS if that's important info)",,,,,Submission,8,0,8
czrvmtv,2016-02-07 23:32:17-05:00,lneutral,,"An emulator named JavaGear was conceived as a University project, and its development and design are thoroughly documented, with the report the student wrote published online (see SMSpower for a [download](http://www.smspower.org/Development/Documents)). It's very helpful.",44nkdi,t3_44nkdi,vito_xmf,,Comment,2,0,2
czrx89o,2016-02-08 00:25:10-05:00,vito_xmf,,Thanks! I'll be sure to check it out!,44nkdi,t1_czrvmtv,lneutral,,Reply,1,0,1
czrjq7s,2016-02-07 18:25:11-05:00,MALON,,"I believe there is a current post over in /r/watchpeoplecode where a guy is making an n64 emulator from scratch. Besides that, you can still find quite a few YouTube videos on creating emulators from scratch.

Good luck, it's pretty complicated, in my opinion.",44nkdi,t3_44nkdi,vito_xmf,,Comment,1,0,1
czrme4o,2016-02-07 19:34:06-05:00,vito_xmf,,"Thanks for the response! I'll be sure to check that out. I'm pretty sure its quite the complicated process but I'm willing to learn, you know what I mean?",44nkdi,t1_czrjq7s,MALON,,Reply,1,0,1
44lqdx,2016-02-07 09:14:04-05:00,yhoyhoj,"What is the place of maths in basic/average neural net theory ? (interested but wondering if it could make a good ""research"" subject)","Hello everyone,
I'm aware that this is not an interesting compsci question, but I thought it would be better to post it here than in the other compsci related subreddits.  
So I'm the equivalent of a first-year undergraduate (not in the UK or the US) in Maths/Physics, but I also have a basic computer science/algorithm course. I have to do a research project (not a lot of time but spread on a long period, 6 months to 1 year) in Maths or Physics.  
I am interested in neural network and machine learning, but I have no formal knowledge, and I thought it could be a good opportunity to learn more on the subject. And taking a comp sci subject would allow me to make a program easily (chemistry/physics experiments and computer programs are highly valued). But I need help to evaluate if that is a sound choice.

The problem is that it need to have a lot of maths and that I won't be able to follow a lot of courses on neural networks before starting doing maths.
So my questions are :
are there interesting maths at the basis of neural networks ?  
For example, let's say I want to code a neural net from scratch to resolve a specific task, I guess it gets algorithmically complicated quite fast. I'm not a total beginner in programming but I won't recreate Google Deep Mind, so is it likely that I reach the limits of what I can possibly code before the maths get interesting ?  
  
I hope you can help me with this ! Here if any clarification is needed.",,,,,Submission,1,0,1
czrijaq,2016-02-07 17:50:59-05:00,jirachiex,,"I would say neural networks fall into applied mathematics and not pure mathematics: that is, it uses math as a tool, but doesn't offer too much in the sense of theory (yet -- maybe the field is just not there right now). It will depend on your project requirements whether that's a good fit.

To understand neural networks, the math is broad, but you don't have to understand it at a very deep level to be able to code one up from scratch. I suggest that the mathematical foundations of neural nets are statistics, linear algebra, and calculus.

Statistics helps define the framework of how to do machine learning. Machine learning has its roots in statistical classification and statistical regression. The task goes like this: you have some data that are pairs of inputs and outputs. You want to create a model of the data that predicts what the outputs are based on only the inputs. So there are two operations you want to do on a model. One, train or fit the model to a given dataset of inputs and outputs. Two, given only an input, use the trained model to predict the corresponding output. In order to compare how one model is better fit than another, you choose a statistically-motivated measure of performance called a loss function. A loss function takes in a model and the dataset and outputs a number: how badly did the model do? If your model fits the data very well, the loss is low. If it sucks, the loss is high.

Linear algebra provides the basic data structures of neural nets. There are lots of possible models out there for machine learning, but neural networks are just one class of models. This specific type says that the inputs are going to be vectors, and we're going to apply a bunch of matrix multiplications by some matrices (which are parameters to the neural network) on the input (plus some other functions) to get the output. So the data and parameters are organized into vectors and matrices. Understanding how vectors and matrices work allows you to implement the predict operation (given an input, compute its output).

But, you still haven't trained the neural net, so this is where calculus comes in. Fitting a model to data is an optimization problem, because you want to find the parameters to the model that minimize the value of the loss function. Well, calculus can provide the tools to find that minimum. Remember that the minima of a differentiable function appear where the derivative equals 0. If you're somewhere where the function is not at a minimum, you can take a step in the direction of a minimum by following the derivatives: this is called gradient descent. It turns out (by no accident) that the loss function and neural nets are differentiable with respect to their parameters, so you can do the same thing to find which parameters to the neural net minimize the loss function. You just have to be comfortable with dealing with calculus on vectors and matrices. That's how you get the parameters of the neural network to be able to do the prediction.

So this is pretty much what you need to know (without all the actual details) to implement a neural net. If you wanted to do research to drive the field forward, to build more effective neural nets that have better performance on some AI task, you would have to have a deeper understanding of statistics, linear algebra, or calculus to provide the mathematical motivation.",44lqdx,t3_44lqdx,yhoyhoj,,Comment,3,0,3
czrjyn5,2016-02-07 18:31:51-05:00,yhoyhoj,,"Thank you very much for your very thourough answer, a very interesting ! Unfortunately I think you confirm what I feared : it being more applied maths I will have to learn a lot of things that are not strictly mathematics before being able to study the underlying maths. It may be a bit too risky to go on this road. I will have to ask my supervisor. Thank you again ! If it doesn't fit for my maths work, it will fit as a personal computer science project",44lqdx,t1_czrijaq,jirachiex,,Reply,1,0,1
czsjddd,2016-02-08 14:30:52-05:00,yhoyhoj,,"According to my supervisor it's not a bad choice, so this is definitely what I will be studying ! ",44lqdx,t1_czrijaq,jirachiex,,Reply,1,0,1
czsn4lx,2016-02-08 15:57:54-05:00,jirachiex,,"Awesome, good luck!",44lqdx,t1_czsjddd,yhoyhoj,,Reply,1,0,1
czr3m1m,2016-02-07 10:45:26-05:00,-missnyankitty,,"Funny, actually coding a basic neural net from scratch right now. You definitely definitely need a solid understanding of linear algebra and matrix operations. Also basic Calculus. Other than that, it's a good practice in designing software well :)",44lqdx,t3_44lqdx,yhoyhoj,,Comment,2,0,2
czsrdpx,2016-02-08 17:41:30-05:00,resavr_bot,,"*A relevant comment in this thread was deleted. You can read it below.*

----

I would say neural networks fall into applied mathematics and not pure mathematics: that is, it uses math as a tool, but doesn't offer too much in the sense of theory (yet -- maybe the field is just not there right now). It will depend on your project requirements whether that's a good fit.

To understand neural networks, the math is broad, but you don't have to understand it at a very deep level to be able to code one up from scratch. I suggest that the mathematical foundations of neural nets are statistics, linear algebra, and calculus.

Statistics helps define the framework of how to do machine learning. Machine learning has its roots in statistical classification and statistical regression. The task goes like this: you have some data that are pairs of inputs and outputs. You want to create a model of the data that predicts what the outputs are based on only the inputs. [[Continued...]](http://www.resavr.com/comment/what-place-maths-basic-2741042)

----


*^The ^username ^of ^the ^original ^author ^has ^been ^hidden ^for ^their ^own ^privacy. ^If ^you ^are ^the ^original ^author ^of ^this ^comment ^and ^want ^it ^removed, ^please [^[Send ^this ^PM]](http://np.reddit.com/message/compose?to=resavr_bot&subject=remove&message=2741042)*",44lqdx,t3_44lqdx,yhoyhoj,,Comment,1,0,1
44kc29,2016-02-07 01:01:15-05:00,a845,Gate 2016 question,There are two different types of led bulbs with same quantity probability of type 1 to light up 100 hrs is 0.7 and type 2 is 0.5 if you choose the bulb randomly  among these bulb what is probability?,,,,,Submission,0,0,0
44k83f,2016-02-07 00:29:54-05:00,BenRayfield,"is Subset Xor npcomplete, like Subset Sum is?","Subset Xor is, given a set of bitstrings, does a nonempty subset exist which xor'ed together is all 0s?

Subset Sum is, given a set of integers, does a nonempty  subset exist which summed equals 0?

The security of complex structures of one-time-pads in crypto depends on this.",,,,,Submission,2,0,2
cztz74p,2016-02-09 17:02:51-05:00,_--__,,No.  You can set it up as a linear programming problem in the integers modulo 2 (and solve it in polynomial time using e.g. Gaussian elimination).,44k83f,t3_44k83f,BenRayfield,,Comment,2,0,2
czu5m56,2016-02-09 19:40:48-05:00,BenRayfield,,Gaussian elimination has a worst case of infinite time since its statistical. Thats worse than npcomplete.,44k83f,t1_cztz74p,_--__,,Reply,1,0,1
czu60td,2016-02-09 19:51:01-05:00,_--__,,"Are you sure you're looking at [the right Gaussian elimination](http://mathworld.wolfram.com/GaussianElimination.html)?  This is most assuredly finite, and solves integer linear programming [in GF(2)] in polynomial time.",44k83f,t1_czu5m56,BenRayfield,,Reply,1,0,1
czux5rb,2016-02-10 12:06:15-05:00,BenRayfield,,"> Gaussian elimination is a method for solving matrix equations of the form matrixMult(A,x) = b

You're saying to find a single column matrix x that says which bitstrings in A to xor? The problem is we only know the last binary digit of b since its ""integers modulo 2"", but doesnt the process depend on knowing all of A and b to derive x? Do you know of some way of reordering the rows (which I vaguely remember in this process from linear algebra in college) using only the last binary digits in each integer in b?",44k83f,t1_czu60td,_--__,,Reply,1,0,1
czv7amu,2016-02-10 15:57:53-05:00,_--__,,"Suppose the integers in A are a[1],a[2],...,a[n] and each needs at most m bits to be represented, and your target xor total is b (which we represent as an m-bit vector **b**).  We define a (m x n) matrix M [over 0,1] as follows: the entry in the i-th row, and j-th column is the i-th bit of a[j].  That is, it is 1 iff the i-th bit of a[j] is 1 (and 0 otherwise).

Then you are trying to find a solution (over the integers mod 2 - by which I mean addition and multiplication is done modulo 2) to M**x** = **b**.  The solution, **x**, will be a n-bit vector (over {0,1}) which represents which elements of A you need to choose to obtain an xor total of b.  Since we can find such a solution in polynomial time (e.g. by using Gaussian elimination over the integers mod 2), we can solve the problem in polynomial time.

The ""integers mod 2"" part comes in *after* we have deconstructed everything to its binary representation.",44k83f,t1_czux5rb,BenRayfield,,Reply,1,0,1
czv80ws,2016-02-10 16:14:38-05:00,BenRayfield,,"> The ""integers mod 2"" part comes in after we have deconstructed everything to its binary representation.

To ""deconstruct"" something it must first exist.

b starts as integers with more than 1 binary digit? Which integers? I'll define Subset Xor as summing to all 1s so we can get rid of the nonempty rule. So all b's integers lowest digits are 1. What are the higher digits?

If no digits (at the same powerOf2 index) ever sum together, then its Set Cover which is npcomplete.

If 3 bit1 digits (and any number of bit0) sum together to 3, you modBy2 that to get 1. But how did you know its 3 instead of any other odd number? The integers in b are used in the algebra to move around whats in M.",44k83f,t1_czv7amu,_--__,,Reply,1,0,1
czv9sg1,2016-02-10 16:54:26-05:00,_--__,,"> I'll define Subset Xor as summing to all 1s so we can get rid of the nonempty rule. So all b's integers lowest digits are 1. What are the higher digits?

If all integers in A have at most k bits, then any xor of them will have at most k bits.  So we can assume that b has (at most k) bits - and as you say, we can assume they're all 1. [For simplicity we can assume everything has exactly k bits (with 0's in the most significant place if it is less than 2^(k-1)-1)].  I don't understand what you mean by ""higher digits"".
",44k83f,t1_czv80ws,BenRayfield,,Reply,1,0,1
czvb8nn,2016-02-10 17:28:58-05:00,BenRayfield,,"Matrix multiply sums like x1*y1 + x2*y2 + x3*y3. I get that all the xs and ys are either 0 or 1, but the sum of 0s and 1s can be more than 1. To handle that you say mod by 2 ""after we have deconstructed everything to its binary representation"".

If this is done after choosing specific integers in b which the ""x1*y1 + x2*y2 + x3*y3"" sums to, it would work, as long as you loop over all possible combinations of odd integers that could be the b matrix.

If we leave that abstract and go with a proof based approach, doing the ""mod by 2"" first, then you still have to rewrite the Gaussian Elimination algorithm to do algebra on numbers whose digits are unknown, such as if ""x1*y1 + x2*y2 + x3*y3"" = 3, or maybe it equals 1, either way its odd, so you must handle the possibilities of 3 and 1 and get the same result, but in algebra if something is 3 or 1 affects the other variables in what math you do to them. So please write out the gaussian elimination algorithm on integers whose digits (except the lowest) are unknown.

You may find useful https://en.wikipedia.org/wiki/Pascal's_triangle andOr https://en.wikipedia.org/wiki/Rule_90 because 

> In Rule 90, each cell is the exclusive or of its two neighbors. Because this is equivalent to modulo-2 addition, this generates the modulo-2 version of Pascal's triangle, which is a discrete version of the Sierpiński triangle.",44k83f,t1_czv9sg1,_--__,,Reply,1,0,1
czvf8yw,2016-02-10 19:14:10-05:00,_--__,,"Lets work through an example.  Let's say A={1,2,5,7,9,11} and we are targetting ""all ones"". Since all our integers can be represented by 4 bits, this means our xor target is 15.  Note that {1,2,5,9} satisfies our requirements.

The matrix M is

    [ 0 0 0 0 1 1 ]
    [ 0 0 1 1 0 0 ]
    [ 0 1 0 1 0 1 ]
    [ 1 0 1 1 1 1 ]

Note the columns are just the binary representations of our numbers.  Our solution (there may be others) is the vector **x**:

    [1]
    [1]
    [1]
    [0]
    [1]
    [0]    

Note the rows with 1's correspond to the elements that form part of our solution.

Now, what is the product M**x**?

    [1]
    [1]
    [1]
    [3]

*But* when we look at the product M**x** modulo 2 we get the vector of all 1's, as required.

Now, let's find a solution from M using Gaussian elimination:

The augmented matrix:

    [ 0 0 0 0 1 1 | 1]
    [ 0 0 1 1 0 0 | 1]
    [ 0 1 0 1 0 1 | 1]
    [ 1 0 1 1 1 1 | 1]

Swap rows 1&4 and swap rows 2&3:

    [ 1 0 1 1 1 1 | 1]
    [ 0 1 0 1 0 1 | 1]
    [ 0 0 1 1 0 0 | 1]
    [ 0 0 0 0 1 1 | 1]

Add r3 to r1: (note that 1+1=0)

    [ 1 0 0 0 1 1 | 0]
    [ 0 1 0 1 0 1 | 1]
    [ 0 0 1 1 0 0 | 1]
    [ 0 0 0 0 1 1 | 1]
    
Add row 4 to row 1:

    [ 1 0 0 0 0 0 | 1]
    [ 0 1 0 1 0 1 | 1]
    [ 0 0 1 1 0 0 | 1]
    [ 0 0 0 0 1 1 | 1]

The matrix is now in row echelon form, so we can find all solutions using back substitution.  Note there are two free variables (x4 & x6).  The solutions are x1=1, x2 + x4 + x6 = 1, x3+x4=1, x5+x6=1.  Or in vector form (since + and - are the same when working modulo 2):

    [1]   [0]     [0]
    [1]   [1]     [1] 
    [1] + [1] s + [0] t
    [0]   [1]     [0]
    [1]   [0]     [1]
    [0]   [0]     [1]

Taking s=t=0 gives our original solution.  Other solutions include (s=1,t=0): {1,7,9}, (s=0,t=1): {1,5,11} and (s=1,t=1): {1,2,7,11}.
",44k83f,t1_czvb8nn,BenRayfield,,Reply,1,0,1
czvfms9,2016-02-10 19:24:46-05:00,wecl0me12,,does this assume P != NP?,44k83f,t1_cztz74p,_--__,,Reply,1,0,1
czvg0ky,2016-02-10 19:35:36-05:00,_--__,,"Yes, but that is usually assumed when deciding whether something is np-complete or not.",44k83f,t1_czvfms9,wecl0me12,,Reply,1,0,1
czr3ykp,2016-02-07 10:57:32-05:00,VideotapeReturn,,Edit: I totally misread the question. Oops,44k83f,t3_44k83f,BenRayfield,,Comment,1,0,1
czva2gw,2016-02-10 17:00:49-05:00,BenRayfield,,"If Subset Xor can be set up to compute AND, then its npcomplete like in SAT.

We already know it can compute NOT since all bitstrings can be extended 1 more bit, and whichever 2 of them the NOT is between would have a 1 bit there.

Subset Xor allows us to define these kind of constraints:

* The solution must have an EVEN quantity of bitstrings from any chosen subset of the bitstrings. Example: For these 3 bitstrings, the solution must include any 1 or 3 of them.

* The solution must have an ODD quantity of bitstrings from any chosen subset of the bitstrings. Example: For these 3 bitstrings, the solution must include any 0 or 2 of them.",44k83f,t3_44k83f,BenRayfield,,Comment,1,0,1
czvfwxw,2016-02-10 19:32:43-05:00,_--__,,"Have a look at [Schaefer's theorem](https://en.wikipedia.org/wiki/Schaefer%27s_dichotomy_theorem).  ""Subset xor"" technically falls into case 6 of the tractable instances - i.e. you can describe it as a conjunction of affine formulae.",44k83f,t1_czva2gw,BenRayfield,,Reply,1,0,1
44jna3,2016-02-06 21:59:17-05:00,theoryofcomp,Help me with MIPS and C (I need to write this a MIPS input program),"http://www.cs.fsu.edu/~carnahan/cda3101/CDA3101_Project1_2.pdf

To be honest, I am not very good with MIPS, and I'm fairly new with C. I am hoping someone can help me with the logic of this problem.
I just need something that I can run so I don't fail this assembly class. I'm more interested in software than the way processors and assembly work.

So where do I even begin with?

I'm assuming I need to declare an array first for the instructions, and then try to parse and open the input file, but I have no idea how to go about it and how to figure out how to identify each type of instruction (J, R, I...) 

How would I even assemble some of these instructions with a pen and paper to begin with ? 
",,,,,Submission,0,0,0
czqsjwi,2016-02-07 00:30:14-05:00,thiagobbt,,"First of all: don't try opening a file, it says on the pdf that you should parse the stdin",44jna3,t3_44jna3,theoryofcomp,,Comment,3,0,3
czqvrt7,2016-02-07 02:58:25-05:00,thiagobbt,,"These links are pretty helpful for correcting your instructions

http://www2.engr.arizona.edu/~ece369/Resources/spim/MIPS_Ref_Card.pdf

https://www.eg.bucknell.edu/~csci320/mips_web/

I used them when I was having MIPS related classes

Now, I have made a sketch that is ugly as hell and probably filled with bugs as I did it half asleep and is manipulating pointers (I'm impressed it's not segfaulting). It only identifies ADDI instructions and would be a mess to add new instructions, but i hope you can take something out of it.

http://pastebin.com/ZKnndGTy",44jna3,t1_czqsjwi,thiagobbt,,Reply,3,0,3
44gln0,2016-02-06 10:24:07-05:00,zSilverFox,Difference between eager and greedy algorithms?,,,,,,Submission,8,0,8
czq8sk2,2016-02-06 14:26:36-05:00,east_lisp_junk,,"What is an ""eager algorithm""?",44gln0,t3_44gln0,zSilverFox,,Comment,3,0,3
czrdjie,2016-02-07 15:30:16-05:00,zSilverFox,,"Good point, I should have googled more. Evaluation != algorithm.",44gln0,t1_czq8sk2,east_lisp_junk,,Reply,1,0,1
44fmo6,2016-02-06 04:57:28-05:00,salemAmeen,How can implement Multi Armed Bandit to choose best models?,"I have hundreds of models and I use big data ""huge data"" as input to those model ""in development time"" so I would like to use any multi armed algorithm to find which best models""say best 10 models"" amount those models. In other words, run some of the data in all models and check the accuracy of the models with helping of MAB try to find the best models. I know might there is a better idea using different techniques but I need to use any kind of Bandit algorithms. I went through many books like""Bandit Algorithms for Website Optimization"" but I could not understand  how to get the mean or mu before running the code.",,,,,Submission,3,0,3
44ekmn,2016-02-05 22:56:21-05:00,aweeeezy,How dependent would a machine learning model of our universe be on our perspective of it?,"I had the thought the other day that went something like this...

Assume we are able to fold space-time and make great leaps across our local cluster or even super cluster. Assume that any such leap is a two-way trip (so that we could retrieve the information gathered from it). If we train a machine learning model to identify constellations, galaxies, or other celestial bodies and then warp a camera w/ computer to an arbitrary point in the universe and back, would we be able to infer where that location was without calculating it beforehand? 

I'm thinking that if the model has no features for the depth of objects of interest in a given image of space, then the model would be hopeless at inferring a relative location to the training location. However, if each star or galaxy had a metric for determining the distance in light years from the origin of the training image, then there would be enough information to deduce the location of the warped camera/computer (assuming a sufficient amount of data is the same in both training and test scenarios).

If you agree that this is possible, what algorithm might you choose to reproduce this experiment and why?",,,,,Submission,0,0,0
44db7o,2016-02-05 17:43:28-05:00,_stewie574,Need help understanding the IAS (von Neumann machine) and address lines,"I'm working on some homework for a Computer Organization and Architecture class. One of the questions states:

>Assume a recent semiconductor memories store 16 Giga-bits on a single chip (it’s the binary Giga, 2^30). Assume that the word size is two Bytes. How many address lines are needed?

My professor told us the answer was 29 (or some number in that ballpark), but I'm not able to figure how he arrived at that answer. ",,,,,Submission,4,0,4
czpnyt5,2016-02-05 22:58:01-05:00,kooppoop,,"It should be 30. You have 16 Gigabits of memory, and words are 16 bits, so, you need enough lines to be able to index into 2^30 (16 Gb / 16 b) addresses, from 0x0 to 0x3fffffff. So, since each line can be active high or active low to represent a bit, you need 30 bits to generate each unique address in that space.",44db7o,t3_44db7o,_stewie574,,Comment,2,0,2
czprdut,2016-02-06 01:06:39-05:00,craiig,,"Count the number of words that the memory can be divided up into. Then, how many bits do you need to identify each one uniquely?

(memory size) / (word size) = number of unique words in the machine",44db7o,t3_44db7o,_stewie574,,Comment,1,0,1
449dwm,2016-02-05 00:13:53-05:00,sadelbrid,How to treat randomness in asymptotic analysis,"I'm trying to analyze something similar to this:

    k = rand(n) //returns 1-n uniformally
    if k < sqrt(n)
        for i = 1 -> n
            for j = 1 -> n
                //stuff

I want to find the expected complexity. Correct me if I am wrong but I expect rand to return n/2. n/2 is only less than sqrt(n) when n < 4. So would the expected complexity be Theta(1)? I feel like I'm forced to make the assumption that n is arbitrarily large.
",,,,,Submission,9,0,9
czolmcl,2016-02-05 02:17:16-05:00,mosqutip,,"It depends on what type of analysis you're trying to do. Generally, complexity analysis is Big-Oh, or the worst case scenario. In the worst case scenario, k = 1, and each loop has complexity O(n), so the total complexity would be O(n^(2)).

In the best case scenario, k >= sqrt(n), so the complexity is O(1). But this seems specious, since we don't really care if the loop isn't running at all.

You're right about the expected value of k, but I'm not sure it's relevant in this situation if you're looking for the time complexity. Are you looking specifically for a tight bound?",449dwm,t3_449dwm,sadelbrid,,Comment,3,0,3
czovm28,2016-02-05 10:28:35-05:00,sadelbrid,,Yeah I'm expected to find a tight bound,449dwm,t1_czolmcl,mosqutip,,Reply,1,0,1
czp7kep,2016-02-05 15:14:53-05:00,_--__,,Expected complexity has a precise meaning - as the average (assumed worst case) complexity over many runs.,449dwm,t1_czolmcl,mosqutip,,Reply,1,0,1
czolpza,2016-02-05 02:22:28-05:00,groumpf,,"> I feel like I'm forced to make the assumption that n is arbitrarily large.

That is indeed the point of asymptotic analysis: estimating the complexity of an algorithm when its parameter tends towards infinity.

In the case you show, you can easily divide your algorithm into a probabilistic part, that generates n, and a fully deterministic part that computes on it, and whose complexity is easy to bound as a function of n. So bounding the expected complexity of the whole algorithm (which is not ""what you expect the complexity to be"", but ""the expectation of the complexity"", where ""expectation"" has its mathematical meaning) is as easy as applying the complexity bound to the expectation of the probabilistic part's output.

Now, if you had probabilistic control-flow or similar weird stuff, you'd be in trouble.",449dwm,t3_449dwm,sadelbrid,,Comment,2,0,2
czp7e2d,2016-02-05 15:10:42-05:00,_--__,,"Critical here is that you are being asked for the **expected complexity**.  That is, for a large number of runs of the algorithm, what is its average worst case (note that this is different from what is the average case - which is taken over a large number of inputs).  Suppose I had a random algorithm that flipped a (fair) coin: if it comes up heads it takes [worst case] O(n) time, if it comes up tails it takes O(1) time.  Then my *expected complexity* is 0.5 O(n) + 0.5 O(1) = O(n).",449dwm,t3_449dwm,sadelbrid,,Comment,2,0,2
czpnkaa,2016-02-05 22:44:37-05:00,VideotapeReturn,,"You can do average case analysis. You treat the time complexity function T(n) as a function of random variables and then find bound it's expected value. Usually for deterministic algorithms the average case is the same asymptotically as the worst case. 

Average case analysis is used to show hash lookups are O(1). First you make an assumption about the hash function that the outputs are uniformly distributed. Then you can show that if the hash table isn't full yet the average lookup is expected constant time. This is a deterministic example. 

Randomized quick sort has an average case of O(n log n) and a worst case of O(n^2) for a randomized example. ",449dwm,t3_449dwm,sadelbrid,,Comment,1,0,1
444c68,2016-02-04 02:53:47-05:00,Rc1777,How does /r/AskComputerScience feel about Codecademy Pro?,"https://www.codecademy.com/pro?utm_campaign=ad_narrative_testing_nov_2015&utm_source=codecademy_platform&utm_medium=internal_ad_syllabus_project&utm_content=wheretostart&utm_term=

Link for info.

I am a Computer Science major at a community college and will be transferring to Davis or UCSD in the fall 2016. I am trying to get prepared for the upcoming coursework and was wondering if this was a good general way to get more knowledge. I plan on doing other studies but this piqued my interest. Any thoughts or comments on this would be greatly appreciated! ",,,,,Submission,5,0,5
cznrwcz,2016-02-04 12:55:05-05:00,scaryred2,,"Haven't tried pro, but I don't really like codecademy. From what I used they show you how to make a var, a function, a hash, etc. But doesn't really help with breaking down the problem and figuring out a solution. 

While it's not completely different, I think tream treehouse is pretty good for a beginner.",444c68,t3_444c68,Rc1777,,Comment,5,0,5
czofo37,2016-02-04 22:43:02-05:00,TeamKennedy,,"CodeAcademy is used as an introduction to programming mainly. It basically just focuses on Web Development and more front end stuff. It also doesn't go too far in depth. (If you asked me it's basically just making a basic website in 10 different languages.) When it comes to actual computations and functions, no it doesn't teach that.  
  
Would I purchase it? No, if you've already completed your Associates for Computer Science it will simply be redundant of what you've already done. (And probably already have a good understanding of)",444c68,t3_444c68,Rc1777,,Comment,1,0,1
443016,2016-02-03 20:16:44-05:00,ramalama-ding-dong,Questions about MIPS Assembly,"So I have a quiz tomorrow and I don't really understand a few things, specifically branching instructions.

For example, I was given a chunk of memory and the corresponding binary code.  I converted all of this into MIPS assembly instructions, and then was asked to convert the MIPS instructions into a high level language.  Finally, execute and write down the return values with the given input arguments and pre-existing values in the register table.

As I worked through this, I got confused because the instructions weren't exactly clear on the best way to accomplish the entire task.  My professor's instructions said to translate each instruction in the IR and execute it immediately.  This meant that with certain branch conditions not being met, I would simply never branch and so some of the machine code was never translated into assembly.

Anyway, since all the data was provided in machine code, the offset/labels of all the branch instructions contained 16-bit signed values.  I understand that in branch instructions, you move however many instructions the offset is, forward if the offset is positive and backward if the offset is negative.  For jump instructions, the target address is calculated by taking the 26-bit target field of the J-type instruction, and plugging it into [PC+4 most significant bit][target][00] to get a 32-bit address to jump to.

Now I know that branches are typically used for if-then-else statements or for loops.  My question is this: if the offset is negative, does this ALWAYS mean that I encountered a loop?  I don't see how a negative offset could be an if-then-else, because if the if-condition is met, then you skip to the else statement which would always be forward, not back.  If the offset is negative, then I would just keep going backward to the same instructions and loop over and over again.

The confusion stems from all examples on the internet and in my book giving actual LABEL names, such as ELSE: , EXIT:, and the like.  However, I'm working strictly with calculated offset targets, so it's not clear how to write the instructions in high-level code.

Finally, if you were asked to translate MIPS instructions into high-level code, can you essentially go down the memory addresses one by one and write every instruction in sequential order to create the high level code?  I think that the branching really only matters during the execution of the code, not the translating, right?",,,,,Submission,11,0,11
czn5vh2,2016-02-03 22:37:01-05:00,mplang,,"Forgive me if I misunderstood anything. Feel free to correct me.

First, were you instructed not to translate unused code? Just because a particular branch might not be taken now doesn't mean that the it won't be later with some other inputs/state.

Second, while a negative offset typically corresponds to a for or while loop, it could also be a simple goto. And remember, just because you jump ""up"" doesn't necessarily mean you're going to repeat code. Say you have some logical sections of code (labels, perhaps), A, B, C, and D. A may branch to C, C jump to B, and B jump to D. That's totally ok.

Lastly, branching does matter when translating. Assume you encounter an infinite loop -- how would you translate that during execution? It would be a really long semester for you, to say the least. So in some way, yes, you can go instruction-by-instruction sequentially and translate the code. However, any time you reach some branching/jumping instruction, you need to take the time to figure out what it's doing and write the appropriate high-level code for the set of instructions involved in the action (goto, for, while, if-then-else).
",443016,t3_443016,ramalama-ding-dong,,Comment,3,0,3
czn6m5o,2016-02-03 22:56:09-05:00,ramalama-ding-dong,,"Thanks for responding :)

Yeah, you're right that the branching does matter when translating into high-level code, because multiple instructions can sometimes be a single line in high level code, and the other way around is true as well.  

And no, I wasn't instructed specifically not to translate unused code.  But myself and many classmates did exactly that, because his instructions said to translate the instruction in the IR, and execute the instruction immediately.  If that were the case, then in a branch statement that evaluates to true we would branch to somewhere further down (with another branch in the future) and eventually the program ended with some instructions being skipped over entirely.  He consistently has grammatical errors and directions out of order, so I think he may have had some oversight on this one.  I'm sure he wanted us to translate ALL the code, as that is REQUIRED in order to write the entire high-level language program.

So I have translated the machine code into MIPS instructions, and they're all in orde.  The thing is, I'm confused because I encountered what you mentioned -- an infinite loop.  Lets say I have something like this:

----
some instructions

----

LABEL2:

instruction: branch to LABEL1 if true

some more instructions

some more instructions

----

LABEL1:

instruction: branch to LABEL2 if true

instruction: branch to LABEL3 if true

some more instructions

some more instructions

----

LABEL3:

some more instructions

some more instructions

END PROGRAM

In the above, if you do NOT branch (the condition is false) then you continue and write the next instructions.  My question is, how do you know when to stop?  For example, in the instruction: branch to LABEL2 if true, I enter an ""else"" block which eventually takes me back to the instruction: branch to LABEL2 if true, and this keeps happening over and over and over again in my high-level code.  Once I've figured out where the LABELS are, am I not supposed to continue down the instructions?  For example, while executing lines of instructions, when you encounter a section that's been LABELED, do you just skip it?  I can't tell where one LABEL section ends and another begins.

Edit: changed the word ""instructions"" to ""directions"" because of the context.
",443016,t1_czn5vh2,mplang,,Reply,1,0,1
czn88x8,2016-02-03 23:45:18-05:00,mplang,,"In my pseudo-pseudocode below, I've swapped LABEL1 and LABEL2, just for my brain. Hopefully it helps your understanding.

    LABEL1:
        if condition is true:
            goto LABEL2
        else:
            ...some more instructions
            ...some more instructions
    LABEL2:
        if condition is true:
            goto LABEL1
        else if some other condition is true:
            goto LABEL3
        else:
            ...some more instructions
            ...some more instructions
    LABEL3:
            ...some more instructions
            ...some more instructions
    END

The important thing here is that instructions are executed in order, literally. The only branches are those which are explicit. Labels are for convenience: the processor knows nothing about them, at all. Assume we're starting at the very top. That is, whatever memory address corresponds to the first instruction (NOTE: It might be better to write each label before the instruction, as in

    LABEL1: if condition is true:

because the label is not some separate thing. The label *is* the address which *is* the instruction.)

So, we're at the first instruction. We evaluate the condition. Say it's false. We enter the else block, which is just another way of saying we execute *the very next instruction.* That would be the first ""some more instructions."" We keep executing instructions sequentially until we reach LABEL2 which, again, is really the ""branch to LABEL1 if true"" instruction. Also, remember that even though I say ""until we reach LABEL2"", the process doesn't really ""know"" that we've reached LABEL2 -- it's just a label for human-readable convenience. As far as instruction execution is concerned, it's just the next instruction.

So, we're at another branch. Say we take it (i.e., the condition is true). We jump back up to LABEL1, i.e., the first instruction. If we don't take the branch, reread the last paragraph. When you're done, skip over this paragraph and go to the next. If we do take the branch, just reread this paragraph. When you're done, continue to the next paragraph as usual.

So, you've made it this far. That means that your branching decisions didn't lead you to an infinite loop. How do you know when to stop? You don't, which is what would've led you to an infinite loop in the first place. To end a loop, something about the condition must change, causing a different branch to be taken (often, this is a counter, as in a for loop). But here we are, at the ""else if"" potion of the branching logic down under LABEL2. We again have two choices. Both, in this case, will eventually land us at LABEL3. If this condition is true, we'll take the shortcut directly to LABEL3. If this condition is false, we fall down into its else block and execute the following two instructions before reaching LABEL3.

After typing this, I realize that maybe I've complicated things a bit, but hopefully it gives you a little more understanding. Good luck!",443016,t1_czn6m5o,ramalama-ding-dong,,Reply,2,0,2
czn8jq8,2016-02-03 23:55:19-05:00,ramalama-ding-dong,,"I appreciate the time you took to write that all out.  You've definitely helped me clear up some things -- especially that if we don't branch, we just continue forward.  

The problem that I had is that every if block has an else block.  However, the else block contains the original if block. And if I write that original if block within the first else, then that if statement again has a new else block (these nest into each other over and over again).

Obviously this is not how it works and I'm just misunderstanding something here.  I will re-write the high level code with the help you've provided for me and will figure this out!  Thanks for your help :)",443016,t1_czn88x8,mplang,,Reply,1,0,1
cznacz9,2016-02-04 01:01:21-05:00,east_lisp_junk,,"> My question is this: if the offset is negative, does this ALWAYS mean that I encountered a loop?

A compiler typically has a lot of freedom to choose how to arrange blocks of straight-line code in memory (of course, if you're starting from machine code, there isn't even a guarantee that a high-level language compiler was involved in producing it).

        add a, b, c
        beq a, x, L1
    L0: add x, y, z
        j L2
    L1: addi b, 10
        beq c, y L0             # negative offset
    L2: add a, b, a

A branch is only looping back if it goes to a label you're guaranteed to have been at before.",443016,t3_443016,ramalama-ding-dong,,Comment,2,0,2
cznaomj,2016-02-04 01:14:28-05:00,ramalama-ding-dong,,"Really clear example, thanks for that.  I will be careful when reading instructions to see if they've been encountered before.",443016,t1_cznacz9,east_lisp_junk,,Reply,1,0,1
czn7c6w,2016-02-03 23:16:58-05:00,SnakeNoir,,"Hello, slug.",443016,t3_443016,ramalama-ding-dong,,Comment,1,0,1
czn7hie,2016-02-03 23:21:28-05:00,ramalama-ding-dong,,is this a joke for procrastinators?,443016,t1_czn7c6w,SnakeNoir,,Reply,1,0,1
czn7mty,2016-02-03 23:25:57-05:00,SnakeNoir,,"No, i assumed you go to ucsc. :)",443016,t1_czn7hie,ramalama-ding-dong,,Reply,1,0,1
czn7p2h,2016-02-03 23:27:52-05:00,ramalama-ding-dong,,"Nope, I was a gaucho in my undergrad and go to Cal Poly Pomona now for masters in CS.  I feel lost most of the time though.",443016,t1_czn7mty,SnakeNoir,,Reply,1,0,1
442f8d,2016-02-03 18:01:37-05:00,theoryofcomp,Does anyone know how to work this? Proof about finite state machines in theory of computation,"Prove that for two machines M and N, M indistinguishable from N does not imply
that M has the same behavior as N. Hint: one way to prove this is to exhibit two
machines, a two state machine M and a three state machine N, both over X = {0,1}
and Y= {0,1} with this property.",,,,,Submission,9,0,9
czmvzjd,2016-02-03 18:15:58-05:00,None,,"It depends on what you mean by ""indistinguishable"". From the hint, I will assume that it means they are trace-equivalent. So, what they want you to prove is that the implication ""M and N are trace-equivalent => M and N are bisimilar"" is false. You can do the proof by contradiction ([hint](http://www.mcrl2.org/release/user_manual/_images/bisim_vs_trace.png)).",442f8d,t3_442f8d,theoryofcomp,,Comment,1,0,1
43yaic,2016-02-02 23:52:56-05:00,xbraatz,Operating Systems and Maximum Ram,"Why is it that different versions of Windows can support different amounts of ram?

**Windows 7:**

* Starter: 8GB
* Home Basic: 8GB
* Home Premium: 16GB
* Professional: 192GB
* Enterprise: 192GB
* Ultimate: 192GB",,,,,Submission,0,0,0
czlywk0,2016-02-03 00:35:24-05:00,i_invented_the_ipod,,"This is a marketing technique known as [Product Differentiation](https://en.wikipedia.org/wiki/Product_differentiation), or possibly Market Segmentation. These products are intended to serve different markets, so the feature sets are adjusted to appeal to the buyers in those markets.",43yaic,t3_43yaic,xbraatz,,Comment,8,0,8
czm079k,2016-02-03 01:27:15-05:00,xbraatz,,Thanks. So there is no difference in the fundamental structure of the OS' to support more RAM?,43yaic,t1_czlywk0,i_invented_the_ipod,,Reply,2,0,2
czm1eks,2016-02-03 02:24:37-05:00,SneakingNinjaCat,,"think of it as a 1,5 liter coffee machine, in to which someone has poured 1 liter of concrete. You can make 500ml of coffee. If you pay more, less concrete will be poured in.",43yaic,t1_czm079k,xbraatz,,Reply,7,0,7
czmc8so,2016-02-03 10:50:11-05:00,xbraatz,,Hmm. Thanks.,43yaic,t1_czm1eks,SneakingNinjaCat,,Reply,1,0,1
czmb34f,2016-02-03 10:20:44-05:00,dxk3355,,wtf Home Premium only supported 16GB?  That's lower than I would expect.,43yaic,t3_43yaic,xbraatz,,Comment,1,0,1
43y7vo,2016-02-02 23:33:03-05:00,SoCalist,How do you develop a mobile app?,"Hello Universe,
I'm interested in developing an app that would be used to connect homeless or destitute college students with people in the community who have extra space in their homes for someone to crash for a night or two. I've got a pretty clear idea of how to tackle the logistics of the program, but I'm completely in the dark about app development and don't know anyone in the field, so I'd really appreciate any constructive input anyone can offer. I'm particularly interested in timeline and budget info, logistics of the software development process (not societal/political logistics), and regulatory concerns.
Thanks folks!",,,,,Submission,0,0,0
czmcrwc,2016-02-03 11:03:18-05:00,KronktheKronk,,"If you're not a software engineer or don't have a software engineering background, you're probably looking at on the order of $50k to pay a single contract developer to make the app for you.  I'd recommend finding and hiring one and letting them handle it.  They'll gather requirements, architect, write, and test the app and backend for you.

Regulatory concerns only exist in the realm of tenant law as far as I'm aware, but coming up with a TOS agreement and figuring out what (if any) risk you take on by putting these parties together is something you'd want to consult a lawyer on before you gain any users.
",43y7vo,t3_43y7vo,SoCalist,,Comment,1,0,1
43xron,2016-02-02 21:39:17-05:00,pikapikaapikachu,Algorithm/Big-Oh Homework Problem,"Hello! I'm in an algorithms class and I have this homework problem that I have no clue how to solve. It has 4 parts. I don't know even know if I understand the problem fully. I don't know if it's just worded badly/unclearly in some parts, or if I'm just stupid. We are just learning Big-oh notation this week and this is the last problem on the homework.

For **part a.)** I know what a binary search is. You have an array of elements and search through them by cutting the search space in half each time. That's the only thing I confidently understand. So that is the ONE thing you don't have to explain to me.

For my homework problem though, we don't have to use a vector/key thing like the book does. My prof just meant to use the idea of binary search. I think we can just use an array or whatever. And the answer can be in pseudo-code.

Like I don't even understand the different between H and H*. So the plate will not shatter under H*? So we are looking for H, the max integer where the plate won't shatter. And H* is the upper bound where if you drop a plate from greater than H*, the plate will shatter.. So isn't H just 1 less than H*?? I probably sound really stupid right now. I mean I don't know!! Ahhh!!

For parts **b-d**, I'm not sure how to even start them? I'm so confused.

Can someone just please give me a really good detailed description about what the questions are asking and how to approach them?? Like how to find how many plates are dropped in the worst case? Do I go line-by-line and get the Big-oh of each line and then add them all up? That's what my prof did in the 1 example she did. I'm also not sure how to count/differentiate how many times the plate is dropped compared to how many times the plate is shattered.

**Homework problem:** http://imgur.com/mstD5FG
**Binary Search:** http://imgur.com/6wKlWPL

Sorry, I'm very lost. I'm not the best a computer science and will appreciate all help. Can someone help me please flesh this out in detail? I'm really sorry, I might need more help than usual.

Thank you in advance! :)",,,,,Submission,6,0,6
czm3b7u,2016-02-03 04:20:00-05:00,zanidor,,"In a ""traditional"" binary search as you describe, there is some list of stuff, and you're testing things in the list at certain points to see if you're too high or too low (trimming the search space with each test).

This problem is asking you to think about plate dropping in the same way, replacing the ""list of stuff"" with ""a range of heights"": and ""testing to see if you're too high / too low"" with ""testing to see if the plate broke"".

Edit: Don't get too hung up on H* -- it's just the top of the range. (If the problem didn't give you some upper bound where the plate definitely breaks, then the search space would be infinite.)",43xron,t3_43xron,pikapikaapikachu,,Comment,2,0,2
czm0kx3,2016-02-03 01:44:11-05:00,_--__,,"No, H\* is a bound that you know plates will definitely shatter, so all you know is that H < H\* (you do also know that 0<H).  The point of the problems is to try and find H using some strategy (part c gives an example of one strategy that could be used to find H).

Don't worry about the concept of binary search for the moment, and don't worry about H\*, just suppose that plates will definitely break if you drop them from 100ft.  Can you come up with *any* method (other than the one listed in part c) for finding H? ",43xron,t3_43xron,pikapikaapikachu,,Comment,1,0,1
czm0mgu,2016-02-03 01:46:11-05:00,MB1211,,I believe H* is the maximum height from which to start searching from. You have the minimum (0) because 0 is where the plate does not fall and could not possibly break. You need an upper bound to start from. So first step is checking if a plate drops from 50...does that help?,43xron,t3_43xron,pikapikaapikachu,,Comment,1,0,1
czmqooe,2016-02-03 16:14:12-05:00,solo1qwerty,,"H* is the upper limit, so there is no need to drop the plate from a height higher than H\*. 

Taking into account that the limit you are searching is a number between 0 and H*, at which height H you would test dropping the plate at first?  Binary search suggests to start looking at the middle of the range of possible values. If it breaks at this initial height which height you will choose next? You have to start again searching with the same algorithm but you will have reduced the options to half.

If you use binary search you will find eventually the limit at which plates stop breaking (low=mid=high in the explanation of your book).",43xron,t3_43xron,pikapikaapikachu,,Comment,1,0,1
43wcgq,2016-02-02 16:16:33-05:00,beatbrot,How do things like ZIP and rar work?,How can they actually shrink files without losing information?,,,,,Submission,24,0,24
czlh38p,2016-02-02 16:29:43-05:00,zombarista,,"Zip works with a technique called Huffman encoding which is a tree that optimizes for sequence frequencies.  Read about it on Wikipedia: https://en.wikipedia.org/wiki/Huffman_coding

RAR works differently, and it's a little difficult to explain, but this page does a nice job: http://www.rarlab.com/technote.htm

They both work mainly by mapping common frequencies of bits.

ELI5: when you reconstitute a compressed file, the decompressor says ""get me chunk 1, then get me chunk 2, then get me chunk 1""

Chunk 1 might be ""haha""
Chunk 2 might be ""that's funny""

so then 1,2,1 it becomes ""haha that's funny haha"".  The full compressed file might look a little like ""1:haha:2:that's funny-121""

The compression ratio sweetens when the compression algorithm is good at picking good chunks.

",43wcgq,t3_43wcgq,beatbrot,,Comment,29,0,29
czljmem,2016-02-02 17:27:20-05:00,Ready_Player_Two,,"To add to the ELI5

If you think of a picture which has a particular section that is completely red for say 20 pixels in a row. Instead of repeating the data required to show red 20 times, you can instead store something like red * 20. 

This can then be applied many times over. ",43wcgq,t1_czlh38p,zombarista,,Reply,15,0,15
czlr13y,2016-02-02 20:42:34-05:00,theobromus,,"Huffman coding is the last step of LZW compression. However, **before the sequence is huffman coded, it is coded in a format that allows it to compress repeated sequences.**

Basically, this code consists of two types of commands:

* it can emit a certain bit

* it can specify how many bits to rewind backwards and how many to copy

So I can say, ""output 'A'"", followed by ""go back 1 byte and copy the next 20 bytes"". Notice that the copying actually can copy the bytes that have just been output by the same command. The result of this is 21 ""A""s in a row. [In my example I've used bytes since it's easier to think about them, but the LZW algorithm works at a bit level]

The set of these commands is then Huffman coded. The lzw algorithm has a standard huffman table and also a way to represent custom huffman trees. The compression algorithm has to decide how to code a certain sequence of commands, i.e. when to construct new huffman trees, etc. There is also some ambiguity in how to far to search backwards to try to find repeated stuff.",43wcgq,t1_czlh38p,zombarista,,Reply,10,0,10
czljxi9,2016-02-02 17:34:44-05:00,DooberBooberDoo,,This just gave me a quick flashback of doing huffman encoding and decoding in my Discrete Structures class. Makes me appreciate the program doing all the work for me.,43wcgq,t1_czlh38p,zombarista,,Reply,5,0,5
czlk2ie,2016-02-02 17:38:11-05:00,zombarista,,"I remember it taking 3-4 minutes to perform my single-threaded huffman encoding on a couple megs of text, and I really appreciate modern archive utilities because of it.",43wcgq,t1_czljxi9,DooberBooberDoo,,Reply,6,0,6
czms8xd,2016-02-03 16:47:51-05:00,lordvadr,,"Pretty much what other people said.  It's kinda beyond the scope here to discuss the nitty-gritty details of how various compression algorithms work--bookshelves have been written and PhD's have been given out, but more or less, they all look for repeated patterns, and encode them as a sequence of patterns, and where they appear.

Here's the example that was given to me.  You could make a very simple compression algorithm that compresses based on how many times a character repeats.  For simplicity sake, let's say our compression algorithm only works on [0-9a-f] so we don't have to worry about special characters.

Then take the following

    000000000011111110000000AAAAAAAAAABBBBBBBBAAAAAAA77777

Which could compress to:

    0{9}1{6}0{6}A{9}B{7}A{6}7{4}

This is obviously a very naive algorithm where even english text would get larger as a function of compression, but you get the idea.  They get A WHOLE LOT MORE COMPLICATED than that.

From a CS standpoint, it's about entropy.  A perfectly random file will not compress under any algorithm.  This is of particular interest to things like images or audio.  Run a wav file through even the best compression and you just won't get much compression because audio and video already looks like random data, which is why they have specialized compression algorithms that looks at the data differently.  For arguments sake, images are split up into brightness and colors, and those compressed separately.  Audio is essentially split up into tones.  For example a sine wave of a particular frequency could be an endless series of chaning data, or simply sin(f*t).

The point being is that the compression is done without losing any information by more efficiently representing low-entropy parts of the file with higher entropy data. ",43wcgq,t3_43wcgq,beatbrot,,Comment,2,0,2
43vgh2,2016-02-02 13:14:00-05:00,karmicnerd,What do I learn to stay with (developer) world ?,"I'm CS major, I know OOPS Concepts and Java and a little bit C# .net. I have hands on of C# but the company I worked for had their own framework so I didn't learn much there, left that and joined other one where I'm again put on .net and UI/UX both of which I hate. What technologies should I learn ? What are some technologies that are worth mastering that would give me the satisfaction of being a developer and also earn a decent salary ?
",,,,,Submission,1,0,1
czlgfql,2016-02-02 16:15:36-05:00,videoj,,">What technologies should I learn ? What are some technologies that are worth mastering that would give me the satisfaction of being a developer and also earn a decent salary ?

This is such an open-ended question that there is no real answer.  Nobody here knows what you would find satisfying  and what you would hate.    

Start with your university professors.  Ask them about their specialty and what they find interesting about it.  If it sounds interesting, ask them about jobs in the field.

Try a [programming multi-reddit](https://www.reddit.com/user/frisky_business2/m/programmingandtech)  and watch for interesting posts.  Sites like slashdot.org and https://news.ycombinator.com/ also have news that may help you find a  field you're interested in.



",43vgh2,t3_43vgh2,karmicnerd,,Comment,1,0,1
43ssf6,2016-02-02 00:17:03-05:00,Wowsoysauce,how much does CS actually need cacl 1... calc 2... calc 3... and 2 years oh physics?,these are the major requirements at my uni. currently in calc 1 and its killin me... ,,,,,Submission,9,0,9
czkrzz5,2016-02-02 02:19:44-05:00,VideotapeReturn,,"If you start getting into engineering applications of cs it'll probably come up. If you do any sophisticated probability all 3 will come up. When you learn big oh notation you do a lot of infinite series and limits which is precalc, but I've bounded functions with integrals and used lhopitals for limits. I've programmed optimization solutions using some calc 1 stuff. In general it isn't guaranteed to be important but it can be and when it does you'll probably wish you knew it. 

Physics is pretty whatever. Electrical engineering uses electromagnetism especially when looking at things like wire capacitance, resistance, etc which could come up in computer engineering. If cs is in the engineering school it'll probably have physics requirements because of the schools requirements. ",43ssf6,t3_43ssf6,Wowsoysauce,,Comment,8,0,8
czkz8c5,2016-02-02 09:17:33-05:00,donghit,,"In grad school currently, and calc has come up quite a bit.  Lots of Linear Algebra and statistics too.",43ssf6,t3_43ssf6,Wowsoysauce,,Comment,4,0,4
czkuxws,2016-02-02 05:33:00-05:00,the_omega99,,"Those physics classes are usually requirements simply for a general intelligence and diversification sort of thing. Most degrees require them. It's not just CS. Very likely you have the option of other classes than physics. Myself, I could have taken bio or chem instead of physics.

Anyway, calculus definitely came in handy, although it's not that important. I didn't use  all of it. AI, particularly my graphics class, utilized some calculus for theory. As well, my stats class (which isn't really CS itself, but also has applications in CS) utilized calculus heavily. A rigid probability theory class will require multiple integrals.

I actually had only taken calc 1 and 2, so had to figure out multiple integrals and partial derivatives myself (which are normally part of calc 3). While calc 2 was an important part of being able to understand these concepts, I didn't utilize most of my calc 2 knowledge to date. I can see some possible applications in graphics work, though.

In general, calculus is only used in some specialized areas of programming. This is why you'll hear so many programmers say that they don't need calculus. It's because their specific field doesn't utilize it. But if you do anything related to AI (computer vision, machine learning, etc) or cutting edge graphics, you can expect calculus to matter a lot more. And even within those fields there's plenty that can be done without touching calculus. So it's not *that* important. Yet quite important if you want to truly understand things. And above all, universities use calculus classes as the math eliminator -- they get rid of all the students who aren't smart enough. ",43ssf6,t3_43ssf6,Wowsoysauce,,Comment,3,0,3
czkvyz2,2016-02-02 06:46:27-05:00,srcreigh,,"They're sort of useful in training your problem solving skills, although I prefer pure math (""discrete math"", analysis, abstract algebra) for training how I think.

If your degree requires them, then there's not much point to worrying about it. Just get them over with and go onto courses you like. If you can drop them and you don't like them then I would---you won't be missing out on much in terms of industry skills!",43ssf6,t3_43ssf6,Wowsoysauce,,Comment,2,0,2
czkypcm,2016-02-02 08:58:36-05:00,artillery129,,"They are not related at all to being a software developer, they are mildly related to being a computer scientist.",43ssf6,t3_43ssf6,Wowsoysauce,,Comment,2,0,2
czl5h14,2016-02-02 12:06:57-05:00,complich8,,"When I was doing my CS degree I'd have rangey meta discussions with my academic adviser, and this came up once or twice.

The way my adviser described it was ... it's probably not that you're going to be doing any multivariate calculus in your day-to-day CS, but a lot of times the ability to work with abstract math (like calculus and linear algebra) and not get too turned around is a reflection of this ""mathematical maturity"" concept.

CS is extremely abstract. In the ""programming"" side, you're creating things, but those things are still abstractions. In the ""theory"" side you're doing much more abstracted things than that (like, proving that all problems of this type can be reduced to problems of that other type). 

Calculus is both an indicator of that mathematical maturity, and a way to develop it. Physics will also help you develop your ability to break down concrete-but-unworkable things into abstract-but-useful parts.

I'd recommend really trying hard to get those. Put in extra time if you need to, seek office hours explanation, do the exercises, you'll be better off for it when you crash into hard theory stuff. ",43ssf6,t3_43ssf6,Wowsoysauce,,Comment,1,0,1
czl6hga,2016-02-02 12:31:07-05:00,Bottled_Void,,"Computer Science teaches you a techniques and concepts. Different types of maths and physics are great tools for solving problems.


But I'm confused. You say they're requirements of your course... So do them. Or don't do the course.",43ssf6,t3_43ssf6,Wowsoysauce,,Comment,1,0,1
czl9d76,2016-02-02 13:37:51-05:00,Aquifel,,"You're pretty lucky, when i was going to school, they used calculus as fillers, we had 6 calculus classes for a 4-year CS degree, although one didn't follow the standard naming format, so, the actual highest was calculus 5.  I would have loved to get a break with physics.

I can't say that i've ever used calculus post-college but, i honestly can't remember a single thing about the classes i took so, there's a chance i'm using it without knowing it.",43ssf6,t3_43ssf6,Wowsoysauce,,Comment,1,0,1
czl1yrk,2016-02-02 10:39:20-05:00,Human_Transmutation,,What you learn in physics and calculus won't help you be a better programmer. But these classes are supposed to teach you critical thinking which will help you become a better programmer. But it is very doubtful triple integrals and magnetic fields are going to show up in your job duty. ,43ssf6,t3_43ssf6,Wowsoysauce,,Comment,0,0,0
czkpv31,2016-02-02 00:49:57-05:00,brennanfee,,"Computer Science needs those things very much.  However, computer engineering a.k.a. programming... not very much at all.",43ssf6,t3_43ssf6,Wowsoysauce,,Comment,-5,0,-5
czks05j,2016-02-02 02:20:01-05:00,BluntnHonest,,Computer engineers are the people who make computer hardware. ,43ssf6,t1_czkpv31,brennanfee,,Reply,8,0,8
czl3d0x,2016-02-02 11:15:29-05:00,brennanfee,,"> Computer engineers are *sometimes* the people who make computer hardware.

FTFY

There are sadly no standards in my industry.  I have had that title twice and didn't/don't deal with hardware other than what runs what I write.",43ssf6,t1_czks05j,BluntnHonest,,Reply,1,0,1
czkz8r4,2016-02-02 09:17:56-05:00,donghit,,You mean Electrical Engineers?,43ssf6,t1_czks05j,BluntnHonest,,Reply,0,0,0
czlaliw,2016-02-02 14:05:45-05:00,BluntnHonest,,"Computer engineers  are like specialized electrical engineers: 

https://en.m.wikipedia.org/wiki/Computer_engineering",43ssf6,t1_czkz8r4,donghit,,Reply,1,0,1
czkuya9,2016-02-02 05:33:48-05:00,the_omega99,,"You seem to be mixing up ""computer engineers"" with ""software engineers"". Big difference.",43ssf6,t1_czkpv31,brennanfee,,Reply,5,0,5
czky6bt,2016-02-02 08:38:20-05:00,nighthawk648,,Even then software engineers do more than just program. There arent softwear engineer processes for a reason ...,43ssf6,t1_czkuya9,the_omega99,,Reply,3,0,3
czky9q4,2016-02-02 08:42:03-05:00,the_omega99,,"True. Although arguably most people who would describe their job as ""programming"" are doing software engineering.",43ssf6,t1_czky6bt,nighthawk648,,Reply,3,0,3
czl3bbp,2016-02-02 11:14:18-05:00,brennanfee,,"True, but there really are no standards for titles as related to the jobs people do.  My industry has a shocking lack of standards which is why I put the ""a.k.a"" in there.",43ssf6,t1_czkuya9,the_omega99,,Reply,3,0,3
43rovd,2016-02-01 19:51:11-05:00,Ipuncholdpeople,What class to enroll in?,"So I'm currently taking an intro computer science class that is really simple and uses python. Next semester I have to choose between c# or Java, and I was wondering which is a good choice. I eventually want to learn C++ to make games, but am just looking for the best education I can get at the moment.",,,,,Submission,11,0,11
czkiu81,2016-02-01 21:45:33-05:00,PastyPilgrim,,"Are they identical classes apart from the language? If it's the same class taught in different languages (most likely due to professor preference) then it really doesn't matter which you pick. Specific languages hardly ever matter because, in computer science, the language is just the tool that you use to learn concepts. Learning to speak with computers (using mathematics) is the skill / language that you're learning, and the programming language you use is just a dialect that should be easy to pick up once you know the language.

Anyway, in my experience, Java is far, far more widespread than C#, but C# is a newer language that I find to be a bit more enjoyable to use than Java. Both languages are (*very*) similar to one another, but neither language is that similar to C++ (don't let C#'s name mislead you, it's more like Java than C or C++). I'd have a hard time recommending one over the other, because while I'd rather use C#, I think it's worth learning Java regardless because it will come up so often in your future. In that respect, Java is probably the better choice because I think it's more mandatory to know than C#.",43rovd,t3_43rovd,Ipuncholdpeople,,Comment,12,0,12
czkj5o8,2016-02-01 21:53:17-05:00,Ipuncholdpeople,,Ok thanks for the input. I'm pretty sure they are equivalent courses since the school I will be transferring to takes either for a certain requirement. I think I'll do Java just so I have have the flexibility of cross-platform programs. Thanks again!,43rovd,t1_czkiu81,PastyPilgrim,,Reply,3,0,3
czkjkpn,2016-02-01 22:03:15-05:00,PastyPilgrim,,"C# can also be cross-platform but it's not quite like Java, which uses a virtual machine to allow for cross-platform support. Even though C# is predominantly Windows (because Microsoft develops C#), Linux has a great C# compiler (mono). All of my computers are Linux and I've never had an issue when using C#.",43rovd,t1_czkj5o8,Ipuncholdpeople,,Reply,4,0,4
czkfg6h,2016-02-01 20:21:21-05:00,mcowger,,"If its just for education - it wont matter that much.  They are both reasonable choices, although C# will mostly limit you to Windows.",43rovd,t3_43rovd,Ipuncholdpeople,,Comment,4,0,4
43rntf,2016-02-01 19:44:34-05:00,theoryofcomp,Help! I need to write a MIPS assembler in C and I don't even know where to start from...,"http://www.cs.fsu.edu/~carnahan/cda3101/CDA3101_Project1.pdf

Can anyone give me any ideas or help ?",,,,,Submission,2,0,2
czkemtc,2016-02-01 20:00:11-05:00,dabbertorres,,"For future reference, /r/learnprogramming is probably a better place for this kind of question.

Anyways, in my experience, it won't really matter. The two languages are similar enough, that knowing one makes it very easy to learn the other.

You'll find people telling you to choose one over the other for many reasons (both valid and invalid), but realistically, at this point in your education/career, I don't think it will make much of a difference.

I would go look up some code samples of both, and go with the one you like better.",43rntf,t3_43rntf,theoryofcomp,,Comment,5,0,5
czlzvfu,2016-02-03 01:13:19-05:00,i_invented_the_ipod,,I'm pretty certain you didn't mean to post this comment here...,43rntf,t1_czkemtc,dabbertorres,,Reply,1,0,1
czkjegv,2016-02-01 21:59:10-05:00,PastyPilgrim,,"I'm not sure why you don't know where to start though, the class and material looks advanced enough that you shouldn't be completely lost on how to at least start the project. Plus, the project doc indicates that you're only using a pretty limited subset of MIPS's ISA. I'm assuming that you know both MIPS and C, right? If you've never used MIPS (or C), then you're in for a real bad time.",43rntf,t3_43rntf,theoryofcomp,,Comment,2,0,2
czlg45u,2016-02-02 16:08:43-05:00,Unpopular_POV,,"Go to class. Pay attention.
",43rntf,t3_43rntf,theoryofcomp,,Comment,2,0,2
czkjsow,2016-02-01 22:08:39-05:00,craiig,,"Looks like fun! You can think of assemblers as basically translators from one format to another. There's not a lot of magic you'll need to do other than reading the input, and then outputting the new format. The trick is to understand the input and output format well enough to be able to write the mapping. Just read over the document a few times and maybe go to an office hour to ask some clarifying questions.",43rntf,t3_43rntf,theoryofcomp,,Comment,1,0,1
43ou0p,2016-02-01 10:09:36-05:00,cstechyguy,Suggestions for Valentine's Day gift,"Hey /r/AskComputerScience! I am trying to find something interesting and educational to suggest to my wife for a Valentine's Day gift, our budget for each other is around $150.  My primary interests that I'd like to begin my venture into are robotics, machine learning, and electronics. Do the fine redditors here have any good suggestions for kits, books, etc to get me started in any of those fields?

My initial ideas were either an Arduino starter kit, or some Python machine learning books, or a RedBot kit, but maybe there are much better options out there! Thanks everyone for your input :) ",,,,,Submission,9,0,9
czjxkvg,2016-02-01 13:05:44-05:00,anamorphism,,https://www.adafruit.com/product/68,43ou0p,t3_43ou0p,cstechyguy,,Comment,4,0,4
czjyqcb,2016-02-01 13:32:28-05:00,blasbido,,A programmable vibrator.,43ou0p,t3_43ou0p,cstechyguy,,Comment,2,0,2
czjyutn,2016-02-01 13:35:21-05:00,cstechyguy,,"No haha, the gift is for me, I meant to come up with a few things to give her an idea of what I'd like. I already have her gift picked out haha the xbox elite controller...simple I know :( ",43ou0p,t1_czjyqcb,blasbido,,Reply,5,0,5
czjvy1x,2016-02-01 12:27:20-05:00,kakalax,,Stop bragging here bro :D,43ou0p,t3_43ou0p,cstechyguy,,Comment,2,0,2
43ohhp,2016-02-01 08:43:19-05:00,Bractude,Looking for twitter datasets for my bachelorproejct,"Hello everyone,

For my bachelorproject (thesis, however you want to call it) I want to do research into Twitter and the way their users treat privacy. Ideally, I'd like to look at the way users give away information (for example when their profiles reveal a lot of information, or if they tweet their geolocation a lot) and if it can be linked to certain groups (they're more likely to follow specific twitter accounts, they live in cities etc). I'm looking for datasets right now, but every Twitter dataset I can find pretty much only has location, tweet, username and date, and none of them have user profiles or what Twitter accounts the users follow other than the one their tweet is directed at.

Now, I am pretty new to datamining so I'm not sure if there's an easy way to link this data. If so, are there any pointers you could give? And in either case, do you know any Twitter datasets with info extending further than just single tweets?

Thanks in advance!

Edit: Whoops @ typo in the title",,,,,Submission,1,0,1
czjpcta,2016-02-01 09:32:14-05:00,luv2lrn,,"You may have seen this already, but just in case, here is a python tutorial showing how to collect data from Twitter using their streaming API: http://socialmedia-class.org/twittertutorial.html",43ohhp,t3_43ohhp,Bractude,,Comment,1,0,1
czjpzh8,2016-02-01 09:52:29-05:00,Bractude,,"The amount of data id be able to get would be inferior to the data sets already in existence, and I don't really have the time to do it. Thanks for the suggestion though",43ohhp,t1_czjpcta,luv2lrn,,Reply,1,0,1
43mmsg,2016-01-31 22:36:27-05:00,NetworkPanda,Need help deciding how to not get obsolete in the Tech industry,"To provide some background, I am a QA Engineer working in the Bay Area in a large networking company. I have a Master's in Electrical and Computer Engineering from a reputed University in the US.

My work involves feature testing networking equipment. I write scripts in Tcl and do some manual testing in a Linux based environment. I want to keep myself updated in the industry so as to not get obsoleted. But I am unsure what to focus on. I know Tcl isn't used a lot in the industry.

So here's my question - How do I make sure I don't pigeon hole myself into writing scripts all my life, and for when the layoffs come, what should I have prepared?",,,,,Submission,4,0,4
czjhxod,2016-02-01 02:38:22-05:00,SneakingNinjaCat,,"The common answer for such questions, when asked by software engineers is: ""Get involved in github.""

In your case, as a software engineer, I might say ""Build cool arduino toys and post youtube videos...""",43mmsg,t3_43mmsg,NetworkPanda,,Comment,5,0,5
czjdmkl,2016-01-31 23:37:29-05:00,mrallen77,,I think this is everyone's fear in the tech industry. Unfortunately I don't have a good answer lol,43mmsg,t3_43mmsg,NetworkPanda,,Comment,3,0,3
czjew5h,2016-02-01 00:21:27-05:00,NetworkPanda,,"I have been reading about not getting obsolete or keeping yourself useful so much lately. Not sure what exactly that entails.
",43mmsg,t1_czjdmkl,mrallen77,,Reply,1,0,1
czjfc8x,2016-02-01 00:38:37-05:00,unicorn_pants,,"what are you interested in? If manual testing, become a domain expert in whatever you're testing. If automation, learn more about framework and test library design. ",43mmsg,t3_43mmsg,NetworkPanda,,Comment,3,0,3
czjfwr9,2016-02-01 01:01:43-05:00,NetworkPanda,,"Thanks! I am interested in the networking industry. I wonder if I should move into a network consultant role (which requires domain expertise) or as a developer (which needs more coding than just Tcl scripting). I like QA, but I have heard from so many people that it gets really difficult to move into development once you have been a QA Engineer.

Also, say I get laid-off tomorrow, how does a QA Engineer take the skills they learnt in one company to another? Chances are the new company has a completely different framework altogether.",43mmsg,t1_czjfc8x,unicorn_pants,,Reply,1,0,1
czjtnaz,2016-02-01 11:31:45-05:00,i_invented_the_ipod,,"> I have heard from so many people that it gets really difficult to move into development once you have been a QA Engineer.

This varies a bit depending on location and industry. There's a fairly-strong stigma around this in Silicon Valley, for instance. One thing that can work in your favor if you want to switch to development is demonstrated competence. Develop some portfolio pieces on your own time that you can show to potential employers. Make them publicly available (source on Github, and available in the appropriate App Store, or hosted somewhere, if it's a web app. ",43mmsg,t1_czjfwr9,NetworkPanda,,Reply,3,0,3
czjxb1f,2016-02-01 12:59:17-05:00,NetworkPanda,,Thanks! Yeah I am not sure how to leverage my background in C in this case though. I have considered making a move to Development in my current company after I know the product well enough as a QA.,43mmsg,t1_czjtnaz,i_invented_the_ipod,,Reply,1,0,1
czjwc1b,2016-02-01 12:36:33-05:00,unicorn_pants,,"Frameworks and products will differ from company to company, but methodologies to testing won't. 

I started out in manual QA and now I'm in an automation development role somewhat by choice. I sort of did what /u/i_invented_the_ipod suggested and started working on side projects to demonstrate my development skills, which actually helped with landing my current job. ",43mmsg,t1_czjfwr9,NetworkPanda,,Reply,1,0,1
czjxdpa,2016-02-01 13:01:02-05:00,NetworkPanda,,"Thanks! Yeah starting to read up about automation frameworks. The company I work at has been around for a really long time, so pretty much all the framework is in place. But I guess it wouldn't hurt to get to know more about it.",43mmsg,t1_czjwc1b,unicorn_pants,,Reply,1,0,1
czjj6yd,2016-02-01 03:55:04-05:00,droidballoon,,"On a positive note, you seem to be fluent in Tcl. That's an expert role! 

My advice is that you pick up another scripting language such as python. You would likely feel at home since there are some similarities in the syntax. You could automate your tcl tests from python for instance. 

You're still a professional QA-engineer, your resume can benefit from experience with some more languages/ecosystems. 

",43mmsg,t3_43mmsg,NetworkPanda,,Comment,3,0,3
czjgn7p,2016-02-01 01:33:13-05:00,ilovelsdsowhat,,"One option is to try and find something useful and specialized in your field that you can learn. Being proficient in something that isn't super common, but essential in certain cases, will make you a very valuable employee. It would also increase your marketability if you need to find a new job. I wish I had a more comprehensive and specific answer for you, but if there is something in your field that fits these criteria, then jump on it. Good luck.",43mmsg,t3_43mmsg,NetworkPanda,,Comment,2,0,2
czjthet,2016-02-01 11:27:41-05:00,i_invented_the_ipod,,"The best thing you can do to stay relevant is to make sure you're at least passingly-familiar with the hot new technology. You may love what you do now, but you're right that TCL is a bit of a niche technology these days. Also, and I'm sure this has occurred to you - a whole lot of QA (and development) in the network infrastructure industry is moving overseas.

If you want security in the face of layoffs, you need to be able to switch to something web- or mobile-related. If you want the single best thing you can do:

1. Buy a Mac
2. Learn to write iOS applications
3. Publish an app on the App Store, and publish the source to Github

If you want to stay in QA, then you're going to want to be an expert in automated testing. Manual testing will inevitably migrate to wherever labor is the cheapest, which is probably not where you are. 

There are a ton of web testing frameworks. Experiment with Selenium, Jasmine, and EggPlant, maybe?",43mmsg,t3_43mmsg,NetworkPanda,,Comment,2,0,2
czjxlnd,2016-02-01 13:06:14-05:00,NetworkPanda,,"Thanks! Yes I have already started looking up stuff on Web development. What are your views on jobs that involve programming in C? I am aware of the scores of web based projects available today. There isn't as much work in C thought, right?",43mmsg,t1_czjthet,i_invented_the_ipod,,Reply,1,0,1
43e2gj,2016-01-30 08:30:26-05:00,lastmoron,What is the technology you are excited about that will be or is available?,Well the title says it all.,,,,,Submission,16,0,16
czhirco,2016-01-30 08:34:11-05:00,None,,Better batteries. ,43e2gj,t3_43e2gj,lastmoron,,Comment,11,0,11
czhlis2,2016-01-30 10:31:44-05:00,TheGrue,,"1) Batteries: I'm with bullshitBill.

Mobile systems are constrained by processor/storage, bandwidth, and power.  Bandwidth is getting better everywhere.  Processor/storage is getting better, and (if you're not trying to do a mobile distributed type thing) can sometimes be offloaded via the network to datacenters/'the cloud'.  Power requirements are still the big limitation to mobile tech.  Longer lasting, faster charging mobile power supplies are a big deal.


2) Augmented Reality:

Obviously we have AR, but a lot of it is toy stuff, workshop stuff, etc.  There are some very neat things that have been done with AR with regards to stuff like battlefield medicine.  This isn't shocking, the military has cash, and will spend it to get the product they need.  As per usual, that tech is going to filter down to the civilian/commercial markets.  The requirements for ubiquitous, inexpensive AR?  See point 1.

3) Rapid Prototyping/Printing:

Again, something that we already have that is going to move forward more and more quickly.  3D printers are out there, and they work.  Most of them are realistically speaking, toys.  But progress is being made on making print quality, materials, etc. better on low-end commercial and consumer printers.  This isn't going to 'revolutionize the supply chain', probably.  At least not yet.  But think about something like Walmart.  They have 15 different SKUs of turbo-encabulator in the automotive department.  It's just a piece of plastic, but it has a different shape depending on the make/model of your car.  They have 3 of each SKU, so 45 items taking up shelf space, having to be inventoried, stocked, and they probably sell one of them a week.

Or they could have printer feedstock, and a catalog of designs.  Even if the part itself is more expensive to make in terms of time and even materials, they can sell it for cheaper - because there's no shipping, stocking, packaging, etc.  It comes off the same roll of ABS as a bunch of other plastic crap they need to print.

Right now, low end commercial printers are great for things you need made out of plastic where the tolerances aren't that tight and the part strength isn't that important.  That's a big limitation, but there are a ton of things out there that meet that description.

There's probably a few more, but I haven't had my second cup of coffee yet.",43e2gj,t3_43e2gj,lastmoron,,Comment,8,0,8
czk6kfv,2016-02-01 16:33:47-05:00,lneutral,, #ImWithBullshitBill,43e2gj,t1_czhlis2,TheGrue,,Reply,2,0,2
czi8vkh,2016-01-30 22:51:30-05:00,whywhisperwhy,,"Links to any of that AR tech you think will trickle down, please?",43e2gj,t1_czhlis2,TheGrue,,Reply,1,0,1
czi9erb,2016-01-30 23:09:48-05:00,TheGrue,,"[This](http://www.futurity.org/military-surgery-telecommunications-989142/) was the first link that came up when I looked for the augmented reality battlefield telemedicine thing.  I have a friend who worked with (not on) a similar (or maybe the same) system as the stateside medical expert. 

The ability to call in a subject matter expert (Teleconferencing) is nothing incredibly new, it's the ability for the expert to do visual mark up on real world objects, and have those notations remain static with respect to the real world, anchored to real surfaces that the local user can then use as guides.

This, combined with advances in whatever we want to call AI these days, will eventually allow intelligent systems to give guidance in a variety of situations to humans 'in the field', thus cutting out the need for a human expert in at least some cases.  ",43e2gj,t1_czi8vkh,whywhisperwhy,,Reply,2,0,2
czi1wy8,2016-01-30 19:09:26-05:00,termi-official,,"1) IPFS/MaidSafe and the whole load of distributed (and decentralized) web technology

2) Next gen C++

3) Vulkan

4) VR/AR

5) WebAssembly",43e2gj,t3_43e2gj,lastmoron,,Comment,3,0,3
czi61v2,2016-01-30 21:17:56-05:00,GiovaniGuizzo,,"I am surprised no one said Self-Driving Cars. I mean, it would be very nice to live in a world of no accidents and no unpleasant drivers. If you want more motivation, see /r/Roadcam.",43e2gj,t3_43e2gj,lastmoron,,Comment,3,0,3
czjh4sd,2016-02-01 01:56:15-05:00,ilovelsdsowhat,,"Having lost my best friend to a car accident, this is something I feel very strongly about, and the fact that the use of self driving cars will be hindered more by people's acceptance of them than by their actual viability is so damn frustrating. In addition to the many industries that are going to lobby against them, a lot of people don't like the idea of driverless vehicles. I've heard people say they don't trust them, that driving is a skill people need to learn, that they don't want to give up driving themselves, and a lot of other objections to self driving cars. I don't know what percentage of people oppose them, but if it's a significant amount then it will be a lot easier for the lobbyists to prevent self driving cars from being made legal.   
   
Even so, I do think that one day we won't have even one driver left on the road. Going from being able to buy a driverless car to everyone having one is going to take a while. I hope we start changing over soon.",43e2gj,t1_czi61v2,GiovaniGuizzo,,Reply,2,0,2
czhpufj,2016-01-30 12:47:22-05:00,RufusStJames,,"Implantables and cybernetics. I'll be first in line when we can eventually *electively* replace limbs with more capable and multipurpose tools. 

Assuming I'm still alive. ",43e2gj,t3_43e2gj,lastmoron,,Comment,2,0,2
czhrrsz,2016-01-30 13:44:10-05:00,LickableLemon,,[Storj](http://storj.io/),43e2gj,t3_43e2gj,lastmoron,,Comment,2,0,2
czhrww8,2016-01-30 13:48:21-05:00,HiDeHiDeHiDeHi,,3D printing anything.,43e2gj,t3_43e2gj,lastmoron,,Comment,1,0,1
czhualh,2016-01-30 14:59:09-05:00,UncleMeat,,"Privacy preserving databases. Medical research is constrained by HIPPA and other privacy laws. Databases that preserve differential privacy allow for access to huge amounts of medical data without worrying about the privacy implications.

The changing nature of the certificate inrfrastructure. Right now it's a pain in the ass to get your website to operate using HTTPS correctly. CDNs have started giving all of their clients useful certs and tools like Lets Encrypt promise to make certs available to everybody. This is a major improvement to the security infrastructure of the internet.

Practical SMT solvers. Proving properties about programs is hard but we've finally reached a point where SMT solvers are practical for real applications. This allows for automated proofs of all sorts of useful properties of programs and can revolutionize software development.",43e2gj,t3_43e2gj,lastmoron,,Comment,1,0,1
czjh78o,2016-02-01 01:59:40-05:00,ilovelsdsowhat,,"Not a specific technology, but increased speeds on cell networks. If I could reliably stream HD on my phone I would gladly pay a higher bill. If there are no data caps, of course.",43e2gj,t3_43e2gj,lastmoron,,Comment,1,0,1
43dhm0,2016-01-30 04:39:15-05:00,seefatchai,Deep learning: end of programming jobs?,Man I thought I was going to be safe...,,,,,Submission,1,0,1
czhsmy9,2016-01-30 14:09:48-05:00,794613825,,"Nah. Training a Neural Network is a really difficult thing, if for no other reason than not having a quantifiable value to train it on in most cases. ",43dhm0,t3_43dhm0,seefatchai,,Comment,2,0,2
czhfsl0,2016-01-30 05:07:34-05:00,Rikkety,,I'm very skeptical.,43dhm0,t3_43dhm0,seefatchai,,Comment,1,0,1
43bq3f,2016-01-29 19:21:29-05:00,baldura49,I want to learn to do proper documentation for a project .What books or sites do you recommend me to look up?,,,,,,Submission,1,0,1
czh696f,2016-01-29 21:38:49-05:00,wolvo,,It might be helpful to know what type of project? Who is your target audience?,43bq3f,t3_43bq3f,baldura49,,Comment,1,0,1
czha0sq,2016-01-29 23:53:39-05:00,baldura49,,"I don't have a particular case  to follow , im just asking  becouse I'm studying my major in CS   and I feel the need to improve in that area.",43bq3f,t1_czh696f,wolvo,,Reply,1,0,1
czhc8y6,2016-01-30 01:27:39-05:00,lneutral,,"The reason that's useful is different kinds of products are often documented differently. Different companies or teams may also have individual standards. 

On Java, for example, code is often documented with special comment blocks used with a system called Javadocs that generates HTML pages for your classes. For C++, there are systems like Doxygen. Some teams prefer to use wikis, especially if they keep their code on Github or a similar website. UML is often used for diagramming out systems. 

There's no one-size-fits-all for this, but if you comment your code consistently, noting things like arguments and outputs from functions, special behaviors, and class relationships, you'll find that when the time comes, you can adapt to whatever the requirements are. ",43bq3f,t1_czha0sq,baldura49,,Reply,3,0,3
43bk1g,2016-01-29 18:41:22-05:00,BenRayfield,Whats so hard about making many computers act as one big computer?,,,,,,Submission,6,0,6
czh204y,2016-01-29 19:23:05-05:00,lneutral,,"Sharing resources is difficult, especially memory. Timing, too, is hard, because the computers have to communicate with each other. 

Finally, distributing the solution to a problem is not always easy. Computer programs are done one step at a time (per thread), and just adding more processors would be like trying to make nine women have one baby in a month. ",43bk1g,t3_43bk1g,BenRayfield,,Comment,16,0,16
czh892i,2016-01-29 22:48:08-05:00,okmkz,,"I had one problem, then I tried to use concurrency: two now I problems have",43bk1g,t1_czh204y,lneutral,,Reply,24,0,24
czh92i9,2016-01-29 23:17:52-05:00,zefcfd,,golden.,43bk1g,t1_czh892i,okmkz,,Reply,2,0,2
czh751z,2016-01-29 22:09:02-05:00,tRfalcore,,"a fair bit of problems or work (or parts of them) can't be worked on in parallel and must be solved in serial.

Also at some point you spend more time breaking up problems and sharing arguments and then collecting results than you save by parallelizing the problem over many computers.",43bk1g,t3_43bk1g,BenRayfield,,Comment,5,0,5
czh950i,2016-01-29 23:20:27-05:00,meltingice,,"TL:DR latency, concurrency, and consensus ",43bk1g,t3_43bk1g,BenRayfield,,Comment,3,0,3
czhbo8d,2016-01-30 01:00:43-05:00,NullEgo,,"Look up the Byzantine generals problem.

I send you a message to tell you meet me somewhere in a week but to reply so I know you will be there otherwise I won't show because I am very busy.  You reply saying you will show but you are also busy and need to know I will show so you tell me to reply with confirmation.  But if I reply I will also have to ask for confirmation etc ad nauseam.  There is no way for us to synchronize when messages between us could be lost or delayed (e.g. network packet loss).  This is just one of many problems that make distributed computing hard.",43bk1g,t3_43bk1g,BenRayfield,,Comment,3,0,3
czjhdmy,2016-02-01 02:08:34-05:00,ilovelsdsowhat,,"Could you help me out? I'm having trouble understanding why that keeps you from synchronizing. If one processor runs part of a code and then waits for the go ahead from another processor. That processor runs a different part and when it's done sends the go ahead signal. I know that doesn't ensure perfect synchronization, but would that still be useful? ",43bk1g,t1_czhbo8d,NullEgo,,Reply,1,0,1
czh91o1,2016-01-29 23:17:01-05:00,zefcfd,,"the reason your computer is fast is because of the extremely low latency and reliablilty of your commuincation busses, and the lack of an entire network protocol in between two processes. when you switch processes on a single system, you just context switch. On a distributed system, you need to figure out how to run them in parallel and communicate all of this stuff.

also, when you take many computers and interconnect them with network fiber, you run into much more difficult issues (like the speed of light). because of this, communication between machines becomes a bottleneck that prevents us working two processes on a distributed system in the same way we'd work with two processes on a single system.

tl:dr - interprocess communication is much slower across distributed systems: timing (in cases of geographical distribution), fault tolerance, network overhead, network failure, etc..., are some of the things that makes it slower/harder. so you could technically distribute an operating system or file system across many computers (there is a lot of research about these things going on right now), but to my knowledge, its not practicle (yet?)",43bk1g,t3_43bk1g,BenRayfield,,Comment,1,0,1
czhu3dw,2016-01-30 14:53:10-05:00,BenRayfield,,"If the network can be trusted to act as ""one big computer"", then either it has some central authority controlling it and we trust that authority (which defeats the purpose of parallel), or each part proves its calculations to eachother (which slows things by computing lots of secureHash andOr many independent parts repeating calculations). Either way theres extra bottlenecks.",43bk1g,t3_43bk1g,BenRayfield,,Comment,1,0,1
czizy78,2016-01-31 17:07:57-05:00,None,,"My understanding that the main problems are:

- Splitting the task into parts. e.g., if step 50 requires step 25 do be completed, that means you have to make sure the machine you assigned to do step 50 doesn't do it before the machine doing step 25 is done. Some problems are such that step n requires steps all steps up to n-1 to be completed, and you can't really distribute that.

- Sharing resources. e.g. if a machine uses a piece of memory to store an intermediate value, you don't want another machine to come in the middle of a calculation and rewrite that intermediate value.

- Reliability of network. The two issues above can only be resolved by machines communicating with each other, but what if you lose information during transfer, or corrupt it?",43bk1g,t3_43bk1g,BenRayfield,,Comment,1,0,1
43bg24,2016-01-29 18:15:37-05:00,twoifihaveto,Simple command line help,"Hello! I am currently taking an introductory coding online course and I needed some assistance in figuring out how to complete this task using commands. Essentially I have to create a directory of music files, within that directory, I must move these music files into subdirectories based on their extension (.mp3, .flac, .wav).

So far i've achieved: mkdir (so e.g. mkdir Sounds /wav creates a directory Sounds, and subdirectory Wav).

the mv is used to move files. And by naming the Directory at the end you can move files to that certain directory.




At this point i've wasted over 5 hours figuring this out. If anyone could explain how can I do this with the least amount of commands possible, that'd be great :D

Will keep updating as a tinker around more.
",,,,,Submission,3,0,3
czh2smn,2016-01-29 19:46:46-05:00,chasecaleb,,"To avoid doing your homework for you 100%, I'll point you in the right direction: look up ""wildcard file globbing""",43bg24,t3_43bg24,twoifihaveto,,Comment,4,0,4
czh80s3,2016-01-29 22:39:59-05:00,wafflestealer654,,Look at the manpages for `grep`,43bg24,t3_43bg24,twoifihaveto,,Comment,-1,0,-1
43acgn,2016-01-29 14:18:06-05:00,studmuffffffin,How do you speak binary?,"So for something like 10101:

Do you say one zero one zero one?  Would you say Ten thousand one hundred one?  Or would you convert it to decimal and say it 21?  Or is there a different way?

I imagine it would be easier to just do the first one for small numbers, but for bigger numbers it would get long.",,,,,Submission,11,0,11
czgt2gw,2016-01-29 15:32:12-05:00,Bottled_Void,,"Depends on the context. Does it correspond to a number or a pattern. How long is it?


For binary number I usually group them up into nibbles (4 bits), since computers will usually store them in multiples of bytes (8 bits) and a nibble corresponds to a hex digit.


So for your number, I'd probably go with zero, zero, zero, one - pause - zero, one, zero, one. If the number of bits were important, I wouldn't add the leading zeros.


I'd usually write it out on a notepad as 0001 0101. Maybe stick | lines around each four or eight bits.


If you need a convenient shorthand, just use hex. Since you don't have to do any complicated arithmetic, each set of 4 bits will go straight into a hex digit. 


If you remember a few hex numbers, it'll make it easier. 0x8 is b1000. 0xF is b1111. 1, 2 and 4 are easy too.


So |0001|0101| is 0x15. If I was shouting this across the office, I'd say 15-hex.",43acgn,t3_43acgn,studmuffffffin,,Comment,11,0,11
czhi02t,2016-01-30 07:50:47-05:00,None,,[deleted],43acgn,t1_czgt2gw,Bottled_Void,,Reply,2,0,2
czhthpy,2016-01-30 14:34:46-05:00,Bottled_Void,,I say fifteen hex.,43acgn,t1_czhi02t,None,,Reply,1,0,1
czgy17d,2016-01-29 17:32:34-05:00,yes_thats_right,,"Are you trying to tell people the binary representation or are you trying to tell people the number that it represents?

If you want to tell them the binary representation, e.g. if you are dictating to them, then I would say *""one-zero-one-zero-one""*

If you want to tell the number which it represents then I would say *""twenty one in binary form""*

General rule: decide what information you want to give, then decide on the easiest way for someone else to understand it.",43acgn,t3_43acgn,studmuffffffin,,Comment,5,0,5
czhi6oe,2016-01-30 08:02:12-05:00,the_omega99,,"Well, you definitively would not say ""ten thousand one hundred one"". That's a decimal thing and leaves confusion for what base we're talking about.

Fortunately, one almost never verbally mentions a binary number. Options are:

1. Speak each digit (""one one zero one"")
2. Speak each hex digit. There's a reason that hex is used to describe binary data -- it's more compact (""one five"")
3. Speak the decimal number and just assume that they can know that they have to represent it in binary. Binary is just a representation, after all. You still think of the number in decimal, typically (""twenty one"")",43acgn,t3_43acgn,studmuffffffin,,Comment,3,0,3
czgr5ha,2016-01-29 14:47:39-05:00,mosqutip,,"Generally you just say each number in turn (your first example). I usually say ""oh"" instead of ""zero"", though, so it would be ""one oh one oh one"".

Calculating what the whole number is without commas is a pain, besides the fact that 10101 is not actually equivalent to 10,101, since it's a base 2 system.",43acgn,t3_43acgn,studmuffffffin,,Comment,2,0,2
czh6t0t,2016-01-29 21:57:35-05:00,BenRayfield,,You make monkey sounds oo oo ah ah oo ah ah ah ah oo oo oo ah This woman does it https://youtu.be/66Q9G_8z1_s?t=31s,43acgn,t3_43acgn,studmuffffffin,,Comment,1,0,1
439475,2016-01-29 09:59:58-05:00,sadelbrid,Thoughts on this recursive runtime complexity?,"I've been trying to wrap my head around recursive runtime problems and I think I've done alright so far. However I'm not even sure how to approach something like this. It may be super simple and I'm just not seeing it. My brain is pretty fried from all of these... Here it is:

    p(n){
        if n <20 ret n^(1/8)
        x = 0, i = n -5
        while i > 10 {
            x += p(i)
            i -= 2
        }
        ret x
    }
    ",,,,,Submission,6,0,6
czgk5j5,2016-01-29 12:07:50-05:00,_--__,,Let T(n) be the runtime of p(n).  Can you come up with an expression for T(n) involving other T(k)'s where k<n?,439475,t3_439475,sadelbrid,,Comment,2,0,2
czgs5p2,2016-01-29 15:11:05-05:00,dandrino,,"Looks like O(n!) at first glance. If you memoize (or refactor into an iterative form) it seems you can make it O(n).
EDIT: well I'm dumb, it's clearly some flavor of exponential, something like O(2^(n/2) )",439475,t3_439475,sadelbrid,,Comment,0,0,0
43716h,2016-01-28 22:36:30-05:00,blood_nja,Making the transition from parse.com,"In the hours since the announcement of the shutdown of Parse.com, I have started attempting to put together a gameplan of how to replace it in all my apps. It is certainly not clear at this point what is the best approach. I have read that amazon provides a similar service, but after a few hours of reading, it is still not at all clear to me how I can create my db and access it via rest calls the way I was doing with parse. User creation and authentication, push notifications, etc etc, all seem to be part of different services, and it seems overwhelming. Maybe amazon is not even the right direction to go in. I guess I'm just looking for some detailed insight from an experienced developer who is also about to go through this transition from parse, and an overview of what your plan is. Thanks",,,,,Submission,6,0,6
czgbmfk,2016-01-29 08:07:26-05:00,roomzinchina,,"Parse has open sourced the Parse server. I'd suggest setting it up on a $5/mo DigitalOcean VM, and just changing the endpoints referenced in your app.",43716h,t3_43716h,blood_nja,,Comment,3,0,3
czgc5rf,2016-01-29 08:29:29-05:00,Haversoe,,You may get more traction for your question by x-posting to /r/programming.,43716h,t3_43716h,blood_nja,,Comment,2,0,2
czg9ix0,2016-01-29 06:15:14-05:00,H3bus,,What services of Parse do you use in your apps? ,43716h,t3_43716h,blood_nja,,Comment,1,0,1
czga26u,2016-01-29 06:47:46-05:00,the_omega99,,You should mention the specific technologies that Parse.com's shutdown leaves you lacking. Otherwise people who have never used Parse.com cannot help you as well.,43716h,t3_43716h,blood_nja,,Comment,1,0,1
czgusrg,2016-01-29 16:12:37-05:00,hs00105,,"It will depend on the app requirement that in which services of Parse you are working on? Also, how much it will impact, if you move your data to another service provider. 
 
Here, we happy to announce that we offer seamless migration from Parse to ShepHertz Backend as a Service. Where we provide complete cloud EcoSystem:

* Marketing Analaytics
* Notification
* Gamification
* User
* Cloud Storage

For migration follow steps mentioned in [Shifting from Parse Services](http://blogs.shephertz.com/2016/01/29/data-migration-parse-services-shephertz-app4/#more-11227) onto App42 and feel free to reach at migration@shephertz.com. ",43716h,t3_43716h,blood_nja,,Comment,1,0,1
czm3j8b,2016-02-03 04:35:49-05:00,babliao,,"You may want to check out [Oursky Parse Hosting](https://parse-hosting.com).We migrate your Parse app to a self-hosted server before the deadline. And we will have the push notification and welcome emails reimplemented :)

[Disclaimer: product manager of Parse Hosting]",43716h,t3_43716h,blood_nja,,Comment,1,0,1
436koh,2016-01-28 20:42:20-05:00,blufox,A question about set-cover,"I have a question about the greedy set-cover algorithm, given [here](http://pages.cs.wisc.edu/~shuchi/courses/787-F07/scribe-notes/lecture02.pdf). Basically, it is shown that the greedy algorithm may not always produce the optimal result, but it is the best possible. My question is, why not go from the other end, removing redundant sets? Is there a name for that strategy? and what is its complexity? (time, and approximation ratio)",,,,,Submission,2,0,2
czggqzb,2016-01-29 10:48:06-05:00,None,,[deleted],436koh,t3_436koh,blufox,,Comment,1,0,1
czgkhkv,2016-01-29 12:15:37-05:00,blufox,,"Essentially,  I take the same strategy but in reverse: Remove the least cost effective sets. In the case of non-weighted set-cover, remove the smallest subset that do not lead to uncovering of elements. If there is a tie, choose randomly.

My phrasing *redundant set* may be the part causing confusion here. I mean removing sets that do not lead to uncovering of elements.

When no further such sets can be removed, we can assert two things: 1) All elements are still covered, and 2) The solution is minimal (in the sense that no subsets of this set can have all elements covered) but not necessarily optimal.",436koh,t1_czggqzb,None,,Reply,1,0,1
czgm16f,2016-01-29 12:50:56-05:00,huck_cussler,,"It is not clear to me how you go about determining whether removing a subset leads to uncovering of any elements.  It can be done, of course, but I don't think it can be done quickly.  How would you do this step?",436koh,t1_czgkhkv,blufox,,Reply,1,0,1
czgn5py,2016-01-29 13:16:11-05:00,blufox,,"Keep counters 1..n for the Universe of n elements. Scan through the entire set of subsets once, and increment the counter for each corresponding element. When this is done, you will have a frequency table of each element. 

Now, for any subset chosen, if any element in the subset has *1* in the frequency table, removing the subset will lead to uncovering of that element. 

When a subset is removed, update the frequency table.",436koh,t1_czgm16f,huck_cussler,,Reply,1,0,1
czjsxlj,2016-02-01 11:13:56-05:00,huck_cussler,,"I agree that would work, and may give a better estimation, though still limited by the O(log n) bound that the pdf talks about.  I believe the cost of what you are proposing is O(n^2).  I'm not sure, but the original approximation algorithm may also cost O(n^2).

So maybe you could get a 'better' O(log n) approximation from this method but it seems you won't be able to get better overall behavior.  I guess it depends really on the application and other implementation details.",436koh,t1_czgn5py,blufox,,Reply,1,0,1
czjwx3x,2016-02-01 12:50:19-05:00,blufox,,"You can't do better than O(log n) for approximation (i.e k log n where k is the original solution), [unless P = NP](https://www.cs.duke.edu/courses/spring07/cps296.2/papers/p634-feige.pdf). Hence, I have no hopes of doing better than the original algorithm in terms of complexity.

However, it [seems](http://cstheory.stackexchange.com/questions/33684/what-is-the-reverse-of-greedy-algorithm-for-setcover) the new algorithm is really even worse in its approximation ratio.",436koh,t1_czjsxlj,huck_cussler,,Reply,1,0,1
czk0th8,2016-02-01 14:20:49-05:00,huck_cussler,,Interesting.  I think I misunderstood that you intended to run this version out of the gates.  I thought the idea was to run the original greedy algorithm and then to go through the resulting sets and pare down where you could do so.,436koh,t1_czjwx3x,blufox,,Reply,1,0,1
434hw2,2016-01-28 13:09:34-05:00,camelslick,Laptop Noise,"My laptop sounds like an airplane engine, I understand that's the AC or cooler. How can I reduce the noise?
",,,,,Submission,0,0,0
czfitq6,2016-01-28 15:17:48-05:00,videoj,,Ask at /r/techsupport,434hw2,t3_434hw2,camelslick,,Comment,2,0,2
czgamab,2016-01-29 07:19:04-05:00,camelslick,,Done.,434hw2,t1_czfitq6,videoj,,Reply,1,0,1
czfdl8j,2016-01-28 13:18:19-05:00,BrokeDiamond,,"This isn't computer science, but look into cleaning out the fan and replacing the thermal grease. It should take no more than an hour and $20 worth of supplies, including rubbing alcohol, a computer screwdriver kit, compressed air, and thermal grease.",434hw2,t3_434hw2,camelslick,,Comment,1,0,1
czfgmkz,2016-01-28 14:27:38-05:00,artillery129,,"I also agree that this is not computer science, but if you have linux use http://linux.die.net/man/8/fancontrol


if you have windows use speed fan
http://www.almico.com/speedfan.php",434hw2,t3_434hw2,camelslick,,Comment,1,0,1
czgaql5,2016-01-29 07:25:23-05:00,camelslick,,"I have windows. Have you used speed fan? 

",434hw2,t1_czfgmkz,artillery129,,Reply,1,0,1
czgcy8x,2016-01-29 08:58:53-05:00,artillery129,,"its been many years since I've had windows, but yes",434hw2,t1_czgaql5,camelslick,,Reply,1,0,1
431ms0,2016-01-27 23:13:02-05:00,tab1360,Internships in high school?,"I am a sophomore interested in cs in Colorado(irrelevant) and was wondering if anybody was able to intern at places in high school? I want to be able to do it probably not the upcoming Summer but the next since I will be able to drive. If you were able to get an internship in high school, what all did you need to know? Thanks.",,,,,Submission,2,0,2
czpmokg,2016-02-05 22:15:21-05:00,KatsTakeState,,"I know a Junior in high school who's an intern at HP right now. He wears his name tag around during classes. Yes I think in the long run that's nice to be able to say he worked for HP in high school but during those times you should just focus on learning more on your own and really figuring out if this is really for you. And Hopefully it is! I find myself looking ahead in the chapter, or googling very deep into subjects that the teacher glances over quick. As long as I have that then I know that comp sci is something I will want to do in the future. And if that's the case for you then an do whatever makes you happy.",431ms0,t3_431ms0,tab1360,,Comment,2,0,2
czewxq0,2016-01-28 02:53:27-05:00,Newmanator29,,"Don't do it. Do some learning if you so wish, but you're in high school. Hell you're not even old enough to drive. You have your whole life to work. Spend some time with friends, play sports, go outside, relax. When you hit college, you'll lose most of your high school friends, you'll be busy all the time, and then when you graduate from college you'll be doing this all day, everyday. Don't let the majority posters be your only frame of reference for the industry. You can still go to Google or Microsoft having only college experience. You don't need to start your career now. You'll only end up looking back on these days wishing you didn't. ",431ms0,t3_431ms0,tab1360,,Comment,4,0,4
czevuwi,2016-01-28 01:57:34-05:00,Pandahx,,"You're gonna be competing with college students and recent grads for internships. Unless you're the next Zuckerberg I don't think interning is very likely.

The best thing you could do right now is work on personal projects to help you get into a good CS school.",431ms0,t3_431ms0,tab1360,,Comment,2,0,2
431his,2016-01-27 22:36:52-05:00,L_Mark,What computer should I buy for CS degree,"Hi,
I recently switched my major to computer science and need to get a laptop for school. To be honest, I am fairly new to the technical aspects of computers. I do not believe that the school has a preference for mac or pc; however, I do not really have the money for a good mac so I think I am going to try to get the best pc I can afford. Anyway I was wondering If any of you had some suggestions or advice? What are the specs that I should be looking for? Any suggestions for manufacturers, ect... I would also like to run some Music DAWs and maybe some games on it (so I don't know if that would change anything drastically when it comes to making my decision)

Anyways, thanks for the help. Sorry to come off a little uninformed, I am honestly completely new to all of this, but I hope to learn.",,,,,Submission,0,0,0
czeqwvv,2016-01-27 22:56:51-05:00,VainWyrm,,"I'm quite sure it depends on the program, but I got through my entire program (just graduated) with a pretty cheap windows machine. Almost none of my coursework was processor or memory intensive, and what little there was I could SSH into a school computer if I needed to.

Edit: I did upgrade the RAM TO 8gb",431his,t3_431his,L_Mark,,Comment,4,0,4
czeqx0v,2016-01-27 22:56:57-05:00,teatacks,,"Most new laptops available today should suit your needs, as long as you stay away from netbooks. Look for at least 4GB RAM (8 GB would be better) and at least 128 GB disk space (a.k.a. HDD or SSD). You can check this list for a few recommendations: http://www.lappylist.com/laptops/best-programming-laptops/

A large and/or high resolution screen will help when programming, but will increase the price more. You can always work around this by learning the keyboard shortcuts for switching between windows.

It'll help to pick a laptop that's a bit more linux-friendly in case you decide to install linux on it: generally stick to Dell, HP or Lenovo in that case, and make sure to search around to see how well a particular model is supported. But, if booting into linux doesn't work out for you, you'll still be able to run it in VirtualBox.",431his,t3_431his,L_Mark,,Comment,2,0,2
czgtgmi,2016-01-29 15:41:16-05:00,L_Mark,,How would I see if a laptop is Linux friendly (I will probably stick to the brands that you suggested). Also thank you for the link!,431his,t1_czeqx0v,teatacks,,Reply,1,0,1
czjcc24,2016-01-31 22:55:06-05:00,teatacks,,"your best source will be to google the model number of the laptop with the phrase ""linux"". You'll see if people have any issues with getting the hardware to work. If you can get access to a floor model, you could try putting a copy of Ubuntu onto a USB stick and try booting it on that laptop. Then see how well the hardware works. http://www.ubuntu.com/download/desktop/create-a-usb-stick-on-windows

Audio and video hardware don't vary all that much anymore, so you should be fine there, but you may encounter issues with wifi adapters and the basic chipset in the machine, which may prevent bootup. Also, Linux doesn't always have the greatest support for power management on laptops, but it has been improving over the past couple of years",431his,t1_czgtgmi,L_Mark,,Reply,2,0,2
czeqx1d,2016-01-27 22:56:58-05:00,RozenKristal,,"For desktop, build your own. For laptop, i think you should get at least an i5 core. I have 8gb of ram and it is sufficient. I have a mac and i like it. Been running strong for 5 years. Idk about other laptop brand though, so cant give you advices about that.",431his,t3_431his,L_Mark,,Comment,2,0,2
czesqn5,2016-01-27 23:54:06-05:00,zombarista,,Almost anything will do these days. But always setup development environments in a VM. You don't want that shit taking over your main workstation! (Speaking from experience with Apache Tomcat/MySQL raping my computers resources once for a web dev class!),431his,t3_431his,L_Mark,,Comment,2,0,2
czewkkx,2016-01-28 02:33:42-05:00,xiongchiamiov,,"You can ask the school. They should be used to answering questions like this, and may even have it in a faq on their website.

The answer will probably be ""anything general purpose machine made in the last five years"", because most programming takes less oomph than most other computer activities.

It would be a good idea to have something capable of running Linux, as an option for later. This is most things.",431his,t3_431his,L_Mark,,Comment,2,0,2
czgtces,2016-01-29 15:38:32-05:00,L_Mark,,"Thank all of you for the help definitely helps with my decisions!
",431his,t3_431his,L_Mark,,Comment,1,0,1
42x8wo,2016-01-27 07:28:14-05:00,camelslick,Computer viruses,"In general, computer viruses don’t discriminate among operating systems. a typical virus can infect any system, regardless of platform

True or false?",,,,,Submission,2,0,2
czdrgd4,2016-01-27 07:41:54-05:00,gurtos,,"Most viruses are written for particular systems in mind.  
Firstly because Windows programs (and viruses are programs) won't work on lets say Linux and vice versa.

Secondly, different OS's operate differently and have different security holes. So even if it did run on different OS, it would have to attack it in very different way.",42x8wo,t3_42x8wo,camelslick,,Comment,9,0,9
czdrgv7,2016-01-27 07:42:36-05:00,iknighty,,False. A virus is like any other program and the target platform needs to understand its code,42x8wo,t3_42x8wo,camelslick,,Comment,5,0,5
czfesei,2016-01-28 13:45:29-05:00,camelslick,,"""False"" is correct.",42x8wo,t3_42x8wo,camelslick,,Comment,2,0,2
cze48fz,2016-01-27 13:32:00-05:00,Kaidelong,,"A virus works by transcribing a part of itself into existing programs. Thus it must be compatible at a low level with other programs on the system so that its payload is meaningful and so that it can spread. On a very simple computer one might imagine that the virus could load up a simple operating system of its choice (for example, by changing the values in the jump table), but generally the virus is going to rely on calls to the operating system to do its input and its output.

Now there is a chance that one OS might be similar enough to another that the virus would do the right thing on both (think something like the 16-bit Windows 3.1, OS/2 could quite possibly have run a virus intended for it). However because the virus does not rely on some compiler existing on the host system, it needs ""binary compatibility"" (clarification: even if the compiler and linker were available, it would still be an ordeal to get any kind of cross-OS compatibility).

So there are two answers here. One is that a typical virus may not go to much effort to discriminate its targets. What would have been a virus spreading to one OS might show up as some other behavior on an unintended target (maybe crash your browser, for example, or firewall hits).

However the answer to the second part is a definitive no. Viruses don't choose where they end up, an infected floppy might wind up on a system that isn't binary compatible with the one it came from, but they can't infect any platform.

Now these things are not desirable to have together, if you can't be cosmopolitan you'd want to be discriminating. It just is outside of the frontier of possibility to achieve either, so viruses are neither.",42x8wo,t3_42x8wo,camelslick,,Comment,3,0,3
czfcodr,2016-01-28 12:57:31-05:00,camelslick,,"That's really detailed, thank you!",42x8wo,t1_cze48fz,Kaidelong,,Reply,1,0,1
czdwsht,2016-01-27 10:39:53-05:00,rfinger1337,,"I agree with everyone else, but I will add that some viruses target multi-platform systems like java or flash.  In that case, it can still infect different OS's, so long as they are running the host system (java)",42x8wo,t3_42x8wo,camelslick,,Comment,3,0,3
cze0jf6,2016-01-27 12:09:27-05:00,TheFlyingGuy,,"Actually this isn't true either, most virusses targetting Flash or Java still try and launch a native payload (usually needed to break out of the jail imposed by the virtual machine). So while a Windows Java exploit might crash a Linux browser, it is unlikely to actually succesfully infect the system.",42x8wo,t1_czdwsht,rfinger1337,,Reply,6,0,6
cze5eje,2016-01-27 13:58:05-05:00,dxk3355,,"With the qualifier in the question of it being a 'typical' virus.  Than it would be false, the vast majority of viruses are OS specific to some extent.  Is this some type of homework question?",42x8wo,t3_42x8wo,camelslick,,Comment,1,0,1
czdrm56,2016-01-27 07:49:43-05:00,visvis,,"False. Like almost all software, the virus relies on the services of the operating system. On different OSes, these are accessed in different ways.

Moreover, to get in one would usually exploit vulnerabilities in the system. These are not just OS-specific, but the exploits often need to be tweaked to the exact version of the system that is used.",42x8wo,t3_42x8wo,camelslick,,Comment,1,0,1
42vnzz,2016-01-26 23:08:49-05:00,metthal,Classical DPLL SAT solving,"I am currently preparing for my exam from Formal Analysis and Verification. I have a question regarding classical DPLL SAT solving algorithm. In our study texts, we always denote assignment that was made during **Decide** step with small index **d**. My question is, can I consider my formula as satisfied if I am not able to perform any step and all clauses are satisfied, even though my assignment contains this ""guessed"" value? Or do I have to somehow **BackTrack** to regular value without this **d**?",,,,,Submission,0,0,0
czditig,2016-01-27 00:00:58-05:00,zifyoip,,"If you have found an assignment of Boolean values to the variables that satisfies all the clauses, then you have solved the problem.",42vnzz,t3_42vnzz,metthal,,Comment,1,0,1
czdwlai,2016-01-27 10:34:49-05:00,metthal,,"Ok, thanks very much.",42vnzz,t1_czditig,zifyoip,,Reply,1,0,1
42tnvu,2016-01-26 15:58:23-05:00,Varzoth,Help finding good security/encryption textbook.,"For some reason there's no reading list for one of my current modules So I'm hoping you guys can point me in the direction of a decent textbook on computer security.

Hopefully it would cover all/most of this:  
-Information Assurance Concepts (Confidentiality, Integrity, Availability)  
- Cryptography: history, main goals, security requirements  
- Types of attacks to cryptosystems  
- Symmetric cryptosystems: DES, AES, operation modes  
- Mathematical background from number theory and finite fields  
- Efficient algorithms for arithmetic and primality  
- Public-key cryptography: Diffie-Hellman key exchange, RSA, ElGamal  
- Digital signatures  
- Hash functions, their use and security  
- Cryptographic protocols: bit commitment, electronic voting  
- Network security, network attack types  
- Use of cryptography for network security  

Thanks in advance for any input <3
",,,,,Submission,5,0,5
czd3eig,2016-01-26 17:17:26-05:00,HenryJonesJunior,,"You mention a diverse set of topics, and you're probably not going to find any one book that covers all of them.

For algorithms for cryptography, signatures, protocols, etc. the definitive go to (last I checked) was still Schneier's [Applied Cryptography](http://www.amazon.com/Applied-Cryptography-Protocols-Algorithms-Source/dp/1119096723/).

For a history of cryptography, I'm fond of Kahn's [The Codebreakers](http://www.amazon.com/Codebreakers-Comprehensive-History-Communication-Internet/dp/0684831309/), but be forewarned that it is a large book.

For Network Security and Information Assurance concepts, I like Anderson's [Security Engineering](http://www.amazon.com/Security-Engineering-Building-Dependable-Distributed/dp/0470068523/), but the state of the art changes so rapidly that it's difficult to recommend a book.",42tnvu,t3_42tnvu,Varzoth,,Comment,3,0,3
czdq61l,2016-01-27 06:27:51-05:00,Towe1,,"Wikipedia is a great place to start. They all include the maths for most algorithms and are generally good information that is relatively reliable, (always check the sources). Read around and don't be afraid of the RFC's.

Add elliptical curve cryptography to your public key section as well.",42tnvu,t3_42tnvu,Varzoth,,Comment,1,0,1
42oxbe,2016-01-25 20:03:14-05:00,wolbis,Specifying complexity in Big Oh without ambiguity,"I think we can prove that n is in O(n), n is in O(n^2 ), n is in O(n^3 ) (Haven't tried it but I guess it should be trivial). So for an algorithm, even if it's running time complexity is O(n), we can say that it has O(n^3 ). So, how is this ambiguity handled when specifying complexity ?
",,,,,Submission,7,0,7
czby4a5,2016-01-25 20:08:43-05:00,_--__,,"You can use Ω to specify lower bounds and Θ to specify tight bounds - i.e. f(n)=n is Ω(n), Ω(log n), Ω(log log n), etc but not Ω(n^(2)). And since it is both Ω(n) and O(n), we can say it is Θ(n).",42oxbe,t3_42oxbe,wolbis,,Comment,3,0,3
czbzdna,2016-01-25 20:40:57-05:00,wolbis,,Thanks. So is there any specific reason why most of the algorithm books use Big Oh notation for specifying the complexity ? I have rarely seen Ω used.,42oxbe,t1_czby4a5,_--__,,Reply,1,0,1
czbzjt2,2016-01-25 20:45:03-05:00,mosqutip,,"[Here's](https://en.wikipedia.org/wiki/Big_O_notation#Related_asymptotic_notations) a Wikipedia link with more on the different types of complexities.

On an unrelated note, if you want to superscript a number with parentheses, write O(n\^(x)) in the editor to get O(n^(x)). The parentheses group the super-scripted text.",42oxbe,t1_czbzdna,wolbis,,Reply,1,0,1
czcg7ka,2016-01-26 07:32:27-05:00,paithanq,,"Usually you're talking about using an algorithm to solve a problem.  You want to know the time complexity of that problem.  If you find an algorithm to solve the problem in some running time (e.g. Selection Sort in Θ(n^2)) then what can you say about the problem?

Just by specifying a single algorithm, you haven't proven the Big-Theta notation for the problem.  By saying Sorting is in O(n^2) due to the Selection Sort algorithm, you're saying that it can be solved in quadratic time and maybe faster.  

I expect that this distinction between the complexity of an algorithm and a problem is the reason that Big Oh notation is more commonly used in these classes where you're studying algorithms and not lower-bounds.  The Big Oh notation is true whether you're talking about either the problem or the algorithm.  Big Theta is not necessarily.",42oxbe,t1_czbzdna,wolbis,,Reply,1,0,1
czc3k3n,2016-01-25 22:22:19-05:00,zifyoip,,"Because most of the time interesting statements are of the form, ""For an input of size&nbsp:n this algorithm will take *at most* f(n) operations.""

But sometimes it's interesting to say something like, ""For an input of size&nbsp:n this algorithm will take *at least* f(n) operations,"" and in that case big-omega notation is what you want.",42oxbe,t1_czbzdna,wolbis,,Reply,1,0,1
czc14g6,2016-01-25 21:24:23-05:00,s1295,,"Everything you wrote is true, but there is no ambiguity, just like the statements 3 < 10 and 3 < 10000 are both unambiguous and true. It is implied that the lowest (known) bound is the property of interest, but specifying any looser bound doesn't yield a false statement. An exercise may ask you to provide a *tight* bound (e: actually, that would be Theta, so I guess ""best"" bound is the term).

Maybe you can rephrase your question if this doesn't satisfy you.",42oxbe,t3_42oxbe,wolbis,,Comment,2,0,2
czc1loe,2016-01-25 21:35:50-05:00,wolbis,,"Thanks, that indeed makes it clear.",42oxbe,t1_czc14g6,s1295,,Reply,1,0,1
czc4ypu,2016-01-25 22:56:57-05:00,umib0zu,,[You should see this](http://www.amazon.com/Introduction-Analysis-Algorithms-Robert-Sedgewick/dp/020140009X) and dive into analytic combinatorics. The master theorem and specifying bounds using asymptotics isn't really very scientific in a sense that you don't have an error bound. Professionals use GF analytic methods.,42oxbe,t3_42oxbe,wolbis,,Comment,1,0,1
czchyfk,2016-01-26 08:47:19-05:00,umib0zu,,"I have no idea why this is downvoted. It's not like Sedgewick isn't a complexity theorist [who directly says Big O notation is bad](https://www.youtube.com/watch?v=-hz95qqOdx0). /u/wolbis questioned the validity of asymptotic techniques. I merely raised the issue that not only is OP right to question their validity, but asymptotic techniques are bad in general and grad level complexity theorists use GFs, where OPs issue is no longer a problem.",42oxbe,t1_czc4ypu,umib0zu,,Reply,1,0,1
42o3tm,2016-01-25 17:15:17-05:00,enigma_x,Question about decidable problems.,"Since we know that a DFA that accepts strings from a certain language is decidable, can I say any model that can be converted to a problem that involves a DFA that accepts a string is decidable? 

For instance, if I can convert a problem involving an NFA into a DFA that accepts strings of a language L, does that NFA become automatically decidable? Sorry if this isn't the right way to ask this question, would love some pointers. Thanks.",,,,,Submission,8,0,8
czbts9i,2016-01-25 18:16:21-05:00,VideotapeReturn,,"I'm having a little trouble understanding the exact question but it sounds like you're asking if languages accepted by DFAs are decidable and the answers yes. Reason being, as you stated, is that checking if a DFA accepts a string is a decidable problem. 

Regular languages = ""languages accepted by DFAs"" and you can prove Regular is a subset of Decidable. ",42o3tm,t3_42o3tm,enigma_x,,Comment,2,0,2
czbtvek,2016-01-25 18:18:34-05:00,enigma_x,,"Thanks. By extension any problem that can be reduced to a DFA accepting strings problem are decidable, am I right?",42o3tm,t1_czbts9i,VideotapeReturn,,Reply,1,0,1
czbtyz0,2016-01-25 18:21:04-05:00,VideotapeReturn,,Yep,42o3tm,t1_czbtvek,enigma_x,,Reply,2,0,2
czbu0k5,2016-01-25 18:22:09-05:00,enigma_x,,Thank you so much. ,42o3tm,t1_czbtyz0,VideotapeReturn,,Reply,1,0,1
42ma1x,2016-01-25 11:39:57-05:00,noobeeee,How does IP address to location conversion work?,"Hi,

How does it work? Do those company have guys running all over the places to collect the data manually or is it possible to detect location from IP address easier?

Will I need to communicate with telecom to get that data?
",,,,,Submission,14,0,14
czbeye9,2016-01-25 12:30:57-05:00,high_side,,"It's a lookup table afaik.  You can download it, just search.",42ma1x,t3_42ma1x,noobeeee,,Comment,5,0,5
czbk6fq,2016-01-25 14:31:28-05:00,worldDev,,"Yes, as is every cached lookup, but I believe the question was probably geared to where does the information originally come from. I don't have the answer, but it's probably along the lines of getting info from those leasing ip addresses and them using their best guess from billing information.",42ma1x,t1_czbeye9,high_side,,Reply,1,0,1
czbkzif,2016-01-25 14:50:02-05:00,cornersnaps,,"Here's a free HTTP API that can ""translate"" IP addresses to geolocation.
https://freegeoip.net",42ma1x,t3_42ma1x,noobeeee,,Comment,2,0,2
czc0y8g,2016-01-25 21:20:14-05:00,noobeeee,,I'm actually looking to know the behind the scene of that website and the data provider.,42ma1x,t1_czbkzif,cornersnaps,,Reply,1,0,1
czbg3pq,2016-01-25 12:57:28-05:00,Syde80,,https://www.maxmind.com/en/home,42ma1x,t3_42ma1x,noobeeee,,Comment,1,0,1
czc0ysf,2016-01-25 21:20:38-05:00,noobeeee,,How will you collect that data like maxmind for your small country?,42ma1x,t1_czbg3pq,Syde80,,Reply,1,0,1
czbjm8p,2016-01-25 14:18:34-05:00,JoTheKhan,,"Public IP addresses are known by the entity that gives out the IP address ranges and most probably by the telecoms that route these IP addresses. Because Public IP addresses are split up by Autonomous Systems I believe that is how people are able to get an IP address to Location conversion. 

For instance my University has the ip address 128.8./16 IIRC, so any ip address that starts in 128.8 is physically located at my University unless someone is using a VPN to get inside the University and then going out to the network from there. 

I'm not exactly sure what steps you need to go through to get an IP address/Location conversion but if there isn't an API out there like there is for making DNS queries then you probably will need to talk to a telecom or some record keeping organization in some form. ",42ma1x,t3_42ma1x,noobeeee,,Comment,1,0,1
czc0xp6,2016-01-25 21:19:52-05:00,noobeeee,,"I see.

In my country, IP to location database from maxmind or others are not that accurate.

So, I'm trying to see what else I can do. :(",42ma1x,t1_czbjm8p,JoTheKhan,,Reply,1,0,1
42lr6v,2016-01-25 10:01:13-05:00,Rllrllrrlrrl,Are there any guides/books out there that teach one how to use google the most effectively when coding?,,,,,,Submission,2,0,2
czbe0qx,2016-01-25 12:08:38-05:00,will6988,,"Sounds sarcastic, but serious answer here. You're better off studying actual programming principles than studying how to google more effectively. You'll learn more and eventually be able to solve issues yourself rather than depending on StackOverflow. ",42lr6v,t3_42lr6v,Rllrllrrlrrl,,Comment,4,0,4
42h4gu,2016-01-24 13:56:31-05:00,TheLastKantian,Is their a need for a mathematically inclined person in Computer Science?,"Hello, I'm interested in doing a computer science PHD but I'm at complete loss at what fields require the most math knowledge in regards to analysis and discrete mathematics. I don't want to go to grad school and spend my time being a ""software engineer"". Can someone who does a PHD in computer science guide me and answer me "" What does a PHD in computer science entail?"" I'm really confused. Thanks !",,,,,Submission,6,0,6
czalhom,2016-01-24 19:04:45-05:00,eygrr,,"A compsci phd typically entails..

1. Reading theoretical papers.
2. Coming up with hypotheses
3. Proving your hypotheses
4. Writing up your hypothesis with proof
5. Crying",42h4gu,t3_42h4gu,TheLastKantian,,Comment,3,0,3
czalkyi,2016-01-24 19:06:47-05:00,TheLastKantian,,"That sounds like what I would like to do. I'm curious about step number 3 the most, care to elaborate a bit more. Thank you!",42h4gu,t1_czalhom,eygrr,,Reply,2,0,2
czalp8l,2016-01-24 19:09:41-05:00,eygrr,,"The proving is ""I need to apply my work in a way that my hypothesis is verifiable"", so you would run experiments that would show your hypothesis is correct e.g. running your new search algorithm against baseline methods and the current best in the field and seeing if it's better.",42h4gu,t1_czalkyi,TheLastKantian,,Reply,2,0,2
czaa4fy,2016-01-24 14:35:10-05:00,SneakingNinjaCat,,What is your background?,42h4gu,t3_42h4gu,TheLastKantian,,Comment,1,0,1
czaauli,2016-01-24 14:54:39-05:00,TheLastKantian,,"I'm currently a sophomore in computer science with up to discrete mathematics under my belt, I plan on taking Analysis next year.",42h4gu,t1_czaa4fy,SneakingNinjaCat,,Reply,1,0,1
czada46,2016-01-24 15:51:23-05:00,SneakingNinjaCat,,What about software engineering do you wish to avoid?,42h4gu,t1_czaauli,TheLastKantian,,Reply,2,0,2
czaffp8,2016-01-24 16:38:16-05:00,CaptainBland,,"I think most likely you would be in a research post for this kind of work, you might find some jobs involving formal verification. I know that Microsoft Research does some work on this.",42h4gu,t3_42h4gu,TheLastKantian,,Comment,1,0,1
czaiwv9,2016-01-24 18:07:53-05:00,fodder008,,"I'm not sure if your question can really be answered. Your typical day as a PhD student will vary greatly depending what sub-field you're working in and what school you attend. 

At the end of the day though pretty much any PhD in computer science is going to have to be implemented. Even the most theoretical theses typically provide a sample implementation. ",42h4gu,t3_42h4gu,TheLastKantian,,Comment,1,0,1
czakpar,2016-01-24 18:46:05-05:00,TheLastKantian,,"Yeah, I don't mind the implementation at all. It's probably one of the most funnest parts, but I don't want the focus of my PHD to be on implementation, period. It's kind of like if a physicist was only concerned with the lab component of his field rather than the theory and mathematical background required, I want both of them. That's why I get scared when people say ""software engineering"", images of people working on boring databases and apps pops up into my head.",42h4gu,t1_czaiwv9,fodder008,,Reply,1,0,1
czalfzz,2016-01-24 19:03:47-05:00,eygrr,,"It would be pretty weird to avoid theory in a PhD, but it might be possible. Best to ask during the application process.",42h4gu,t1_czakpar,TheLastKantian,,Reply,1,0,1
czalj9a,2016-01-24 19:05:45-05:00,TheLastKantian,,"no, I don't want to avoid it. Theory is what I want to do the most!",42h4gu,t1_czalfzz,eygrr,,Reply,2,0,2
czall3z,2016-01-24 19:06:54-05:00,eygrr,,"PhD's are focused on proving hypotheses, which is a theoretical process completed by coding. If you are a good computer scientist, you minimize the time it takes to prove your hypothesis, which means you minimize the amount of time coding through use of libraries etc. The more hypotheses you prove and publish, the better a computer scientist you are. So, if you are interested in succeeding, you will spend more time doing theory than implementation, even if your theory is being backed up by experimental data.",42h4gu,t1_czalj9a,TheLastKantian,,Reply,1,0,1
czanppf,2016-01-24 19:58:43-05:00,nuclearqtip,,"I worked in academia for a while. I witnessed two of my friends get a PHD in CS. The focus is rarely on having a ""solid implementation"".

Usually what you'll find is that they spend most of their time developing the theory and then they either (a) write some simple proof-of-concept scripts (e.g. one of my friends wrote really short perl scripts that just proved his ideas), or (b) pass it off to lab assistants and have them implement your theory (assuming you have a CS research lab).

Alternatively sometimes you can find startup companies where theory thrives. I.e. go find companies that get their money from research grants (e.g. SBIR). That's what my other friend did, he basically got his PHD while being paid to do his day job.

Actually most of my friends' time was spent (a) developing the theory, and (b) getting it published in journals. Very little time doing software engineering. However YMMV, obviously, depending on all kinds of circumstances.",42h4gu,t1_czakpar,TheLastKantian,,Reply,1,0,1
czaljyt,2016-01-24 19:06:11-05:00,None,,[deleted],42h4gu,t3_42h4gu,TheLastKantian,,Comment,1,0,1
czalmvm,2016-01-24 19:08:04-05:00,TheLastKantian,,This is exactly what I wanted to hear.,42h4gu,t1_czaljyt,None,,Reply,1,0,1
czanqy2,2016-01-24 19:59:33-05:00,Merad,,"You should be talking to your professors about this.  All (or most) of them have gone through getting a PhD and can tell you about their experiences. If you're seriously interested in the PhD route then you should also start trying to figure out what research areas you're interested in, and get experience doing research as an undergrad if at all possible.  

Speaking very generally, getting a PhD means you're researching something that hasn't been done before (""expanding the scope of human knowledge"", etc). The exact details of what you do will very wildly depending on your research area, even within CS.",42h4gu,t3_42h4gu,TheLastKantian,,Comment,1,0,1
42gufz,2016-01-24 12:56:41-05:00,Biscuitoid,What are some problems that are much easier for a human to do than a computer?,Bonus points if instances of the problem can themselves be generated by an algorithm.,,,,,Submission,15,0,15
cza868j,2016-01-24 13:41:41-05:00,mosqutip,,"* Image recognition: determining what objects exist in a photograph, or what a photograph depicts ([relevant xkcd](https://xkcd.com/1425/)).

* Natural language processing: figuring out the semantic meaning of text, such as the sentiment of a body of text, deciphering metaphors, etc.

* Complex physical actions in 3-D space: something like a robotic football player, for example, would (at this point in time) be completely outmatched by even a novice human player.

* Anything having to do with our definition of ""intelligence"". This includes establishing new problems and a set of steps to solve a problem from a collection of never-before-seen data, or pattern-matching.

It's important to note that computers might eventually be as good or better than humans in these tasks, especially if we can figure out how to invent strong AI. But for the time being, humans are clearly better.",42gufz,t3_42gufz,Biscuitoid,,Comment,21,0,21
czaggjz,2016-01-24 17:01:55-05:00,CaptainBland,,"I'd like to add that for image recognition, people are only better at generalised image recognition. For specialised cases, such as face recognition, computers have been able to outperform humans. https://medium.com/the-physics-arxiv-blog/the-face-recognition-algorithm-that-finally-outperforms-humans-2c567adbf7fc#.g9moypb5g",42gufz,t1_cza868j,mosqutip,,Reply,5,0,5
czagzla,2016-01-24 17:23:13-05:00,mosqutip,,"Very true. Computers are better at specialized tasks, and quickly catching up for more general ones.",42gufz,t1_czaggjz,CaptainBland,,Reply,2,0,2
cza86q9,2016-01-24 13:42:03-05:00,xkcd_transcriber,,"[Image](http://imgs.xkcd.com/comics/tasks.png)

[Mobile](http://m.xkcd.com/1425/)

**Title:** Tasks

**Title-text:** In the 60s, Marvin Minsky assigned a couple of undergrads to spend the summer programming a computer to use a camera to identify objects in a scene. He figured they'd have the problem solved by the end of the summer. Half a century later, we're still working on it.

[Comic Explanation](http://www.explainxkcd.com/wiki/index.php/1425#Explanation)

**Stats:** This comic has been referenced 624 times, representing 0.6432% of referenced xkcds.

---
^[xkcd.com](http://www.xkcd.com) ^| ^[xkcd sub](http://www.reddit.com/r/xkcd/) ^| ^[Problems/Bugs?](http://www.reddit.com/r/xkcd_transcriber/) ^| ^[Statistics](http://xkcdref.info/statistics/) ^| ^[Stop Replying](http://reddit.com/message/compose/?to=xkcd_transcriber&subject=ignore%20me&message=ignore%20me) ^| ^[Delete](http://reddit.com/message/compose/?to=xkcd_transcriber&subject=delete&message=delete%20t1_cza86q9)",42gufz,t1_cza868j,mosqutip,,Reply,2,0,2
czaco7c,2016-01-24 15:38:59-05:00,umib0zu,,"[Relevant econtalk](http://www.econtalk.org/archives/2014/10/david_autor_on.html).

tl:dr Machines need environments with low variability. Machines generally can't adapt to highly variable environments. Any problem associated with an environment is generally hard.",42gufz,t3_42gufz,Biscuitoid,,Comment,2,0,2
czagxf5,2016-01-24 17:21:35-05:00,rfinger1337,,"Natural speech in english. 

Compared to all other forms of computing, in this area we are still stuck in the 80's.  ",42gufz,t3_42gufz,Biscuitoid,,Comment,2,0,2
czb8a82,2016-01-25 09:37:24-05:00,SolarPolarMan,,Riddles? Hard to generate though.,42gufz,t3_42gufz,Biscuitoid,,Comment,2,0,2
czaddvx,2016-01-24 15:53:45-05:00,imh,,"To generalize on what /u/mosqutip said, it's basically things involving the natural world or people.

But an additional area in machine learning is learning from few examples. ""One shot learning"" is a really cool current research direction.",42gufz,t3_42gufz,Biscuitoid,,Comment,1,0,1
czb7bvv,2016-01-25 09:04:20-05:00,dxk3355,,Sounds like a homework question .,42gufz,t3_42gufz,Biscuitoid,,Comment,1,0,1
czbd3ok,2016-01-25 11:46:36-05:00,Biscuitoid,,"Nope, actually thinking about problems you could set humans that would take a certain amount of time to do but wouldn't be sped up (i.e. cheated at by) a computer. I'm thinking captchas is probably the best way at the moment, I just wondered if people had other ideas.",42gufz,t1_czb7bvv,dxk3355,,Reply,1,0,1
4293bw,2016-01-22 22:11:06-05:00,petabit,What does transparent (cryptography|compression|...) mean in operating systems?,"I'm reading about operating systems from various sources and I don't get what's meant by ""transparent."" For example, in wiki page for [ext3](https://en.wikipedia.org/wiki/Ext3#Compression):

> e3compr is an unofficial patch for ext3 that does **transparent** compression.

In a [mach operating system paper](http://www.cdf.toronto.edu/~csc469h/winter/handouts/mach_usenix86.pdf) it says mach has

> support for **transparent** remote file access between autonomous systems

What do they mean by ""transparent"" and what would be a non-transparent remote file access?",,,,,Submission,6,0,6
cz8ljia,2016-01-22 22:38:18-05:00,njaard,,"It means that applications running on the OS can read/write those files even though they are compressed without needing to do anything special/being aware that they're compressed/encrypted.

If it were ""opaque"" (that is, not transparent), then software would need to be modified in order to access the encrypted/compressed files.",4293bw,t3_4293bw,petabit,,Comment,6,0,6
cz8urxt,2016-01-23 06:53:36-05:00,ldpreload,,"Transparent remote file access is something like NFS: you mount a remote server, and you access it as a normal path on the filesystem. Or, for a more modern example, Dropbox. You just use your files normally.

""Opaque"" remote file access would be something like FTP or Google Drive, where you have to manually copy files back and forth, or, at best, each application needs to know how to access the remote server so that it can copy files back and forth for you. (It's true that since modern OSes are extensible, you can get extensions that make this access transparent.)",4293bw,t3_4293bw,petabit,,Comment,2,0,2
cz936kz,2016-01-23 12:35:27-05:00,zefcfd,,"this is what i've known transparnecy as 

http://www.cl.cam.ac.uk/~jac22/books/ods/ods/node18.html",4293bw,t3_4293bw,petabit,,Comment,2,0,2
cz9dt6j,2016-01-23 17:34:58-05:00,darthandroid,,"*Transparent* in the general software/computing world means that anything using the transparent thing doesn't have to know about it: Generally used in context of a new feature, where existing software can make use of that feature without knowing about it.

*Transparent Compression* means that applications don't need to decompress the files when reading them from disk, nor do they need to compress them before writing them to disk. The filesystem driver instead *transparently compresses and decompresses* the files on the fly as they are read and written to disk without the application knowing about it.

*Transparent File Access* means that applications can access the file like a normal file on the local hard drive, even though it may not be a local file or stored on a local harddrive. When you mount a network share with the OS (i.e., mapping a drive in Windows or OSX's Connect to Server feature), any program can access those files *transparently*, without any special code to support them being remote. Similarly, when you plug a USB drive into the system, any application can access those files *transparently*, without any special code to support them being connected over USB.",4293bw,t3_4293bw,petabit,,Comment,2,0,2
4289fi,2016-01-22 18:37:00-05:00,geographybuff,Computer science career goals,"I'm a high schooler hoping to graduate next year, and I have varying interests, including world languages. I thought I should learn more about computer science to build my resume and increase my career possibilities, but I have gotten somewhat disoriented along the way, and I thought I might as well share what's on my mind here. I've studied java and html, though I would consider myself a beginner in both languages. I'm just trying to get used to the syntax and general ideas. While I haven't really been able to set concrete goals when it comes to computer science, the biggest thing I've wanted it for has been data analysis, specifically analysis of written texts. I've thought about going into computational linguistics, but I'd really like a job where I can work on or in more than just English. Right now, taking a course while I'm still in high school is only a remote possibility for me. I'm pretty much left to the internet. So here are some things I'm thinking about. What programming languages go best with data analysis, linguistics, and world languages? What are some good ways of learning these languages? And what are some good programming activities I can take part in to increase my skills?

Confused but hopeful,
geographybuff",,,,,Submission,5,0,5
cz8ewpw,2016-01-22 19:08:58-05:00,videoj,,">What programming languages go best with data analysis, linguistics, and world languages?

Python: [Natural Language Toolkit](http://www.nltk.org/book/)  The book includes a basic intro to Python, as well as an overview of the field.  

Java: [Standford's CoreNLP](http://stanfordnlp.github.io/CoreNLP/) and [Apache's OpenNLP](https://opennlp.apache.org/)

There are other languages used in the field, but those are good starts.  


Also visit r/linguistics/, /r/compling and /r/machinelearning.  For learning programming, try /r/learnprogramming",4289fi,t3_4289fi,geographybuff,,Comment,3,0,3
cz8f3pg,2016-01-22 19:14:32-05:00,PastyPilgrim,,"For starters, HTML is not a programming language, so you can completely stop looking at it unless you want to make websites.

Other than that, I think you should be careful about getting ahead of yourself. It's important to have goals, dreams, etc., but you should focus on learning the basics of computer science and programming before you start getting into things like data analysis and computational linguistics. Learning your first programming language is hard, but learning subsequent languages is easy (if you want to compare it to learning spoken languages, think of the ability to speak with computers as a new language, with programming languages being dialects). So it doesn't matter all that much which language you start with (as long as it's a respectable language).

If you start out trying to do applied computer science before learning foundational computer science then you're going to have a real bad time. You can learn some programming on your own, but learning computer science on your own is very difficult. If you go to college for computer science, you'll spend your first semester learning how to program and the following 7 semesters learning computer science. In college, you learn a lot faster than you would on your own, so to learn CS on your own would take many years (in my opinion).

So, if you're driven to get started now, then you should learn programming, as that can be done on your own. The three languages that I would consider starting with are Java, Python, or C. Python is the easiest to learn because it is designed to be very readable: it is also the highest level language of the three (it's the furthest removed from what is actually happening on the hood). Java is a medium level language. C is a low level language. In my opinion/experience, the higher level the language, the easier it is to learn at the start, but the harder it is to learn low level languages later. The lower level the language, the harder it is to learn at the start, but the easier it is to learn high level languages later. You should pick the language that best suits you. If you care, my order of learning was Java -> Python -> C and my order of preference (now that I am a computer scientist) is Python > C > Java.

They're like starter Pokemon: one of them will give you an immediate edge in the early gyms where you have the type advantage, but require that you put a lot of effort into raising other Pokemon to handle the type disadvantage later. Another one will give you the early disadvantage, but reward you later. And the third will be a neutral early start.",4289fi,t3_4289fi,geographybuff,,Comment,3,0,3
cz8gbol,2016-01-22 19:50:09-05:00,geographybuff,,"Thanks for the pointers. I guess the reason I've wanted to learn about computer science and not just languages is that I have spent hours on codecademy in the past and learned about different operators and syntax elements, and still wondered how on earth these things could be used in the real world. It's nice to know how these things work together into real programs.

And it seems learning programming before college is quite popular nowadays. I'd like to explore this stuff while I still have time to think about it. If you don't mind sharing, what was your alma mater? Did you do the one semester of programming and seven of computer science like you described?",4289fi,t1_cz8f3pg,PastyPilgrim,,Reply,1,0,1
cz8h5tj,2016-01-22 20:15:38-05:00,applesaucosaurus,,"I don't know if this is common but when I graduated with my cs degree I interviewed with several companies that wanted to know if I spoke a second language. If you can speak two languages and know some coding stuff in both languages you can act as a bridge between an English speaking team and an overseas team.

Also check out codecademy.com, they have some cool tutorials that start out very basic and build up.",4289fi,t3_4289fi,geographybuff,,Comment,2,0,2
424rjf,2016-01-22 03:53:50-05:00,an_actual_human,"What theory do I need to learn to write a toy version of a calculator (as in Mathematica, not Casio)?","I want to write a calculator that would be able to evaluate strings such as ""(2 + 3) * sin(pi)"", ""200 USD in EUR"" or ""a = 2; 2*a"". 

Please recommend me some reading and keywords to search. I do have a modest exposure to CFGs (as in I could write a script to parse an arithmetic expression), but none in formal semantics (to the degree I'm not sure it's relevant; is it?). 

I imagine the Dragon book would enlighten me enough, but I'm not trying to make a compiler or a full-blown language such as Mathematica for that matter, so that might be an overshot.",,,,,Submission,7,0,7
cz7o1b9,2016-01-22 07:14:55-05:00,Teemperor,,"Write a parser (e.g. define a grammar, put in parser generator) to parse the input string in a parse tree. Then interpret the tree (e.g. `if (node.type == ""convertUSDToEUR"") convert(node): else if (node.type == ""arithmeticExpression"") ...`

EDIT: The Dragon book actually covers many of the things you want to do. Cherrypick the parts about parsing and maybe runtime environment.",424rjf,t3_424rjf,an_actual_human,,Comment,3,0,3
cz7ozqa,2016-01-22 08:03:03-05:00,an_actual_human,,Then draw the rest of the owl!,424rjf,t1_cz7o1b9,Teemperor,,Reply,12,0,12
cz7qz7z,2016-01-22 09:17:53-05:00,tonylearns,,He actually gets you closer than just two circles. This issue is this is something that is generally covered in an entire course and not really something that a reddit comment is going to be able to solve for you. If the dragon book is too much I know that coursera has a free compliers course and I would bet opencourseware does to.,424rjf,t1_cz7ozqa,an_actual_human,,Reply,1,0,1
cz7rklv,2016-01-22 09:36:21-05:00,an_actual_human,,"It was about the part of the comment before ""EDIT"". Anyhow, I certainly don't expect (or want for that matter) people to explain things in comments, just a couple of hints where to look would be nice. Like what the relevant part of the art that is not parsing is called?",424rjf,t1_cz7qz7z,tonylearns,,Reply,2,0,2
cz7s4gv,2016-01-22 09:52:38-05:00,Teemperor,,"OP's right, I edited shortly before he commented (and I also removed two ""just"" in there that made the whole thing sound easier than it is).

What you want to write would I call an *interpreter* and you follow the typical steps for building one:

1. Define a grammar in a format that for example *GNU bison* can use to build you a parser. The `Infix calc` example [here](http://www.gnu.org/software/bison/manual/html_node/Examples.html) is a good start as it is quite similar to your task. With this parser you get a parser tree.

2. Anaylyse and evaluate your parser tree. The tree could look similar to that for your `(2 + 3) * sin(pi)` example:

^^^^^^^^^^^DummyTestBecauseRedditFormattingIsFuckedUp

     Program-Node
     |----ArithmeticExpression
       |----Multiplication
           |----Addition
              |-----Literal(2)
              |-----Literal(3)
           |----FunctionCall(sin)
              |-----GetVariable(Pi)


Step 2 is a bit harder to find resources on because it is really specific to the type of language you want to build.

EDIT: Formatting...",424rjf,t1_cz7rklv,an_actual_human,,Reply,2,0,2
cz7t3n7,2016-01-22 10:20:01-05:00,an_actual_human,,"I'm pretty comfortable with the stuff you've explained, I want to learn things that go beyond. Thanks for your time though.",424rjf,t1_cz7s4gv,Teemperor,,Reply,-1,0,-1
cz7tagf,2016-01-22 10:24:57-05:00,tonylearns,,"What do you mean by ""go beyond""? This is pretty much how it's all done.",424rjf,t1_cz7t3n7,an_actual_human,,Reply,-1,0,-1
cz7tcv0,2016-01-22 10:26:44-05:00,an_actual_human,,"Beyond what you've explained (i.e. beyond parsing). For arithmetic expressions evaluation is enough without any theory, of course, but I would like to know more.",424rjf,t1_cz7tagf,tonylearns,,Reply,-1,0,-1
cz7tst5,2016-01-22 10:38:09-05:00,tonylearns,,"Once you've parsed it, everything is effectively a method call. Once you know it's addition you call add, once you know it's currency conversion you call that method. For an interpreted language these methods will most likely be written in the target language or at least an intermediary target.",424rjf,t1_cz7tcv0,an_actual_human,,Reply,0,0,0
cz7tx2o,2016-01-22 10:41:12-05:00,an_actual_human,,"Well, that much I do understand, but I'm pretty sure there is more to know. I guess I'll peruse something on compilers for lack of a better keyword.",424rjf,t1_cz7tst5,tonylearns,,Reply,-1,0,-1
cz7tmsm,2016-01-22 10:33:54-05:00,oji816,,"I remeber one of my Data Structures course projects was to build a small program that uses a stack, to evaluate a postfix arithmetic string... Basically you push the literals to the stream (for infix operators I assume you could do it with two stacks or queues) and once the string is over,  pop literals from the stack while applying the operation literals of it... 
Not sure if that's what you want, or if this idea could help you or not, but you could make use of it.. ",424rjf,t3_424rjf,an_actual_human,,Comment,1,0,1
cz7tp18,2016-01-22 10:35:30-05:00,an_actual_human,,This is a small part of what I want that I already know.,424rjf,t1_cz7tmsm,oji816,,Reply,-1,0,-1
cz7yz6z,2016-01-22 12:41:17-05:00,videoj,,What I think you're asking about is a [Computer Algebra Systems](https://en.wikipedia.org/wiki/Computer_algebra_system).  The wiki article linked provides a lot of references to information and algorithms.,424rjf,t3_424rjf,an_actual_human,,Comment,1,0,1
423g43,2016-01-21 21:26:04-05:00,sadelbrid,While loop runtime complexity,"I'm trying to figure out the runtime of something like this

    i = 1
    while i < n^2
        for j = 0 -> i
            ...
        i *= 3

I think the while loop runs log(n) times. Does that make the runtime n^2 * log(n)?",,,,,Submission,9,0,9
cz7m4xb,2016-01-22 05:12:36-05:00,thewataru,,"Yes, outer loop runs log(n) times. You can bound whole number of steps as O(n^2 log n). But it's not a tight bound. To get a tight bound you have to actually count number of steps:   

    1+3+9+27+...+3^k < n^2

This sum can be simplified to  

    (3^{k+1} - 1)/2

Now, using the fact that 3^k can be as big as n^2 we can get the upper bound O((3*n^2 - 1)/2) which is O( n^2 )

EDIT: formulas",423g43,t3_423g43,sadelbrid,,Comment,3,0,3
cz7pis4,2016-01-22 08:25:43-05:00,sadelbrid,,"Does that account for the inner loop?
",423g43,t1_cz7m4xb,thewataru,,Reply,1,0,1
cz7swp0,2016-01-22 10:14:49-05:00,thewataru,,"yes, what i did is just counted how many operations inner loop performs. At first outer iteration it's 1. Then it's 3. At the next outer loop iteration i will be 9 and inner loop will perform 9 operations. Sum it all and get O(n^2). Even if you account for additional operations in outer loop (like increase of i) it all will be only O(logn) and will not contribute to the final asymptotic complexity.",423g43,t1_cz7pis4,sadelbrid,,Reply,1,0,1
cz7tlm7,2016-01-22 10:33:03-05:00,sadelbrid,,Gotcha. I didn't realize you were referring to the inner loop. Thanks,423g43,t1_cz7swp0,thewataru,,Reply,1,0,1
cz7jlxu,2016-01-22 02:23:11-05:00,dandrino,,"It's O(n^2 ). The largest value of i (let's call it i_max) that occurs in the final iteration of the while-loop fits the following inequality: n^2 / 3 <= i_max < n^2. All other values of i will sum to a constant factor (more or less because it is a finite sum) times i_max because they form a geometric sum. Because i_max grows quadratically, the total number of iterations grows quadratically, too.",423g43,t3_423g43,sadelbrid,,Comment,1,0,1
cz7d2rv,2016-01-21 22:21:21-05:00,JoTheKhan,,"i goes from 1 to n^2 by a factor of 3. So (1/3)(n^2) times which is Big O of n^2.

for j = 0 -> i, j goes to n^2 because at i's greatest value it is n^2 - 1, so big O of n^2.

So it looks like to me its

(1/3) * (n^2 ) * (n^2 ) = (1/3)(n^4 ) or big O of n^4 ",423g43,t3_423g43,sadelbrid,,Comment,0,0,0
cz7e4u1,2016-01-21 22:53:09-05:00,lordvadr,,"It's not += 3, it's *=3, so the outer loop is absolutely log(n).  The while loop runs k times with the relationship 3^k >= n^2.

    3^k >= n^2
    k*log(3) >= 2*log(n)

Drop the constants and you get k ~ log(n).

I'm gong to have to work out how to calculate the inner loop (or the total for that matter).  It's sum(1,n^2) which is on the order of n^3, but I don't know how to deal with the exponential increase of i.

BTW, CS degree 10 years ago and really rusty.  So my guess would be n^3 log(n), or log( n^3 ), or maybe log(n)^3 ",423g43,t1_cz7d2rv,JoTheKhan,,Reply,2,0,2
41uxiv,2016-01-20 10:55:28-05:00,moneyking123,Downloading videos,"Hello! 
I wanted to know how to download videos from a website. Potentially get the link to where the video was uploaded.
Any help would be appreciated!
Thanks",,,,,Submission,0,0,0
cz5dm5z,2016-01-20 12:14:55-05:00,hellscyth,,"That depends what site you're talking about. For downloading specific files from a website I personally use ""wget"".    If that doesn't work do a google search and see if someone has already made a tool for the specific site.",41uxiv,t3_41uxiv,moneyking123,,Comment,1,0,1
41un73,2016-01-20 09:50:28-05:00,gllass,Help showing critical section example in pseudocode and using semaphores,"I'm studying for my operating system exam by going through old exams and answering the questions. I stumbled upon one that I don't really get. I know what a critical section is but I don't get how to show one that is composed of two separate segments. The questions I'm trying to figure out are:



""a.)Give a pseudocode example on a situation where critical section is composed of two separate code segments. Give a multihreaded scenario for your example where the final result is erroneous because the critical section was not protected properly?""


and 


""b.)Show (with pseudocode) how the critical section in your example should be protected with semaphores. Remember to define all used semaphores with their initial values. Explain why the erroneus scenario (in part a) can not occur now.""



Any help/explanation would be much appreciated. I think I might just be thinking about this question wrong. I can't find any examples in our material of critical sections being divided.",,,,,Submission,4,0,4
cz5bzl0,2016-01-20 11:36:28-05:00,invisiblewardog,,"It sounds like a producer/consumer problem. One process with two threads, one of which writes to a variable or buffer, and the other reads from it.

[Try this page](http://www.embedded.com/design/mcus-processors-and-socs/4007127/Threading-and-Parallel-Programming-Constructs-used-in-multicore-systems-development-Part-2), in particular figures 4.7 (part a of your question) and 4.8 (part b).

Edit: I think they have a typo in 4.8.  Try [this article too](https://en.wikipedia.org/wiki/Producer%E2%80%93consumer_problem#Using_semaphores).",41un73,t3_41un73,gllass,,Comment,3,0,3
cz5drq6,2016-01-20 12:18:30-05:00,gllass,,"Initially I thought it might be this too, but the next question in this exam was about the producer-consumer problem. I doubt he would have two questions on the same thing. I'll have a look at those pages for the next question though, thanks!",41un73,t1_cz5bzl0,invisiblewardog,,Reply,2,0,2
cz5e3gg,2016-01-20 12:26:02-05:00,invisiblewardog,,"I had a professor do that too.  My response to the next question was ""everything is the same as the previous question except xyz"" and he was fine with it.  Your results may vary.",41un73,t1_cz5drq6,gllass,,Reply,2,0,2
cz5ebdg,2016-01-20 12:31:01-05:00,gllass,,"Haha nice. Risky, but nice. I don't think I have the balls to do that.",41un73,t1_cz5e3gg,invisiblewardog,,Reply,1,0,1
cz5crzx,2016-01-20 11:55:10-05:00,None,,"Concurrency is all about Reads and Writes. Show a set of read and write actions happening in a parallel system that would cause problems.

Here's an exercise in Go: https://gobyexample.com/mutexes",41un73,t3_41un73,gllass,,Comment,3,0,3
cz58802,2016-01-20 10:01:06-05:00,gllass,,Does the question ask for two processes that each have their own critical section or one process that has two? That's where I'm having most of the problems. If I just need to show two processes that fail because they try to enter their own critical sections at the same time it's much easier.,41un73,t3_41un73,gllass,,Comment,2,0,2
cz5adcn,2016-01-20 10:57:00-05:00,incredulitor,,"It's really hard to tell from the wording given.  My gut feeling is that it's talking about a case where you have one process, with two distinct pieces of code that could run at separate times, but there's some situation where they both have to be run as a single critical section in order for the program to be correct.  In that case you could use a semaphore in the caller surrounding both pieces of called code that gets signalled after both complete.",41un73,t1_cz58802,gllass,,Reply,2,0,2
cz5b019,2016-01-20 11:12:36-05:00,gllass,,I also felt the wording just wasn't clear. I hate when teachers do that :/ I think your idea might be right. Lets hope this particular question isn't on the exam though. Thanks for the idea :),41un73,t1_cz5adcn,incredulitor,,Reply,2,0,2
41q98o,2016-01-19 14:21:40-05:00,niysso,Computer shutting down randomly.,"Hello, I have Lenovo g510 computer. My computer just randomly shuts down all the time. Though it might be overheating issue. Cleaned my fan, didn't helped. Computer shuts down in normal temperature. I don't know what to do. Can you help me? Thank you for your answers.",,,,,Submission,0,0,0
cz49eig,2016-01-19 14:33:16-05:00,ShittyAlgorithms,,No. This is /r/AskComputerScience. Computer science doesn't have anything to do with fixing hardware. We do algorithms and discrete math and shit. You need /r/TechSupport.,41q98o,t3_41q98o,niysso,,Comment,9,0,9
41q729,2016-01-19 14:09:40-05:00,strikerx,"If you are given a file without an extension, how would you figure out the file type?","You cannot use any ""File Identifier"" programs, just text editors and command prompt. I've already used a text editor to open the file and no text shows up.",,,,,Submission,22,0,22
cz49tbk,2016-01-19 14:42:52-05:00,SneakingNinjaCat,,Look at the file's magic number...,41q729,t3_41q729,strikerx,,Comment,20,0,20
cz4efa0,2016-01-19 16:28:16-05:00,nawfel_bgh,,[Wikipedia link for the lazy](https://en.wikipedia.org/wiki/File_format#Magic_number).,41q729,t1_cz49tbk,SneakingNinjaCat,,Reply,5,0,5
cz4hus0,2016-01-19 17:48:09-05:00,naetus,,"Open it with an hex editor and read the first few bytes, then compare them with the list of well known signatures.

As an example, assuming you have a file named /bin/ls in your file system:

    hexdump -C -n 8 /bin/ls

This command will show you the first 8 bytes of the /bin/ls file in hex format: in this case the result is:

    00000000  7f 45 4c 46 02 01 01 00                           |.ELF....|

This means that at offset 0, the next 8 bytes are 7f 45 4c 46 02 01 01 00: if you check the [list of known signatures](http://www.garykessler.net/library/file_sigs.html), you'll find that 7F 45 4C 46 is the signature of an ELF file (executable file on *nix).

You can, of course, use any editor in hex mode to do exactly the same thing: hexdump is a pretty nifty and useful command on *nix, though.

EDIT: fixed the formatting.",41q729,t3_41q729,strikerx,,Comment,10,0,10
cz6ts7z,2016-01-21 14:13:31-05:00,strikerx,,"Wow! Thanks for the info. So I opened the file in a program called HEX for OS X and the first few bites read ""504B0304"", as I looked up on databases, I found it to be a .PK file. Prior to this I came to the conclusion it was a Zip file but changing the extension didn't work. ",41q729,t1_cz4hus0,naetus,,Reply,2,0,2
cz4aqok,2016-01-19 15:04:04-05:00,anamorphism,,"well, there are pretty much two primary forms of files: ascii and binary.

opening an ascii file in a text editor will show you text. opening a binary file in a text editor will show you a bunch of garbage characters (text editor converts all of the bits to characters resulting in a bunch of randomness).

as for figuring out what type of ascii file it is, you would judge it by its contents. whole bunch of c# code? probably a .cs file. html, css, javascript? you get the drill. file extensions for ascii files are just there so your operating system can define applications that handle opening them by default.

as for binary files? most standard binary file formats start with a predefined header. for example, bmp files start with a 14 byte header followed by an additional fixed size header from one of 7 different formats. https://en.wikipedia.org/wiki/BMP_file_format",41q729,t3_41q729,strikerx,,Comment,7,0,7
cz4nxsu,2016-01-19 20:24:16-05:00,lgastako,,You may be interested in the unix `strings` command.  Use the man page for more info (`man strings`).,41q729,t3_41q729,strikerx,,Comment,2,0,2
cz4oetq,2016-01-19 20:36:43-05:00,None,,[deleted],41q729,t3_41q729,strikerx,,Comment,2,0,2
cz6tvzs,2016-01-21 14:15:54-05:00,strikerx,,Nothing appears when opened with display. But my question has been [answered.] (https://www.reddit.com/r/AskComputerScience/comments/41q729/if_you_are_given_a_file_without_an_extension_how/cz4hus0) The file is a .pk file.,41q729,t1_cz4oetq,None,,Reply,1,0,1
cz4jxlo,2016-01-19 18:39:28-05:00,artillery129,,"Look at the mime data! You can use lib magic, can check file type

http://linux.die.net/man/3/libmagic",41q729,t3_41q729,strikerx,,Comment,1,0,1
cz6qkkl,2016-01-21 13:00:56-05:00,doverthere,,"This is probably one of the quicker solutions. Languages like php have a built in function to determine the mime type  
  
e.g. in php:  
  
    <?php  
    echo mime_content_type('hello.gif')  
  
outputs  
  
    image/gif  
  
",41q729,t1_cz4jxlo,artillery129,,Reply,2,0,2
cz6qnov,2016-01-21 13:02:54-05:00,artillery129,,"Yes, absolutely, but you'd have to setup a PHP server then :D",41q729,t1_cz6qkkl,doverthere,,Reply,1,0,1
cz6qqi4,2016-01-21 13:04:41-05:00,doverthere,,"No, you just need php installed on your computer and appropriate permissions on the file. You don't need a dedicated server",41q729,t1_cz6qnov,artillery129,,Reply,1,0,1
cz499rk,2016-01-19 14:30:07-05:00,pinch_it,,"This is an interesting question.

It's easy to find a text file, using your text editor. If the file is an object file or an image, the text editor would be filled with illegible characters and symbols. This is usually the stuff only machine understands.

Since your file is empty when you cat it, it's probably en empty text file.",41q729,t3_41q729,strikerx,,Comment,1,0,1
cz4er02,2016-01-19 16:35:38-05:00,lodolfo,,"See if this helps. If you're using something unix-like and your file is named ""hello"", type this on the terminal:

    $ file hello    

",41q729,t3_41q729,strikerx,,Comment,-3,0,-3
cz4eyg4,2016-01-19 16:40:20-05:00,mishac,,"> You cannot use any ""File Identifier"" programs, 

He specifically asked how you'd do it *without* without using programs like ""file"".",41q729,t1_cz4er02,lodolfo,,Reply,3,0,3
41piqs,2016-01-19 11:55:40-05:00,OVDU,What is the main difference between Coroutine and Subroutine?,,,,,,Submission,5,0,5
cz45w7s,2016-01-19 13:12:12-05:00,daymi,,"Subroutine runs its course and returns to the caller once:

    def f(x):
        return 2
        return x # useless

Coroutine ""returns"" to the caller many times, continuing the coroutine in-between:

    def f(x):
        yield 2
        yield x # useful

Extreme case of coroutine:

    def f():
        i = 0
        while True:
             yield i
             i += 1

Data can also flow from the caller into the coroutine in the middle of the thing:

    def f():
        i = 0
        while True:
             j = yield i # value for j comes from the caller: a new one for every `yield'
             i = j + 1

Caller usage of coroutines similar to lists or generators.",41piqs,t3_41piqs,OVDU,,Comment,3,0,3
cz4cdud,2016-01-19 15:42:02-05:00,OVDU,,Thanks a lot ,41piqs,t1_cz45w7s,daymi,,Reply,2,0,2
cz49ei4,2016-01-19 14:33:15-05:00,pinch_it,,"oroutines are generalizations of the normal ""subroutines"". The main difference is that each invocation of a subroutine has the same starting point and the same end point all the time, while a coroutine has multiple entry points and multiple pathways. The rigurous definitions are not easy to understand, so, to put it simple, a coroutine is a function that works as a state machine, and for each state the coroutine behaves differently. You can see the coroutine as a collection of multiple routines that share the same code, and are linked by a state machine.",41piqs,t3_41piqs,OVDU,,Comment,3,0,3
cz4cdi8,2016-01-19 15:41:49-05:00,OVDU,,Thanks a lot ,41piqs,t1_cz49ei4,pinch_it,,Reply,2,0,2
41pbn7,2016-01-19 11:14:12-05:00,Shrubberer,Pumping Lemma for a CFG,"I have to find a proof that a given grammar isn't context free using the pumping lemma. I had a go at it and my conclusion is, that I still don't get it.

The Grammar is as follows: a^k b^m a^k where k,m >= 0 and k>=m.

I started a case by case analysis and most of them are trivial. Thought I'm stuck where uwx = a^i b^m a^i (assuming the form z = uvwxy). uwx is greater than 0 obviously and any integer i won't change, that the word is element of the language. It get's even more confusing when I skip the 'b' alltogether (m=0) and only conctruct strings of a's.

Since the assignment explicitly asks us to proof that the language is NOT context free, I can only assume, that I have to look at the |vwx| <= p contraint. That's where I'm lost. What is that p number even supposed to mean?
",,,,,Submission,10,0,10
cz4b74l,2016-01-19 15:14:42-05:00,_--__,,"You sound very confused about the whole idea behind a PL proof.  Can you please write out a proof using the PL for regular languages, say proving the language a^(k)b^(m) where k>=m is not regular?",41pbn7,t3_41pbn7,Shrubberer,,Comment,3,0,3
cz4dcjh,2016-01-19 16:03:53-05:00,Shrubberer,,"Alright then. If I choose k=m and select k as my pumping length, the resulting string is a^k b^k, just to make sure I get a word longer than k. 

I have to cast that word into the form xyz. Obviously 'x' will only be a's and z only 'b'. As for y, I choose one of the a's. Since the total amount of 'a' is exactly the pumping length, I don't violate the rule when I take a subset of x. 

Now, since any word a^g (a^h)^i  b^p with g+h=p for any integer i is in the language, I can only assume that the given language is regular. 

Needless to say, Yes, I'm totally confused. ",41pbn7,t1_cz4b74l,_--__,,Reply,1,0,1
cz4ebyp,2016-01-19 16:26:11-05:00,_--__,,"You are getting mixed up with what you can choose and what you can't.  

* At the start you are correct, you can select k as the pumping length - you cannot select the pumping length (you have not), the PL asserts that *if* the language is regular *then* there is some length p which satisfies <conditions of PL>.
* You can choose k=m, but that's not really what you are doing.  Technically, you are choosing the word w=a^(k)b^(k) (where k is the pumping length).  This word is in the language because it is of the form a^(k)b^(m) where k>=m.  This is important because it means we can apply the PL to it.
* You cannot choose how w breaks up into xyz - the PL asserts that it does it somehow (with certain conditions on x,y, and z), but because you cleverly chose your w you are imposing some conditions on x,y,z yourself.  For example, because |xy| has to be at most the pumping length, and the first k letters of w are all 'a', we know that x=a^r and y=a^s for some r & s (but we don't know what r & s are, we just know that r+s<=k).  Secondly, because |y|>0 we know that s>0.  Finally, because of how things break up, we know that z = a^(k-r-s)b^(k).
* Now we work towards the contradiction.  The PL asserts that xy^(i)z is in the language for all i>=0, in particular xz is in the language.  But we know that xz = a^(r)a^(k-r-s)b^(k) = a^(k-s)b^(k). Since s>0, k-s < k, so xz is not in the language.  Contradiction.

Does that make a bit more sense?",41pbn7,t1_cz4dcjh,Shrubberer,,Reply,3,0,3
cz4bbxa,2016-01-19 15:17:46-05:00,dogcowpigaardvark,,"""If a language L is context-free, then there exists some integer p ≥ 1 (called a ""pumping length"") such that every string s in L that has a length of p or more symbols (i.e. with |s| ≥ p) can be written as
s = uvwxy

with substrings u, v, w, x and y, such that

1. |vwx| ≤ p,

2. |vx| ≥ 1, and

3. u v^n w x^n y is in L for all n ≥ 0."" - [Wikipedia](https://en.wikipedia.org/wiki/Pumping_lemma_for_context-free_languages)


You have to prove by contradiction that the grammar is not context free: ""Suppose that a^k b^m a^k where k,m >= 0 and k>=m  *is*  a context-free grammar. We will have to find some number that satisfies all the conditions of the pumping lemma for any pumping length p. An example string in this language is ""aaabbaaa"" for k = 3. {We are trying to find a contradiction, and any combination of numbers will probably do that, but I'm just going to start with k = 3 and see what happens.} 
Break the string up into:

u =  aa

v^p = ab 

w = empty

x^p = ba

y = aa

Pumping this string once results in the string ""aaababbabaaa"", which is clearly not in the language described by the grammar... therefore L is not context-free. If it were you'd find any pumping length, would result in a string that is in the language. But I just showed one that isn't. QED


You may have to prove that **any** combinations of aaabbaaa in uvwxy are not in the language. This knowledge is good for algorithm analysis and developing a ""testing mentality"". Or so I was told. ",41pbn7,t3_41pbn7,Shrubberer,,Comment,3,0,3
cz4ce5f,2016-01-19 15:42:14-05:00,Shrubberer,,"But I could find such a string in a context free language as well. Let's say a^n b^n. If I choose vwx to be one of the a's, any pumping will create a word not in the language. 
",41pbn7,t1_cz4bbxa,dogcowpigaardvark,,Reply,2,0,2
cz4effi,2016-01-19 16:28:21-05:00,dogcowpigaardvark,,"You could indeed. But in THIS language, there exists a combination of aaabbaaa in uvwxy *that abide by those three conditions of the pumping lemma* but compute strings that are not in the language. So, even though you can make strings that abide, it's still NOT context free because it doesn't create ALL strings that follow the grammar. The set of ALL strings produced by the grammar must still be in the language for some pumping length p. You can show that there is one that breaks the rule. Contradiction. The grammar describes the language. So if you increase the pumping length p and the string produced can't be generated by the grammar any more, then you have just used the pumping lemma to PROVE that the language is not context free: if it were, the pumping lemma could be used to create ANY string in the language. 

You're just trying to prove that the given grammar can be broken up into uvwxy in a way that produces strings when pumped that are no longer in the language. The pumping lemma for regular languages can be used to disprove a^n b^n. a^n b^n .",41pbn7,t1_cz4ce5f,Shrubberer,,Reply,2,0,2
41oxv0,2016-01-19 09:37:32-05:00,nawfel_bgh,Questions about spectrum division in mobile telephony networks,"Spectrum is a limited resource. Therefore, mobile carriers need to reuse their frequency bands without causing interference. This problem (Frequency Assignment Problem) can be modeled as a graph coloring one. Mobile networks operators nowadays use frequency division or Code division to let multiple users to connect to the network.

In GSM networks, frequencies are assigned to base stations and are used to link between cell phones and the nearest base station. Base stations are assigned more frequencies in the cells (geographic regions) having high demand

3G networks use frequency division, other networks use code division (CDMA), and LTE (4G) networks use Orthogonal Frequency division (OFMDA). 

### Questions

A) Are these little frequency bands:

1. **assigned by user** (mobile phone): A base station communicating with N phones needs N bands.
2. Or are they **shared between phones using an access control protocol** (like CSMA/CA for example): A base station communicating with N phones needs M bands. with M <= N.

B) What is the difference between old school frequency division and OFDMA?

C) Can all of these methods (Frequency division, OFDMA and CDMA) be modeled as a graph coloring problem?

-----------

Links to good reading (or video) materials will be appreciated.

[I submitted the same question in /r/AskEngineers](https://www.reddit.com/r/AskEngineers/comments/41ow0z/questions_about_spectrum_division_in_mobile/).",,,,,Submission,2,0,2
41naru,2016-01-19 01:15:37-05:00,TheUndead96,A Question about Huffman Coding,"I have been dabbling in information science and I have a question regarding Huffman coding. 

-Does it make sense to have a code where there are multiple word lengths? 

-Surely in a real world computer scenario a code corresponding to certain values would need to be fixed to the length of its longest word?

Any help would be greatly appreciated.",,,,,Submission,2,0,2
cz49pbr,2016-01-19 14:40:15-05:00,pinch_it,,"Your maximum length of an encoded letter or character would be the height of the Huffman tree. 

Huffman coding can have multiple word lengths.

[Example](https://www.youtube.com/watch?v=MleGSpPpHXs)",41naru,t3_41naru,TheUndead96,,Comment,1,0,1
41n5gi,2016-01-19 00:29:44-05:00,fakeabuela,Can CPU caches store data without power?,,,,,,Submission,1,0,1
cz3op6y,2016-01-19 01:34:37-05:00,_chrisc_,,Caches are (virtually) always made out of [SRAM](https://en.wikipedia.org/wiki/Static_random-access_memory).  They require constant power.,41n5gi,t3_41n5gi,fakeabuela,,Comment,5,0,5
cz3pq6b,2016-01-19 02:24:24-05:00,fakeabuela,,Thanks! I vaguely remember hearing about SRAM so this helps a lot!,41n5gi,t1_cz3op6y,_chrisc_,,Reply,1,0,1
cz49cee,2016-01-19 14:31:50-05:00,Delwin,,"They can't... with a few minor caveats.

If you drop them in liquid nitrogen they keep their data for a little while without power.  This is used only in computer forensics and even then very rarely.",41n5gi,t3_41n5gi,fakeabuela,,Comment,2,0,2
41mw6s,2016-01-18 23:19:25-05:00,revereddesecration,Do I understand neural networks correctly?,"Let's say I used a microphone to record 1000 files of myself saying ""hello world!"" - not 100 duplicate files but 100 separate recordings - and used that data to train a neural network. Could I then use that neural network to generate new audio files of myself saying ""hello world!"" that are not the same as the files I fed into the network but still distinguishably my voice and those words?

If so, is there any resource/article/post that explores this idea? If not, what kind of process would be used to achieve this goal?",,,,,Submission,9,0,9
cz3mjca,2016-01-19 00:10:20-05:00,katkov,,"Neural nets typically don't generate data, but are used in to identify patterns. In this case it would be more similar to feeding it 100 files, some saying hello world, some not, and telling it which is which. After training on those files the goal is to feed it unknown files and have it say if its you saying Hello World or not.

What your describing is similar to Deep Dream, which is a neural net trained as above on an image, but then reversed, in order to generate images instead",41mw6s,t3_41mw6s,revereddesecration,,Comment,9,0,9
cz3no3u,2016-01-19 00:52:07-05:00,zardeh,,"While I agree entirely, the real answer to OPs question is yes, just that in their current form, we're pretty bad at this.

NNs, in their purest form are just a method of compression. We want to represent the essence of ""hello world"" or a picture of a dog, or whatever, in a smaller format than {the collection of all images of dogs} or {the collection of all audio recordings of hello world}.

If we can efficiently create a classifier that can, with 100% accuracy, classify pictures of dogs vs. pictures of not dogs, it stands to reason we can use the data within that classifier to produce pictures of dogs. (a trivial way to do this would be to throw random images at it until it returns true, but you can easily do better).

You can see some examples of this, [in google's MINST tensorflow example](https://www.tensorflow.org/versions/master/tutorials/mnist/beginners/index.html), you can see the positive and negative weights which in many cases resemble the numbers. 

Similarly, deepdream is the best example of a system like that. Obviously, far from perfect, but theoretically possible.

",41mw6s,t1_cz3mjca,katkov,,Reply,3,0,3
cz3neb9,2016-01-19 00:41:37-05:00,revereddesecration,,"I had indeed heard about Deep Dream, so it's a variation on a neural network or is it a layer built on top if a neural network? Seems like I should be reading up on Deep Dream primarily then?",41mw6s,t1_cz3mjca,katkov,,Reply,1,0,1
cz3ouip,2016-01-19 01:41:15-05:00,ebix,,"If you're interested in ""generative models"" you might want to check out [this paper on generative adversarial networks](http://arxiv.org/pdf/1406.2661v1.pdf) which attempt to build neural networks that generate elements of a class from random noise.

Basically the approach is, train two NNs in tandem. (1) a ""discriminator"" which attempts to tell the difference between elements in the training set and elements generated by (2) a ""generator"" that takes in random noise, and uses it to generate an element in the set. 

The generator is then trained by trying to make the discriminator perform as poorly as possible. By alternating between training the two networks you can get pretty interesting generators. 

EDIT: I might add that generating elements in a class is one of the harder problems in ML, and even state of the art methods are going to be pretty mediocre at it (depending on the complexity of the class and it's elements). Additionally, state of the art methods are going to require a lot of background understanding in ML to implement well. I certainly don't want to discourage exploration, but to avoid disappointment I would recommend tempering your expectations. ML isn't magic :-). 

EDIT2: The paper linked above is the canonical introduction to GANs, but I realized now that there are probably a lot more interesting presentations from a non-researcher perspective. In particular if you're willing to wade through some code this is pretty interesting. https://github.com/Newmu/dcgan_code ",41mw6s,t1_cz3neb9,revereddesecration,,Reply,2,0,2
cz3utql,2016-01-19 07:36:15-05:00,revereddesecration,,"Thanks for the thorough response! I had a strong suspicion that things would not be as simple as I wanted them to be. I'll certainly be keeping my eyes out for further progress in this field though, it's fascinating.",41mw6s,t1_cz3ouip,ebix,,Reply,1,0,1
cz97v5n,2016-01-23 14:44:58-05:00,Bottled_Void,,"Neural networks essentially from a set of logical expressions to come up with an algorithm to process any certain input.


So let's say you had a single node input and a single node output. You could pass in a value X and train it to return the results X^2. That's simple enough. Well except it may actually return the results of F(X) = X (X-X/1000).


I'm reminded of a neural network that was taught to recognise tanks. Except for the sample set it was trained with all the pictures with tanks were taken on a foggy day. So they were asking, ""Can you see a tank"" and it was responding with, ""It's quite foggy today"".


If you give a sample set of 100 similar files and say, look for these. You're not telling it what to ignore. It could be looking for a certain pitch or a certain volume instead. So along with your 100 files, you'll also have to give it many many more files which you don't want it to match.


Matching up a voice file is complicated because it's a long sequence of data with slight variations, it would make it simpler if you could pre-process it to reduce that down to something more manageable.",41mw6s,t3_41mw6s,revereddesecration,,Comment,2,0,2
cz3moxo,2016-01-19 00:15:50-05:00,yes_thats_right,,"Neural Networks are usually used to take in many pieces of data to learn patterns and then when presented a new piece of data, be able to match it to one of the rules it has learned.

To bring this into the context of your question, you could play 1000 audio files to your neural network program and hope that it learns which ones are you saying ""hello world!"" and then once it has finished learning it would hopefully be able to identify whether a new sound is also saying ""hello world"" or not.

I don't quite understand the purpose of what you want to achieve. Usually if we want to reproduce a phrase, we just record it and then play it back. If you want it to sound different then apply a filter or distort it.",41mw6s,t3_41mw6s,revereddesecration,,Comment,1,0,1
cz3nlt1,2016-01-19 00:49:38-05:00,revereddesecration,,"The concept in my head was, if a neural network can distill the essence of some audio data into a set of nodes, could it then use the nodes to reconstruct the audio? Is there a reason why this would not be possible?",41mw6s,t1_cz3moxo,yes_thats_right,,Reply,3,0,3
cz418xd,2016-01-19 11:20:23-05:00,videoj,,"Take a look at [Markov Chains](https://en.wikipedia.org/wiki/Markov_chain#Music).  Also, you might get a better answer at /r/machinelearning.",41mw6s,t1_cz3nlt1,revereddesecration,,Reply,2,0,2
cz3zdiq,2016-01-19 10:30:24-05:00,earslap,,"It is possible to some extent, just not with pure feed forward neural networks. See this for example (IIKC this should be Geoffrey Hinton's work): http://www.cs.toronto.edu/~hinton/adi/

Select a digit from the top, and press play (possibly after increasing speed) and it will show you *generated* novel hand written digits deduced from its training data which the network *believes* will be similar to the digit you have chosen.

More info here: http://www.cs.toronto.edu/~hinton/digits.html",41mw6s,t1_cz3nlt1,revereddesecration,,Reply,1,0,1
cz3mz67,2016-01-19 00:26:00-05:00,high_side,,"> I don't quite understand the purpose of what you want to achieve. Usually if we want to reproduce a phrase, we just record it and then play it back. If you want it to sound different then apply a filter or distort it.

Seems like a cool idea - to have software generate variations on the input phrase that aren't produced mathematically or randomly.

It's not what ANN does, but it's an interesting project.",41mw6s,t1_cz3moxo,yes_thats_right,,Reply,2,0,2
cz3nysv,2016-01-19 01:03:58-05:00,revereddesecration,,It's not a concept that will change the world but it could add a little bit of extra depth to various voice-related applications. ,41mw6s,t1_cz3mz67,high_side,,Reply,1,0,1
41frzo,2016-01-17 17:20:53-05:00,coconutscentedcat,What do CompSci students do for fun?,"
Do people often get together and game? Any lan parties? I'm going into CS next year and it would be nice to find gaming buddies. I've always gamed alone. Such loneliness.

Do you meet up and learn programming together? programming competitions? I want to program with people, learn github and make things together.
",,,,,Submission,15,0,15
cz20h6g,2016-01-17 17:47:38-05:00,mosqutip,,The same things non-CS students do for fun?,41frzo,t3_41frzo,coconutscentedcat,,Comment,78,0,78
cz20dr9,2016-01-17 17:45:10-05:00,UniverseCity,,Anything besides CS. ,41frzo,t3_41frzo,coconutscentedcat,,Comment,59,0,59
cz26wbf,2016-01-17 20:36:09-05:00,Zaveno,,"I can't speak for all CS students, but I myself like to play Magic the Gathering. Fun way to make friends and have a god time.",41frzo,t3_41frzo,coconutscentedcat,,Comment,9,0,9
cz2g7zp,2016-01-18 01:13:51-05:00,coconutscentedcat,,Oh that would be fun to get into. Do many people play MTG are your college or just a few ppl? I hope I could find a club at the college im attending,41frzo,t1_cz26wbf,Zaveno,,Reply,2,0,2
cz2gn64,2016-01-18 01:30:45-05:00,Zaveno,,A good number of us play. There's a local game store nearby that hosts Friday Night Magic events that are always fun to attend.,41frzo,t1_cz2g7zp,coconutscentedcat,,Reply,2,0,2
cz2uhen,2016-01-18 11:59:25-05:00,fitzjack,,"That's one of the most expensive hobbies I have. Only my comic book collection and car hobby trump it in spending. Wizards keep making it harder and harder to get into as well, especially with the proxy bans here lately. My local game store has completely quit now which sucks but it's to be expected when so many changes keep happening that splinter the playerbase.",41frzo,t1_cz2gn64,Zaveno,,Reply,2,0,2
cz31uyt,2016-01-18 14:57:59-05:00,ShittyAlgorithms,,"I'm a CS major and I:

* Do yoga
* Practice ballet
* Go to the gym
* Program things (alone usually)
* Work in research
* Grab drinks/eat dinner/party with friends
* Connect with people on GitHub/Twitter/etc.
* Netflix and chill
* Attend conferences
* Go to the theatre
* Travel
* Crack really nerdy inside jokes with friends who also know CS stuff
* Go on dates
* Sing songs and dance around my apartment (often in the style of musical theatre)
* Reddit, unfortunately... how^do^I^stop^.^.^.^? :'(

I'm a CS major and I *don't*:

* Hackathon (I really dislike the atmosphere/culture)
* Stay inside all day
* Game (aside from the occasional holiday Civ V/Sims session)
* Live with my parents
* Watch anime
* Play chess/go/etc. regularly

Stereotypes about CS students are overblown. Usually individuals fit a couple of stereotypes, but almost no one fits very many. If you fit a CS stereotype you will probably find similar people who fit that same stereotype, but most won't.",41frzo,t3_41frzo,coconutscentedcat,,Comment,6,0,6
cz224ai,2016-01-17 18:30:15-05:00,None,,[deleted],41frzo,t3_41frzo,coconutscentedcat,,Comment,17,0,17
cz317ya,2016-01-18 14:42:34-05:00,ShittyAlgorithms,,"> change the new tab page of the web browsers on the school's networked computers to a gay porn site

Does this kind of stuff still happen at other schools? Put up *gay* porn solely because making it *gay* porn is somehow funnier? Failing that, put up straight porn just to make a decent portion of women in the department feel understandably uncomfortable in public study spaces? Great joke.

I'm really thankful that I go to a school where this kind of shit doesn't happen. You know, because people around here understand that my sexuality isn't a punchline.

These sorts of descriptions remind me why this field is struggling to retain anyone from demographics that don't include ""straight"", ""white"", and ""male"".",41frzo,t1_cz224ai,None,,Reply,-3,0,-3
cz31ccz,2016-01-18 14:45:29-05:00,None,,[deleted],41frzo,t1_cz317ya,ShittyAlgorithms,,Reply,2,0,2
cz31yta,2016-01-18 15:00:35-05:00,ShittyAlgorithms,,"Case and point. Anyone who doesn't find your joke funny is a SJW or some variation on that same theme.

God forbid someone doesn't look and think the same way as you. They might be gay, or female, or vegan. What a funny joke.",41frzo,t1_cz31ccz,None,,Reply,-3,0,-3
cz3240t,2016-01-18 15:04:06-05:00,None,,[deleted],41frzo,t1_cz31yta,ShittyAlgorithms,,Reply,-3,0,-3
cz36iid,2016-01-18 16:47:33-05:00,keepdigging,,Well he is running shitty algorithms...,41frzo,t1_cz3240t,None,,Reply,1,0,1
cz2c7so,2016-01-17 23:01:01-05:00,theacorneater,,frat parties and such. CS people are normal people too.,41frzo,t3_41frzo,coconutscentedcat,,Comment,10,0,10
cz23ryb,2016-01-17 19:13:14-05:00,simonorono,,"I learn new languages... 

In the last six months I learnt Scala, Kotlin and the LLVM IR.

Also, I built an application using JavaFX, FXML and SQLite.

This led me from being a Java hater to a JVM user (I loved Kotlin, still kind of hating Java).

Thanks to this I got interested in compilers, and I'm curently creating a new programming language with graphs as data type (using Kotlin and ANTLR 4 to compile the new language and output LLVM IR) as an undergrad research work.",41frzo,t3_41frzo,coconutscentedcat,,Comment,7,0,7
cz286f0,2016-01-17 21:13:00-05:00,mrbrianxyz,,WATCH ANIME,41frzo,t3_41frzo,coconutscentedcat,,Comment,9,0,9
cz2qud8,2016-01-18 10:19:29-05:00,imaghostspooooky,,Any favorites from this season?,41frzo,t1_cz286f0,mrbrianxyz,,Reply,3,0,3
cz211hb,2016-01-17 18:02:45-05:00,artillery129,,"Go ahead and attend hackathons, they sound like what you are describing, many programmers do those for fun. Also pick up hobbies outside of gaming and computer science",41frzo,t3_41frzo,coconutscentedcat,,Comment,6,0,6
cz29p1o,2016-01-17 21:52:54-05:00,g0pats,,"My suggestion  
  
Join the CS club of your school  
Find local meetups (meetup.com) regarding these interests  
Form study groups next year - maybe these guys will be into gaming and might want to start an open source project with you ",41frzo,t3_41frzo,coconutscentedcat,,Comment,2,0,2
cz2d5w0,2016-01-17 23:29:33-05:00,skunkwaffle,,"When I was in college I would play music with my friends, go to comedy shows, play ticket to ride and munchkin, watch TV with my roommates (Buffy and Fringe were the  big ones), have picnics in the summer and go sledding in the winter, go out to dinners, have sex, and play Minecraft. ",41frzo,t3_41frzo,coconutscentedcat,,Comment,2,0,2
cz2rd2q,2016-01-18 10:34:57-05:00,DemianJ,,"I like to:

- play guitar:
- hang out with friends: 
- work out: 
- play video games:
- program (personal 'fun projects'):
- cook:
- read.

I do not know whether CS students have a particular niche in hobbies. 
I like the occasional LAN party but generalizations are just what they are, not always correct. ",41frzo,t3_41frzo,coconutscentedcat,,Comment,2,0,2
cz39we6,2016-01-18 18:11:32-05:00,Darkflux,,"\#1 recommendation is try out a bunch of student clubs. If gaming is something you're interested in and there isn't a gaming club, make one (your student union can help make this less scary). I helped out with running my university's gaming club and I had a blast, met some lifelong friends along the way. My only regret is that I didn't get involved with clubs sooner.

I rarely think about my grades today, but I think about those friends all the time.",41frzo,t3_41frzo,coconutscentedcat,,Comment,2,0,2
cz8dvgu,2016-01-22 18:39:48-05:00,XisuperninjaiX,,As a CS student it is all about the LAN parties for me and my buddies. We also spend a good couple of hours a week working on improving our Java. Java is 10/10,41frzo,t3_41frzo,coconutscentedcat,,Comment,2,0,2
cz23p8s,2016-01-17 19:11:27-05:00,Not_Ayn_Rand,,"Tech-related: at my school gaming was a big thing for sure. There's a game center and a whole game dev track and everything. There was also a programming team, and people who were into that *really* got into it. Lots of startup challenges and the school supporting entrepreneurial pursuits. Security labs that let people hack into networks and computers for practice. 3D printing.

Non-tech: normal people stuff. My school was a big hipster place and most people were into live music/concertgoing/etc. But mostly, people had interests all over the place. There were people whose hobbies were all tech-related, but those people tended to stick to the same circles and weren't very social. There's so many different clubs you can join, and you definitely want to meet people outside of CS.",41frzo,t3_41frzo,coconutscentedcat,,Comment,2,0,2
cz2df4a,2016-01-17 23:37:46-05:00,srcreigh,,"I'm in 4th year CS. Judging from your interests, it seems you'll really like hackathons. They're student-organized huge competitions where you form teams and make things for profit and glory. Some of the bigger ones are TreeHacks (at Stanford), MHacks (at UMich), PennApps (at UPenn), and Hack the North (at UWaterloo in Canada: this is my school.)

CS interns tend to make pretty good money, so a lot of my friends take up more expensive hobbies like traveling, photography, investing, eating out at fancy restaurants, bartending (i.e. practicing mixing drinks with $1k worth of booze), clubbing, and drug use.

Video games like smash, DDR/ITG, dota, hearthstone, etc. are very popular. The math society at my school also has an *enormous* collection of board games.",41frzo,t3_41frzo,coconutscentedcat,,Comment,2,0,2
cz2ka81,2016-01-18 04:40:11-05:00,CrazyKiwiCake,,"I'm a photographer so I do that, listen to and make music, smoke weed.



Imagine The Dude, age 19.



But yea.. We're humans too, man. ",41frzo,t3_41frzo,coconutscentedcat,,Comment,1,0,1
cz2uprq,2016-01-18 12:05:19-05:00,SockPuppetDinosaur,,"Brew Beer, Drink Beer, Play Video Games, Watch TV, Bike around my city...

Pretty much anything except program :P",41frzo,t3_41frzo,coconutscentedcat,,Comment,1,0,1
cz2ym5n,2016-01-18 13:40:21-05:00,billyc74,,dota 2,41frzo,t3_41frzo,coconutscentedcat,,Comment,1,0,1
cz4388y,2016-01-19 12:08:58-05:00,mmej2,,"I hang out with friends (CS major and others), watch movies and TV shows, play videogames, read books, browse reddit, play piano, crochet, go for a walk or run, etc. We do the same as all other students. 
Source: Am a CS student",41frzo,t3_41frzo,coconutscentedcat,,Comment,1,0,1
cz2kj7m,2016-01-18 04:56:56-05:00,Use_My_Body,,"I waffle between trying to learn new and interesting programming things, and slutting myself to people in online roleplay ♥

Wanna have some fun~? :)",41frzo,t3_41frzo,coconutscentedcat,,Comment,-3,0,-3
41f3ah,2016-01-17 14:51:15-05:00,StephenSwat,Why is the reduction of a decision problem in terms of complexity theory a one-way process,"[Edit: Excuse the title. I'm not implying that's true, but that's the most likely fault I see in my logic.]

Hello everyone,

I was looking a little deeper into the NP-complete class of problems today and there is one problem with the class that keeps bugging me. As far as I know, a problem is NP-complete if you can reduce any NP problem to that problem in polynomial time. It would seem intuitive that reducing a problem into another problem in polynomial time would also imply that you could expand the NP-complete problem into the NP problem from which it was reduced in polynomial time. That seems to imply, however, that NP and NP-complete are equal to each other: if you can reduce NP problems A and B into NP-complete problem C (which should be possible due to the definition of NP-completeness) and the reduction is reversible, then you could reduce B into A in polynomial time (because a polynomial multiplied by a polynomial is also a polynomial) by first reducing B into C and then expanding C into A. That would seem to make every problem in NP, NP-complete.

Now, that seems unlikely because that would imply that NP == NP-complete. I assume that means that the reduction of a problem is not reversible, but I can't really see why. Would anyone take the time to quickly explain where I'm going wrong here?

Thanks in advance!",,,,,Submission,3,0,3
cz1xkm7,2016-01-17 16:33:34-05:00,ldpreload,,"Probably this gets easier to understand if we are more rigorous about terminology. Instead of ""problem"", let's talk about _classes_ of problems (2SAT, 3SAT, subset sum, factoring) and individual _instances_ of problems (whether this particular formula is satisfiable, whether there's a subset of these numbers that adds to a target, what the factors of this number are). Complexity classes like NP are about classes of problems, not instances.

The thing to observe is that classes have easier and harder instances, and there are instances (specific problems to solve) that are in multiple classes of problems. For example, factoring the number 2^57,885,161 is easy, but factoring the number 2^57,885,161 - 1 is hard. This means that, as a class, factoring is hard, because there exist instances of factoring that are hard: an algorithm for solving a class of problems has to solve _all_ of the problems in the class. Any instance of 2SAT (at most 2 terms per clause) is also an instance of 3SAT (at most 3 terms per clause), and the 2SAT problems remain easy when expressed as 3SAT, but that doesn't mean 3SAT as a class is easy. The only problems you can easily convert back from 3SAT to 2SAT are those where each clause happens to not have a third term.

So when you reduce one class to another, what you're really doing is reducing one class to a subset of another. Most of the time, that's a (very) proper subset. If you reduce 3SAT to clique, you're reducing any 3SAT instance to a clique instance that has a particular structure. So it _could_ be the case that there are also some clique instances that don't have that structure, that are much harder than 3SAT. That is, the reduction is only reversible for clique instances that have this particular structure, that look like they came from 3SAT. If they don't have the structure, there's no guarantee that they can be expanded back into a 3SAT problem: therefore those clique instances could be harder to solve than 3SAT.

The way that Karp's 21 NP-complete problems worked is that he first stated that they were all in NP, placing an upper bound on their difficulty (specifically, by using the Cook-Levin theorem that any problem in NP could be reduced to SAT). Then he reduced SAT to those problems, placing a lower bound on their difficulty (if you can solve the new problem in polynomial time, you can solve SAT too). If he just did the second step, they'd have been NP-hard, which could well be much harder than NP-complete.",41f3ah,t3_41f3ah,StephenSwat,,Comment,7,0,7
cz1y1qt,2016-01-17 16:45:43-05:00,StephenSwat,,Yes! This is perfect. Thank you very much!,41f3ah,t1_cz1xkm7,ldpreload,,Reply,2,0,2
cz1xp28,2016-01-17 16:36:45-05:00,umib0zu,,/u/StephenSwat think of these classifications as sets. Where do you think NP-Complete falls in a set diagram consisting of NP and the compliment of NP?,41f3ah,t3_41f3ah,StephenSwat,,Comment,2,0,2
cz1xxjc,2016-01-17 16:42:43-05:00,StephenSwat,,"As far as I'm aware, NP-complete sits inside of NP, together with P.",41f3ah,t1_cz1xp28,umib0zu,,Reply,2,0,2
cz1ytuj,2016-01-17 17:05:16-05:00,umib0zu,,"So if you take a problem class A in NP, and you can take a problem class B in NP-Complete, and convert an instance of B into an instance of A, is A in NP? Also, is A in NP-Complete?",41f3ah,t1_cz1xxjc,StephenSwat,,Reply,2,0,2
cz1z1zb,2016-01-17 17:10:53-05:00,StephenSwat,,"A is in NP, but not necessarily in NP-complete because you don't know if all NP problems can be reduced to it, right?",41f3ah,t1_cz1ytuj,umib0zu,,Reply,1,0,1
cz1z6xh,2016-01-17 17:14:27-05:00,umib0zu,,"So, if I can reduce B in NP-complete to A, and B is by definition, a problem such that any C in NP can be reduced to B, can't I take and C in NP, reduce it to B, then reduce it to A?",41f3ah,t1_cz1z1zb,StephenSwat,,Reply,2,0,2
cz1ziy1,2016-01-17 17:23:04-05:00,StephenSwat,,"Well, yes.",41f3ah,t1_cz1z6xh,umib0zu,,Reply,1,0,1
cz20quq,2016-01-17 17:54:42-05:00,umib0zu,,"So... is A then NP-complete then if I can take any C in NP, reduce it to B, then reduce it to A? When you think of these things like sets (or classes), keep in mind as well these proofs are generally constructive. The problem with the complexity zoo is *finding* these transformations. If you can transform a class A in NP to a class B in NP-complete, cool. But if you can then take a class B in NP-complete and transform it into a problem in class A, you essentially have constructed a proof that A is in NP-complete.",41f3ah,t1_cz1ziy1,StephenSwat,,Reply,2,0,2
cz2119l,2016-01-17 18:02:36-05:00,StephenSwat,,"Alright, that makes sense. Thanks!",41f3ah,t1_cz20quq,umib0zu,,Reply,1,0,1
41b098,2016-01-16 18:53:03-05:00,HandsomeJesus,[ELI5] How an arbitrary Turing Machine can be encoded as the input to a Universal Turing Machine,"Gotta know this for an exam and the actual explanation provided is wayyyyyy too long.
Thanks",,,,,Submission,0,0,0
cz10pse,2016-01-16 20:28:08-05:00,lneutral,,"Think about the fact that a Turing machine can be represented as a tuple of states, symbols and rules for reading, writing, and moving the head. ",41b098,t3_41b098,HandsomeJesus,,Comment,1,0,1
419gxb,2016-01-16 13:08:22-05:00,moocow1452,Will we ever see a solution for running x86 and ARM software natively?,"I know that emulation exists for running ARM code on x86 and the other way around, but could a hardware solution be devised that could run compiled code and apps in both architectures with similar performances, or will it always and forever be more practical to look for a software solution?

Edit: Title fail.",,,,,Submission,4,0,4
cz0lrnh,2016-01-16 13:36:57-05:00,IgnorantPlatypus,,"It's totally possible to make hardware that can run both instruction sets. But it won't be as fast as either alone (since you need more silicon to do both), and there's no money in it. No one will spend the millions of dollars to make such a chip.

If you want an app to run natively on both architectures, you're better off just porting the software.",419gxb,t3_419gxb,moocow1452,,Comment,8,0,8
cz18ns7,2016-01-17 00:12:53-05:00,ToplessTopmodel,,Anyone who wants to think a little bit outside of this posters boyndarys sgould google fpga.,419gxb,t1_cz0lrnh,IgnorantPlatypus,,Reply,-5,0,-5
cz0n3br,2016-01-16 14:15:35-05:00,PastyPilgrim,,"It wouldn't be worthwhile. Unless I'm mistaken, ARM has a simpler architecture than x86, so a version of x86 that supports all of the instructions of an ARM chip might be possible, but they wouldn't have the same performance (because the chip would need to be physically larger). ARM supporting the x86 architecture would not be feasible, because x86 is a Frankenstein monster that we've been adding to as the goals of technology change over time. Plus, the point of an ARM chip is to be low power. If you had the power to support all of x86's operations then you'd just use an x86.

Instead, we write compilers that can optimize code for different instruction sets. It's much more efficient to just have computer scientists write compilers that allow for efficient-ish cross platform code deployment rather than messing with hardware.",419gxb,t3_419gxb,moocow1452,,Comment,3,0,3
cz0ncav,2016-01-16 14:23:02-05:00,BonzoESC,,"The hardware solution is to have both ARM and x86 hardware. If i'm working on Linux software from my Mac, there's not much difference between testing it on an x86 Linux VM, x86 Linux hardware, or ARM Linux hardware.",419gxb,t3_419gxb,moocow1452,,Comment,1,0,1
cz0p4n9,2016-01-16 15:15:22-05:00,Randolpho,,I seem to remember back in 90s that there were PowerPC Macs that included an Intel daughter card for dual-boot-without-emulation capabilities. ,419gxb,t1_cz0ncav,BonzoESC,,Reply,1,0,1
cz0w58y,2016-01-16 18:27:36-05:00,BonzoESC,,"[Yup!](https://en.wikipedia.org/wiki/Power_Macintosh_6100) Since then, networking has gotten a lot faster (although mostly slower than the NuBus technology that era of macs used), increased computer power and CS research has allowed us to get smarter about distributed computing (consider the [buffering and prediction overhead of mosh-shell](https://mosh.mit.edu) and the development of [the CAP theorem](https://en.wikipedia.org/wiki/CAP_theorem) that helps us understand distributed computing better), computer costs have both gone down and become much more flexible, and our expectations for what computing even means have changed.

Consider that nobody was renting remote machines for gaming twenty years ago, and now: http://lg.io/2015/07/05/revised-and-much-faster-run-your-own-highend-cloud-gaming-service-on-ec2.html",419gxb,t1_cz0p4n9,Randolpho,,Reply,2,0,2
419e5b,2016-01-16 12:49:57-05:00,GeatMaster,Why are non volatile RAM technologies considered so exciting?,"I'm not referring to the speed boost of using it as a HD, but the idea of having one memory block to replace both Disk and RAM.

Most computer problems seem to be fixed by reseting, but if the RAM was nonvolatile, that wouldn't work.  So wouldn't having non-volatile RAM be a bad thing?",,,,,Submission,13,0,13
cz0nm9a,2016-01-16 14:31:09-05:00,TheFlyingGuy,,"A couple of reasons spring to mind (straddling the line between EE and CS), but a big one tends to be that they take up less surface area then SRAM, while requiring less power then even idle DRAM. So they are great for increasing density and performance on mobile devices. 

But in the same way, they also enable up to 6x (if they take up as little space as DRAM) as much memory on a CPU die. This is also aided by the fact that several of the potential techanologies are more compatible with the process used for producing CPUs then DRAM is to begin with. (DRAM and CPUs are produced with slightly different processes and doing logic in the DRAM process takes up more space, just as doing DRAM in the CPU process does), so they would enable more high-speed cache near the CPU.

If you have any more questions feel free to ask. (and I should cite sources, but I can't find them quite so quickly, most of this is from books from the 1990s, when the original ones where being investigated)",419e5b,t3_419e5b,GeatMaster,,Comment,7,0,7
cz0tco3,2016-01-16 17:15:21-05:00,gatsbysghost,,"NVRAM becomes really important when you understand how network infrastructure devices work. Routers, Switches, Firewalls--basically every functionally invisible device between you and the servers you want to access across the Internet--all of these devices store their configurations in NVRAM and then copy them into DRAM for runtime at boot.

Why? Because these devices need to be HEAVILY fault-tolerant. When you reboot an enterprise Firewall appliance, you need to be POSITIVE that it reloads the exact configuration it loaded before you rebooted it (hence the importance of the ""Non-Volatile"" part of ""NVRAM""). It stores your software configuration long-term.

These devices basically ARE the Internet, from a certain point of view. So advances in the technology that stores their configuration is absolutely a big deal--just not so much for user workstations.",419e5b,t3_419e5b,GeatMaster,,Comment,2,0,2
cz12r83,2016-01-16 21:25:39-05:00,GeatMaster,,"And if it's last state before rebooting was a kernel panic, wouldn't that just load back into the kernel panic?",419e5b,t1_cz0tco3,gatsbysghost,,Reply,1,0,1
cz174ns,2016-01-16 23:28:18-05:00,faledale,,"""configuration"" not state. The contents of the NVRAM isn't changing to reflect DRAM. NVRAM copies to DRAM on boot, and then DRAM goes on its merry way. (just rewording what gatsbysghost said. I don't claim to know anything about NVRAM... yet)

Edit: Syntax Error: expected ')' 2:93",419e5b,t1_cz12r83,GeatMaster,,Reply,2,0,2
cz1q2ay,2016-01-17 13:30:08-05:00,GeatMaster,,"i sort of assumed he meant state because I can't imagine the configuration being changed without a file changing as well. Furthermore the point still stands, a bad configuration could cause a segfault or something: you might not want to do it again.

The obvious answer is that that's not going to happen in production software, but the root of the whole problem is that I think that NVRAM ignore's murphy's law.",419e5b,t1_cz174ns,faledale,,Reply,1,0,1
cz124yx,2016-01-16 21:08:07-05:00,Devvils,,"> having one memory block to replace both Disk and RAM.

That's what virtual memory does. 

Flash is non-volative & fast. I don't think non-volatile RAM solves much.",419e5b,t3_419e5b,GeatMaster,,Comment,1,0,1
cz16mjx,2016-01-16 23:15:37-05:00,ldpreload,,"Virtual memory does that at very visible performance cost: even swapping to regular flash/SSD has a clear impact on performance (though much less than that of spinning disk, yes). A cold read from SSD has latency on the order of a millisecond: a cold read from RAM has latency on the order of nanoseconds. Flash is fast, yes, but RAM is a million times faster.

The new technologies in this area bridge this gap, reducing the impact of spilling things from RAM to disk.",419e5b,t1_cz124yx,Devvils,,Reply,3,0,3
cz12spd,2016-01-16 21:26:51-05:00,GeatMaster,,"i don't think that's what virtual memory does. 

E.g. HP is working on ReRAM, Intel on 3D XPoint, both are about 1/2 the speed of RAM and as inexpensive as Flash: so intel is talking about chipsets that wouldn't need RAM, it would just use one 3D XPoint device for both RAM and Disk.",419e5b,t1_cz124yx,Devvils,,Reply,2,0,2
cz1k2rm,2016-01-17 10:34:40-05:00,Syde80,,"That is not what virtual memory does.  Virtual memory is basically a way of adding program address space beyond the actual amount of RAM that you have. It does this by storing memory pages on your main storage.  Typically referred to as a swap file or partition.  It's function of the OS where it will move pages that are not currently in use by an active process to swap file to free up RAM for processes that are active.  

It's not at all about trying to bridge the gap between RAM and main storage, its about giving you enough address space to allow more programs to run simultaneously and also to prevent processes from failing when they try to allocate more memory than what is available.",419e5b,t1_cz124yx,Devvils,,Reply,1,0,1
cz1s5e1,2016-01-17 14:18:48-05:00,Bottled_Void,,"Well you would still have regular RAM too I believe.


Like the Harvard Model of computing, it separates program memory space and data memory space. The Program Memory space is usually non-volatile and the Data Memory is volatile.",419e5b,t3_419e5b,GeatMaster,,Comment,1,0,1
4199bf,2016-01-16 12:18:30-05:00,leo-g,"Why did we drop the ""Visual Basic"" form of programming?","I have kind of dabbled with some programming in my life. Usually the web stuff because the pay off is immediate.

Recently wanted to try to write something for fun and realized, damn. Things gotten a lot more complex. When did we kind of lose the ""visual"" programming feel?",,,,,Submission,0,0,0
cz0kzfx,2016-01-16 13:14:24-05:00,None,,[deleted],4199bf,t3_4199bf,leo-g,,Comment,8,0,8
cz0lq2g,2016-01-16 13:35:39-05:00,leo-g,,"Absolutely loved the layering of ""actions""
I can draw a button and then ""drill down"" into that button's onclick( ) and do what I needed to do when the button is clicked. Kind of powerful for the above average user.",4199bf,t1_cz0kzfx,None,,Reply,2,0,2
cz0nogn,2016-01-16 14:32:58-05:00,seabrookmx,,"> Kind of powerful for the above average user

I'd argue the opposite actually. I find the visual style of programming is great for people just getting started with programming. It certainly helped me at the start of my career, but for larger, more advanced apps it really isn't feasible. 

For example, having an ""onClick"" for every button doesnt scale for large apps. You'll have thousands of them. Attempting to keep your code modular then requires a bunch of work from the developer to break down their business logic into classes/modules (which you should always do) but wiring up those onClick events to them leads to tons of boilerplate. 

As for WYSIWYG editors, they have a really hard time handling things like text reflow, window resizing, and high DPI support (ie: responsive design).  This style of programming is still available with C# and VB.NET in the way of Windows forms (or Web Forms for if you want a Website not a desktop app), but I'd only ever use it for a quick 1hour prototype, or for teaching amateurs how to code. Having markup for a UI and then using a framework to bind it to your code provides you with much greater flexibility, performance, and keeps your code cleaner (separation of concerns). Most professional developers prefer this once they get over the initial learning curve.",4199bf,t1_cz0lq2g,leo-g,,Reply,2,0,2
cz0jo5y,2016-01-16 12:36:55-05:00,dxk3355,,I don't get what you mean.  I programmed in VB 6 and VB .net and don't understand.,4199bf,t3_4199bf,leo-g,,Comment,2,0,2
cz0lt4k,2016-01-16 13:38:10-05:00,leo-g,,"Vb6 had a absolute simplicity to everything. You focused on designing your interfaces, then link it up by opening the ""code view"" of that particular button and do whatever you needed ",4199bf,t1_cz0jo5y,dxk3355,,Reply,1,0,1
cz0lx22,2016-01-16 13:41:22-05:00,dxk3355,,Vb .net does that still just double click on the object.  C# does it too.,4199bf,t1_cz0lt4k,leo-g,,Reply,1,0,1
cz0m3zm,2016-01-16 13:46:58-05:00,leo-g,,The syntax is crazy weird. Pretty sure it is a learning curve thing. And was thinking of making something for my Mac and Windows. Gonna try xojo.,4199bf,t1_cz0lx22,dxk3355,,Reply,1,0,1
cz0lk78,2016-01-16 13:30:55-05:00,UncleMeat,,"There have been a million papers over the years trying to move us away from linear text based programming. Some of the ideas are interesting. 

But there is a TON of inertia in programming languages and even more inertia in programming methodologies. Not only do you need to convince people that your new paradigm is better (this is hard, see functional programming) but you need to build a robust system of libraries and make your language interoperable with existing systems if you want anybody to use it in the real world.",4199bf,t3_4199bf,leo-g,,Comment,2,0,2
416lvw,2016-01-15 22:18:27-05:00,bryceguy72,What's a fast yet simple programming language for computer graphics?,"I'm not looking for a sophisticated object-oriented fancy pants programming language with bells and whistles and complex data structures, like C# or C++ or what not.   I just need something that can plot a pixel at X,Y of color RGB and also read the RGB values from coordinates X, Y after loading a photo.  Something simple, fast, and easy to use.  I'm going to use it mainly for making fractals so it should be fast at math.  Thanks. ",,,,,Submission,10,0,10
cz0226m,2016-01-15 22:48:47-05:00,AbouBenAdhem,,Try [Processing](https://processing.org).,416lvw,t3_416lvw,bryceguy72,,Comment,10,0,10
cz03oh0,2016-01-15 23:44:53-05:00,bryceguy72,,"Thank you, I've spent the last hour learning about it.",416lvw,t1_cz0226m,AbouBenAdhem,,Reply,5,0,5
cz07f6e,2016-01-16 02:27:51-05:00,HappyBirthmus,,I prefer Cinder over Processing.,416lvw,t3_416lvw,bryceguy72,,Comment,1,0,1
cz07tau,2016-01-16 02:49:56-05:00,bryceguy72,,"Cinder is a C++ library and although it may be very powerful,  I wanted something simple, easy to learn, and not object oriented. I don't want to learn a complex language like C just to draw simple pixels.
",416lvw,t1_cz07f6e,HappyBirthmus,,Reply,2,0,2
4147kt,2016-01-15 12:59:11-05:00,_amogh_,How can I find out linkedin like relationship path between two twitter users ?,Will I need a graph database and keep adding user ids as nodes along with relationships ?,,,,,Submission,0,0,0
cz33069,2016-01-18 15:25:21-05:00,ShittyAlgorithms,,"[Dijkstra's algorithm](https://en.wikipedia.org/wiki/Dijkstra%27s_algorithm) is a solution to the more general [shortest path problem](https://en.wikipedia.org/wiki/Shortest_path_problem).

In terms of representation you should probably do:

* User = node
* Follow = unweighted (or weight 1) directed edge",4147kt,t3_4147kt,_amogh_,,Comment,1,0,1
413ype,2016-01-15 12:05:30-05:00,android765,Safety of virtual PCs against malware / keyloggers / worms etc?,"I am thinking of creating a virtual machine (standard style included with windows 10) in order to have something to run potentially unsafe apps. Is there any possibility for a malware to escape somehow from the Virtual PC and affect the host pc and / or the network? 

What if I nest a virtual pc inside another virtual pc? Do I get better results?

Thanks a lot.",,,,,Submission,11,0,11
cyzdxl8,2016-01-15 12:20:38-05:00,None,,"If you're going to use it for running unsafe apps, you should try sandboxie instead",413ype,t3_413ype,android765,,Comment,7,0,7
cz01peb,2016-01-15 22:37:07-05:00,moop__,,"IT security worker here.

Escaping from most virtual environments is easy if you have a modern toolset and the host isn't hardened. Nesting is typically pointless. 

You'd want to harden it by firstly using a host-only network on your VM. This, alongside iptables rules will provide the VM with internet but not provide it with easy access to the host machine. I use the following on VirtualBox: 

    iptables -A FORWARD -o enp6s0 -i vboxnet0 -s 192.168.56.0/24 -m conntrack --ctstate NEW -j ACCEPT
    iptables -A FORWARD -m conntrack --ctstate ESTABLISHED,RELATED -j ACCEPT
    iptables -A POSTROUTING -t nat -j MASQUERADE
    sysctl -w net.ipv4.ip_forward=1
    # Ubuntu-like
    apt-get install dnsmasq 
    service dnsmasq start
    # Arch
    pacman -S dnsmasq
    systemctl enable dnsmasq

Then you should be good. 

-----

But here in real life you're probably fine using sanboxie with a decent AV.",413ype,t3_413ype,android765,,Comment,7,0,7
cz0geo6,2016-01-16 10:59:33-05:00,__Ephemeral,,How do they break out of the vm?,413ype,t1_cz01peb,moop__,,Reply,2,0,2
cz0ymw5,2016-01-16 19:33:22-05:00,BonzoESC,,"Many ways, but they're usually patched quickly, since VMs are big business. http://xenbits.xen.org/xsa/advisory-135.html is one example.",413ype,t1_cz0geo6,__Ephemeral,,Reply,2,0,2
cyzvbui,2016-01-15 19:23:19-05:00,Natanael_L,,"Virtual machines are good but not perfect. Breaking out of them happens from time to time. 

Nested virtual machines are inefficient because of how the x86 architecture handles various instructions and memory management functions. It doesn't work in practice outside specialized uses with custom virtualization software.",413ype,t3_413ype,android765,,Comment,6,0,6
411475,2016-01-14 21:49:48-05:00,sadelbrid,Logarithmic runtime complexity,"I'm trying to wrap my heap around the runtime complexity of a situation like this (n being the input size):

    j=1
    while(j < n^2)
        j*=5

I want to say the runtime complexity is log(n) but the whole (n^2) part makes me doubt that. Thoughts?",,,,,Submission,5,0,5
cyytq0r,2016-01-14 23:01:32-05:00,bhrgunatha,,Remember that log(n^2) = 2 log(n),411475,t3_411475,sadelbrid,,Comment,7,0,7
cyyu9pc,2016-01-14 23:17:52-05:00,sadelbrid,,Thanks for that. I often need reminders like that being that I'm not fantastic at math,411475,t1_cyytq0r,bhrgunatha,,Reply,2,0,2
4111s2,2016-01-14 21:33:32-05:00,Nougati,Can anybody who understands soft heaps explain them to me?,"I don't know how frequented this subreddit is, but here goes nothing.

I read some stack overflow questions and the 2000 Paper by Bernard Chazelle but I still don't quite understand the nature of the structure. As a background to what I *think* I understand I'll gloss over my knowledge of soft heaps. If you don't care, the specific question I want to ask will be in the final paragraph.

My understanding is this:
The soft heap is a variation of a heap which gets amortized constant complexity on all but operations but insert, which is O(log1/ε), where ε is the 'Error Rate' residing between 0 and 0.5.
The error rate dictates the number of 'corrupted' nodes in the heap, where ""the runtime of the operations gets cheaper and cheaper as ε goes up (and, therefore, the output gets less sorted) and the operations get more expensive as ε goes down (in which case the output gets more and more sorted)."" (Quoted from [Stack Overflow](https://stackoverflow.com/questions/26126170/soft-heaps-what-is-corruption-and-why-is-it-useful%5C))

This is what I understand less:

Corruption involves raising the priority of some nodes, chosen by some method by the data structure. The number of them again is dictated by epsilon. Supposedly, this reduces the entropy? (Is this means like expected value of runtime... How?). 

The premise of corrupting nodes makes sense, what exactly is the term 'entropy' referring to in this context, and how does raising some arbitrary selection of nodes' priority reduce it? I usually learn well from visuals on data structures but there is very little on soft heaps, so if you can help me visualise the process of reaching a better performance as opposed to a regular heap I would be grateful.",,,,,Submission,3,0,3
410kyy,2016-01-14 19:39:33-05:00,tool1138,Question Concerning CS classes,I go to cc and am about to register for classes in a couple of days.  I'm registering for the intro cs course at my college but my counselor also suggested that for the sake of my schedule I take the class on computer architecture and assembly language as well.  The class doesn't have any prerequisites but I'm concerned about taking that class as a beginner in cs.  Any advice on whether I should take the computer architecture and assembly class in conjunction with my intro to cs course?,,,,,Submission,5,0,5
cyytpzd,2016-01-14 23:01:30-05:00,_--__,,"Although every course is different, I suspect you would be able to cope with the computer architecture part with no problem.  For the assembly part, having done some programming before might be useful, but with the type of mind that is looking to go into CS you will probably be able to manage even if you don't have any programming experience.",410kyy,t3_410kyy,tool1138,,Comment,4,0,4
cyzcq59,2016-01-15 11:52:36-05:00,Quakerz1,,"It should be doable, yes. In my university, the computer architecture class is on the first semester, along with the calculus and intro cs classses.",410kyy,t1_cyytpzd,_--__,,Reply,1,0,1
cyytyc4,2016-01-14 23:08:16-05:00,dxk3355,,There's a community college offering assembly and computer architecture?  That's pretty impressive compared to what the one here offers.  If the assembly is anything like what I took then you'd be better having taken a programming course first or at least know basic programming. ,410kyy,t3_410kyy,tool1138,,Comment,3,0,3
cyz08wy,2016-01-15 03:31:39-05:00,macnor,,Having transferred from a community college I'd suggest you do it and worst case you drop the course and take it later. For financial aid (at least in California) you don't get penalized as long as you remain enrolled in classes at 2/3s load (9 semester units).,410kyy,t3_410kyy,tool1138,,Comment,2,0,2
cyz3bmg,2016-01-15 06:50:18-05:00,artillery129,,"Give the courses a try, worst case scenario you can just drop them",410kyy,t3_410kyy,tool1138,,Comment,2,0,2
40wr8g,2016-01-14 03:28:03-05:00,noobeeee,How do I record FM radio on my computer?,"Hi,

I need to record a FM channel which is being aired at 87.9MHz. 

I need to be able to record it and then process it on my computer.

I'm using Linux. What kind of devices do I need to do so? I'm looking at USB FM receiver from eBay. I'm not sure if that's the only thing that I will do.

What other ways I can do that since my radio station doesn't have online streaming yet?",,,,,Submission,0,0,0
cyy0143,2016-01-14 10:59:04-05:00,Concordiaa,,Seems pretty straight forward.  Get a USB FM receiver like you suggested - should work fine.,40wr8g,t3_40wr8g,noobeeee,,Comment,1,0,1
cyy56mz,2016-01-14 12:59:53-05:00,noobeeee,,"Hi,

However, I need to be able to record five different channels at once. Can I do that with five USB FM receiver with ONE linux computer or will I do five different computers?

Thanks.",40wr8g,t1_cyy0143,Concordiaa,,Reply,1,0,1
cyy92bs,2016-01-14 14:25:10-05:00,Concordiaa,,"Hm.  If you have to record five channels I imagine you would need to get five different receivers unless you can find one that will let you listen to multiple at once (not sure if these exist, and if they do I imagine they would be expensive).  If you're able (have enough USB ports and processing power on your computer) to hook up five different channels at once you should be able to do it with one machine.",40wr8g,t1_cyy56mz,noobeeee,,Reply,1,0,1
cyydrst,2016-01-14 16:11:44-05:00,artillery129,,"It is a hardware limitation, you can only tune from one frequency at a time, you would require five receivers, here's a fun link about using a single board to listen from many at the same time



http://cowlet.org/2014/05/05/listening-to-200-radio-stations-at-once.html",40wr8g,t1_cyy56mz,noobeeee,,Reply,1,0,1
cyyps34,2016-01-14 21:16:56-05:00,noobeeee,,"Thanks. If I have five FM receivers and one powerful computer, will it be enough? I just worry I might need five FM receivers and five computers.",40wr8g,t1_cyydrst,artillery129,,Reply,1,0,1
cyyzkar,2016-01-15 02:53:14-05:00,artillery129,,"five FM receivers with one powerful computer is enough, even with a non-powerful computer it's probably enough, just give it a go",40wr8g,t1_cyyps34,noobeeee,,Reply,1,0,1
40w393,2016-01-13 23:46:09-05:00,mad_scientist42,List of simple general algorithm centered computer science problems.,"I am currently in 4th Semester (Computer Science) and am looking for some relatively simple computer science problems upon which maybe I could work on. 

I am really interested in Voice Assistants and Optimization area. Or even something like Travel-Salesman Problem. ",,,,,Submission,13,0,13
cyxpogc,2016-01-14 03:18:15-05:00,Devvils,,Maybe [Schönhage–Strassen](https://en.wikipedia.org/wiki/Sch%C3%B6nhage%E2%80%93Strassen_algorithm).,40w393,t3_40w393,mad_scientist42,,Comment,3,0,3
cyz4a45,2016-01-15 07:42:07-05:00,mad_scientist42,,Interesting. ,40w393,t1_cyxpogc,Devvils,,Reply,1,0,1
cyxokqy,2016-01-14 02:16:45-05:00,seandemmer,,https://www.hackerrank.com/domains/algorithms/warmup,40w393,t3_40w393,mad_scientist42,,Comment,2,0,2
cyz46yp,2016-01-15 07:37:41-05:00,mad_scientist42,,"I am looking for something which could really benefit humanity in any way. Problems, on which I can start working today and really solve them in long way.",40w393,t1_cyxokqy,seandemmer,,Reply,2,0,2
cz6ij6f,2016-01-21 09:43:53-05:00,seandemmer,,"Not sure you're going to find ""relatively simple computer science problems"" that ""really benefit humanity"". I'd imagine the low-hanging fruit has already been picked.",40w393,t1_cyz46yp,mad_scientist42,,Reply,1,0,1
40pv4g,2016-01-12 21:26:30-05:00,erinsmith280,Urgent question concerning collecting videos over the internet,"I am a high school researcher who would be much appreciative of any help/input. 
On my current project, I need to collect videos from a computer webcam of certain medical patients watching short video clips. Does anyone know how to do this?  I need data collection, and this tiny problem is completely stalling my research.

I am so grateful for any words of advice and help.",,,,,Submission,0,0,0
cywdavw,2016-01-13 01:36:25-05:00,BigBoom3599,,"I think you're in the wrong sub for this :D, and try to be little more specific as well (what webcam, what is it hooked up to, ...)",40pv4g,t3_40pv4g,erinsmith280,,Comment,2,0,2
cyxc4c9,2016-01-13 19:34:02-05:00,lordvadr,,"This isn't /r/techsupport.  Also, you'll have HIPPA/Privacy problems and all sorts of stuff.",40pv4g,t3_40pv4g,erinsmith280,,Comment,1,0,1
cyw54ei,2016-01-12 21:28:57-05:00,erinsmith280,,"Ideally, I want to be able to collect these videos over the internet, having them virtually sent to me. I have the patients, I just need to technological capacities to collect the data, and I feel really stuck.",40pv4g,t3_40pv4g,erinsmith280,,Comment,0,0,0
40oeq0,2016-01-12 16:02:52-05:00,Cat_Templar,Need help with Modelling Questions! (Relations),,,,,,Submission,1,0,1
cyvryle,2016-01-12 16:03:35-05:00,Cat_Templar,,I've just encountered this question on my Modelling past paper (first year computer science student) and I'm slightly confused on this question. Any explanations/answers would be appreciated :),40oeq0,t3_40oeq0,Cat_Templar,,Comment,2,0,2
cywwo8d,2016-01-13 13:44:17-05:00,thegreattimics,,"I'm not sure what background your course offered on binary relations in general, but basically you can find the definitions of the properties pretty much anywhere on the internet (https://en.wikipedia.org/wiki/Binary_relation), and you only have to match them with the abstract notation. 

idA represents pairs of equal elements, so whether it is included or disjoint with R will represent reflexivity and irreflexivity.

R^(-1) is the inverse of R, so whether the sets are equal or disjoint indicate symmetry and antisymmetry.

This leaves A to represent transitivity, and what the notation says is basically that if (x,y) and (y,z) are part of the relation, then their composition, (x,z) is too.

Hope this helps!

",40oeq0,t1_cyvryle,Cat_Templar,,Reply,2,0,2
40n27n,2016-01-12 11:21:48-05:00,VerifiedMod,Can computers keep getting faster?,or is there a limit to which our computational power will reach a constant which will be negligible to the increment of hardware power,,,,,Submission,17,0,17
cyvguzt,2016-01-12 11:54:59-05:00,Khuprus,,"This is a fun question!
  
*As a layperson* thinking this over, I think it becomes obvious that transistor-based computers will eventually hit a limit. This limit will be based on physical properties (for example, a transistor somehow made of a single atom won't be able to be improved to being a transistor 1/1000th the size of an atom). Miniaturization can only go so far.
  
This of course doesn't mean that alternative methods of computing won't be invented. You can think about this as the difference between the first steam-powered locomotive and the modern day rocket. Both are inventions that convert energy into work, but with tremendously different results. This might end up being quantum computing, optical computing, bio-inspired computing, or most likely something altogether new.
  
The ultimate limit to computational power is the limit of how much energy you can feed it. There's a fun story called [""The Last Question"" by Isaac Asimov](http://www.physics.princeton.edu/ph115/LQ.pdf) that kind of addresses this.",40n27n,t3_40n27n,VerifiedMod,,Comment,29,0,29
cywewra,2016-01-13 02:53:51-05:00,Bman1296,,So current binary computing is like steam locomotives versus Saturn V (quantum computing)?,40n27n,t1_cyvguzt,Khuprus,,Reply,2,0,2
cywjp9k,2016-01-13 07:45:19-05:00,Hakawatha,,"Actually, yeah, in the sense that the Saturn V is well-suited to problems that aren't otherwise easily solved (like, how can I get to the moon?), but are entirely ineffective for a lot of day-to-day stuff (how do I get to the store?). The Saturn V is a quantum computer: your car is a classical computer. You can't get to the moon with your car, but you can't get to the store with your Saturn V.",40n27n,t1_cywewra,Bman1296,,Reply,4,0,4
cyxdlp7,2016-01-13 20:14:37-05:00,Bman1296,,"Well, I could. Might die, but I still made it. (ha)",40n27n,t1_cywjp9k,Hakawatha,,Reply,1,0,1
cyvjixk,2016-01-12 12:56:33-05:00,Chandon,,"I'm going to respond to a slightly different question:

> Will processor vendors stop shipping new, more powerful processors any time soon?

Probably not. There are a bunch more things that can be done to make your processor do more computation in the same amount of time.

They do seem to have hit some limits:

* They can't scale clock speed much, because power consumption increases at the cube of frequency.
* They can't reduce the size of transistors that much more. Intel's at 14nm, and no one's really sure how to get below ~5nm or so.

Here's what they can still do, no problem:

 * Increase the size of chips. Nvidia has shipped chips that are 10x the size of standard Intel chips.
 * Layer chips. Stacking 10 chips on top of each other would work great - the new HBM GPUs are already solving the technical problems.

Simply having more chip, as I suggest, means you have to reduce power consumption. But remember - power increases at frequency cubed. So if you have a 100W chip and you cut the clock speed in half, it'll only take 12W. So you can have a chip that's 10 times as big at half the frequency, which will give you 5 times the total compute power (assuming you just copy and paste for more cores).",40n27n,t3_40n27n,VerifiedMod,,Comment,9,0,9
cyvzhe8,2016-01-12 19:00:42-05:00,kyuubi42,,"While you're not totally wrong, neither of the two advances you mention really apply to CPUs (at least currently). Nvidia can get away with massive die sizes because they use very large processes / run at low frequencies which helps mitigate the yield and timing issues. HBM only works because the only thing which is being stacked there is DRAM, which is very low power compared to a cpu or gpu core.",40n27n,t1_cyvjixk,Chandon,,Reply,4,0,4
cyw945l,2016-01-12 23:12:44-05:00,Chandon,,"Nvidia is generally about one process node behind Intel, so it's not like their giant chips are 90nm or something. And yes, giant chips have to run at lower frequencies - I'm convinced that's the future, and once we have 16 core laptop processors then 64 core laptop processors at half the clockspeed will sound like an upgrade.

Although my more specific bet is that the big/LITTLE archetecture we're seeing from ARM is even closer to the future. We'll keep our dual or quad core 3GHz cpus, but we'll get progressively bigger arrays of low power coprocessors which will converge with GPUs and then take over most of the processing work, leaving the ""main"" CPU in low power states all the time except for legacy apps.",40n27n,t1_cyvzhe8,kyuubi42,,Reply,1,0,1
cywssks,2016-01-13 12:16:09-05:00,kyuubi42,,"While I can agree with you that big.little is interesting, I would be shocked if silicon based cpus ever grow to be massively parallel with homogenous cores. There will always be workloads which do not benefit from parallelization, and large die sizes / core counts require you to make tradeoffs which either get prohibitively expensive or completely sell out single thread performance.",40n27n,t1_cyw945l,Chandon,,Reply,1,0,1
cyx9x0b,2016-01-13 18:38:01-05:00,Chandon,,"> There will always be workloads which do not benefit from parallelization, 

I've done some work in the area, and my conclusion is that while there are some theoretical edge cases, in practice any task that is compute-bound can be solved with a parallel computation.

The problem is that, in general, parallel algorithms cause a slowdown compared to sequential algorithms which then needs to be made up by hardware parallelism. That's why the algorithms you see today on GPUs are significantly different from the CPU algorithms. It's still efficient to use a sequential algorithm on a quad-core CPU. On a 64-core CPU, we'd see parallel algorithms for pretty much everything.",40n27n,t1_cywssks,kyuubi42,,Reply,1,0,1
cyw7rqv,2016-01-12 22:36:32-05:00,Sqeaky,,"> no one's really sure how to get below ~5nm or so

We thought this about 22nm and 45nm and 90nm. 

With the current state of naysaying I wouldn't be surprised if they invented a new subatomic particle just to keep getting smaller once atoms are thought too big.",40n27n,t1_cyvjixk,Chandon,,Reply,0,0,0
cyw91e3,2016-01-12 23:10:42-05:00,Chandon,,"The complication is that the plan for getting *to* 5nm is to switch off silicon. So even if they figure out how to make gates out of quarks or whatever, there will be a stall at 7nm (maybe 10nm) while the completely new process gets developed.",40n27n,t1_cyw7rqv,Sqeaky,,Reply,2,0,2
cyvyob2,2016-01-12 18:39:39-05:00,heywire84,,"Some improvements in speed may be possible by changing how some of the individual operations are carried out.  

For example, in the old days on an 8086 chip, the great grandfather of all our modern x86 chips, it took 80 clock cycles to divide a number.  Today, on a brand new Skylake i7 chip, the same exact division instruction would take 6 (ish) clock cycles to compute.  The algorithm that the chip uses to do the division has changed and become better and faster, meaning that the same math can be done using less ticks and tocks because there are fewer steps.

Also, there are many tricks that today's chips take advantage of to increase their computation speed.  That 8086 could only work on one thing at a time.  If you ask it to divide a number, that was all it could do for all 80 cycles.  Today's chips can work on many things at the same time.  If you ask it to divide a number, it starts, then the next clock tick asks for another instruction.  This is called pipelining.  Chips can also execute more than one instruction at the same time.  A chip capable of that is called superscalar (this is separate from multicore, a single core can be superscalar).  A chip that is both superscalar and pipelined has a throughput of more than one instruction per clock cycle, as long as it can sustain it's pipeline.

It is reasonable to assume that not all of these tricks have been discovered or implemented, so even if clock speed does not improve, the algorithms and ways that cpus crunch their numbers will.",40n27n,t3_40n27n,VerifiedMod,,Comment,6,0,6
cyvfwxk,2016-01-12 11:32:20-05:00,EquationTAKEN,,"Computing power relies on how many transistors you can fit in your CPU. The size of transistors is asymptotically moving towards atom-sized. At that point, it's going to get exponentially more difficult to make them any smaller, which is why quantum computing is a pretty important buzzword these days.",40n27n,t3_40n27n,VerifiedMod,,Comment,7,0,7
cyvs42i,2016-01-12 16:06:59-05:00,anamorphism,,"that's the current paradigm, but computing power didn't always rely on the transistor and any significant advances in computing power will most likely come from the replacement of the transistor.

just like replacing vacuum tube triodes with transistors was a huge milestone.",40n27n,t1_cyvfwxk,EquationTAKEN,,Reply,8,0,8
cyvtni6,2016-01-12 16:39:55-05:00,UncleMeat,,"Computing power is much more than just this. Algorithmic design, compiler optimizations, and chip design can all make incredible improvements in performance without changing the commonly measured numbers in hardware like bus speed, clock speed, and transistor count.",40n27n,t1_cyvfwxk,EquationTAKEN,,Reply,2,0,2
cyvtjla,2016-01-12 16:37:44-05:00,OpticCostMeMyAccount,,"Dumb question here, why can't we just make the CPU bigger? ",40n27n,t1_cyvfwxk,EquationTAKEN,,Reply,3,0,3
cyvts1q,2016-01-12 16:42:31-05:00,UncleMeat,,"A few problems. One is heat. A larger CPU generates more heat and dissipating that heat is expensive. Two is that it simply takes longer to access data that is further away. There is a reason why L1 cache sits right next to the CPU. A larger CPU necessarily has larger latencies. 

But the most important reason is that we've grown accustomed to performance increasing exponentially. An exponentially increasing CPU size will become impractical very very quickly. If we need to double the size of our CPU every two years then in 20 years our CPUs need to be *one thousand* times larger. In another 20 years they are *one million* times larger than the original chips. That's just not feasible.",40n27n,t1_cyvtjla,OpticCostMeMyAccount,,Reply,6,0,6
cyvuf14,2016-01-12 16:56:07-05:00,PastyPilgrim,,"Perhaps the biggest bottleneck in computer speed is the length of the wires inside a CPU. They may be microscopically small, with light being extremely fast, but when you need to change the information on wires billions of times every second, that transit time becomes a real problem. Delving a little more into computer architecture, you often need to limit each stage of operation to the length of the longest possible operation that can happen at that time if you want to keep everything synced up and working nicely. So adding in some extra functionality further away from everything else will slow down everything else.

By making a CPU bigger you need to increase the length of wires. This is why CPUs getting smaller is actually better. This is also why we've moved towards having more CPUs in a computer as opposed to having bigger CPUs. It's much easier to put 4 CPUs (quad core) in a computer to solve 4 problems simultaneously than it is to make a processor that can solve all 4 of those problems in 1/4 the time.",40n27n,t1_cyvtjla,OpticCostMeMyAccount,,Reply,3,0,3
cyw4w0x,2016-01-12 21:22:59-05:00,Devvils,,Heat density. A modern CPU will be 10 cm^2 and user over 100 watts. They will fry if you try to crank up the speed.,40n27n,t1_cyvtjla,OpticCostMeMyAccount,,Reply,1,0,1
cyvyf43,2016-01-12 18:33:02-05:00,tholenst,,"There are physical limits, though they are far from what we can achieve. See:
https://en.wikipedia.org/wiki/Limits_to_computation#Physical_limits

I find [Landauers Principle](https://en.wikipedia.org/wiki/Landauer's_principle) particularly interesting.",40n27n,t3_40n27n,VerifiedMod,,Comment,3,0,3
cywbdpm,2016-01-13 00:22:56-05:00,taricorp,,"[""Universal limits on computation""](http://arxiv.org/abs/astro-ph/0404510) came to mind. These are quite definitive answers to the question, but not particularly meaningful on a human scale.",40n27n,t1_cyvyf43,tholenst,,Reply,1,0,1
cyw6yrs,2016-01-12 22:15:59-05:00,Mines_of_Moria,,"This isn't really what your asked but in case you don't know, the biggest limitation to computing isn't processing speed, it's usually writing and reading from memory, which is really slow in comparison. That's why things like paging were invented, to optimize the amount of time the CPU is working vs waiting for slower things to complete. 

",40n27n,t3_40n27n,VerifiedMod,,Comment,3,0,3
cywkcug,2016-01-13 08:15:52-05:00,Master_Rux,,"Shrinking transistor size isn't going to work for much longer. We'll need to come up with new solutions. Much like how we now have hyper-threading, multiple cores and gpu's, we'll change the architecture and how we approach our problems in code.  We'll come up with something new to keep us going. ",40n27n,t3_40n27n,VerifiedMod,,Comment,3,0,3
cyvqy3t,2016-01-12 15:41:43-05:00,Delwin,,"Short answer: yes.

Long answer: There was a time not all that long ago when mechanical computation was limited by the size of the vacuum tube.  Eventually that ended when we made the transition to transistors.  That migrated into integrated circuits and we've been governed by Moore's Law since then.

At some point we will hit the physical limits of the 2D integrated circuit (about 5nm or so) and we will need to make the transition to something new.  Market forces will keep pressure on the issue and we're already seeing the beginnings.

3D integrated circuits.

At some point (likely a few decades, maybe as many as 5 or 6) we will hit the limits there and we will need to transition to something else - my bet is photonics.  There's already the baby steps in labs needed for optical computing and photons can multiplex whereas electrons can't so there's a whole dimension worth of computing power waiting for the hardware people to figure out how to harness it, and then us software people to figure out how to program it.",40n27n,t3_40n27n,VerifiedMod,,Comment,5,0,5
cyvpt3v,2016-01-12 15:16:42-05:00,Filmore,,"You can almost always always pre-compute or approximate results. With modern cluster and parallelization techniques, the question becomes three dimensional across speed, Cost, and accuracy. You Want computations fast and cheap? It's not going to be accurate. You want it fast and accurate? It will cost you. You want it cheap and accurate? Prepare to wait. 

The result will never be free and never be instant. So you really need to decide how much error you can tolerate and then push on tech to reduce cost and increase speed for your accuracy requirements. ",40n27n,t3_40n27n,VerifiedMod,,Comment,-5,0,-5
40mwn0,2016-01-12 10:44:01-05:00,MachNineR,CAS latency is measured in cycles. Is it independent of speed? If ram speed is downgraded or overclocked will it use the same number of cycles to respond or the same number of milliseconds?,,,,,,Submission,0,0,0
cyvf5iw,2016-01-12 11:13:26-05:00,sefsefsefsef,,"CAS is configurable.  You can set this just like you can set the memory frequency (which is what I assume you mean when you say ""speed"").  If you use the same CAS latency (measured in cycles) for a higher memory frequency, then you will actually reduce your memory latency (measured in nanoseconds).  If you use the same CAS latency (measured in cycles) for a lower memory frequency, then you will actually increase your memory latency (measured in nanoseconds).  

Your CPU's performance is only dependent on the nanosecond latency of your memory, and CAS is just one input variable to determine what that will be.",40mwn0,t3_40mwn0,MachNineR,,Comment,1,0,1
cyvtq5o,2016-01-12 16:41:26-05:00,MachNineR,,Thanks for the corrections. So I can just approximate the latency after changing the frequency and see if I get any memory errors?,40mwn0,t1_cyvf5iw,sefsefsefsef,,Reply,1,0,1
40hzn4,2016-01-11 13:08:46-05:00,thefunkygibbon,"Why is it that you can stream >1080p HD quality video with surround sound, but video calls like Hangouts looks like you are communicating via potatoes?","I get that it is using UDP and that packets are not guaranteed.  I also get that you are uploading as well as downloading and are at the mercy of the upload speed of both sides.  But those reasons do not sit well with me given that when I try to call my dad who a) is in the same city and ISP and b) we both have sufficient upload speed (enough to stream HD videos from home, ie via plex).",,,,,Submission,31,0,31
cyub2eb,2016-01-11 13:45:24-05:00,Khuprus,,"You also have to consider that streaming video is either a full file being served in parts (streaming a movie), or on a sizable delay (a ""live"" broadcast). This allows a buffer where the content can be downloaded ahead of being displayed. It also allows a program like Netflix to seamlessly switch between HD and SD if your internet connection speed changes, because it has enough warning ahead of time to make the changes.
  
With a video call, latency is everything and even a 1-2 second delay is frustrating when trying to have a conversation. To ensure super low latency, video quality suffers in this tradeoff.
  
Also take into consideration that most front-facing cellphone cameras and laptop cameras are made to be small and cheap, with low resolution captures. You're comparing a movie (shot with nice high end cameras with good lighting) to a shot of yourself in a dark living room with a camera smaller than a dime.",40hzn4,t3_40hzn4,thefunkygibbon,,Comment,30,0,30
cyudiqi,2016-01-11 14:42:15-05:00,sandwichsaregood,,"Buffering and latency are the biggest reasons as you say, but it's also worth noting that Netflix spends a [tremendous amount of effort picking the optimal encoding strategy for *each* video](http://techblog.netflix.com/2015/12/per-title-encode-optimization.html). They basically encode the video many times using dozens of different encoder settings and then cherry-pick the best ones to serve.

So while Hangouts can adjust the bitrate up and down depending on your connection, it doesn't have the luxury of knowing the ideal encoding parameters for each bit rate. I expect they use general settings that work alright in most scenes, but it can't really get that extra little bit of quality from using ideal settings like Netflix can.",40hzn4,t1_cyub2eb,Khuprus,,Reply,11,0,11
cyubc9v,2016-01-11 13:51:42-05:00,battmender,,"Exactly this. If you're streaming a YouTube video, the server knows what's going to be coming in the future. In Hangouts, it cannot without delaying the stream significantly. ",40hzn4,t1_cyub2eb,Khuprus,,Reply,6,0,6
cyudjyd,2016-01-11 14:43:03-05:00,Zalack,,"I am a video editor, not a computer scientist, so take this answer with a grain of salt:

One thing that hasn't been touched on yet is compression.

The most common compression scheme for movies is H264, which can creat very small good looking files.

However, encoding video as an H.264 file is incredibly computationally intensive. Most systems cannot handle real-time encoding, as it has to happen on a pretty decent graphics card. The result is that the video from your camera is far more data than the movie you are watching, since it has not been compressed as heavily/efficiently.

In order to send it over the web, your computer has to decrease that data somehow, and since it cant use a pretty, but difficult compression like H264 since it doesn't have the time, it chooses instead to more haphazardly strip out data. ( still a compression scheme, just a easier one)

It may also be the camera is still sending H264, just a much less high quality version that can be encoded much more quickly.

Hope that helps!",40hzn4,t3_40hzn4,thefunkygibbon,,Comment,12,0,12
cyul8tl,2016-01-11 17:44:28-05:00,haagch,,"> However, encoding video as an H.264 file is incredibly computationally intensive. Most systems cannot handle real-time encoding, as it has to happen on a pretty decent graphics card.

Well, any (most?) low end GPUs can do it, they just have to be from the last few years (2011+). My ivy bridge cpu in my laptop has dedicated hardware for video encoding: https://en.wikipedia.org/wiki/Intel_Quick_Sync_Video

My HD 7970M also has dedicated hardware for video encoding: https://en.wikipedia.org/wiki/Video_Coding_Engine

And nvidia does too.",40hzn4,t1_cyudjyd,Zalack,,Reply,0,0,0
cyuot7n,2016-01-11 19:19:06-05:00,Zalack,,"Sure, but at what quality? The best H264s take a lot of horsepower. I can max out my Titan and get barely faster than realtime with all of the quality settings dialed up out of a professional program.

Also, the highest quality files are encoded at 2-pass, which means it encodes the file once, then goes through a second time and optimizes the file again, which obviously can't happen in realtime as the video is being recorded.",40hzn4,t1_cyul8tl,haagch,,Reply,5,0,5
cyupddy,2016-01-11 19:34:23-05:00,haagch,,"The quality is usually okay, but the colors suffer. Example: https://www.youtube.com/watch?v=x8F4G4N3xGk. That was recorded and encoded ""live"" with ivy bridge while playing. (the random hangs in the video are usually not there, that's a separate bug). Well, and it took (almost) no horsepower away from the CPU, because it's all done in hardware.",40hzn4,t1_cyuot7n,Zalack,,Reply,2,0,2
cyuppul,2016-01-11 19:43:55-05:00,Zalack,,I would consider color a big part of the quality. ,40hzn4,t1_cyupddy,haagch,,Reply,3,0,3
cyufhu6,2016-01-11 15:28:14-05:00,GenFussypants,,"There are several reasons for the differences between Netflix/Amazon style content and ""real-time"" content.  
&nbsp:  
First let's discuss what you are getting over that ""HD"" stream from Netflix. What do you consider HD? Is it just the resolution (i.e. 1920x1080) or is it the ""quality"" in the video itself? Perhaps both? Well, Netflix (and others) use technologies like MPEG-DASH and HLS (Adaptive Rate Control). This streaming technology allows them to encode a single video in a number of different resolutions and ***bitrates***. It then breaks each of these videos into (a minimum of) 10s *segments*. As the video is played on your local receiver the bandwidth is monitored. If the bandwidth is under-utilized then on the next *segment* a better ""quality"" encoding will be chosen. If your bandwidth is being over-utilized then on the next *segment* a lower ""quality"" encoding will be chosen. This means from segment to segment you may get the same resolution but wildly different ""quality"".  
&nbsp:  
It is difficult (but not impossible) for real-time streaming to do this. You have one encoding source that is having to capture data at real-time, encode, and shove it out a network. If you change resolutions you will have to restart the encoder which will cause all kinds of hiccups in the stream.  
&nbsp:  
Now let's talk a little about the encoding being done on the video. The big thing here is you want high quality with bandwidth consumption as low as possible. The three big knobs you have to turn are resolution, quality (bitrate), and framerate. Raising, or lowering, the resolution will have the most profound impact on encoding vs. quality. Followed by your quality tools (bitrate), and lastly by your framerate. Let's jump over resolution and look at quality for a second. The most commonly used codec right now is H.264 (MPEG-4 AVC Part-10). H.264 has several ""Profiles"" that dictate how the encoding is accomplished. There are 3 types of ""frames"" generated by H.264: Intra-frame (I-frame), Predictive (P-frame), and Bidirectional (B-frame). Order goes from largest to smallest. A arbitrary relative examplel would be 128KB (I), 2-3KB (P), and 100-200B (B). Big differences between these three frame types! Why wouldn't I always just use B? Or P? Well P and B frames are derivatives of an I-frame. This means if you miss a P or B frame you will get corruption on the screen. It takes an I-frame to clear that and start a new reference sequence.  
&nbsp:  
Now, a company like Netflix uses something like MPEG-DASH (which is TCP based) and your live stream uses some UDP based item (again, doesn't have to be). The MPEG-DASH technology will not loose frames as it is TCP based and heavily buffered. Thus it can use a ""High"" H.264 Profile with large spacing between I-frames. This reduces the overall bandwidth used allowing for greater quality. Your live stream is using UDP which will drop frames over the Internet(*). Most live streams do NOT use B-frames in a loss possible environment as the corruption created would not be mask-able. So if you drop one of those P-frames over UDP you will see a little bit of corruption. If you drop a number of those P-frames you will see a lot of corruption. Remember that it takes an I-frame to fix this. Thus you want an I-frame fairly often. This has a big impact on your overall bandwidth.  
&nbsp:  
In conclusion companies like Netflix can stream higher quality because they can take full advantage of streaming technologies and codecs. Your live stream is restricted by bandwidth, real-time configuration changes, and overall performance.",40hzn4,t3_40hzn4,thefunkygibbon,,Comment,4,0,4
cyur59k,2016-01-11 20:22:45-05:00,Shadowsoal,,"Don't think has been mentioned yet, but you also have to keep in mind that when you stream video you are generally download the data from a CDN (content distribution network), compared to video calls where your download is bound by the other participant's upload speeds.

A CDN's entire existence hinges on being able to rapidly and reliably deliver data to consumers. Everything else mentioned here matters, but if you're talking with someone whose uplink is bad you're not going to have a great experience.",40hzn4,t3_40hzn4,thefunkygibbon,,Comment,4,0,4
cyusz6z,2016-01-11 21:10:48-05:00,earslap,,"It has to do with latency. 2 way voice communication is *extremely* sensitive to latency. The most one can tolerate without issues is around 0.1 seconds (especially if you can hear your voice from the other side) and guaranteeing that has significant tradeoffs.

When the voice / video packets leave your machine, it goes through possibly dozens of different devices to reach its destination: and your priority is not any higher than anyone else's. Guaranteeing that all of those machines will deliver each of your packets in at most 100ms in total is a tall order: so tradeoffs have been made in encoders and decoders that wrap the data of those communications. They are made specifically to cope with missing packets, unordered packets and such. They literally have code in them to decide what to emit / display when the next packet is unavailable, or if there is a gap in the stream. All dependent on predicting what should happen in between.

Most of the glitches you experience in streams are actually content put in place in place of missing data. The data arrives in the end, just later than expected. Late data is useless, they are discarded if they arrive then, because well, the time to display / emit that data has already passed.

If the algorithm decides that it is frequently missing data to the detriment of the realtime communication, they renegotiate the settings they use (between the parties, behind the scenes) to use a lower quality stream until the situation gets better. So if the algorithm decides that the infrastructure between you and your dad cannot handle low latency communication in HD, they will lower the quality until the packets arrive in time more reliably.

Now none of this would occur if you could buffer, say, 2-3 seconds of data. Then you'd be shielded against any short-term delays in communication. You couldn't get any data for 300ms? No biggie, you have 3 seconds buffered ahead of you, the late data will eventually arrive by then (this is what video streaming you're accustomed to does).

For realtime communication, this is not an option. You need low latency, and the hardware that connects you and your dad is far too unreliable. The problem gets bigger as your data gets bigger. So those apps try to do more with less data (lower quality audio / video), fill in the gaps when latency demands are not met (glitches / predicted output) to make realtime communication possible.

A final note in showing how bandwidth is not everything: You can load a ship full with high density disk drives from China and transfer enormous amounts of data in weeks, when averaged, might look impressive in ""speed"" (I transferred 1234 yobibytes in 2 weeks, that means it was 2 gigabytes per second!). But the latency was very high. It took you 2 weeks to get your first bit, but after that you got all. Doesn't change the fact that you can't use the method for realtime communication. So the thing is not dictated solely by your internet speed. Telephony applications should be robust against the latency problems in the connection.",40hzn4,t3_40hzn4,thefunkygibbon,,Comment,2,0,2
cyuyh4w,2016-01-11 23:32:40-05:00,tRfalcore,,streaming a netflix movie has tons of servers sending data.  The netflix app says I need XYZ and a thousand servers can respond with XYZ.  When I stream my hot omegle porn to you directly from my webcam it can only come from my computer.,40hzn4,t3_40hzn4,thefunkygibbon,,Comment,1,0,1
40hjm9,2016-01-11 11:39:06-05:00,theacorneater,I bought a keyboard and only 8 keys work in it. How can I fix this?,"I bought this backlit QWERTY Keyboard on Amazon. It's from some Asian manufacturer. The only keys that work are 

""asdfjkl;""


Is this a common occurrence? Could someone help me with this? I've tried using it on a Windows, Mac and Linux machines.

I don't know if this is the right place for this question, but I hope you can help me. ",,,,,Submission,0,0,0
cyu5xr4,2016-01-11 11:41:04-05:00,videoj,,Try /r/techsupport,40hjm9,t3_40hjm9,theacorneater,,Comment,6,0,6
40frm7,2016-01-11 02:47:43-05:00,RexBox,Is there a way to see if one colour is the shade of another colour by using things like RGB values?,"We are doing a project for school, and our goal is to reckognise the same object in different photos.

Now, some photos are made in darker shade than others. Like, we have a red ball, and first we take it with normal lightning, and then with a bit decreased lightning, so the photo is darker. Are there any techniques for checking if the object is still the same colour?",,,,,Submission,8,0,8
cytua2d,2016-01-11 02:50:21-05:00,tyggerjai,,"Look into HSV - Hue, Saturation, Value. It's a method of categorising color which is very common in computer vision, and which basically does exactly that.

The OpenCV library is a good place to start, with some good tutorials.",40frm7,t3_40frm7,RexBox,,Comment,13,0,13
cytuod9,2016-01-11 03:11:47-05:00,RexBox,,"Thank you a lot! I just tried it on some photos, and it seems to work for the most part! The V(alue) doesn't seem to change much if you make it darker, and the H(ue) changes slightly, but the S(aturation) changes a lot. Thanks for the help man :)",40frm7,t1_cytua2d,tyggerjai,,Reply,3,0,3
cytur85,2016-01-11 03:16:21-05:00,tyggerjai,,"Yep, the Hue is basically the color, the saturation is how ""colourful"" it is. You may find value more useful as they get really dark.",40frm7,t1_cytuod9,RexBox,,Reply,3,0,3
cyuoknd,2016-01-11 19:12:31-05:00,Rangsk,,"HSV is also great for chroma-key implementations, and also if you need to interpolate between two colors in a logical way.",40frm7,t1_cytuod9,RexBox,,Reply,1,0,1
40ch1d,2016-01-10 13:32:33-05:00,RunnrX,"New, self-taught software developer asking about how to become more marketable in software field","Hi, and thanks for reading this.  To keep it short, I've recently enjoyed changing jobs from unskilled and low paying ones into one where I work privately for an investor helping to automate his strategic ideas.  I am self taught and only have experience working with simple code that works with time & price based data series in c# and have a loose working knowledge of objects and so on.

I have a feeling that the current job may not be a long term situation and I'd really rather not leave this and go back to things like driving trucks or doing night security :/

I've seen that people who work as web developers or misc software developers seem to just require about 2 years of college to get a rather rewarding career for themselves, but I was wondering if anyone could direct me towards any effective options besides taking 2 years of college.  For example, I'm currently just working through learning the book ""c# for dummies"" and getting through the codeacademy website for an intro to html, css, javascript and so on..  but I'd love any advice anyone has on what would be the most effective thing to try to learn and what may be some in-demand jobs I could try to repurpose myself for, I don't really care which ones they are at this point.

I'll happily provide more details if anything is needed - thanks!",,,,,Submission,10,0,10
cyt4yy5,2016-01-10 14:23:24-05:00,sloec,,Write and publish a free app. Contribute to open source projects so I can go look at your work. Basically provide enough real world experience that the lack of college becomes less important. We hired a young guy that did not go to school but showed us a lot of real world contributions and spoke intelligently about topics not covered by his work. ,40ch1d,t3_40ch1d,RunnrX,,Comment,11,0,11
cytfyc3,2016-01-10 18:59:02-05:00,l4mpSh4d3,,"Related, if you have an idea about an application you are interested in developing, create a github account and make the code accessible. Make sure you document your code and write it using all the software engineering knowledge you can get from books online resources etc. It's a double edged sword because if your code is bad, it will show.",40ch1d,t1_cyt4yy5,sloec,,Reply,3,0,3
cyu8bj4,2016-01-11 12:40:40-05:00,RunnrX,,"This also is good - thank you, I can make good use of this.",40ch1d,t1_cytfyc3,l4mpSh4d3,,Reply,2,0,2
cyu8art,2016-01-11 12:40:10-05:00,RunnrX,,"I hadn't thought of this, but I find it quite helpful - thank you!",40ch1d,t1_cyt4yy5,sloec,,Reply,1,0,1
cytqax3,2016-01-10 23:57:49-05:00,brennanfee,,"The first (and I believe best) piece of advice I can give you is be honest in the interview.  It is ok, often even good, to utter the phrase ""I don't know.""  Even better is ""I don't know but here's how I would go about finding out.""

Second, be able to demonstrate what you *do* know.  Interviewers are trying to see if our skills are what they need, if you can't even discuss what you have done and how or why you make a particular choice it is very difficult for them to understand how you might apply what you know for them.

Third, keep coding.  Keep doing it.  As much as you can.  Read as much code as you can.  If you don't understand something... figure out how it works, read up on it, toy with it.  Learning the how, why, and the theory is much more important than remembering inane information like what method is used to do X.",40ch1d,t3_40ch1d,RunnrX,,Comment,3,0,3
cyu88t7,2016-01-11 12:38:52-05:00,RunnrX,,"Thank you for the good advice - and for the first point, you're right - this is how I got where I am today.  My current employer asked me if I knew how to automate his trading strategies for him.  The answer to that was no, but instead of telling him to look up someone else, I just did a crash course in coding tutorials and became someone who knew how to do it.  I then found that any challenge I would meet where I had no idea how to do something, I would be able to eventually get an answer or some guidance by looking up pre-existing posts on stackoverflow or something like it.  Being able to find out what I don't know has been a fun and interesting path.",40ch1d,t1_cytqax3,brennanfee,,Reply,1,0,1
cyurev8,2016-01-11 20:29:55-05:00,brennanfee,,"Great, keep it up and you'll do fine.",40ch1d,t1_cyu88t7,RunnrX,,Reply,2,0,2
cyt5t3n,2016-01-10 14:44:42-05:00,piratebroadcast,,"Look into doing an immersive web development course at General Assembly. This just might be ideal for you. There are also ways to finance the education, etc. https://generalassemb.ly/education/web-development-immersive/",40ch1d,t3_40ch1d,RunnrX,,Comment,3,0,3
cyu89y1,2016-01-11 12:39:38-05:00,RunnrX,,I like having links to learning resources - thanks.  I'll look into this :),40ch1d,t1_cyt5t3n,piratebroadcast,,Reply,1,0,1
cyt5ard,2016-01-10 14:31:47-05:00,None,,[deleted],40ch1d,t3_40ch1d,RunnrX,,Comment,5,0,5
cyu85kx,2016-01-11 12:36:41-05:00,RunnrX,,"Very good point.  I am working full time at software development already, in a rather loose sense.  I have hours per day available while running time-consuming simulations on one computer where I could study and practice my coding, and as far as having guides I've found a lot of benefit to using discussion forums up until now.. I was hoping that I could somehow gain the same learning experience without having to take the path of a 2 year course, but I can imagine it forcing me to stay on focus more than if I leave it all up to myself..",40ch1d,t1_cyt5ard,None,,Reply,1,0,1
cytwm9n,2016-01-11 05:17:30-05:00,LoganGyre,,Colleges teach at such a slow pace. They also want you to waste time taking unnecessary courses like visual basic which I am so happy i wasted over a grand to find out was worthless. Every programming course i have ever taken moved slower then molasses and in the end covered only a portion of the materials. Maybe its just my area but it seems like allot of people take programming courses just because they think it will be all gaming and browsing the web.... ,40ch1d,t1_cyt5ard,None,,Reply,1,0,1
cytx8xg,2016-01-11 06:01:31-05:00,None,,[deleted],40ch1d,t1_cytwm9n,LoganGyre,,Reply,1,0,1
cyty90g,2016-01-11 07:06:12-05:00,LoganGyre,,the difference a state can make. All the schools in my area have this shitty deal with Microsoft where we all get nice cool new software on all the machines but they teach a shit ton of microsoft specific courses unless I want to go to a for profit or religious school. In fact 2015 was the first year they had dropped VB from the coursework.,40ch1d,t1_cytx8xg,None,,Reply,1,0,1
cyu82cd,2016-01-11 12:34:30-05:00,RunnrX,,"This is one of the things that worried me about paying for a college course.  There are some private colleges in my area that are notorious for charging 3x the tuition that a community college would cost for fluff course content that gets you nowhere.  I learned about how I should avoid them, but now I still have my doubts about the value of a reputable community college course and its costs vs just doing self teaching for less cost but a higher requirement of personal action & responsibility.",40ch1d,t1_cyty90g,LoganGyre,,Reply,1,0,1
cyti3y7,2016-01-10 19:55:00-05:00,NullEgo,,"If you aren't completely in love with web development, you could look into languages like Java and C++ to do back end services.  I'm in software development and rarely touch the front end stuff.  Just another option for you to consider.",40ch1d,t3_40ch1d,RunnrX,,Comment,2,0,2
cyu7yr2,2016-01-11 12:32:06-05:00,RunnrX,,"Something like this would be more appealing to me actually, thank you for pointing that out.",40ch1d,t1_cyti3y7,NullEgo,,Reply,1,0,1
cytxa7o,2016-01-11 06:04:00-05:00,SneakingNinjaCat,,">  I'm currently just working through learning the book ""c# for dummies""

I learned C# from the book ""Essential C# 4.0"" on my first university year in Software Engineering.

Object oriented programming and the C# programming language counted as 5 credits (ECTS): the entire education (M.Sc. in Software Engineering) is 300 ECTS.

Passing a university course on C# moved me 1,7% towards getting an engineering degree. Being a software developer is much much more than knowing a programming language.
",40ch1d,t3_40ch1d,RunnrX,,Comment,2,0,2
cyu7y3l,2016-01-11 12:31:40-05:00,RunnrX,,"Thank you for giving this perspective.  I knew there would be much more to it than just knowing a language, but I don't know yet what the other elements to seek out and learn are.  If you have another moment, listing even a couple of topics in point form would be greatly appreciated and then I can go look it up.",40ch1d,t1_cytxa7o,SneakingNinjaCat,,Reply,1,0,1
cywapvy,2016-01-13 00:00:43-05:00,Mines_of_Moria,,"operating systems, algorithms, discrete math, other math like linear algebra depending on what you're doing, databases, networking, security, learning more about hardware, architecture, design patterns, version control, optimization, distributed systems, functional programming, object oriented programming, web development....and there is so much more 

You should pick a language, like C or Java, and start going through all the primary libraries. Learn how to use operating systems as well, learn how to do multithreading, use fork(), stuff like that in at least one or two operating systems. learn how to do things from the command line. learn a scripting language. 

look up projects for your language of choice and try to implement. ",40ch1d,t1_cyu7y3l,RunnrX,,Reply,2,0,2
cywmowi,2016-01-13 09:40:17-05:00,RunnrX,,"Thank you, this will be a huge help to know about :)",40ch1d,t1_cywapvy,Mines_of_Moria,,Reply,1,0,1
40cglt,2016-01-10 13:30:13-05:00,allconfigured,What kind of website would be good for the domain allconfigured.com?,,,,,,Submission,0,0,0
40a6xd,2016-01-10 01:20:09-05:00,AestheticFC,Possible to reverse engineer censorship?,"Sorry if stupid question but is it possible (or at least in theory) to obtain the original image of [this (for example)](http://i.huffpost.com/gen/1322389/thumbs/o-CHINA-CONFESSION-570.jpg?7) if it was known what algorithm was used to generate that blurred area over the original? If so, why have I never heard of this before?",,,,,Submission,2,0,2
cysodew,2016-01-10 01:57:49-05:00,mosqutip,,"It's [unlikely](https://photo.stackexchange.com/questions/35097/is-it-possible-to-re-focus-recover-an-intentionally-blurred-image) that you'd be able to reverse the blur, even if you know what algorithm was used. Standard blurring techniques usually introduce some element of randomness.

Using a strict Gaussian blur, reversal may be possible. See [this](https://photo.stackexchange.com/questions/38435/how-can-i-undo-an-intentional-gaussian-blur-when-i-dont-have-the-originals) for a more in-depth description.",40a6xd,t3_40a6xd,AestheticFC,,Comment,4,0,4
cyv6ls3,2016-01-12 05:37:04-05:00,drummyfish,,True. Also there is [this] (http://smartdeblur.net/) nice program.,40a6xd,t1_cysodew,mosqutip,,Reply,2,0,2
408umg,2016-01-09 19:03:34-05:00,lost_angel25,Associative Memory Database,"I have a set of key value pairs that I would like to sort into a database, but I want the location in the database to be based on the semantic relation between two entries.  I've been working on it for a while, but is this possible through a finite state machine? I am trying to frame it as an algorithmic problem if that helps.",,,,,Submission,3,0,3
cysngtj,2016-01-10 01:19:11-05:00,sathoro,,"I feel like all the posts in this subreddit ask the most ambiguous, theoretical questions. Why don't you tell us what you are attempting to accomplish or what ""sort into a database"" even means?",408umg,t3_408umg,lost_angel25,,Comment,2,0,2
cysmh23,2016-01-10 00:40:32-05:00,Mines_of_Moria,,">  but is this possible through a finite state machine?

Why wouldn't it be? ",408umg,t3_408umg,lost_angel25,,Comment,1,0,1
408ocy,2016-01-09 18:24:03-05:00,HumanID,How does a relational database sort datasets?,And why it does it do it in that manner?,,,,,Submission,6,0,6
cyshvb1,2016-01-09 22:09:05-05:00,BonzoESC,,"Relational databases sort in many ways at many times: at insert time, at update time, and a few different ways at query time.

When doing a write like insert or update, most relational databases prefer to perform a complete and consistent write, or failing that, no write at all: and to do this quickly. In a very basic case, it works like this:

0. Set any calculated fields, such as autoincrementing/sequenced primary keys.
1. Perform validation on data, such as unique index constraints.
2. Append the write operation to the journal/write ahead log.
3. Flush the journal to disk.
3. Tell the client application the write has been committed.
4. Go back and file the journaled row in the right place in the table files, index files, etc.
5. Put a checkpoint in the journal saying ""all writes before here have been committed""

The journal, being append-only, is in chronological order. In the index and table files, things get more complicated: see /u/Cadoc7 's comment, but both indexes and tables kind of work the same way: a data structure that makes both scanning (`SELECT * FROM tweets ORDER BY created_at DESC`) and seeking (`SELECT * FROM tweets WHERE id=682431544653381632`) cheap in terms of disk time.

Many databases provide multiple kinds of table or index format that provide different operations for different kinds of data.

At query time, the planner has to make a choice about what sorting strategy to use, based on index availability and the ""shape"" of the data.

If an ordered index like a B-tree is available to sort with, just seek to the first index entry to be read, follow the row-ids in the index to the table file, return those rows to the client, and repeat with the next index entry up to the last one.

If the data can't be sorted cheaply with an index, but fit into RAM, quicksort them there (using short indexes to the array of rows in memory, instead of expensively copying rows around during the sort), and return them to the client in order.

For a non-indexed sort field on data that don't fit into RAM, read the sort fields and row-ids into a temporary B-tree on disk, and sweep it out of there.

Some reading:

* Postgres index types: http://www.postgresql.org/docs/current/static/indexes-types.html
* Postgres journal/write ahead log: http://www.postgresql.org/docs/current/static/wal.html
* Postgres files: http://www.postgresql.org/docs/8.3/static/storage.html
* LevelDB file format: https://leveldb.googlecode.com/svn/trunk/doc/impl.html
* and of course the wikipedia for every data structure that's been name-dropped :P",408ocy,t3_408ocy,HumanID,,Comment,4,0,4
cysaikk,2016-01-09 18:37:20-05:00,Rikkety,,"In theory, relational data has no order. 

In practice, of course, the data has to actually be stored on a disk or in memory, so there has to be *some* order. What determines that order depends on the implementation of the DBMS. 

An ordered dataset allows some more effective search techniques, so the obvious way to order the set is to use a field which is often used for querying, typically the primary key. 

It's also possible the dataset is simply ordered chronologically, i.e. the order in which the records were added. In case of an auto-incremented primary key, this would mean it's also automatically order on the primary key. ",408ocy,t3_408ocy,HumanID,,Comment,1,0,1
cyspyly,2016-01-10 03:19:56-05:00,Devvils,,One common way is to store the data so it can be retrieved in sorted order. A common way to do this is a [B-Tree](https://en.wikipedia.org/wiki/B-tree) which you can store on disk.,408ocy,t3_408ocy,HumanID,,Comment,1,0,1
cysc21v,2016-01-09 19:19:31-05:00,Cadoc7,,"They don't actually sort datasets because that would be horribly expensive to do on every query. Databases lets the data structure they use sort for them. Databases traditionally store data in a balanced tree, commonly a B or a B+ tree. This structure is used because it works really well with block-oriented storage system (aka filesystems) with efficient point and range lookup speed (the B+ tree is particularly great at contiguous range queries).

In memory databases will use some other data structures optimized for RAM instead of disk, but the principal is the same: sorting is done by the data structure, not by a sorting algorithm.",408ocy,t3_408ocy,HumanID,,Comment,1,0,1
404os1,2016-01-08 22:42:18-05:00,OutofPlaceOneLiner,"Is studying computer science more like a new language, math, or even physics?","Probably going to minor in it and I have 0 experience in it, but I realize the look it gives a resume ",,,,,Submission,10,0,10
cyri1dn,2016-01-08 23:42:40-05:00,eygrr,,"Computer Science is just mathematics with computers. If you can think abstractly, you'll do great.",404os1,t3_404os1,OutofPlaceOneLiner,,Comment,12,0,12
cyrleb5,2016-01-09 02:00:57-05:00,PastyPilgrim,,"Computer Science is a branch of math. Learning your first computer language is like learning a new language (not so much for subsequent languages), but that language is just a tool for expressing mathematics.

So, to answer your question, it's most like studying math because it is studying math.",404os1,t3_404os1,OutofPlaceOneLiner,,Comment,9,0,9
cyrgqel,2016-01-08 22:57:33-05:00,MCPtz,,"Then you'll know a bit about programming languages and a bit about general purpose solutions to solving problems on computers and a bit about learning how to learn about how to solve problems.

It's up to you where you want to go.

For all three, I'd say it has the similar distinction of you get what you put in. If you put in hard work, you'll learn, and if you continue to find it useful, you can, if you want, continue to get better at using computers to solve problems.",404os1,t3_404os1,OutofPlaceOneLiner,,Comment,3,0,3
cyrrbdx,2016-01-09 08:39:46-05:00,AyySStim,,"I just finished my degree up last year.

I'd say the learning code part is equivalent to learning the alphabet. While it's one of the foundations of your regular education, the real meat and potatoes of your lower grade years is  the classic lit, grammar, how to format your use of the alphabet in a way easily digested by others (long way of saying how to write papers). It's similar in Comp sci, you move on to the rest after learning the alphabet(code).

What you learn about the inner workings of a computer, to learning about the types of languages out there and how one can out perform another in certain cases and be totally rekt in others (Javaaaaaaa !!!)

And as my fellow comp sci nerds in here have mentioned it's more about abstraction and problem solving. For whatever reason people fear the nitty gritty of computers, comp sci guys are just problem solvers that got past the computer being a big bad machine that works on magic and dreams.

I went on to pursue something else that developed for me in my last year of school, but for one saying you have a degree in comp sci feels good, and two, knowing about computers is just crazy convenient. 

I've had too many times to count where just having been around comp sci for that many years has saved me never ending headaches.

This is going to be a bit technical, but just today I couldn't figure out why OpenDNS wouldn't play well with my pc. It turned out when I updated some drivers a setting for Ipv6 turned back on in my video card settings and apparently Chrome's singular option of ""Use a web service to complete blah blah"" uses Ipv6. 

Don't worry about ""look it gives to a resume"". If you bust your ass and *really* want those jobs you're looking for, you'll get them.

Last but not least, I'm not that smart, I had to take one of the upper level courses *three* fucking times, and I'm pretty sure my linear algebra teacher decided he wanted to see what would happened if he passed the retarded kid. But like the job, if you **really** want something, it will come.

",404os1,t3_404os1,OutofPlaceOneLiner,,Comment,4,0,4
cyrptad,2016-01-09 07:01:43-05:00,None,,"It's like a mix of mathematics and engineering. You prove stuff, but you also make stuff.

",404os1,t3_404os1,OutofPlaceOneLiner,,Comment,2,0,2
cyrvd04,2016-01-09 11:25:43-05:00,A_SALAMI,,"I'd say learning a programming language itself is like learning every other language. Actually coding, however, is more like learning how to be a lawyer (but, you know, with computers, using math). 


I'm totally spit balling here because I have no law experience, but you can memorize syntax and how everything is used, but really getting good at it involves knowing how to use what you know to solve a problem, like a good debate. Coincidentally, both involve a degree of logic to hold it all together. 


Documentation and UI is like putting stuff plainly enough for a jury to understand. Knowing everything and being a savant does nothing for you if people ultimately can't use your software. Maybe having no experience with comp sci will help you see both sides and make you better. ",404os1,t3_404os1,OutofPlaceOneLiner,,Comment,1,0,1
cyu92fj,2016-01-11 12:58:22-05:00,yendrdd,,"Computer Science is more like math. Programming languages, just like math, are designed to communicate information to computers in words that we humans can understand. 

So, arguably 95% math and 5% language depending on how you look at it.",404os1,t3_404os1,OutofPlaceOneLiner,,Comment,1,0,1
401nfl,2016-01-08 10:59:21-05:00,Salty_Dugtrio,[Networking] UMTS Frequency Division Duplexing,"I'm currently taking a networking course, and after looking online, I couldn't find a proper answer to these questions I was having.

In UMTS, both Spreading and Scrambling are used. 

Spreading is used with an Orthogonal Variable Spreading Factor (OVSF) in order to generate Orthogonal codes, to realize the duplexing of 2 or more streams from the same sender. (Spreading is constrained in that it requires all streams to be synchronized at a common time t.) Spreading also has the downside(?) of increasing bandwidth usage.

Scrambling is used in order to combine the data streams of 2 different terminals with the use of Gold codes. 

My question is, why is scrambling not used to achieve the combining of 2 streams from the same terminal? Using Spreading with a high OVSF factor causes a higher bandwidth to be used, whereas Scrambling guarantees a constant BW. Is there a specific overhead in Scrambling that Spreading does not have? ",,,,,Submission,4,0,4
3zxwmh,2016-01-07 17:27:30-05:00,TheBeardofGilgamesh,"Which would be more performant, a best case 0(2^n) algorithm in a fast language like Java, or a worst case O(log(n)) algorithm in something slow like Ruby?",Assuming n > 100 thousand,,,,,Submission,1,0,1
cypwrvk,2016-01-07 17:49:05-05:00,Lothlorne,,"In general, assuming that your input size is sufficiently large (and in this case it is), differences in complexity are going to outweigh practically anything else. One of these algorithms will resolve within a small fraction of a second: the other will still be running well after the heat death of the universe. You should be able to determine for yourself which is which.",3zxwmh,t3_3zxwmh,TheBeardofGilgamesh,,Comment,10,0,10
cyq0y8k,2016-01-07 19:37:12-05:00,i_invented_the_ipod,,"In general, you can't say. Different algorithms will have different amounts of overhead, and different numbers of operations per iteration, none of which is accounted for by the big-O notation.

In the case where n is ""sufficiently large"", the big-O notation WILL tell you which algorithm is faster. Interestingly, this is independent of how fast/slow each individual iteration is.

The fastest you can do anything on a modern super-computer is on the order of 1 picosecond. At that speed, an algorithm that takes 2^n operations will take at least 2.3x10^30073 times longer than the age of the universe to complete, for n=100000.

By comparison, even if your O(log(n)) algorithm involves you writing down the problem, mailing it to someone on the other side of the world, having them work it out on paper, and then send the result back, you'll only have to do that process 12 times total. So even at a year per iteration, the more-efficient algorithm will be *vastly* faster.

The crossover point at which the ""slow"" algorithm gets to be faster in this example will work out to around n=69. ",3zxwmh,t3_3zxwmh,TheBeardofGilgamesh,,Comment,3,0,3
cyq034f,2016-01-07 19:13:56-05:00,zifyoip,,"Strictly speaking, your question is unanswerable, because big-O notation does not say anything at all about specific values of the function.

Also, O(2^(n)) just means that the function is asymptotically bounded *above* by c⋅2^(n) for some constant&nbsp:c. So a *constant* function is O(2^(n)).

An algorithm that takes one step and finishes, for all&nbsp:n, runs in O(2^(n)) time, because any constant function is asymptotically bounded above by&nbsp:2^(n). An algorithm that takes 10^(10^10^100)log&nbsp:n steps runs in O(log&nbsp:n) time. In this case, for any n&nbsp:≥&nbsp:2 the number of steps taken by the second algorithm is vastly greater than the number of steps taken by the first algorithm.

So just the fact that one algorithm takes O(2^(n)) steps and a second algorithm takes O(log&nbsp:n) steps tells you absolutely nothing at all about which one takes more steps for any specific value of&nbsp:n.

That's true even if you are more precise and specify that the first algorithm takes Θ(2^(n)) steps and the second takes Θ(log&nbsp:n) steps.",3zxwmh,t3_3zxwmh,TheBeardofGilgamesh,,Comment,3,0,3
cypwfkk,2016-01-07 17:41:10-05:00,mosqutip,,"The second case, easily. Look at this handy graph: http://bigocheatsheet.com/img/big-o-complexity.png

O(2^(n)) complexity is really, really bad. It grows out of hand extremely quickly, regardless of the specific algorithm or language being used. The differences in speed between individual languages is relatively minor compared to a difference in algorithmic complexity this large.

On another note, a better comparison would probably be something like C or even assembly, instead of Java.",3zxwmh,t3_3zxwmh,TheBeardofGilgamesh,,Comment,1,0,1
cypzrz5,2016-01-07 19:05:42-05:00,TheBeardofGilgamesh,,"i know i was just joking, java actually kinda sucks which is why I wrote Java. I have looked at quite a few performance bench marks on generic algorithms, and Java initially is kinda slow but eventually optimizes and has performance that is only slightly worse than than C. But C's performance is linear, it starts fast and continues on being the fastest. ",3zxwmh,t1_cypwfkk,mosqutip,,Reply,-4,0,-4
3zxfo3,2016-01-07 15:49:13-05:00,GreenAce92,Will quantum computing make string matching instantaneous?,"I am vaguely aware of the premise for quantum computing. I listen to NPR like everyday at work and today they mentioned quantum computing. But the guy's explanation was so vague, surface level knowledge. He used the double-slit experiment and ripples in a pond but didn't actually talk about say a CPU design or the additional state was it? Instead of just 0 and 1 there will now be 2 as well? I don't know, anyway


I'm using grep right now in Linux to search for a keyword in a text file, so I imagine it has to open every file and see if that string exists inside the file. Whereas the qunatum version as mentioned in the brief NPR session, would look in everything simultaneously so the answer should be immediately found right? Or at least in one check rather than several checks resulting from sequential search? 

Read a book!

PS. How do you insert a line break in reddit? Is it html or not?

<br />",,,,,Submission,5,0,5
cyptar3,2016-01-07 16:28:53-05:00,UncleMeat,,"No. Quantum computation does not ""do everything simultaneously"", like is commonly stated. If this was true then Grover's Algorithm would run in O(1) time rather than O(sqrt n) time. ",3zxfo3,t3_3zxfo3,GreenAce92,,Comment,8,0,8
cyqe389,2016-01-08 02:41:51-05:00,Rothon,,"Grover's Algorithm runs in O(sqrt(n)), not O(logn) I believe.",3zxfo3,t1_cyptar3,UncleMeat,,Reply,3,0,3
cyqv8tp,2016-01-08 13:24:29-05:00,UncleMeat,,Er.. right. That's a pretty big error on my part.,3zxfo3,t1_cyqe389,Rothon,,Reply,1,0,1
cyqolqb,2016-01-08 10:49:58-05:00,GreenAce92,,Are you sure nephewmeat says otherwise,3zxfo3,t1_cyptar3,UncleMeat,,Reply,1,0,1
cypvgi3,2016-01-07 17:18:03-05:00,permanonnnnn,,"Hi. I'm afraid I'm not sufficiently qualified to give you a rigorous formal answer, but I've been investigating potential applications of quantum computing for a while at work, so happy to share my take so far.

In essence there are 3 main issues outstanding to my understanding:
1) not many people are entirely sure anyone has built an effective quantum computer yet. Some argue strenuously that the D-Wave and d-wave 2 are good examples: the controversy amongst some pretty heavy hitters is surprising

2) no one seems confident yet they know how to code to take advantage of quantum computing. There are many coding paradigms that fundamentally need to be rethought to work on a quantum basis

3) there is a class of problems most suited to theoretically taking advantage of quantum computing. I would describe that class of problems (in my view) as being problems that involve a stochastic rather than deterministic model being evaluated. But in truth, I don't think the class of problem is well understood

The other commenter's reply is also absolutely correct.",3zxfo3,t3_3zxfo3,GreenAce92,,Comment,6,0,6
cyq36og,2016-01-07 20:38:00-05:00,anon_smithsonian,,"IIRC, the controversy around the D-Wave was generally due to two things:

1. The D-Wave is completely proprietary, so the details of *how* it actually runs is known to very few. They are essentially black-boxes, and D-Wave offered little to academia in terms of proof, instead focusing on convincing their potential sales markets (the Dept. of Defense, NASA, etc.) with closed-door demos. This is what initially led to the widespread skepticism following D-Wave's initial announcement and claims.

2. As it became harder to refute D-Wave's clearly superior performance on certain types of computations that traditional computers struggle with, it became more a debate about the semantics of what if the true definition of *a quantum computer*. The last I recall reading about the controversy, one of the more vocal skeptics of the D-Wave had essentially ceded that the D-Wave *does* appear to take advantage of certain quantum phenomena in order to improve its performance on certain types of calculations,  however it is unlikely that it would be considered a ""true"" quantum computer.

(It was a couple years since I read this, so I could be wrong, misinterpreting some things, or completely omitting important points... but this is my recollection to the best of my knowledge.)

I kind of got the impression that it was mostly just a pissing match that really stemmed from the long-time grudge between the private sector and academics (where most of the quantum computing technology advances originate). ",3zxfo3,t1_cypvgi3,permanonnnnn,,Reply,2,0,2
cyqgp8x,2016-01-08 05:21:55-05:00,lapfaptap,,"The controversy surrounding d wave is that there is no clear reason why it should work. They completely ignore error correction and the coherence time is way below what one would expect to actually perform quantum computations. Even if it somehow managed to work as advertised it's not even clear the underlying *theoretical* model provides any speed up over classical models.

Theyve thrown a bunch of stuff together and sort of just hope it does something interesting. Now, this is okay for what it is. If people want to spend their time and money on somethibg, go ahead. Hell, they might somehow manage to get lucky and defy expectations from people in the field. The real problem is their insistence on making extraordinary claims about their machine's potential without even ordinary evidence. And despite the headlines you've seen over the years they still haven't provided proper evidence that there is any quantum speed up. No, the 100 million speed up or whatever Google's most recent claim is, is not over classical computers. It's over a specific classical algorithm they chose. They even admit in the paper that there are classical algorithms that might beat the machine, but they chose to ignore those. But, as always when we talk about d wave, the revolution is just around the corner. Shut up with the claims and provide some freaking evidence. ",3zxfo3,t1_cyq36og,anon_smithsonian,,Reply,2,0,2
cyqh0s4,2016-01-08 05:43:58-05:00,anon_smithsonian,,"Thank you for helping clarify what the controversy about it comes from. I wasn't aware of the error correction and cohesion parts. Clearly there is still a great deal of debate about it. 

It's been a few years since I really read much about the D-Wave... I should brush up on the topic.",3zxfo3,t1_cyqgp8x,lapfaptap,,Reply,1,0,1
cyqhn2y,2016-01-08 06:26:46-05:00,lapfaptap,,"Not much new imo. I don't really think Google's most recent paper helped d wave's case. It worked great in the PR machine, but as actual evidence in scientific terms it was actually pretty disappointing. There were no asymptotic improvement which left them with a constant factor improvement. Again, not over classical algothims. Over one specific classical algorithm. And it was essentially simulating the behavior of the d wave machine. You know what can simulate a rock being dropped into a pond better and faster than any computer we have? Actually throwing a rock into a pond. Hardly evidence for saying my rock dropping device is a quantum computer. ",3zxfo3,t1_cyqh0s4,anon_smithsonian,,Reply,1,0,1
cyq2idw,2016-01-07 20:19:42-05:00,anon_smithsonian,,"Ok, so disclaimer: I'm not a CS, Quantum Mechanics, or Mathematics expert: however I do find Quantum Mechanics fascinating and have spent a lot of my free time learning about it, and in the process, spent a lot of time learning about Quantum computing. I'll try to condense my takeaway to help explain why the answer to your question is ""no."" 

As you heard, quantum computing allows for bits (called qubits) that have **three** states: just like computers today, they have a 0 state and a 1 state. (All of our modern digital technology boils down to 0s and 1s.) Qubits can also have a third state, which is a *[superposition](https://en.wikipedia.org/wiki/Quantum_superposition)* of 0 and 1: essentially both 0 ***and*** 1 at the same time. 

What does this mean, *really*? That's where the mathematics I'm not qualified to explain come in... but it essentially opens up the possibility of new algorithms that can *only* be run on a quantum computer. Most famous of those is [Shor's Algorithm](https://en.m.wikipedia.org/wiki/Shor%27s_algorithm). (This one is really important for cryptography, but it's also math-heavy and I don't feel comfortable trying to explain it correctly.)

Now, where quantum computers are expected to really shine is on what is known as [NP hard](https://en.wikipedia.org/wiki/NP-hard) problems. A good example of this is the ""delivery driver routing problem"": you are a delivery driver and have to deliver these 50 packages at 50 different places around the city today. What is the most efficient route to deliver those packages? 

Well, that's an example of a type of problem that our current computers can't do very well. Right now, we'd have to basically brute-force every single possible combination of routes and, at the end, compare the results of each route in order to determine which is the best. But due to the different algorithms that can be run on quantum computers, this problem can be solved exponentially faster. (Again, I don't have the math background needed to explain *why* or *how* it does this, but if it helps, you can think of it as it is and to ""see"" the most optional route without having to calculate each route individually.)

But, basically, Quantum computers don't just run ""faster"" than modern computers: instead, they just solve **very specific types** of computations **more efficiently**. There are plenty of algorithms that quantum computers are actually expected to run far more *poorly* than traditional, binary computers.

Also, part of the reason it couldn't run your text search faster is because you are working with a very specific set of data... the calculation it is doing is basic substring matching... which, by itself, isn't very computationally intense, it just has to do it *a lot*. In this scenario, you would see far greater results by (1) having more threads running asynchronously and (2) having enough data bandwidth to keep the processors from wasting clock cycles waiting for data to be moved. 

Hopefully this helped to clarify your understanding of things. I think it's really common for people to misunderstand what quantum computing will actually mean and what sort of advances we can expect to see from it. There is a lot of bad information being portrayed in popular media either because the people themselves don't understand it or it's over-simplified to the point of being misleading. At one time--not that long ago--I, too, thought that quantum computing would mean nearly instantaneous computations for *everything!* But, sadly, that isn't the case. In reality, we can expect quantum computing to provide the most impact on scientific and technical fields that work with a lot of NP-hard problems... but it's unlikely that it will ever straight-up replace the role that binary computing plays in our everyday lives.",3zxfo3,t3_3zxfo3,GreenAce92,,Comment,3,0,3
cyq3ifg,2016-01-07 20:46:55-05:00,bobtheterminator,,"I would recommend this article: http://www.scottaaronson.com/blog/?p=208

It explains Shor's algorithm at a fairly high level, and hopefully will help explain why quantum computers are faster than classical computers even though they can't just try every answer and resolve the right one.",3zxfo3,t3_3zxfo3,GreenAce92,,Comment,2,0,2
cyq0rlz,2016-01-07 19:32:08-05:00,anon_smithsonian,,">PS. How do you insert a line break in reddit? Is it html or not?

It's `&nbsp:` with an empty line above it and below it.

(To remember it, **nbsp** = **n**on-**b**reaking **sp**ace... essentially it's a space character that's treated like a normal character)",3zxfo3,t3_3zxfo3,GreenAce92,,Comment,1,0,1
cyqp12v,2016-01-08 11:00:28-05:00,GreenAce92,,"This is not a line break my friend. It is a space. I want a horizontal divide between paragraphs eg. Line break. 
Few methods come to mind.
/n
<br>",3zxfo3,t1_cyq0rlz,anon_smithsonian,,Reply,1,0,1
cyqp7kk,2016-01-08 11:04:56-05:00,anon_smithsonian,,"> It's `&nbsp:` **with an empty line above it and below it.**

Look:

&nbsp:

Line breaks!

No nbsp between lines?



No line break.

(empty line + nbsp + empty line)?




&nbsp:

Line break.


&nbsp:


&nbsp:


&nbsp:


&nbsp:


&nbsp:

Lots and lots of line breaks.",3zxfo3,t1_cyqp12v,GreenAce92,,Reply,2,0,2
cyqqasr,2016-01-08 11:30:53-05:00,GreenAce92,,"Thanks.

&nbsp:

I deserved that.",3zxfo3,t1_cyqp7kk,anon_smithsonian,,Reply,1,0,1
cyqqst7,2016-01-08 11:42:30-05:00,anon_smithsonian,,"The reason it works is due to how reddit renders the Markdown formatting. Basically, two (or more) consecutive CrLf (paragraph returns) is treated as a single `<p />`. So *{Return}{Return}`&nbsp:`{Return}{Return}* works because the Markdown renderer treats `&nbsp:` as if it were a non-whitespace character instead of treating it as whitespace, as it does with a normal space.


",3zxfo3,t1_cyqqasr,GreenAce92,,Reply,1,0,1
cyqr2gf,2016-01-08 11:48:42-05:00,GreenAce92,,"Is this for XSS or did they just choose to do this? Also how do you know? Read it somewhere or are you a developer for Reddit? Don't mean to sound skeptical I'm just curious, I see people pick up tricks here and there like embedding clickable hyperlinks and I have to search how to format it.",3zxfo3,t1_cyqqst7,anon_smithsonian,,Reply,1,0,1
cyqs31y,2016-01-08 12:12:16-05:00,anon_smithsonian,,">Is this for XSS or did they just choose to do this?

It isn't a reddit-specific behavior, it's just the way that [Markdown](http://daringfireball.net/projects/markdown/) formatting works. 

&nbsp:

> Also how do you know? 

Years of redditing, I guess? 

&nbsp:

* Surround text in one set of `*asterisks*` and you get *italic*
* `**double-asterisks**` is **bold**
* `***triple asterisks***` is ***bold-italic***
* Hyperlinks are `[display text](URL)`
* `~~Double tildes~~` is ~~strikethrough~~

&nbsp:

It isn't really that complicated (apart from tables, which I still can never remember without having to double-check)... after a while, you just remember it. Markdown is used for text formatting in other places, as well, such as GitHub. 

* [Reddit's comment formatting guide](https://www.reddit.com/wiki/commenting) 
* [Another more in-depth guide to Markdown on reddit](https://www.reddit.com/r/reddit.com/comments/6ewgt/reddit_markdown_primer_or_how_do_you_do_all_that/c03nik6) 
* [The Markdown website with a lot more information on it.](http://daringfireball.net/projects/markdown/basics)
* [An in-browser Markdown editor that shows you how your text will look with formatting in real-time](http://dillinger.io/)",3zxfo3,t1_cyqr2gf,GreenAce92,,Reply,1,0,1
cyqt8tb,2016-01-08 12:39:00-05:00,GreenAce92,,"Thanks and sorry again for earlier, gotta learn how to read",3zxfo3,t1_cyqs31y,anon_smithsonian,,Reply,1,0,1
cyqqrk9,2016-01-08 11:41:43-05:00,GreenAce92,,Pretty sure I'm a psychopath,3zxfo3,t1_cyqp7kk,anon_smithsonian,,Reply,0,0,0
3zwgdd,2016-01-07 12:26:01-05:00,whereswalden90,Why don't more programming languages use colon for assignment?,"Since the beginning of time (or at least programming languages) [there has been criticism](https://en.wikipedia.org/wiki/Assignment_(computer_science)#Assignment_versus_equality) of the use of `=` as an assignment operator. In attempting to mitigate this issue, many languages choose to use `:=`,  `<-`, and others, but very few have used `:` (the only vaguely popular example I could find was Rebol). Why not? It seems a natural choice and it would solve several language design issues.

For example, Python has chosen to forbid assignment in conditional and loop statements because `if foo = bar` is most commonly a typo for `if foo == bar`. Using a colon for assignment (`if foo: bar`) would be unambiguous and typo-risk-free.

Using a colon would also make things simpler for beginners, since `=` would mean what it does in mathematics.

So why isn't this more common? Does it create other language design problems I haven't thought of? Is it simply the continuing legacy of C?",,,,,Submission,4,0,4
cypnt7u,2016-01-07 14:25:35-05:00,the_omega99,,"1. The `=` vs `==` issue is totally overplayed. It's a bigger issue for beginners than experienced programmers. And people don't usually develop programming languages around beginners. Myself, I don't think I've ever made this mistake.
2. The issue of typos can be almost entirely avoided for languages that have a boolean type and require all conditionals to use it, such as Java, C#, etc. In that case, the only kind of assignment that could result in valid code is assignment that results in a boolean, and that's pretty rare, anyway. Bear in mind that an experienced programmer would never write `foo == true` or such, so pretty much the only case where it applies is as a replacement for xor (`foo == bar`), which is a pretty rare case.
    * It can be entirely eliminated if we stopped making assignment evaluate to the assigned value. Scala did this. Eg, `foo = true` assigns `true` to `foo`, but evaluates to `Unit`, which is Scala's version of `void`. As a result, this error is impossible to make in Scala. This does prevent one from doing stuff like `foo = bar = 123`, though, as well as things like `while(stillReading = getline(file, line))` (which is arguably ugly, though).
3. In languages where assignment is allowed and often done, such as C and C++, there's usually compiler warnings for doing it. The standard way to turn off the warning is a set of parenthesis around the assignment.

Not that it's unusable or a flat out terrible idea or anything. CSS uses `:` for what's essentially the CSS version of assignment. JSON also uses the colon this way, as does the JSON-like object syntax in JS (which lets you do things like `{key: value}`. Mostly it's just not a big enough deal and `=` is the traditional assignment operator that everyone is familiar with and reads well.",3zwgdd,t3_3zwgdd,whereswalden90,,Comment,6,0,6
cypma7w,2016-01-07 13:51:30-05:00,HenryJonesJunior,,"A colon for assignment isn't intuitive from any language standpoint.  In programming languages, colon has a history of use with labels, switch statements, etc.  In natural language while = can indicate that two things are equal/becoming equal, : has no such connotation.  := at least preserves the equals sign, and an arrow has some visual significance, but a colon is confusing.",3zwgdd,t3_3zwgdd,whereswalden90,,Comment,4,0,4
cypu8k2,2016-01-07 16:49:54-05:00,Bottled_Void,,": doesn't scream assignment to me.

Ada uses : as a type delimiter. (And := as assignment)


And you can't just blame C, Fortran used = as assignment too.",3zwgdd,t3_3zwgdd,whereswalden90,,Comment,3,0,3
cypodc0,2016-01-07 14:38:10-05:00,drummyfish,,"I think it's just the legacy. Most compilers will give you a warning if you use assignment in a condition, so I don't think it's such a problem. It may however confuse beginners, which is why I like :=, or <- (which I think is the best way and shows most clearly what it does) better, but then again it's two characters and no one's got time for typing that :)",3zwgdd,t3_3zwgdd,whereswalden90,,Comment,2,0,2
cysz9q0,2016-01-10 11:47:17-05:00,ShittyAlgorithms,,"`:` for assignment makes absolutely no sense. `:` already used for too many things, and using it for assignment is not intuitive to most programmers or mathematicians. However, `:=` for assignment makes a lot of sense as it conforms to the mathematical syntactic conventions.

I expect one reason why `:=` for assignment isn't used in languages like C, Java, or Python could be because assignments are used so frequently (I would argue more frequently than `==`). And really the `=` vs. `==` problem is something people get over quite quickly. It doesn't trip up experienced programmers.",3zwgdd,t3_3zwgdd,whereswalden90,,Comment,1,0,1
3zv1fp,2016-01-07 06:04:27-05:00,justrandomdude,Virtualization and HLT,"Assume we have some virtual machines, running on the same physical computer. If one of the virtual machines executes the Halt instruction, it would only put that VM into the idle state, not the physical machine, right?",,,,,Submission,4,0,4
3ztwkz,2016-01-06 23:48:54-05:00,themoviehero,"Associates, or BS in Computer Science?","The Associates is actually in ""Computer Programming"" but covers quite a few languages it seems. I can post the program if it helps. I have a BA in an unrelated field. I work a full time job, and the drive to the BS is about an hour away, and an hour back, so it would be a lot more effort and an Associates is a great deal cheaper. I have had people tell me that a BS in Comp Sci is not as great as a certificate, which the associates comes with, simply because Comp Sci changes so rapidly that it's hard to guage it and a degree can be outdated by the time the ciriculum is posted. I can post either program here if it helps with the information.

Thanks for your help in advance.",,,,,Submission,6,0,6
cyp2fdn,2016-01-07 01:18:23-05:00,watsreddit,,"Not sure what they were telling you about the nature of a BS in Computer Science. Technologies change for sure, but Computer Science isn'r about learning technology, it's about the theoretical, mathematical underpinnings of computation. It's incredibly useful, and the things you learn will be applicable for a long time. Hell, most algorithms you learn about weren't even created this century.

While getting a job without a BS is doable, it's certainly a hell of a lot easier if you go for the BS. The thing about a bachelor's degree in CS is that it is often a filter used by companies, so those without it are at a large disadvantage. I would suggest finding a way to make a BS work if possible.",3ztwkz,t3_3ztwkz,themoviehero,,Comment,8,0,8
cyp53jz,2016-01-07 03:31:12-05:00,themoviehero,,"I appreciate the advice. The advice given to me was given with the reasoning that languages rise and fall so often, that it's tough to really get a degree that is current, since the classes are outdated by the time it's worked out. But my program looks like it covers Java, C++, -C and several others as well as several pillars of math that seem to have been predominant throughout the years, so I'm not sure how viable or applicable it is in my situation. I have a few days left to drop if I choose to, so I guess I have some thinking to do. It'll just take a while, but i think it might be best.",3ztwkz,t1_cyp2fdn,watsreddit,,Reply,1,0,1
cypby5e,2016-01-07 09:33:27-05:00,watsreddit,,"It's true that languages and technologies come and go, but they really aren't the focus of a degree in CS. They are just tools to help you learn CS principles. As is often quoted of Dijkstra (though perhaps mistakenly): ""Computer Science is no more about computers than astronomy is about telescopes."" 

The issue I think is that too often computer science gets conflated with software engineering. While it's true that CS majors make up a large portion of software engineers, they aren't the same thing. What languages you learn and use are largely irrelevant in the long run, but data structures, algorithms, discrete mathematics, complexity theory, and so on are all absolutely essential for a robust understanding of computing.",3ztwkz,t1_cyp53jz,themoviehero,,Reply,1,0,1
cypv4dj,2016-01-07 17:10:13-05:00,themoviehero,,"I appreciate that. After speaking with the AS head, I decided to tough it out and go for the CS BS, since it will take about the same amount of time. He nearly echoed what you said. He had a Computer Science Degree, and said that ""I would learn about Computer Programming, as well as much more invaluable information to not only make me a good programmer, but give me a fundamental understanding of the ins and outs of computers that would give me the ability to be a great problem solver, as well as a great programmer which will help in the job market, as well as my own growth in the computing field"" (Paraphrasing of course). He really urged that with my credits, since it would take about the same amount of time for an AS and BS since I had transfer credits for all the core general education requirements, I go for the BS, so I settled on that. I appreciate all of your advice. I'm interested in the coding and software aspect  more than anything else, but I don't want to be a general coder who can't solve problems presented, so I think I will be happy with that.",3ztwkz,t1_cypby5e,watsreddit,,Reply,2,0,2
cyp1h5z,2016-01-07 00:41:44-05:00,None,,"What's stopping you getting the AS then transferring? That's exactly what I (and many people I know) are doing.  

If you can get the BS, do it. But why not test the waters?",3ztwkz,t3_3ztwkz,themoviehero,,Comment,3,0,3
cyp2bjq,2016-01-07 01:14:04-05:00,themoviehero,,"That's what I'm thinking of doing. I'm signed up for the BS, but I can still drop with a refund, and do late enrollment in the AS. I also get the AS at a discount (Parent is a faculty member, so I get half price). I may do just that. You think a Computer Programming would transfer well into a Computer Science BS? I'm going to ask tomorrow, but insight would be good. How far are you into your AS? You liking it? ",3ztwkz,t1_cyp1h5z,None,,Reply,2,0,2
cyp2ngu,2016-01-07 01:27:49-05:00,None,,"My situation does not seem like it translates, to be honest.  

I'm going for an AS so I can get some kind of degree. It will be an AS, in CS. The plan is to go to a BS, which may or may not work for you, you need to look into it. I'm halfway through my associates. IF that program will transfer, you likely have nothing to lose. But they don't always transfer to where you want to go. Your uni should have a transfer page, look into it. ",3ztwkz,t1_cyp2bjq,themoviehero,,Reply,2,0,2
cyp510a,2016-01-07 03:27:00-05:00,themoviehero,,"Will do, thanks for the advice!
",3ztwkz,t1_cyp2ngu,None,,Reply,1,0,1
cyp8sjj,2016-01-07 07:26:48-05:00,None,,[deleted],3ztwkz,t3_3ztwkz,themoviehero,,Comment,2,0,2
cypug7v,2016-01-07 16:54:42-05:00,themoviehero,,"I have one in English. I spoke with the program head at the AS college, and he recommended that with my credits I go for for the BS, since they are about the same amount of classes, so it will take me about 2 and a half years, on 2 or 3 classes a semester. So I guess I'm going to tough it out on it, thank you though, I was thinking a BA and an AS would get me far!",3ztwkz,t1_cyp8sjj,None,,Reply,1,0,1
3zq4zx,2016-01-06 10:06:15-05:00,Deleunes254,Looking for a good basic tutorial :),"Hi everyone,

Next year I'm going to university to study computer sciences. I was wondering if there are some experts here that now any good tutorials to prepare myseld for the next schoolyear. :) Maybe not like the basics of specific languages (I have codeacademy for that!), but maybe some tutorials about programming/coding in general.

Thanks in advance!
",,,,,Submission,3,0,3
cyo3oww,2016-01-06 10:45:43-05:00,CoopNine,,"Buy this book.

https://en.wikipedia.org/wiki/Design_Patterns

It's not a tutorial.  It's a very good and relevant book which discusses commonly used patterns in software design, which is integral to creating good software, yet many developers don't have a strong knowledge of these.

It may not be something you can grasp entirely today, but this book provides a strong basis for damn near everything a developer does.",3zq4zx,t3_3zq4zx,Deleunes254,,Comment,4,0,4
cyo4vo9,2016-01-06 11:14:57-05:00,Deleunes254,,Wow this looks really fantastic! I'll gladly read it :) thank you sir!,3zq4zx,t1_cyo3oww,CoopNine,,Reply,1,0,1
cyo74bs,2016-01-06 12:06:07-05:00,king3730,,"A classic, insightful book to read is Mythical Man-Month by Fred Brooks. It is sort of goes in depth on project management with software engineering. 

If anyone has a better description ^feel ^^free ^^^to ^^^^share.


EDIT: word",3zq4zx,t3_3zq4zx,Deleunes254,,Comment,2,0,2
cyo7gsb,2016-01-06 12:13:48-05:00,Deleunes254,,Alright! Thanks I will check that out as well :),3zq4zx,t1_cyo74bs,king3730,,Reply,1,0,1
cyof1go,2016-01-06 15:04:51-05:00,74AC153,,"For me, the two biggest things that would have helped:

1. Find out the predominant programming language that your prospective university uses for teaching and become familiar with it. If it's python, try this: https://docs.python.org/3/tutorial/

2. Try to familiarize yourself with various topics under the 'discrete math' umbrella so you can hit the ground running in your classes. For example, https://www.cs.princeton.edu/courses/archive/fall06/cos341/handouts/mathcs.pdf

I graduated a number of years ago having studied computer science, and in hindsight I wish I'd been told this.

edit: 3. become proficient with a version control system and use it for all your assignments. https://git-scm.com/book/en/v2",3zq4zx,t3_3zq4zx,Deleunes254,,Comment,2,0,2
cyp36ak,2016-01-07 01:50:26-05:00,Deleunes254,,"Amazing! I will definitely check this out. These seem really usefull.

Thank you, kind sir!",3zq4zx,t1_cyof1go,74AC153,,Reply,1,0,1
3zn5ud,2016-01-05 19:35:25-05:00,SolveAllProblemsNow,Linear Algebra: What's the best Web source for learning linear algebra? Particular for CS,"**update: need a source that teaches like this gem --** https://www.reddit.com/r/learnmath/comments/3zml1q/what_does_a_vector_do/cynfknm

---

also what's the most helpful thing to know that any 'average kid' can understand to help learn linear algebra easier?",,,,,Submission,19,0,19
cynkp57,2016-01-05 21:29:25-05:00,IAmNotMyName,,"MIT open courseware is pretty good.

http://ocw.mit.edu/courses/mathematics/18-06-linear-algebra-spring-2010/",3zn5ud,t3_3zn5ud,SolveAllProblemsNow,,Comment,3,0,3
cyo8yzq,2016-01-06 12:47:42-05:00,MirrorLake,,"There is an OCW ""scholar"" version of this same course--seems to be designed to make it easier to follow the syllabus.  Same lecture videos. 

http://ocw.mit.edu/courses/mathematics/18-06sc-linear-algebra-fall-2011/index.htm

",3zn5ud,t1_cynkp57,IAmNotMyName,,Reply,1,0,1
cynq4s8,2016-01-06 00:08:29-05:00,sullage,,"I'm a software engineer for a big insurance company.
I took 4 semesters of calculus, linear algebra and diff eq in college. I don't think I've ever used it. ",3zn5ud,t3_3zn5ud,SolveAllProblemsNow,,Comment,2,0,2
cynuxdr,2016-01-06 03:52:43-05:00,the_omega99,,"Linear algebra has to be one of the most useful fields of math for a programmer.

Things that use it include:

1. Literally any low-level graphics programming. The whole idea of modern computer graphics is built on linear algebra.
2. Machine learning. Many common algorithms use LA heavily or have optimizations that would use LA.
3. Scientific computing tends to favour programming languages (MATLAB/Octave) and libraries (NumPy, Breeze, etc) that are very LA driven.
4. Image processing is basically applied LA. Things like filtering, sheering, edge detection, etc.
5. Various random algorithms use LA. Eg, balancing chemistry equations, calculating network flow, etc.

Although these are more specific applications, so many programmers might not need this. It totally depends on what you're doing.

Calculus, on the other hand, is far less useful. It's most useful for theoretical CS (eg, it came up a lot on my computer vision and machine learning classes). A programmer would likely not need calculus directly, though.

Stats is probably the second most useful mathematics for programming. Or even the most useful if we consider the use of averages. Probability theory is a field where we'd have to use calculus. In programming, it would come up in some complex simulation systems, and obviously for statistical systems (but that's basically cheating for the topic of using math in programming).",3zn5ud,t1_cynq4s8,sullage,,Reply,10,0,10
cyo78hp,2016-01-06 12:08:41-05:00,SolveAllProblemsNow,,"but maybe you have, and you aren't aware of that. quora has highly educated and very elite ppl, and LA seems to be used in some things --    
http://www.quora.com/What-is-the-point-of-linear-algebra --   
https://www.quora.com/What-are-the-core-Linear-Algebra-topics-Id-need-to-complete-Stanfords-online-AI-course  
http://www.quora.com/Why-is-Linear-Algebra-a-prerequisite-behind-modern-scientific-computational-research  
http://www.quora.com/Computer-Science/How-important-is-linear-algebra-in-computer-science  
http://www.quora.com/Linear-Algebra/What-are-the-real-life-applications-of-linear-algebra-that-is-where-we-use-things-like-fields-groups-rings-etc-mention-any-problem-which-we-encounter-in-daily-life-that-can-be-solved-using-topics-of-linear-aglebra  ",3zn5ud,t1_cynq4s8,sullage,,Reply,4,0,4
cyob4ab,2016-01-06 13:36:04-05:00,brandonism,,"I've heard good things about this class: 
https://www.coursera.org/course/matrix

But I haven't done it personally.",3zn5ud,t3_3zn5ud,SolveAllProblemsNow,,Comment,1,0,1
czrl0ku,2016-02-07 19:01:00-05:00,o_edo,,Try out the following web site. There are both course and exercises and nice plots helping in representing things. http://studybyyourself.com/seminar/linear-algebra/course/?lang=en,3zn5ud,t3_3zn5ud,SolveAllProblemsNow,,Comment,1,0,1
3zn4ew,2016-01-05 19:26:25-05:00,PKAhistorian123,Computer and Information Systems Security VS Computer Science,would it be a good idea to get a bachelors in BOTH? what are the pros and cons of both majors?,,,,,Submission,1,0,1
cyosb6j,2016-01-06 20:15:43-05:00,Furgera,,"Well both of these majors are quite interesting in their own terms. I think /u/PastyPilgrim covered the generality of both.

I'd none the less would like to add in a few things of my own that will no doubt be of great help for you in choosing ! Eat a dick kid and do the giveaway is what i'd like to suggest.",3zn4ew,t3_3zn4ew,PKAhistorian123,,Comment,3,0,3
3zluda,2016-01-05 14:53:38-05:00,Made503,I need advice on where to start with making an app.,Hey there! I have a few questions and hopefully someone can help me out. I have a pretty good idea for a new business that would be run through an app. It's an idea that I've never seen out there so I obviously want to get started on it sooner rather then later. Only problem is I don't know how to write code. I know that there is websites out there where you can hire freelancers to make apps for you. Any thoughts on what would be the best route to take with this? Also what's an average starting price someone charges when making an app? I have no idea where to start on this so any advice or tips would be greatly appreciated. Thank you!,,,,,Submission,0,0,0
cyn8jmr,2016-01-05 16:23:56-05:00,Aquifel,,"Best start, learn yourself, start here: https://www.codecademy.com/learn
Once you've taken a few courses, you should have the information needed to further develop knowledge more tailored to what you want.

You're developing an app, android apps are usually written in Java, codecademy has a tutorial for that (I would start out with something easier and work your way up to Java).  iPhone app development may be more involved, it uses a language called 'Swift' although, i believe that you can use C (do not hold me to that, i am not sure).

If you did want to hire someone, there are many places but, this is an option: https://www.upwork.com/cat/mobile-developers/
Whatever concept you may have, it will likely take considerably more hours than you think it will. If you have no coding background, budget for 5-10x the amount of time that you think it will take, remember things that seem incredibly simple to you may actually be monumentally difficult from a programming perspective.  Maintain good relations with whoever you hire, you will need modifications/fixes and, noone is likely to understand their code better than them (which will save you man hours).

I know you think that your app is unique but, be 100% sure before you put any money down.",3zluda,t3_3zluda,Made503,,Comment,3,0,3
3zlkvy,2016-01-05 13:59:34-05:00,enigma_x,Theory of computation - grad level,"I am a master's student in computer science and spring is my graduating semester. As a part of graduation requirements I've to take a grad level course in theory of computation. I have taken the undergraduate equivalent at the university where I did my undergrad, just that it's been a while and I didn't really breeze through my class or anything back then. I need a B minimum on the course to graduate. I'm looking for ways to be prepared for the course (a refresher on the foundations may be) plus every which way I can do well in the class. Looking forward to suggestions from people who've taken the course and/or are comfortable with turing machines and related topics. I'm ready to put in the hours just that I want to make sure I'm studying it right. 

 Thanks in advance. ",,,,,Submission,6,0,6
cynfxm8,2016-01-05 19:21:44-05:00,Al__Gorithm,,"*'Discrete Mathematics and its Applications'* is a book I have referenced a lot in my module covering the theory of computation, it covers a very wide specification and is akin to a textbook, I got an old edition dirt cheap so take a look!",3zlkvy,t3_3zlkvy,enigma_x,,Comment,3,0,3
cyntytt,2016-01-06 02:55:35-05:00,enigma_x,,Thanks :),3zlkvy,t1_cynfxm8,Al__Gorithm,,Reply,1,0,1
cyn8mcy,2016-01-05 16:25:36-05:00,_--__,,What topics does the course cover?,3zlkvy,t3_3zlkvy,enigma_x,,Comment,2,0,2
cynu2g9,2016-01-06 03:01:19-05:00,enigma_x,,"Turing completeness, reduceability, P/NP. It assumes a fairly broad undergraduate background with all the usual finite automata, regular languages and the like. ",3zlkvy,t1_cyn8mcy,_--__,,Reply,1,0,1
cyodpzd,2016-01-06 14:35:44-05:00,mpdehnel,,Sipser's [Introduction to the Theory of Computation](http://www.cin.ufpe.br/~jjss/Introcuction%20to%20Theory%20of%20computation%20by%20Micheal%20Sipser%20Ist%20Ed..pdf) is excellent. [Amazon link](http://www.amazon.com/Introduction-Theory-Computation-Michael-Sipser/dp/053494728X/ref=sr_1_4?ie=UTF8&qid=1452108889&sr=8-4&keywords=sipser). Doesn't entirely matter which edition you get.,3zlkvy,t3_3zlkvy,enigma_x,,Comment,2,0,2
cyofgx5,2016-01-06 15:14:19-05:00,enigma_x,,Thanks a lot. This is the text book for the class. I'm already working through it so I can be familiar with the material. ,3zlkvy,t1_cyodpzd,mpdehnel,,Reply,1,0,1
cz4ywfj,2016-01-20 02:12:25-05:00,bluelite,,"This Udacity course could help with some of the topics: https://www.udacity.com/courses/cs313

It covers NP-completeness, reduction of NP-complete problems, the Halting Problem, and approximation.",3zlkvy,t3_3zlkvy,enigma_x,,Comment,2,0,2
cz6nhy4,2016-01-21 11:50:13-05:00,enigma_x,,Thank you :),3zlkvy,t1_cz4ywfj,bluelite,,Reply,1,0,1
cz6kspu,2016-01-21 10:45:13-05:00,romcabrera,,"This set of lectures: https://www.youtube.com/watch?v=TOsMcgIK95k&list=PLbtzT1TYeoMjNOGEiaRmm_vMIwUAidnQz

(choose the topics you need to review)

Also this free course from Udacity: https://www.udacity.com/course/computability-complexity-algorithms--ud061",3zlkvy,t3_3zlkvy,enigma_x,,Comment,2,0,2
cz6nhhp,2016-01-21 11:49:56-05:00,enigma_x,,Thanks a lot!,3zlkvy,t1_cz6kspu,romcabrera,,Reply,1,0,1
cz7b16m,2016-01-21 21:23:53-05:00,romcabrera,,"Good luck man, right now I'm taking the graduate course on complexity, computability, and algorithms in gatech's omscs",3zlkvy,t1_cz6nhhp,enigma_x,,Reply,1,0,1
cz7bpei,2016-01-21 21:42:34-05:00,enigma_x,,Good luck to you too! I have only this course between me and the degree and I hope to nail it! I hope you graduate with the highest honors as well. ,3zlkvy,t1_cz7b16m,romcabrera,,Reply,1,0,1
3zl5en,2016-01-05 12:28:04-05:00,UsuallyQuiteQuiet,Regarding compatibility across CPUs,"This always bugs me and I can't really find a cohesive explanation online. 

Can someone explain microcode, machine code, assembly code and a single instruction within the context of each other? I realise this looks like a massive ask.

How I understood it is that Assembly is simply a human readable way of writing machine code. A line of this corresponds to an instruction which is broken down into microcode which is specific to whatever CPU you're running your program on.

i.e. the same machine code could run on different CPUs but is translated to microcode specific for a particular CPU.

So something like a sum instruction from registers A and B corresponds to machine code which is then broken down into individual steps for a CPU,.",,,,,Submission,3,0,3
cyn352r,2016-01-05 14:20:15-05:00,PastyPilgrim,,"Humans write assembly, which is translated to machine code, which then ***may*** be used as a microcode. Microcode is something that only some processors use to set control points, which allow a computer to actually perform an operation. A single instruction is just an operation that can be performed with only one instruction.

For example:

Assembly: `add $t0, $t1, $t2` (human written instruction to set register t0 = t1 + t2)

Machine code: 0110010111101011010101100101110 (computers understand 0s and 1s, not 'add's '$t0's and so on, so a program is used to translate your instructions into the format that a computer can read). Machine code is usually similar, if not identical to assembly, just with everything you've written translated into binary. The assembler will do other things, like optimizing the order of your instructions, expanding pseudoinstructions (instructions that you write as a single instruction for convenience, but actually need more than one instruction to perform), and so on, but assembly languages usually mirror the machine language pretty closely.

Both of those are single instructions.

This:

`add $t0, $t1, $t2`

`addi $t0, $t0, 10`

Is not a single instruction, because you're supplying two instructions.

Microcode is an entirely different concept that requires some understanding of how hardware works to explain.",3zl5en,t3_3zl5en,UsuallyQuiteQuiet,,Comment,2,0,2
cynhu3h,2016-01-05 20:12:00-05:00,UsuallyQuiteQuiet,,"I have some understanding of the underlying hardware so if you do have an explanation for how microcode relates to assembly, I'm all ears. Thanks for the comprehensive answer. I appreciate it.",3zl5en,t1_cyn352r,PastyPilgrim,,Reply,1,0,1
cynir9j,2016-01-05 20:36:40-05:00,PastyPilgrim,,"Well, inside a processor you have the data paths and the control unit. The data paths are the roads where all the traffic flows. The control unit is the component that decides what to activate and when to activate it to allow for something operation to occur.

A control unit is either driven by a hardware or a software process. When it's hardware, you pass it an instruction like 01010100010011... and it uses a chain of hardware (gates, multiplexers, decoders) to determine what control points to activate. This is super fast, simple, and efficient, but it's not all that powerful because you're limited by the space need for all that decoding hardware and the fact that it's all fixed from the moment the processor is manufactured. Simpler CPUs might use this because it's all they need.

When you use a software process to handle instructions, then you're using microcode. The actual implementation will depend on the processor and its goals, but, basically, you have a unit that decodes an instruction into a set of control points that go into a register. These control points (microcode) are just data, so it's easy to change/manipulate them.

A processor that uses microcode has a lot of flexibility, because you can, in theory, make your own instructions as a combination of control points (but you probably wouldn't, because that would require a perfect understanding of the hardware). However, it does allow the processor to change how it does something (like x86, which needs to keep supporting the same instructions, but might change how those instructions are executed). And it allows you to do some really weird things, such as changing what instructions do. For example, the Vax computer, which is famous for having native instructions to do any and everything (rather than most modern computers that do everything as a combination of simple instructions (CISC v RISC)), had instructions that allowed you to change the mode that it was in to natively emulate the instructions of another kind of CPU. It did this by loading a different piece of decoding software (most likely just a table) into the unit that decodes instructions into control points.

Anyway, that's a brief explanation of a complex topic. I have a couple of diagrams that show what these kinds of machines might look like, if you want to see them.

[Hardware Unit](http://i.imgur.com/drg0v1U.jpg)

[Microprogrammed Unit](http://i.imgur.com/G1eEHP2.jpg)",3zl5en,t1_cynhu3h,UsuallyQuiteQuiet,,Reply,2,0,2
cynjcj4,2016-01-05 20:52:50-05:00,UsuallyQuiteQuiet,,"Thanks again. I'll probably start looking into particular processors that used microcode then. It's hard for me to visualise how it would function as a piece of software as opposed to hardware gates and selectors etc.

When I'd first heard of microcode I thought it would be the signals required to control the selectors to control where the data went.",3zl5en,t1_cynir9j,PastyPilgrim,,Reply,1,0,1
cynbkmv,2016-01-05 17:33:36-05:00,Steve132,,"C code (or any other high level language) is a platform-independent way to write code.

    int a=b*c+c:

The compiler uses its knowledge of a particular platform specification, called the ISA (such as x86 or x64) to translate the statement into assembly language for the target platform.  For example, the multiply and add instruction for a mips platform or mips5 isa
    
     mad r3 r1 r2 r2  %Computes r3=r1*r2+r2

This gets translated by a program called an assembler into machine code for that ISA.  Machine code is the same thing as the assembly but in binary.  For example, the highest 8 bits might give the code for which instruction it is, so ""mad"" is the 121'st instruction the machine understands alphabetically, so maybe the first 8 bits are 01111001.  Then each register is specified as 6 bits (because maybe there are 64 possible registers)...so r3,r1,r2,r2 as 4 arguments would be 000011,000001,000010,000010, respectively...so in machine code it looks like  01111001000011000001000010000010 for that instruction.

Now suppose I want to write my own processor that is compatible with the above hypothetical instruction set (so that it can run my users programs) but to save money I don't actually have a ""mad"" instruction.  Suppose my machine is *internally* designed as a 2-operand format only (one source and one destination).   In my *internal* machine code, I have to do the steps in this order:

    mov r3 r1  %
    mul r3 r2
    add r3 r2
    
Now it's 3 instructions in my modified simpler machine code assembly.  

That means that in my my modified simpler machine code (two args, maybe only 6 bits for the instruction) we have maybe mov=33,mul=35,add=2...so the first instruction breaks down as

    mov r3 r1  %100001,000011,000001
    mul r3 r2   %100011,000011,000010
    add r3 r2   %000010,000011,000010

So, the final microcode would be  

    100001000011000001 100011000011000010 000010000011000010

So, basically, we have this:
Source Code:

        int a=b*c+c:

ISA assembly language:

	mad r3 r1 r2 r2 

ISA machine code:

	01111001|000011|000001|00001|0000010

Internal Microcode Assmbley (never generated)

    mov r3 r1 
    mul r3 r2
    add r3 r2

Microcode executed on the processor:

	100001|000011|000001 
	100011|000011|000010 
	000010|000011|000010


",3zl5en,t3_3zl5en,UsuallyQuiteQuiet,,Comment,2,0,2
cynhyhs,2016-01-05 20:15:17-05:00,UsuallyQuiteQuiet,,Saved. Thanks a lot this helps clear it up.,3zl5en,t1_cynbkmv,Steve132,,Reply,1,0,1
3zjchs,2016-01-05 03:53:10-05:00,OVDU,How memory segmentation works?,Can someone please explain it to me in a really simple way.,,,,,Submission,7,0,7
cympv3l,2016-01-05 08:15:36-05:00,ldpreload,,"The basic idea of segmentation is very simple: you tell the CPU, every time you access a code pointer, add this number: every time you access a data pointer, add this other number: etc. So if you have an instruction like `MOV [SI], AX`, which usually means ""Move the data at address SI to the AX register,"" the CPU will actually read it as, ""Add SI and the value you gave me earlier, and move the data at _that_ address to the AX register."" The longer way of writing this would be `MOV [DS:SI], AX`, where DS is the ""data segment"" base register.

So then there's the question of _why_ you want the CPU to do that. On the x86, the boring explanation is that memory addresses can only be 16 bits wide, which would only let you access 2^16 bytes = 32 kilobytes of memory. However, the segment bases get an extra hex digit `0` at the end when used, so even though those are also 16 bits, you effectively have 20 bits to use, and 2^20 bytes = 1 megabyte. If your DS register is set to `F000`, and you access address `FFFF`, you're actually accessing `F0000` + `FFFF` = `FFFFF`.

This allowed the x86 to use 1 megabyte of RAM without needing to change the pointer size (and affect all existing code), add more wires everywhere in the processor, etc. To get more than 1 megabyte you had to use 32-bit mode, though.

The slightly better explanation is that, if you have a program that only needs 32 kilobytes of memory (or really, 32 kilobytes of code, 32 kilobytes of data, etc.), the OS can put it anywhere where it has a spare 32 kilobytes, and it can set your segment registers appropriately in advance. Then, as long as you don't _change_ your segment registers, you can run the exact same code and not have to worry about where exactly the OS put you. This is useful for multitasking OSes.",3zjchs,t3_3zjchs,OVDU,,Comment,8,0,8
3zgoia,2016-01-04 16:09:26-05:00,gentlechentle,What do I need to LEARN to become a successful computer programmer?,"I've tried searching this question on Google, but all I get in the search results is ""How to become a computer programmer,"" or ""What you should study in school to be a programmer,"" and stuff similar to that.

I have already gone through school and I got my bachelor's in math. My question isn't how I should start, because I already know some computer languages, like HTML, CSS, Java, Python, etc. I have no problem learning things on my own, because I learn very easily.

My problem is that I have no idea what I need to learn. I've got some languages down, but that's pretty much it. I don't know how to make a program, or do anything practical; just how to code.

My question is what should I teach myself before I begin applying to jobs? I know every job requires different things, but what are some absolute must haves in every programming/software developing job?

If anybody happens to have an answer, I would tremendously appreciate it!",,,,,Submission,19,0,19
cylyugo,2016-01-04 16:23:41-05:00,lneutral,,"You *cannot* learn to be a successful programmer by studying, only by doing. 

To be a good programmer, you need to know a bit about algorithms, data structures, software engineering best practices, and some math. But personal experience is crucial. Plenty of people watching the super bowl understand the aspects of the game: the rules, the strategy, analysis of each player's performance, and even the politics of the sport - how many of them could be described as football players? 

The trick is - what do you want to do enough to practice it? Do you want to be a mobile apps developer? Write a few mobile apps - Android has a free SDK with tutorials and documentation. Games? Unity or Unreal Engine are both a good place to start. Don't know?  Try mocking up some small demos in Python with PyGame or what have you. Your imagination, curiosity, and drive will have to sustain you through a process of writing, confusion, and enlightenment that you can't get by trying to study programming the same way you might study biology or history.

Which kind of companies do you want to work for? Depending on what your goals are, there may be tutorials out there that could get you started. ",3zgoia,t3_3zgoia,gentlechentle,,Comment,22,0,22
cymbcnv,2016-01-04 21:44:27-05:00,charlesbukowksi,,"Startup founder, where's the tutorial for that?",3zgoia,t1_cylyugo,lneutral,,Reply,2,0,2
cymk6qd,2016-01-05 02:35:10-05:00,Zakafein,,"Read up, autobiographies of successful start up finders? Coming up with an original idea and being able to execute it? ",3zgoia,t1_cymbcnv,charlesbukowksi,,Reply,4,0,4
cymuux7,2016-01-05 11:00:31-05:00,the_omega99,,"* Most fundamentally, you need practice. ""Book smarts"" aren't worth much in the programming industry. You need experience and preferably something to show you have experience. This typically comes in the form of projects. You make something large and useful. By large, I mean something that's more than, say, a few weeks of work. It doesn't have to be useful or innovative. Just something non-trivial that applies what you've learned.

    In the creation of a project, you'll naturally come across topics that you need to learn. Eg, if you decide to implement a card game, you might decide to make it multiplayer over the internet, for which you'll realize the need for networking knowledge. You might realize the need to create a GUI, to deal with threading, etc.

* Algorithms and data structures is a critical aspect of programming that you'd want to learn. It's fundamental and usually a core part of programming interviews.

* You should recognize that knowing languages is different from knowing how to program, and that learning more languages is not going to make you a better programmer. Knowing many languages is something that is useful, since certain things are best done in certain languages, but if you find yourself with a desire to learn a new language in the near future, repel it. It's a common beginner mistake to keep jumping from language to language, thinking that that's how you become a better programmer. It's just going to cause the opposite.

    You want to focus on learning specific techniques (which are mostly a language agnostic thing). Eg, game dev, network programming, web development, creation of GUIs, etc. These aren't aspects of programming languages, but things you'd do with that language once you know it.

* It's not something to worry about early on, but at some point of time before considering yourself employable, you are going to want to read a good book on software engineering. SE is all about the creation of software: the process of software development, how to develop changes for large projects (ones so large no one knows how everything works), how to refactor code, software testing, debugging techniques, design patterns, build systems, dependency management, etc. It's a massively broad field, but you should at least know most of the basics.

* Debugging is something you need serious practice with. It's a major, major part of programming. It can range from 20 to 80 percent of your time, so naturally you need to be good at it. It's something you can't truly learn from books. You can learn some general techniques, but mostly it's acquired through experience (you simply naturally pick up patterns over time).

* On a minor note, it's important to learn some of the best practices that apply to your language and programming in general. For example, a Java developer would need to be familiar with the fact that strings are immutable, and thus if you need to concatenate many strings, you'd use a `StringBuilder`. And for programming in general, you'd want to know how to format your code well, how to name things appropriately, how to comment your code (not too much, not too little: especially explain the why, not the how), appropriate sizes for methods, etc.

* It's absolutely vital that you know at least one type of version control system before you consider applying for a job. It's pretty much mandatory knowledge for all jobs and something every programmer needs to know, anyway. Git is the most popular and thus most likely to be useful, but really once you learn one, it's not a major step to learn another (sure, there's some major differences, but the core ideas of why you'd use a VCS and what functionality to expect from one are the most important things to know).

* You don't want to start with this, but you definitely want to work with other people's code before applying for jobs. Most likely this would be done via finding an open source project and contributing to it. Really any open source project would work, preferably a large one, though. It's fundamental that you can work with very large projects that you know nothing about, because that's what most commercial environments are going to be like. It's totally unlike working with your own code from the ground up. Everything is alien and the majority of your time is probably going to be spent simply *finding* where to make the change. The importance of practicing this cannot be understated. And don't be discouraged if minor changes take a few hours. This is normal and it does get easier as you get more familiar with the codebase.

* You should try and identify a more specific field that you're interested in. You might think that you'd want to be a jack of all trades, but the reality is that this isn't very feasible, at least not at first. Specializing in an area for a while allows you to acquire useful knowledge faster and become employable in a reasonable timespan. These areas are still actually quite broad, so there's a lot of room to learn and expand. I'm talking things like web development, game dev, mobile development, etc.

    This doesn't have to be some final choice. Just an area to focus on for now. Even more, you'd probably want to narrow things down a bit to a subset of the area that interests you with regards to platform, language, frameworks, and general architecture. Eg, if you want to do web dev, you might decide to do it with Python and Django. If you do mobile, you might start with Android and Java. This, too, is simply to avoid trying to tackle too much at once and avoid switching between things before you have an in-depth, useful knowledge of how to apply that technology (you don't want to be relearning new web frameworks every month -- you want to be picking one and using it to do new things).",3zgoia,t3_3zgoia,gentlechentle,,Comment,3,0,3
cymvaa0,2016-01-05 11:11:46-05:00,gentlechentle,,"Thank you, this helps a lot. I'll definitely do some research on what I might be interested in. I'll look at GitHub and see what I can find that interests me and that I might be able to help with.",3zgoia,t1_cymuux7,the_omega99,,Reply,1,0,1
cym2aax,2016-01-04 17:44:48-05:00,avent606,,"There are many different types of programmers. I work at a midsize company (about 6000 people globally) with a IT department  of about 400. We look for different skills and attributes depending on the role, at a very high level categorize into:
 
Back office Development - typically larger projects for back office applications  (ERP, compliance, systems of record stuff).  Good to understand the SDLC, languages like Java J2EE, ASP.net and some older legacy things like Lotus notes and VB. Knowledge of some of the big ERP apps ( SAP, PeopleSoft) is helpful. 

Full Stack Developers. These people develop/steward our internal and external portals and web based environments.  These are J2EE (WebSphere)and integrate with various content management applications. Good skills: Node JS, LAMP, Portal technologies ( WebShere, WebLogic..).   

Mode 2 Developers: If you google ""Bimodal IT"", its opening up new opportunities for developers  that work with business units outside of traditional IT (mode 2).  These are the more creative positions. We look for people comfortable across many scripting languages, JQuery, Bootstrap... can bend the DOM to their will and know basic API and DB concepts. Mode 2 developers typically work on short lifecycle, ""fast fail"" projects.

This list is programming related, Im leaving out a big list: DBA's, integration specialist, environment provisioning, security...data scientists...  

Keep in mind this is a single companies view. 

On your ""what to learn"" question, I would get an understanding of the tremendous breadth of positions that that need developers. Look at what the market is hiring. Fill your gaps based on what they are asking for and get a high level understanding of what they do.  Its easier to get the job if you have a basic understanding of the companies  business as well as the skills they need. 

Outside of programming, its helpful to understand a bit about different types of application architectures, information architecture and  familiarize your self with a few SDLC approaches (RAD, Waterfall, Agile...).  






",3zgoia,t3_3zgoia,gentlechentle,,Comment,3,0,3
cym41nj,2016-01-04 18:28:56-05:00,gentlechentle,,"Alright, thanks! This list will really help in what I should be trying to learn.",3zgoia,t1_cym2aax,avent606,,Reply,1,0,1
cylz13v,2016-01-04 16:27:50-05:00,mcowger,,">I don't know how to make a program, or do anything practical: just how to code.

This statement doesn't make any sense.  If you can write code, you can make a program.

Perhaps what you are asking is closer to ""How to I learn to break down a problem, apply some basic concepts and solve the problem with out the steps of that solving being handed to me""?

If so, your first step is practice.  Find a problem worth solving (common one here is scraping a website for prices or something) and try to break it down and solve it yourself...without a tutorial.

If you want a better understanding of the *algorithms* in use (like why do different kids of sorting do better), there are tons of online courses on the science aspects.

>what are some absolute must haves in every programming/software developing job?


* Do you have a body of work you can share?
* Is it coded well, to the standard in use by the community?
* Is it efficient?  Are you proud of it?  Is it *documented*?

",3zgoia,t3_3zgoia,gentlechentle,,Comment,5,0,5
cym3uj5,2016-01-04 18:23:57-05:00,gentlechentle,,"Sorry, I should have been more clear. I mean I don't know where I would go about writing code to make a program. I can download a compiler online, but I'm not sure how I would release that to the public for other people to see.",3zgoia,t1_cylz13v,mcowger,,Reply,2,0,2
cym4lk7,2016-01-04 18:43:39-05:00,mcowger,,"When you write code today, how are you doing it?  What programs and systems are you using after you download the compiler?",3zgoia,t1_cym3uj5,gentlechentle,,Reply,1,0,1
cymct9v,2016-01-04 22:22:05-05:00,gentlechentle,,The websites I use to learn to code have space to practice coding.,3zgoia,t1_cym4lk7,mcowger,,Reply,4,0,4
cymsev7,2016-01-05 09:50:34-05:00,nsaisspying,,the platform I found easiest to begin learning how to make applications for was the browser. try and set up a local server on your computer (check out wamp if you wanna use php)  and come up with a simple idea for an application that interests you. and try and take it from there.  start by building a simple landing page using no frameworks. use plain old html-css-javascript for front end and php for backend. the  try and connect it to a database to store some things. that will give you an over view of the things you can do and how to go about doing things. since you can just pull up the end result on a browser you will be able to see the results of your work immediately you will find it very motivating. ,3zgoia,t1_cymct9v,gentlechentle,,Reply,3,0,3
cympxjr,2016-01-05 08:18:34-05:00,thechao,,"Install Python. Clone a medium sized Git repo from GitHub. Write a ""Git repo analyzer"" that finds all the files in the repo and tries to guess what they do.

Bonus 1: track your work using a Git repo.
Bonus 2: parse the Git files, and display information about their contents.
Bonus 3: port your script to C.

By the time you're done with (3) you should be solidly in the ""beginner"" and out of ""raw newbie"".

Now, for intermediate stuff. Set up a Jenkins slave/master that is edge triggered on commits to your repo. Run edge triggered regressions on your project against a number of other projects. Also run a ""nightly"" on a very large project.",3zgoia,t1_cymct9v,gentlechentle,,Reply,2,0,2
cym980f,2016-01-04 20:47:50-05:00,IAmNotMyName,,GitHub,3zgoia,t1_cym3uj5,gentlechentle,,Reply,1,0,1
cyn2kz2,2016-01-05 14:07:11-05:00,thecodingturtle,,"I am in a similar position. I am a senior in high school and have done an internship, so I now have  experience writing programs in C# with Visual Studio, but my goal is to write Android apps, which I do not know how to do. 

I'm taking a Java class at school, but I also recently started a course that teaches Android Marshmallow development. It has been extremely helpful and would be great for anyone looking to get into making Android apps. 

If I were you I would go to a website such as Udemy that has courses that teach this sort of thing. Also, check out the Techno Buffalo Deals store. They have courses you can buy for cheap that will help you use your programming knowledge to make actual programs. 

Hopefully this helps!
",3zgoia,t3_3zgoia,gentlechentle,,Comment,2,0,2
3zfqn0,2016-01-04 12:40:15-05:00,timozattol,"What is the best ""hacking"" thing I can do inside a <style> tag?","Here's the story: on my school's website, we can modify some of the fields of our public user profile. On those fields, special characters are not escaped, so we can basically enter any html we want and ""customise"" our profile. They blacklisted all <script> tags though, to avoid XSS injection.

I had a bit of fun inserting a <style> tag and creating an animation on my name, which now moves in the air. I can of course change the background of the page, etc. But I'm wondering, what is the most amazing thing I can do with that? Would it be possible to completely change the webpage's content using CSS? Would it be possible to trigger a ping to a custom server to store ip adresses of people who visit my profile? (is that even legal?) Other crazy things?",,,,,Submission,0,0,0
cylrp8l,2016-01-04 13:37:53-05:00,vikksorg,,"As a forewarning, please don't be *that guy* and do anything malicious.

Declaring a background-image or @font-face with a URL to a file on your server, and then logging requests for said file, would allow you to grab IP addresses of those who visit your profile.  Interesting, but not entirely useful unless you can somehow come up with a mapping of users to IP addresses.

Taking it a step further, imagine the following element:

    <div class=""user-greeting"">Hello, <span class=""user-name"">John Smith</span>!</div>

You could apply [this method](http://mksben.l0.cm/2015/10/css-based-attack-abusing-unicode-range.html) to create log entries on your web server for each unique letter in the .user-name element.  One could then reconstruct the actual names of each person looking at your page, with the caveat that it will only do so for each *unique* letter (so if the user's last name is Green, you would log Gren.)",3zfqn0,t3_3zfqn0,timozattol,,Comment,10,0,10
cym3rno,2016-01-04 18:21:57-05:00,timozattol,,"Woa that's actually a nice hack! :D Quite advanced.

Are we sure that the browser is going to fetch the letters in order? Also can't packets change order in the network, depending on their trajectory?",3zfqn0,t1_cylrp8l,vikksorg,,Reply,1,0,1
cylpz8n,2016-01-04 12:56:00-05:00,big-blue,,"You could change content, as described [here](http://stackoverflow.com/questions/7896402/how-can-i-replace-text-with-css).

Technically, as [HTML + CSS3 is Turing complete](http://stackoverflow.com/questions/2497146/is-css-turing-complete), you could do anything that's possible with a computer using just HTML and CSS3, but it would be incredibly complex.

Abusing the mechanic of the website you mentioned could lead to repercussions, though, keep that in mind.",3zfqn0,t3_3zfqn0,timozattol,,Comment,3,0,3
cylqi8s,2016-01-04 13:08:56-05:00,timozattol,,"Thanks! That's interesting

I don't intend to go further deep in the dark side tough, that's just out of curiosity.",3zfqn0,t1_cylpz8n,big-blue,,Reply,2,0,2
cze4lmr,2016-01-27 13:40:10-05:00,X7123M3-256,,"They only blacklist <script> tags? You can add event handlers as HTML attributes. You could therefore do something like <div onload=""doBadStuff()"">Hello!</div>. If they are only looking for the <script> tag and don't escape anything else, this might pass.",3zfqn0,t3_3zfqn0,timozattol,,Comment,1,0,1
3zc06k,2016-01-03 18:09:39-05:00,95-,How do I go about this question about Direct-Mapped Cache?,"I'm currently in my first semester for ComSci and as part of a revision I've got this question. I'm really confused on how to answer it. Could someone please point me in the right direct. 

[Question](http://i.imgur.com/y537y0O.png)

Find i and t.

I'm sorry if this is actually really simple. 

Update:

I can't believe how simple it is. I was trying to do various calculations on the address line, instead of just sticking with the obvious. Thank anyway people haha

It's been a long night haha",,,,,Submission,3,0,3
cyl30nm,2016-01-03 21:37:56-05:00,craiig,,"Based on the diagram at the bottom, which bits in the address correspond to the tag and line?",3zc06k,t3_3zc06k,95-,,Comment,1,0,1
cylizvb,2016-01-04 09:38:48-05:00,95-,,"I managed to find the answers, i = 591|24F and t = 1194|4AA, however I'm still confused as to how these re found. 

Update:
Ignore this, I get what you said now. I can't believe it was that simple haha. I was trying to do various calculations on the address line, instead of just sticking with the obvious ",3zc06k,t1_cyl30nm,craiig,,Reply,2,0,2
3z8z66,2016-01-03 02:22:06-05:00,JPSYCHC,"Let's play a game: by your best estimate, how many how bytes of space are there in the world?",how did you arrive at your number?,,,,,Submission,0,0,0
cyk8zzh,2016-01-03 05:01:30-05:00,LET-7,,"Sloppy question, sloppy answer: 

On the same order as the number of moles of electrons.",3z8z66,t3_3z8z66,JPSYCHC,,Comment,2,0,2
3z88ey,2016-01-02 22:35:31-05:00,cardboardxbox,How to make data persist on a NodeJS server + login capabilities?,"So, I'm looking to start a small open source project that supports login capabilities. Each user will need to store data and that data will need to persist.

Heroku refreshes daily with your code, so if your POST/GET requests interacts with a sqlite server, only the sqlite commands within the code stay. For example, if a user inputs ""hi"" to a text box and my post request adds that to the sqlite server, it will be gone in a day when it refreshes. How can that data persist?


Additionally, how can I keep passwords secure? I haven't learned much on security, so a starter link or tutorial you could recommend would help.

Thank you!",,,,,Submission,5,0,5
cykmteu,2016-01-03 14:40:22-05:00,WarInternal,,"From my *very* brief reading, it looks like Heroku's filesystem is ephemeral, since your application may change machines. Therefore, storing things in memory and on disk doesn't work. You need external storage, I'm seeing postgres as one option.

You also get hobby tier Redis for free, you might try that for your session store instead of postgres, as I believe you tell redis to expire an entry, or you might switch to JWT, which doesn't need a persistence layer.

Not seeing much of anything for disk storage.

---
As for securing passwords, I'm partial to node's crypto.pbkdf2 functionality.

[Here's](https://gist.github.com/skeggse/52672ddee97c8efec269) one way to use that.
",3z88ey,t3_3z88ey,cardboardxbox,,Comment,3,0,3
cyk16nm,2016-01-02 23:13:48-05:00,tRfalcore,,"hash the passwords, or for more security hash the salted passwords, or if you're super srs encrypt the hashed salted passwords

to persist anything it must be stored in <something> like a database which stores that last beyond system restarts and session timeouts",3z88ey,t3_3z88ey,cardboardxbox,,Comment,1,0,1
cyk1qn6,2016-01-02 23:31:52-05:00,cardboardxbox,,"I'm wondering what the difference would be that would allow my data to persist in the sqlite database? I had a project earlier and on a certain request it would insert a row, but daily it would refresh the entire website and that would go away. Thanks for the advice!",3z88ey,t1_cyk16nm,tRfalcore,,Reply,1,0,1
cyk2eft,2016-01-02 23:54:18-05:00,tRfalcore,,"gotta tell sqlite to persist to file

I'm not a sqlite pro, but I've used other ""lite"" databases, and often times they just default to in memory only.  so they wipe on application or system restart.",3z88ey,t1_cyk1qn6,cardboardxbox,,Reply,1,0,1
cykmc0v,2016-01-03 14:27:59-05:00,776865656e,,"Perhaps look into [Auth0](https://auth0.com/). Using it for a startup I'm working on, seems really nice so far.",3z88ey,t3_3z88ey,cardboardxbox,,Comment,1,0,1
3z7rf1,2016-01-02 20:26:53-05:00,Throwamay_,Could you tell me if compsci right for me? Please take a look at this!,"Hey!

Ill get right into it. Computers have been a huge part of my life for over 10 years. I'm 17 and was hooked on the moment I got one. I've built my own, run my home network and help out with the school network. I'm techsupport for all my peers, tinker with phones. I love it all and cannot get enough of any of it!

I do well in mathematics at school when I want to but I don't have a burning passion for it. If I put my head down and work I can do very well. Do I need to absolutely love it?

Problem is, programming. I like the idea of programming and have made countless attempts at learning every single language.

Python, Java, C, C#, C++, MySQL everything..

I cannot get past the first 10 hours of a language before losing interest. Booleans, functions, loops I have a fairly good understanding of them and they make sense but I trail off after some time and come back to it every 2 months for a week. Should I stick at it?

I want to study computer science but I don't want to come into the course with the wrong mindset. ",,,,,Submission,2,0,2
cyk7czw,2016-01-03 03:14:54-05:00,starly396,,"Do it.

I was sorta like you. I was tampering with system settings and causing blue screens since I was 8, dismantling all the electronics in the house. I was the tech support, the Wiresharker and the port forwarder, the Wii homebrewer, and I got as far with programming as replacing the Firefox icon with a 'shutdown /s' script on the lab computers or writing a Jeopardy! game with nothing but ECHOs. But the few times I tried to sit down and start a Python or Javascript tutorial, I couldn't keep myself focused and petered out in a day. Not even sites like [codecombat](https://codecombat.com/) could suck me in. I did well in math, but it definitely wasn't my favorite. But I still enjoyed technology enough to try it in college.

Fast forward, I'm a junior and I've learned Python, Java, C, Javascript, SQL, Ruby, Assembly and more, and I've loved every second! I also learned what ports and sockets really are, how routing really works, and enough to configure my own DNS server and use a UDP for music projects. I feel totally capable of wielding code to do my bidding, and I even get to teach it.

I think 1) having goals to achieve instead of letting yourself drown in the possibilities and never settling on one, plus 2) having peers on your level that are just as interested in technology as you are, go a long way in shaping what you're capable of. And, like playing an instrument, there's still plenty of things I'm really interested in that I can't seem to motivate myself to do. But if you can master the self-motivation to learn something new, something challenging, you're already in the top percentage of humans who ever lived.

So pursue it! I think it's worth the while. In the meantime, try and find tutorials with a goal in mind, like a [text game](http://thepythongamebook.com/en:python:goblins:start) or small projects like a [port scanner](http://www.primalsecurity.net/0x1-python-tutorial-port-scanner/). If you don't understand something, look it up! But the whole point is getting experience and having fun. Message me if you have other Qs, hope this helps!",3z7rf1,t3_3z7rf1,Throwamay_,,Comment,2,0,2
cyogcs2,2016-01-06 15:33:31-05:00,drummyfish,,"You'll be just fine, I think you're more than good to go. You don't need to love math, most of my classmates are just ""good with math but not absolutely into it"". Also most of them started programming in their first year of college and they're fine. I think your problem with programming seems to be just laziness, the school will make you write programs and learn to program - and once you get into it a little, you'll start writing scripts for everything and you won't be able to live without it anymore.",3z7rf1,t3_3z7rf1,Throwamay_,,Comment,1,0,1
cykbhyl,2016-01-03 08:04:31-05:00,CaptainBland,,"You've kind of got to bear in mind that anything with any practical value has some kind of  tedious part, but  learning a language is a bit like long distance running. You get through the easy stuff, then you hit a wall - once you get through that wall what you get is proficiency, that's like your second wind. When the pieces start falling into place you can start implementing projects in that language and when you encounter an obstacle in your project that will give you a reason to learn more techniques and so be more ambitious - the learning process at that point sort of feeds into itself. You've got to get through the wall, though.

Remember though that a programming language is as much a practical instrument as it is a collection of theoretical constructs so it's important to practice and improvise rather than just just blindly following tutorials.  A good way to keep motivated and informed is to join in on conversations about your technology of choice.",3z7rf1,t3_3z7rf1,Throwamay_,,Comment,1,0,1
cyktlbn,2016-01-03 17:27:00-05:00,techhead57,,"Go for it. I had similar problems trying to learn these things on my own. Don't get me wrong, I enjoy programming, but I'm much more interested in understanding problems. I can use little programs or scripts to better understand problems and solve them. Programming isn't all you will do with a computer science degree. It's the tedious but sometimes extremely satisfying part that you get to do as part of your job.

Trying to learn programming on your own is like trying to learn to do a lot of things on your own, difficult. If you study computer science you'll want it to be your full time job (college makes this easy, it can be done without a degree, but I know someone who did this, but it's becoming more and more difficult).

Basically, it's easier to learn it in a structured environment where experienced learners and teachers are giving you guidance. There are tutorials on the internet, sure. Some of them are even pretty good. But, you're 17, and you've got other things going on. That's going to make it hard to spend the amount of time you need to learn this stuff on your own.

It'll get easier. The more you know going in, the easier getting the degree will be. But don't worry if you can't test out of the first class or two. Teaching a mid-level CS class or two, the students who skipped a few classes, are bad students as often as they are good students, because they skipped around on what they learned and don't have the fundamentals they need to excel later.",3z7rf1,t3_3z7rf1,Throwamay_,,Comment,0,0,0
3z67ir,2016-01-02 13:53:15-05:00,TheBeardofGilgamesh,"during a technical whiteboard interview how much ""Marker Sniffin"" is too much ""Marker Sniffin""?","On Wednesday I had a technical interview, and whenever I am using a white board I periodically sniff the marker, especially while thinking about a possible solution. 


Now I was only kind of aware of this, and I was unaware that the interview would notice I was sniffing the markers until he said that I shouldn't smell the markers so much, and I colored my nose blue. 


I have yet to hear back from the company, and while the interview was the day before New Years Eve. I am starting to think that it's possible my marker sniffin' may have given off a poor first impression. ",,,,,Submission,17,0,17
cyjixjh,2016-01-02 14:04:34-05:00,None,,"Go home, you are still drunk.",3z67ir,t3_3z67ir,TheBeardofGilgamesh,,Comment,26,0,26
cyjufr7,2016-01-02 19:46:23-05:00,None,,Yeah it may have given off a poor first impression. But if you were great then it wouldn't matter. Find another job.,3z67ir,t1_cyjixjh,None,,Reply,1,0,1
cyk1u5q,2016-01-02 23:35:05-05:00,matrix2002,,"Any is too much. Don't sniff markers, it's weird.",3z67ir,t3_3z67ir,TheBeardofGilgamesh,,Comment,11,0,11
cyjk45t,2016-01-02 14:38:19-05:00,Artichoke93,,Everyone knows its 1 sniff per 43 minutes. ,3z67ir,t3_3z67ir,TheBeardofGilgamesh,,Comment,15,0,15
cyjnqgu,2016-01-02 16:26:04-05:00,TheBeardofGilgamesh,,"id say a good sniff should be expected when asked questions like ""what would be the worst case time complexity for this particular implementation and in what kind of scenario would you expect to see your implementation perform worst case""


That's when I in response say ""well let me see. . . ""(takes massive whiff of marker)",3z67ir,t1_cyjk45t,Artichoke93,,Reply,5,0,5
cyjl499,2016-01-02 15:07:48-05:00,Silverlight42,,"methinks you may be in the wrong sub.

try /r/nostupidquestions where you're allowed to ask all the stupid questions you like!",3z67ir,t3_3z67ir,TheBeardofGilgamesh,,Comment,11,0,11
cyjnsqb,2016-01-02 16:28:01-05:00,TheBeardofGilgamesh,,"if not to as an elaborate excuse to sniff white board markers, then how else can you explain their prevalence?  ",3z67ir,t1_cyjl499,Silverlight42,,Reply,4,0,4
cyjooie,2016-01-02 16:54:26-05:00,Silverlight42,,"it's not only the dry erase markers.... works with permanent markers too.  Might even be better.

and did you know that you can erase permanent marker with a dry erase marker in case you accidentally use one on a white board?",3z67ir,t1_cyjnsqb,TheBeardofGilgamesh,,Reply,1,0,1
cyjosh6,2016-01-02 16:57:43-05:00,TheBeardofGilgamesh,,"no I did not, but the smell of permanent markers sure is better. ",3z67ir,t1_cyjooie,Silverlight42,,Reply,2,0,2
cyjzxds,2016-01-02 22:34:36-05:00,smeezy,,https://www.youtube.com/watch?v=VuqoCWQEFbY&t=1m57s,3z67ir,t1_cyjnsqb,TheBeardofGilgamesh,,Reply,1,0,1
cyjmhmu,2016-01-02 15:48:13-05:00,Jurby,,Any marker sniffing is too much market sniffing,3z67ir,t3_3z67ir,TheBeardofGilgamesh,,Comment,7,0,7
cykh50a,2016-01-03 12:08:16-05:00,orlybach,,He's probably wondering what other chemicals you sniff in your free time. ,3z67ir,t3_3z67ir,TheBeardofGilgamesh,,Comment,1,0,1
cyksx90,2016-01-03 17:10:02-05:00,TheBeardofGilgamesh,,"well I don't sniff but consume adderall, which allows me to code 15 hours a day non stop without lunch or dinner breaks! ",3z67ir,t1_cykh50a,orlybach,,Reply,1,0,1
cyl0h6r,2016-01-03 20:26:50-05:00,orlybach,,seems reasonable. I recently read an article about programmers taking small hits of LSD to stay focused and calm,3z67ir,t1_cyksx90,TheBeardofGilgamesh,,Reply,1,0,1
cyszkb9,2016-01-10 11:56:05-05:00,ShittyAlgorithms,,"I know people are being super harsh here but I don't think this is a big deal. 

I don't see this as different from someone who might bite their nails or scratch their chin. Lots of people have nervous habits or things they do when they're thinking hard/under pressure. If an interviewer doesn't understand this then they should get their head out of their ass.

If you don't get a call back it's probably for some other reason. If you wrote a good answer I don't see why your unintentional blue nose should disqualify you. Focus on nailing the interview next time.

That said, stop sniffing markers. It's still not a good idea. It probably won't affect the outcome of your interview, but there's still a risk that you will one day get a judgemental prick interviewing you for a job you really want.",3z67ir,t3_3z67ir,TheBeardofGilgamesh,,Comment,1,0,1
cyk84dc,2016-01-03 04:00:49-05:00,robbyoconnor,,"This is for intelligent questions. Odds are your marker sniffing, which combined with your asking this asinine question, is likely why you're not going to get a call back. ",3z67ir,t3_3z67ir,TheBeardofGilgamesh,,Comment,1,0,1
cyjxtry,2016-01-02 21:29:11-05:00,eygrr,,"I think perhaps it would have negatively impacted your chances, however there is no sure way to tell just yet. Have you tried analysing its statistical relevance in relation to interview norms?

edit: Perhaps a Neural Network approach?",3z67ir,t3_3z67ir,TheBeardofGilgamesh,,Comment,1,0,1
3z4vkx,2016-01-02 07:15:36-05:00,OmioKonio,looking for some way to simulate a layman's physics theory (involving some new particle),"Hi

So i don't know what's available for simulating something (in 3d space), and i don't have any programming skills.

i **think** i know the rules it would require to work, but maybe it will need many tries. 


The general idea:

The idea here is that planets, gas giants, and stars (and pretty much everything else of big size) are all the same thing at different stages. 
There exists a fluid composed of neutral particles that fill the entire visible space. the fluid's waves are electromagnetic waves, its *wind* is gravity, and its (thousandth of a proton sized) particles are electron-positron pairs. Those particles split when disturbed by physical matter when they pass through it. The electron part is harvested by any molecule around, while the positron continues it's course (with the fluid wind) while gathering other fluid particles around it until it's a proton which impacts physical matter (appearing inside of it).

For more info about the theory, check my username, i posted a serie about it.


The simulation would be about the transformation process (from planet to star). It should start with a body of matter big enough (rotating slowly on itself) inside and endless supply of *the fluid*.

Thanks for anybody interrested",,,,,Submission,2,0,2
cyjk6b4,2016-01-02 14:40:02-05:00,dirac_majorana_weyl,,"Is there a way to simulate physics theories? Yes. To do so, you have to know the math behind the theories well enough to write a computer program that will simulate it. So the question is: do you know the math behind both your new theory and behind GR and the standard model? ",3z4vkx,t3_3z4vkx,OmioKonio,,Comment,1,0,1
cyjmrox,2016-01-02 15:56:40-05:00,OmioKonio,,"I don't know any math.

i though that if some computer program can simulate a gas (how it moves with a great number of particles) it could simulate my theory.",3z4vkx,t1_cyjk6b4,dirac_majorana_weyl,,Reply,1,0,1
cyjs5v6,2016-01-02 18:39:41-05:00,dirac_majorana_weyl,,"There are lots of computer programs out there that do that. You'll have to figure out how your theory looks mathematically so you can write whatever additional things in your theory into the existing software. 

Also please don't downvote me. I did particle physics research for several years and genuinely do want to help you get on the right track. ",3z4vkx,t1_cyjmrox,OmioKonio,,Reply,1,0,1
cyk5rt8,2016-01-03 01:56:34-05:00,helpful_hank,,I downvoted you. I'm protective of weird theories and thought you were being dismissive. My bad. Downvote reversed.,3z4vkx,t1_cyjs5v6,dirac_majorana_weyl,,Reply,1,0,1
cyjsb38,2016-01-02 18:44:02-05:00,dirac_majorana_weyl,,"Want to add something: I just read your posts on the critical shower thoughts subreddit, and it looks like you are trying to come up with a theory defending aether theory to discredit current physics theories. This would be all fine and dandy if you understood current physics, but you don't, so you are going to have a hard time writing a software program to simulate your theories. If you are looking to learn the existing math and physics, I can make some recommendations. ",3z4vkx,t1_cyjmrox,OmioKonio,,Reply,1,0,1
cyk7e4f,2016-01-03 03:16:44-05:00,OmioKonio,,"thanks for the interrest.

i think my theory would be quite simple to implement, i'll just have to refine the variables:

* physical matter *X* is known physical matter, but is permeable to aether (permeability variable to define and adjust) it also disturbs it by causing a small percentage of aether particles to split (disturbing variable to define and ajust)

* aether fluid *Y* has a density (variable to define and adjust) a compressability (variable to define and adjust) a pressure (variable to define and adjust)

* planetary body critical mass to reach a big enough diameter to allow matter appearance inside of it (variable to define and adjust) a rotation on itself (variable to define and adjust)

and maybe some other variables to define after that, until the simulation shows a planet growing, loosing its crust (becoming rings) and so on.

Also, i would love to know what good software there is outhere that i could use (at my level) if you know any (so i don't have to install to much stuff on my computer)",3z4vkx,t1_cyjsb38,dirac_majorana_weyl,,Reply,1,0,1
cyk7kid,2016-01-03 03:26:51-05:00,dirac_majorana_weyl,,You should try Mathematica. They have a lot of fluid dynamics examples you can find easily by googling. ,3z4vkx,t1_cyk7e4f,OmioKonio,,Reply,1,0,1
cyjosf4,2016-01-02 16:57:40-05:00,helpful_hank,,"Do you want to simulate something, or just describe/portray it? Simulation involves calculations, but you could probably ""3D draw"" it if that's not what you're going for. I have an idea or two.",3z4vkx,t3_3z4vkx,OmioKonio,,Comment,1,0,1
cyjpxld,2016-01-02 17:32:32-05:00,OmioKonio,,"my idea is to have a sandbox filled with what i posted, and let it work out and see how it behaves (to see if it's close to reality or not)",3z4vkx,t1_cyjosf4,helpful_hank,,Reply,1,0,1
cyk49c2,2016-01-03 00:56:58-05:00,helpful_hank,,"Yeah, that might take some math. I have some things like that I'd like to test too, so let me know if you find anything or make some progress. (I'm also interested in your results of course.) In the meantime, I'd recommend not telling people outside CST the details of your theory. ",3z4vkx,t1_cyjpxld,OmioKonio,,Reply,1,0,1
3z2rm5,2016-01-01 19:06:17-05:00,Narroo,"So, I know how to write code, but I don't know how to actually program.","I took a course in C++ back in college, and then I went through an entire book in C++ (Programming Principles and Practices in C++ By Stroustrup.)  Afterwards, I even tried contributing to a GitHub project.

And you know what?  I still don't how actually program.  Sure, I can write something in MS Visual C++, play with it in the debugger a bit, and maybe make some kind of executable.  But that's it.  I have no idea how to do anything else.  So sure, I write a ""Hunt the Wompus"" program for myself, but for any practical applications I still have no idea what I'm doing.",,,,,Submission,15,0,15
cyiuuf8,2016-01-01 20:52:39-05:00,Jurby,,"Very few people do.  When they decide (or are told) to do something practical or interesting, they look up how to do things in that subject area.

I have no idea how to do basic network communication.  That's a crazy fundamental thing I've just never done.    I recently needed to fit work, so I just started googling.

Programming isn't about knowing everything, it's about knowing the basics and being able to learn more based on those basics.",3z2rm5,t3_3z2rm5,Narroo,,Comment,20,0,20
cyiybeg,2016-01-01 22:48:54-05:00,bluelite,,"I feel for you. I think that the Internet and mobile apps have warped our sense of what it is to ""program."" It can be discouraging to look at large, complex apps and think ""how could I possibly do that?""

What you're feeling is normal. It's like a carpenter saying they know how to use all the tools but have no idea how to actually build anything. Sure, you can read the instruction manuals and do a few exercises, but what you need is *practice, practice, practice.*

Don't force yourself to write programs. That's not as effective as finding programs that you *want* to write. They don't have to be big or complicated. But the desire has to come from within.

I've been programming for some forty years. When I started as a kid, computers were simpler. Input/output consisted of mostly text with some simple graphics. The programs I wrote were no more than a few dozen lines, but I wrote a *lot* of them. Some of the earliest programs I can remember writing:

* Make the letter O bounce around the screen. (Remember, all I had was text and basic graphics.)
* Calculate whether a number was prime or not.
* Wrap text around the screen, putting in word breaks at the right places.
* Generate random x-y coordinates and plot them on the screen.
* A basic ""paint"" program that used the joystick to control a ""cursor"" on the screen. Press the fire button to ""paint:"" press another button to ""erase.""

Keep in mind that none of these programs pre-existed as libraries that I could simply download and use. I had to write them from scratch and they took no more than a dozen lines or so. I think that's what separates a programmer from a coder. The latter knows how to use the tools: the former knows how to build stuff from scratch.

So build stuff. Just build stuff. There's no magic line that you'll cross to become a programmer, but you'll know you're there when you can confidently say ""I know how to tackle this problem.""",3z2rm5,t3_3z2rm5,Narroo,,Comment,7,0,7
cyj8tjz,2016-01-02 07:53:01-05:00,Narroo,,"Thank you but let me explain: I can do exercises like that.  To keep with the carpenter example, it's more like I don't know how to get building permits or contracts, or how to hire someone to lay a foundation, paint the house, etc.  I can nail wood together and that's it.

That said, I do understand the art of practicing (I went through almost every program in Stroustroup's book.) Thank you for the encouragement!",3z2rm5,t1_cyiybeg,bluelite,,Reply,2,0,2
cyje3yb,2016-01-02 11:45:26-05:00,Lawlbooobz,,"So it's the logic that you don't understand? 
Think of a problem that needs solved, write out step by step instructions that take you to the solution, then switch out those steps with applicable coding. 

Pseudocode or flow charts are really helpful. :)

Sorry if that's not what you meant! ",3z2rm5,t1_cyj8tjz,Narroo,,Reply,1,0,1
cyjsb6g,2016-01-02 18:44:06-05:00,Narroo,,"No no, it's not the code: it's the ""Supporting skills"".",3z2rm5,t1_cyje3yb,Lawlbooobz,,Reply,1,0,1
cyjft63,2016-01-02 12:35:58-05:00,None,,If you want to move beyond exercises your best bet is to work with other people. A good mentor is generally better than all of the books.,3z2rm5,t1_cyj8tjz,Narroo,,Reply,1,0,1
cyivv0o,2016-01-01 21:26:30-05:00,Merad,,"Really the only way to get past that is practice.  Think up some project ideas and work on them.  Maybe consider switching to a language that's easier to use than C++, so you can focus more on solving problems and producing results than dealing with intricacies of the language.",3z2rm5,t3_3z2rm5,Narroo,,Comment,3,0,3
cykq8bt,2016-01-03 16:03:57-05:00,madwilliamflint,,"It's not a programming problem. It's a design and ideas problem.

Pick something concrete that you don't know how to write and start a little list of things it needs to do.

I think you'll find relatively quickly that the problem is in framing and perspective more than technical skill and aptitude.


",3z2rm5,t3_3z2rm5,Narroo,,Comment,2,0,2
cyj78mi,2016-01-02 05:55:55-05:00,None,,Make something from scratch yourself. Pick a project and jump in. You'll get stumped right off the hop and have to google through the mud and the dirt. ,3z2rm5,t3_3z2rm5,Narroo,,Comment,1,0,1
cyjj26a,2016-01-02 14:08:14-05:00,TheBeardofGilgamesh,,"What I would suggest is find some kind of **convention over configuration framework** and then build a few things with it, then slowly you will learn how the framework was built and start to understand how the whole thing is structured. 

And also try at first to not ""read about how to"", but instead just try learning as you go, I mean if you want to know how to get around a city, often times just exploring and going around the city may be better than just staring at a map. 

But when I said ""don't read up on it"" I mean initially, since it's easier to understand the concepts if you're familiar/have experience with the thing you're building. 

also I am not sure what you're trying to program, but if you want a web based thing maybe try and write a CPP module that does something like Double Hash photos in a boolean list https://github.com/kkaefer/node-cpp-modules",3z2rm5,t3_3z2rm5,Narroo,,Comment,1,0,1
cyjssww,2016-01-02 18:58:24-05:00,Narroo,,"I'm a science graduate student, so I need programming for research, eventually.  Of course, that's not formally taught - you have to teach it yourself.  Worse, My actual courses at the moment usually take up all my time: I have very little free time which is why I've been trying to avoid learning by haphazard ""learn as you go"" since that's far too inefficient given how much time I'm spending on my actual courses.  That said, I had a feeling that there's no real guide on supporting programming skills/knowledge.

Also, as an example, your suggested exercise completely mystifies me.  Even after googling double hashes and node.js, I'm still at a completely loss at what to do, if that help explain my situation.  (Why do I need to write a sever module function...thing in order to double hash something?  What exactly is a boolean list?  Etc.  

And that's just programming theory.  I'm even more concerned about things like: How to efficiently make a stand-alone program: use compiler features correctly, how to use any-and-all libraries correctly and efficiently , supplemental tools, and all the techniques and knowledge used in programming, but not actually writing the core logic of the program itself.  For example, a chef may be able to cook, but he needs to know how to sharpen his knives, stock his kitchen, clean, etc which is all supplemental.)",3z2rm5,t1_cyjj26a,TheBeardofGilgamesh,,Reply,1,0,1
cyk5oau,2016-01-03 01:52:18-05:00,TheBeardofGilgamesh,,"oh a boolean list is a word i made up to describe an array of booleans which is essentially an array of bytes that are either true or false. Now a [boolean](https://en.wikipedia.org/wiki/Boolean_data_type) is essentially how computers eventually break down all problems its essentially the **1** True/something and **0** False/void (just an FYI an array of booleans is 8 bits(1 byte), even though it could be represented in single bits, for the purpose of efficiency true or false takes up 8 bits of space ). 


Anyways what I meant by double hash is a common and simple technique to filter out false positives. So lets say a website is trying to filter out or 'ban' photos of something like a Troll picture that says something outrageous like ""Caitlyn Jenner is not a stunning and Brave woman"", in order to identify offensive microaggressions like this you can hash the original banned photo several different aways like lets says 4 different hashing algorithms and place a true on each value it lands on. Now lets say every time a user uploads a photo, if the photo lands atleast a few of the trues on then the photo can be sent off to a service to be analyzed in more detail. 


That way you can have a proper filtering system, and not have to have every photo be processed and scanned for microaggressions ",3z2rm5,t1_cyjssww,Narroo,,Reply,1,0,1
cykai7x,2016-01-03 06:52:58-05:00,Narroo,,"Alright, thanks!",3z2rm5,t1_cyk5oau,TheBeardofGilgamesh,,Reply,1,0,1
cykfuxe,2016-01-03 11:26:14-05:00,TheBeardofGilgamesh,,"whoa, I was pretty drunk when I wrote that reply last night, and originally I planned on going into more detail, but half way through I forgot what my ""point"" was. 

Also I didn't even see the second half of your reply since I was responding through my notification feed. 


in response to some questions:


**(Why do I need to write a sever module function...thing in order to double hash something?**

Oh you don't actually need server anything it's just a common thing people use. But Hashing is just a fast way to store and find things in memory. 


Also if you wanna learn some of these things, why not watch a few videos on Lynda.com on C++?  ",3z2rm5,t1_cykai7x,Narroo,,Reply,1,0,1
cyktk2l,2016-01-03 17:26:06-05:00,Narroo,,Because I've never heard of Lynda.com. ,3z2rm5,t1_cykfuxe,TheBeardofGilgamesh,,Reply,1,0,1
